{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1and6 = Path(HERE).resolve().parents[1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4550522</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[5547132, 2349897, 7998790, 3060600, 13538872,...</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL221655</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[3357707, 3346815, 1052992, 6116087, 30496230,...</td>\n",
       "      <td>-0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3339032</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1705, 13925431, 5032800, 4181618, 729515, 693...</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3670680</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3568556, 3234227, 1578989, 11354240, 6804879,...</td>\n",
       "      <td>-0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3622727</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2060065, 2673342, 2475943, 1535176, 3310692, ...</td>\n",
       "      <td>-2.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4550522  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       CHEMBL221655  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3339032  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3670680  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3622727  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \n",
       "0  [5547132, 2349897, 7998790, 3060600, 13538872,...               0.82  \n",
       "1  [3357707, 3346815, 1052992, 6116087, 30496230,...              -0.35  \n",
       "2  [1705, 13925431, 5032800, 4181618, 729515, 693...               0.19  \n",
       "3  [3568556, 3234227, 1578989, 11354240, 6804879,...              -0.94  \n",
       "4  [2060065, 2673342, 2475943, 1535176, 3310692, ...              -2.32  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1and6/\"HDAC1and6_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type_HDAC1</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>type_HDAC6</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>SelectivityRatio</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>109.647820</td>\n",
       "      <td>6.96</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.141254</td>\n",
       "      <td>9.85</td>\n",
       "      <td>776.247117</td>\n",
       "      <td>2.89</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>616.595002</td>\n",
       "      <td>6.21</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.169824</td>\n",
       "      <td>9.77</td>\n",
       "      <td>3630.780548</td>\n",
       "      <td>3.56</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4243347</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.995262</td>\n",
       "      <td>8.70</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.199526</td>\n",
       "      <td>9.70</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4247128</td>\n",
       "      <td>C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>83.176377</td>\n",
       "      <td>7.08</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.251189</td>\n",
       "      <td>9.60</td>\n",
       "      <td>331.131122</td>\n",
       "      <td>2.52</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4126811</td>\n",
       "      <td>CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>436.515832</td>\n",
       "      <td>6.36</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.331131</td>\n",
       "      <td>9.48</td>\n",
       "      <td>1318.256739</td>\n",
       "      <td>3.12</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL4167599</td>\n",
       "      <td>NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>4073.802778</td>\n",
       "      <td>5.39</td>\n",
       "      <td>IC50</td>\n",
       "      <td>50.118723</td>\n",
       "      <td>7.30</td>\n",
       "      <td>81.283052</td>\n",
       "      <td>1.91</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL4282471</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3388.441561</td>\n",
       "      <td>5.47</td>\n",
       "      <td>IC50</td>\n",
       "      <td>117.489756</td>\n",
       "      <td>6.93</td>\n",
       "      <td>28.840315</td>\n",
       "      <td>1.46</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL470843</td>\n",
       "      <td>O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6309.573445</td>\n",
       "      <td>5.20</td>\n",
       "      <td>IC50</td>\n",
       "      <td>173.780083</td>\n",
       "      <td>6.76</td>\n",
       "      <td>36.307805</td>\n",
       "      <td>1.56</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>28.183829</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Ki</td>\n",
       "      <td>245.470892</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>6.45</td>\n",
       "      <td>IC50</td>\n",
       "      <td>295.120923</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.202264</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "1         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "2         CHEMBL4243347  O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...   \n",
       "3         CHEMBL4247128  C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...   \n",
       "4         CHEMBL4126811  CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...   \n",
       "...                 ...                                                ...   \n",
       "1905      CHEMBL4167599  NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...   \n",
       "1906      CHEMBL4282471  CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...   \n",
       "1907       CHEMBL470843          O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO   \n",
       "1908      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "1909      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "     type_HDAC1  Standard_Value_HDAC1  pChEMBL_HDAC1 type_HDAC6  \\\n",
       "0          IC50            109.647820           6.96       IC50   \n",
       "1          IC50            616.595002           6.21       IC50   \n",
       "2          IC50              1.995262           8.70       IC50   \n",
       "3          IC50             83.176377           7.08       IC50   \n",
       "4          IC50            436.515832           6.36       IC50   \n",
       "...         ...                   ...            ...        ...   \n",
       "1905       IC50           4073.802778           5.39       IC50   \n",
       "1906       IC50           3388.441561           5.47       IC50   \n",
       "1907       IC50           6309.573445           5.20       IC50   \n",
       "1908         Ki             28.183829           7.55         Ki   \n",
       "1909       IC50            354.813389           6.45       IC50   \n",
       "\n",
       "      Standard_Value_HDAC6  pChEMBL_HDAC6  SelectivityRatio  \\\n",
       "0                 0.141254           9.85        776.247117   \n",
       "1                 0.169824           9.77       3630.780548   \n",
       "2                 0.199526           9.70         10.000000   \n",
       "3                 0.251189           9.60        331.131122   \n",
       "4                 0.331131           9.48       1318.256739   \n",
       "...                    ...            ...               ...   \n",
       "1905             50.118723           7.30         81.283052   \n",
       "1906            117.489756           6.93         28.840315   \n",
       "1907            173.780083           6.76         36.307805   \n",
       "1908            245.470892           6.61          0.114815   \n",
       "1909            295.120923           6.53          1.202264   \n",
       "\n",
       "      SelectivityWindow            label  \n",
       "0                  2.89  HDAC6-selective  \n",
       "1                  3.56  HDAC6-selective  \n",
       "2                  1.00      Dual-binder  \n",
       "3                  2.52  HDAC6-selective  \n",
       "4                  3.12  HDAC6-selective  \n",
       "...                 ...              ...  \n",
       "1905               1.91   Semi-selective  \n",
       "1906               1.46   Semi-selective  \n",
       "1907               1.56   Semi-selective  \n",
       "1908              -0.94      Dual-binder  \n",
       "1909               0.08       Non-binder  \n",
       "\n",
       "[1910 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1and6/\"HDAC1and6_SemiSel_dataset.csv\", )\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca3dbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 'Dual-binder']['SelectivityWindow'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4550522</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[5547132, 2349897, 7998790, 3060600, 13538872,...</td>\n",
       "      <td>0.82</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL221655</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[3357707, 3346815, 1052992, 6116087, 30496230,...</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL3339032</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1705, 13925431, 5032800, 4181618, 729515, 693...</td>\n",
       "      <td>0.19</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3670680</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[3568556, 3234227, 1578989, 11354240, 6804879,...</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4550522  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1       CHEMBL221655  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL3339032  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3      CHEMBL3670680  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \\\n",
       "0  [5547132, 2349897, 7998790, 3060600, 13538872,...               0.82   \n",
       "1  [3357707, 3346815, 1052992, 6116087, 30496230,...              -0.35   \n",
       "2  [1705, 13925431, 5032800, 4181618, 729515, 693...               0.19   \n",
       "3  [3568556, 3234227, 1578989, 11354240, 6804879,...              -0.94   \n",
       "\n",
       "         label  Class  \n",
       "0  Dual-binder    3.0  \n",
       "1  Dual-binder    3.0  \n",
       "2  Dual-binder    3.0  \n",
       "3   Non-binder    4.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"selectivity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.SelectivityWindow >= 2.0].index, \"selectivity\"] = 1.0\n",
    "df.loc[df[df.SelectivityWindow <= -2.0].index, \"selectivity\"] = 1.0\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y = df[\"SelectivityWindow\"].values\n",
    "Y_cat =  df[\"selectivity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_cat\"] = np.array(list(df.iloc[index_tr_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['SelectivityWindow'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_cat\"] = np.array(list(df.iloc[index_te_all[i]]['selectivity'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.689245     0.071103\n",
      "1                    TP        17.500000     2.273030\n",
      "2                    TN        98.600000     1.173788\n",
      "3                    FP         1.900000     1.100505\n",
      "4                    FN        15.900000     2.726414\n",
      "5              Accuracy         0.867074     0.021280\n",
      "6             Precision         0.902773     0.051079\n",
      "7           Sensitivity         0.524871     0.074117\n",
      "8           Specificity         0.981110     0.010904\n",
      "9              F1 score         0.661080     0.064462\n",
      "10  F1 score (weighted)         0.853262     0.026337\n",
      "11     F1 score (macro)         0.789183     0.038459\n",
      "12    Balanced Accuracy         0.752990     0.037331\n",
      "13                  MCC         0.620971     0.064955\n",
      "14                  NPV         0.861520     0.020888\n",
      "15              ROC_AUC         0.752990     0.037331\n",
      "CPU times: user 1min 15s, sys: 168 ms, total: 1min 16s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_reg =  RandomForestRegressor(random_state=1121218, max_features = None, n_jobs=4,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_reg.fit(x_train, y_train)\n",
    "        y_pred = rf_reg.predict(x_test)  \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=4, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggest_categorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggest_categorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestRegressor(**param_grid, n_jobs=4, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 15:47:14,322] A new study created in memory with name: RFRegressor\n",
      "[I 2023-12-19 15:48:06,405] Trial 0 finished with value: 0.6863943445696583 and parameters: {'n_estimators': 320}. Best is trial 0 with value: 0.6863943445696583.\n",
      "[I 2023-12-19 15:50:18,076] Trial 1 finished with value: 0.6864746129117472 and parameters: {'n_estimators': 797}. Best is trial 1 with value: 0.6864746129117472.\n",
      "[I 2023-12-19 15:52:21,063] Trial 2 finished with value: 0.6863950803117922 and parameters: {'n_estimators': 642}. Best is trial 1 with value: 0.6864746129117472.\n",
      "[I 2023-12-19 15:55:19,318] Trial 3 finished with value: 0.6870257557219486 and parameters: {'n_estimators': 904}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 15:57:10,308] Trial 4 finished with value: 0.6866406481777363 and parameters: {'n_estimators': 572}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 15:57:42,043] Trial 5 finished with value: 0.6855481457232355 and parameters: {'n_estimators': 163}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 15:58:10,887] Trial 6 finished with value: 0.6862307145726863 and parameters: {'n_estimators': 145}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:00:18,637] Trial 7 finished with value: 0.686452621120927 and parameters: {'n_estimators': 647}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:02:49,157] Trial 8 finished with value: 0.6865169779001936 and parameters: {'n_estimators': 775}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:05:10,818] Trial 9 finished with value: 0.6859380676280672 and parameters: {'n_estimators': 729}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:08:14,837] Trial 10 finished with value: 0.686899866121544 and parameters: {'n_estimators': 946}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:11:31,382] Trial 11 finished with value: 0.6868858319049236 and parameters: {'n_estimators': 994}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:14:38,371] Trial 12 finished with value: 0.6868685330730688 and parameters: {'n_estimators': 979}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:17:31,675] Trial 13 finished with value: 0.6867778536954827 and parameters: {'n_estimators': 892}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:18:52,462] Trial 14 finished with value: 0.6856536132685414 and parameters: {'n_estimators': 421}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:21:46,078] Trial 15 finished with value: 0.6867778536954827 and parameters: {'n_estimators': 892}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:23:10,520] Trial 16 finished with value: 0.6858877232424676 and parameters: {'n_estimators': 430}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:25:54,637] Trial 17 finished with value: 0.6867756189335072 and parameters: {'n_estimators': 856}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:28:12,102] Trial 18 finished with value: 0.6861618097652049 and parameters: {'n_estimators': 698}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:31:11,872] Trial 19 finished with value: 0.6868830685083007 and parameters: {'n_estimators': 943}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:33:52,450] Trial 20 finished with value: 0.686708178503525 and parameters: {'n_estimators': 848}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:36:56,686] Trial 21 finished with value: 0.6868524557281889 and parameters: {'n_estimators': 965}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:40:02,348] Trial 22 finished with value: 0.68688909847122 and parameters: {'n_estimators': 956}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:42:36,450] Trial 23 finished with value: 0.6864540575047785 and parameters: {'n_estimators': 800}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:45:27,814] Trial 24 finished with value: 0.6870181791165216 and parameters: {'n_estimators': 900}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:48:18,159] Trial 25 finished with value: 0.686829859968377 and parameters: {'n_estimators': 887}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:50:40,084] Trial 26 finished with value: 0.6863026645670057 and parameters: {'n_estimators': 746}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:52:17,076] Trial 27 finished with value: 0.6869214694599812 and parameters: {'n_estimators': 503}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:53:47,189] Trial 28 finished with value: 0.6860986708953888 and parameters: {'n_estimators': 471}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:54:43,294] Trial 29 finished with value: 0.6863764073895708 and parameters: {'n_estimators': 302}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:56:29,506] Trial 30 finished with value: 0.6863924040406733 and parameters: {'n_estimators': 550}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:57:29,836] Trial 31 finished with value: 0.6862965529699427 and parameters: {'n_estimators': 307}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 16:59:02,755] Trial 32 finished with value: 0.6861166872101757 and parameters: {'n_estimators': 475}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 17:01:05,698] Trial 33 finished with value: 0.6864546890353297 and parameters: {'n_estimators': 644}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 17:03:41,062] Trial 34 finished with value: 0.6865383040295342 and parameters: {'n_estimators': 808}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 17:05:22,332] Trial 35 finished with value: 0.6868631682361039 and parameters: {'n_estimators': 541}. Best is trial 3 with value: 0.6870257557219486.\n",
      "[I 2023-12-19 17:08:18,702] Trial 36 finished with value: 0.6870515679367852 and parameters: {'n_estimators': 923}. Best is trial 36 with value: 0.6870515679367852.\n",
      "[I 2023-12-19 17:10:13,209] Trial 37 finished with value: 0.6866723852117405 and parameters: {'n_estimators': 601}. Best is trial 36 with value: 0.6870515679367852.\n",
      "[I 2023-12-19 17:12:53,805] Trial 38 finished with value: 0.6866254707324618 and parameters: {'n_estimators': 843}. Best is trial 36 with value: 0.6870515679367852.\n",
      "[I 2023-12-19 17:13:38,031] Trial 39 finished with value: 0.6860754878247091 and parameters: {'n_estimators': 226}. Best is trial 36 with value: 0.6870515679367852.\n",
      "[I 2023-12-19 17:14:49,033] Trial 40 finished with value: 0.6857348617383524 and parameters: {'n_estimators': 376}. Best is trial 36 with value: 0.6870515679367852.\n",
      "[I 2023-12-19 17:17:47,584] Trial 41 finished with value: 0.686989051282178 and parameters: {'n_estimators': 936}. Best is trial 36 with value: 0.6870515679367852.\n",
      "[I 2023-12-19 17:20:44,035] Trial 42 finished with value: 0.6870515679367852 and parameters: {'n_estimators': 923}. Best is trial 36 with value: 0.6870515679367852.\n",
      "[I 2023-12-19 17:23:39,215] Trial 43 finished with value: 0.6870515679367852 and parameters: {'n_estimators': 923}. Best is trial 36 with value: 0.6870515679367852.\n",
      "[I 2023-12-19 17:26:47,317] Trial 44 finished with value: 0.6868536658793823 and parameters: {'n_estimators': 999}. Best is trial 36 with value: 0.6870515679367852.\n",
      "[I 2023-12-19 17:29:41,322] Trial 45 finished with value: 0.6870886734241599 and parameters: {'n_estimators': 922}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:32:34,115] Trial 46 finished with value: 0.6870879177299244 and parameters: {'n_estimators': 915}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:35:07,475] Trial 47 finished with value: 0.6867070629758553 and parameters: {'n_estimators': 819}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:37:32,168] Trial 48 finished with value: 0.6864092835725711 and parameters: {'n_estimators': 768}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:40:29,536] Trial 49 finished with value: 0.686977957503691 and parameters: {'n_estimators': 930}. Best is trial 45 with value: 0.6870886734241599.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6871\n",
      "\tBest params:\n",
      "\t\tn_estimators: 922\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFRegressor\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.685238\n",
      "1                    TP   27.000000\n",
      "2                    TN  200.000000\n",
      "3                    FP    2.000000\n",
      "4                    FN   39.000000\n",
      "5              Accuracy    0.847015\n",
      "6             Precision    0.931034\n",
      "7           Sensitivity    0.409091\n",
      "8           Specificity    0.990100\n",
      "9              F1 score    0.568421\n",
      "10  F1 score (weighted)    0.823641\n",
      "11     F1 score (macro)    0.737725\n",
      "12    Balanced Accuracy    0.699595\n",
      "13                  MCC    0.553642\n",
      "14                  NPV    0.836800\n",
      "15              ROC_AUC    0.699595\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_rf_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet0 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_0_cat = np.where(((y_pred_rf_0 >= 2) | (y_pred_rf_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_rf_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_rf_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_rf_0_cat)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 17:43:27,231] Trial 50 finished with value: 0.6650062214730194 and parameters: {'n_estimators': 858}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:46:17,738] Trial 51 finished with value: 0.6649380661020325 and parameters: {'n_estimators': 910}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:48:25,742] Trial 52 finished with value: 0.664941955635052 and parameters: {'n_estimators': 690}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:51:13,355] Trial 53 finished with value: 0.665019072294076 and parameters: {'n_estimators': 878}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:54:18,152] Trial 54 finished with value: 0.6657610123077536 and parameters: {'n_estimators': 995}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:57:09,394] Trial 55 finished with value: 0.6651068650706653 and parameters: {'n_estimators': 918}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 17:57:27,748] Trial 56 finished with value: 0.6613214100811677 and parameters: {'n_estimators': 103}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:00:29,804] Trial 57 finished with value: 0.6655391883437204 and parameters: {'n_estimators': 964}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:03:03,191] Trial 58 finished with value: 0.6650690967174955 and parameters: {'n_estimators': 836}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:05:25,622] Trial 59 finished with value: 0.6651975618135538 and parameters: {'n_estimators': 770}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:08:09,872] Trial 60 finished with value: 0.6650259455726082 and parameters: {'n_estimators': 876}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:11:00,391] Trial 61 finished with value: 0.6648453247266626 and parameters: {'n_estimators': 908}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:13:56,640] Trial 62 finished with value: 0.6655972232737765 and parameters: {'n_estimators': 968}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:16:48,310] Trial 63 finished with value: 0.6649575650298353 and parameters: {'n_estimators': 933}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:19:27,080] Trial 64 finished with value: 0.6650384682756455 and parameters: {'n_estimators': 875}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:21:57,412] Trial 65 finished with value: 0.6649324555874636 and parameters: {'n_estimators': 818}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:24:08,227] Trial 66 finished with value: 0.6651502892758754 and parameters: {'n_estimators': 727}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:26:54,215] Trial 67 finished with value: 0.6649305053064319 and parameters: {'n_estimators': 903}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:29:17,406] Trial 68 finished with value: 0.6650117088995993 and parameters: {'n_estimators': 790}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:32:14,556] Trial 69 finished with value: 0.665653223779683 and parameters: {'n_estimators': 973}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:35:05,375] Trial 70 finished with value: 0.6651817758256996 and parameters: {'n_estimators': 946}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:37:56,769] Trial 71 finished with value: 0.6649761649316812 and parameters: {'n_estimators': 930}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:40:32,926] Trial 72 finished with value: 0.6649625334464381 and parameters: {'n_estimators': 867}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:43:26,126] Trial 73 finished with value: 0.6651583248623715 and parameters: {'n_estimators': 943}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:46:15,457] Trial 74 finished with value: 0.6649104904758076 and parameters: {'n_estimators': 907}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:48:50,713] Trial 75 finished with value: 0.6651258698260454 and parameters: {'n_estimators': 841}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:51:54,087] Trial 76 finished with value: 0.6656539902197699 and parameters: {'n_estimators': 977}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:54:36,904] Trial 77 finished with value: 0.6649792207941114 and parameters: {'n_estimators': 882}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 18:57:34,732] Trial 78 finished with value: 0.6651714687383834 and parameters: {'n_estimators': 951}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:00:07,305] Trial 79 finished with value: 0.6650139824382795 and parameters: {'n_estimators': 834}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:03:10,838] Trial 80 finished with value: 0.6656608073547088 and parameters: {'n_estimators': 991}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:06:03,850] Trial 81 finished with value: 0.6649429293210953 and parameters: {'n_estimators': 932}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:08:49,772] Trial 82 finished with value: 0.6650106341801172 and parameters: {'n_estimators': 892}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:11:39,696] Trial 83 finished with value: 0.6651429750160696 and parameters: {'n_estimators': 920}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:14:37,896] Trial 84 finished with value: 0.6652828689588051 and parameters: {'n_estimators': 955}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:17:16,721] Trial 85 finished with value: 0.6650062214730194 and parameters: {'n_estimators': 858}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:20:03,729] Trial 86 finished with value: 0.6649908358772625 and parameters: {'n_estimators': 893}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:22:51,298] Trial 87 finished with value: 0.6651429750160694 and parameters: {'n_estimators': 920}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:25:55,860] Trial 88 finished with value: 0.6657801435154443 and parameters: {'n_estimators': 1000}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:28:57,769] Trial 89 finished with value: 0.6656738927011262 and parameters: {'n_estimators': 975}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:31:22,675] Trial 90 finished with value: 0.6650063780886712 and parameters: {'n_estimators': 796}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:33:13,712] Trial 91 finished with value: 0.6654526470707239 and parameters: {'n_estimators': 588}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:34:31,848] Trial 92 finished with value: 0.6645487841291859 and parameters: {'n_estimators': 423}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:36:13,673] Trial 93 finished with value: 0.6652488198215571 and parameters: {'n_estimators': 543}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:37:49,737] Trial 94 finished with value: 0.6649734406154466 and parameters: {'n_estimators': 511}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:39:23,189] Trial 95 finished with value: 0.6651757763920527 and parameters: {'n_estimators': 501}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:40:37,368] Trial 96 finished with value: 0.6651357671652015 and parameters: {'n_estimators': 400}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:42:03,883] Trial 97 finished with value: 0.6645936241586508 and parameters: {'n_estimators': 458}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:43:11,037] Trial 98 finished with value: 0.6631413079578984 and parameters: {'n_estimators': 347}. Best is trial 45 with value: 0.6870886734241599.\n",
      "[I 2023-12-19 19:45:07,960] Trial 99 finished with value: 0.6654972376813022 and parameters: {'n_estimators': 623}. Best is trial 45 with value: 0.6870886734241599.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6871\n",
      "\tBest params:\n",
      "\t\tn_estimators: 922\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.685238    0.711248\n",
      "1                    TP   27.000000   33.000000\n",
      "2                    TN  200.000000  197.000000\n",
      "3                    FP    2.000000    3.000000\n",
      "4                    FN   39.000000   35.000000\n",
      "5              Accuracy    0.847015    0.858209\n",
      "6             Precision    0.931034    0.916667\n",
      "7           Sensitivity    0.409091    0.485294\n",
      "8           Specificity    0.990100    0.985000\n",
      "9              F1 score    0.568421    0.634615\n",
      "10  F1 score (weighted)    0.823641    0.841646\n",
      "11     F1 score (macro)    0.737725    0.773326\n",
      "12    Balanced Accuracy    0.699595    0.735147\n",
      "13                  MCC    0.553642    0.600128\n",
      "14                  NPV    0.836800    0.849100\n",
      "15              ROC_AUC    0.699595    0.735147\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_rf_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_1_cat = np.where(((y_pred_rf_1 >= 2) | (y_pred_rf_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_rf_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_rf_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_rf_1_cat)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 19:48:21,578] Trial 100 finished with value: 0.6904270604882713 and parameters: {'n_estimators': 898}. Best is trial 100 with value: 0.6904270604882713.\n",
      "[I 2023-12-19 19:51:23,143] Trial 101 finished with value: 0.6904485120731224 and parameters: {'n_estimators': 930}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 19:54:27,907] Trial 102 finished with value: 0.6904317829638156 and parameters: {'n_estimators': 936}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 19:57:24,781] Trial 103 finished with value: 0.6902981889202078 and parameters: {'n_estimators': 910}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 20:00:11,334] Trial 104 finished with value: 0.6902895238064768 and parameters: {'n_estimators': 865}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 20:02:56,993] Trial 105 finished with value: 0.6900809128566434 and parameters: {'n_estimators': 854}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 20:05:45,115] Trial 106 finished with value: 0.6901626433453847 and parameters: {'n_estimators': 861}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 20:08:29,734] Trial 107 finished with value: 0.6901099170296121 and parameters: {'n_estimators': 860}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 20:11:09,917] Trial 108 finished with value: 0.6903694547297611 and parameters: {'n_estimators': 827}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 20:13:49,030] Trial 109 finished with value: 0.6903152450521992 and parameters: {'n_estimators': 826}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 20:16:29,929] Trial 110 finished with value: 0.6903197357270834 and parameters: {'n_estimators': 825}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 20:19:11,444] Trial 111 finished with value: 0.6903299431185171 and parameters: {'n_estimators': 829}. Best is trial 101 with value: 0.6904485120731224.\n",
      "[I 2023-12-19 20:21:35,137] Trial 112 finished with value: 0.690602734498472 and parameters: {'n_estimators': 744}. Best is trial 112 with value: 0.690602734498472.\n",
      "[I 2023-12-19 20:24:13,367] Trial 113 finished with value: 0.690418728202162 and parameters: {'n_estimators': 818}. Best is trial 112 with value: 0.690602734498472.\n",
      "[I 2023-12-19 20:26:43,295] Trial 114 finished with value: 0.6905731650376558 and parameters: {'n_estimators': 779}. Best is trial 112 with value: 0.690602734498472.\n",
      "[I 2023-12-19 20:29:07,784] Trial 115 finished with value: 0.690602734498472 and parameters: {'n_estimators': 744}. Best is trial 112 with value: 0.690602734498472.\n",
      "[I 2023-12-19 20:31:26,784] Trial 116 finished with value: 0.690714117038414 and parameters: {'n_estimators': 722}. Best is trial 116 with value: 0.690714117038414.\n",
      "[I 2023-12-19 20:33:42,804] Trial 117 finished with value: 0.6908478088778522 and parameters: {'n_estimators': 699}. Best is trial 117 with value: 0.6908478088778522.\n",
      "[I 2023-12-19 20:35:54,060] Trial 118 finished with value: 0.6908320920033193 and parameters: {'n_estimators': 680}. Best is trial 117 with value: 0.6908478088778522.\n",
      "[I 2023-12-19 20:38:04,752] Trial 119 finished with value: 0.690927760230506 and parameters: {'n_estimators': 676}. Best is trial 119 with value: 0.690927760230506.\n",
      "[I 2023-12-19 20:40:16,579] Trial 120 finished with value: 0.690930110045472 and parameters: {'n_estimators': 675}. Best is trial 120 with value: 0.690930110045472.\n",
      "[I 2023-12-19 20:42:23,635] Trial 121 finished with value: 0.6908162810513041 and parameters: {'n_estimators': 665}. Best is trial 120 with value: 0.690930110045472.\n",
      "[I 2023-12-19 20:44:36,406] Trial 122 finished with value: 0.6908921747617696 and parameters: {'n_estimators': 678}. Best is trial 120 with value: 0.690930110045472.\n",
      "[I 2023-12-19 20:46:47,123] Trial 123 finished with value: 0.6910061165456158 and parameters: {'n_estimators': 671}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 20:48:56,240] Trial 124 finished with value: 0.6910061165456158 and parameters: {'n_estimators': 671}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 20:51:05,163] Trial 125 finished with value: 0.6909550533490827 and parameters: {'n_estimators': 672}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 20:53:15,730] Trial 126 finished with value: 0.6910061165456157 and parameters: {'n_estimators': 671}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 20:55:26,138] Trial 127 finished with value: 0.6909550533490828 and parameters: {'n_estimators': 672}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 20:57:34,428] Trial 128 finished with value: 0.6909294619978646 and parameters: {'n_estimators': 673}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 20:59:45,724] Trial 129 finished with value: 0.6908869878447274 and parameters: {'n_estimators': 669}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 21:01:53,686] Trial 130 finished with value: 0.6908869878447274 and parameters: {'n_estimators': 669}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 21:04:04,261] Trial 131 finished with value: 0.6908313518089685 and parameters: {'n_estimators': 668}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 21:06:15,519] Trial 132 finished with value: 0.6909277602305057 and parameters: {'n_estimators': 676}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 21:08:28,384] Trial 133 finished with value: 0.6908632261882444 and parameters: {'n_estimators': 684}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 21:10:42,145] Trial 134 finished with value: 0.6908269481468573 and parameters: {'n_estimators': 706}. Best is trial 123 with value: 0.6910061165456158.\n",
      "[I 2023-12-19 21:12:45,289] Trial 135 finished with value: 0.691645718478438 and parameters: {'n_estimators': 627}. Best is trial 135 with value: 0.691645718478438.\n",
      "[I 2023-12-19 21:14:44,380] Trial 136 finished with value: 0.6916064841305326 and parameters: {'n_estimators': 629}. Best is trial 135 with value: 0.691645718478438.\n",
      "[I 2023-12-19 21:16:45,802] Trial 137 finished with value: 0.691645718478438 and parameters: {'n_estimators': 627}. Best is trial 135 with value: 0.691645718478438.\n",
      "[I 2023-12-19 21:18:48,573] Trial 138 finished with value: 0.6917909965989291 and parameters: {'n_estimators': 635}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:20:48,017] Trial 139 finished with value: 0.691604311464589 and parameters: {'n_estimators': 623}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:22:49,734] Trial 140 finished with value: 0.6916453913608482 and parameters: {'n_estimators': 626}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:24:50,270] Trial 141 finished with value: 0.691645718478438 and parameters: {'n_estimators': 627}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:26:52,057] Trial 142 finished with value: 0.6917269447759876 and parameters: {'n_estimators': 632}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:28:49,130] Trial 143 finished with value: 0.6917788918195829 and parameters: {'n_estimators': 612}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:30:49,002] Trial 144 finished with value: 0.6915385649685064 and parameters: {'n_estimators': 625}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:32:49,982] Trial 145 finished with value: 0.691645718478438 and parameters: {'n_estimators': 627}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:34:50,861] Trial 146 finished with value: 0.6915415528614481 and parameters: {'n_estimators': 624}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:36:51,579] Trial 147 finished with value: 0.6916674791692763 and parameters: {'n_estimators': 621}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:38:52,390] Trial 148 finished with value: 0.6916453913608482 and parameters: {'n_estimators': 626}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:40:52,333] Trial 149 finished with value: 0.6916674791692763 and parameters: {'n_estimators': 621}. Best is trial 138 with value: 0.6917909965989291.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6918\n",
      "\tBest params:\n",
      "\t\tn_estimators: 635\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.685238    0.711248    0.698779\n",
      "1                    TP   27.000000   33.000000   34.000000\n",
      "2                    TN  200.000000  197.000000  196.000000\n",
      "3                    FP    2.000000    3.000000    4.000000\n",
      "4                    FN   39.000000   35.000000   34.000000\n",
      "5              Accuracy    0.847015    0.858209    0.858209\n",
      "6             Precision    0.931034    0.916667    0.894737\n",
      "7           Sensitivity    0.409091    0.485294    0.500000\n",
      "8           Specificity    0.990100    0.985000    0.980000\n",
      "9              F1 score    0.568421    0.634615    0.641509\n",
      "10  F1 score (weighted)    0.823641    0.841646    0.843090\n",
      "11     F1 score (macro)    0.737725    0.773326    0.776569\n",
      "12    Balanced Accuracy    0.699595    0.735147    0.740000\n",
      "13                  MCC    0.553642    0.600128    0.598763\n",
      "14                  NPV    0.836800    0.849100    0.852200\n",
      "15              ROC_AUC    0.699595    0.735147    0.740000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_rf_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_2_cat = np.where(((y_pred_rf_2 >= 2) | (y_pred_rf_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_rf_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_rf_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_rf_2_cat)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 21:43:00,022] Trial 150 finished with value: 0.6656234373694757 and parameters: {'n_estimators': 620}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:44:45,972] Trial 151 finished with value: 0.665707236575979 and parameters: {'n_estimators': 571}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:46:42,091] Trial 152 finished with value: 0.6656020261183226 and parameters: {'n_estimators': 628}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:48:32,272] Trial 153 finished with value: 0.6657686081809958 and parameters: {'n_estimators': 601}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:50:27,682] Trial 154 finished with value: 0.6655745483343282 and parameters: {'n_estimators': 637}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:52:13,053] Trial 155 finished with value: 0.6657344951721004 and parameters: {'n_estimators': 572}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:54:04,225] Trial 156 finished with value: 0.665703547935208 and parameters: {'n_estimators': 607}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:56:05,010] Trial 157 finished with value: 0.6656319481173566 and parameters: {'n_estimators': 648}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:58:04,444] Trial 158 finished with value: 0.6656189027362378 and parameters: {'n_estimators': 646}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 21:59:50,507] Trial 159 finished with value: 0.665707236575979 and parameters: {'n_estimators': 571}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:01:37,481] Trial 160 finished with value: 0.6657646380067035 and parameters: {'n_estimators': 595}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:03:31,944] Trial 161 finished with value: 0.6656468920927054 and parameters: {'n_estimators': 613}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:05:29,970] Trial 162 finished with value: 0.6655464391503718 and parameters: {'n_estimators': 644}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:07:19,218] Trial 163 finished with value: 0.6658250269879967 and parameters: {'n_estimators': 590}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:09:13,377] Trial 164 finished with value: 0.665660011662429 and parameters: {'n_estimators': 626}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:11:13,846] Trial 165 finished with value: 0.6656319481173566 and parameters: {'n_estimators': 648}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:13:02,294] Trial 166 finished with value: 0.6658244162216439 and parameters: {'n_estimators': 584}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:14:54,636] Trial 167 finished with value: 0.6656468920927054 and parameters: {'n_estimators': 613}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:16:37,104] Trial 168 finished with value: 0.665778255205201 and parameters: {'n_estimators': 555}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:18:34,426] Trial 169 finished with value: 0.6656433619170674 and parameters: {'n_estimators': 633}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:20:31,964] Trial 170 finished with value: 0.6656509126072375 and parameters: {'n_estimators': 650}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:22:42,350] Trial 171 finished with value: 0.6657197676251703 and parameters: {'n_estimators': 710}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:24:33,625] Trial 172 finished with value: 0.6656969548963931 and parameters: {'n_estimators': 609}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:26:27,495] Trial 173 finished with value: 0.6655465933733209 and parameters: {'n_estimators': 630}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:28:27,385] Trial 174 finished with value: 0.6656937400565145 and parameters: {'n_estimators': 656}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:30:19,986] Trial 175 finished with value: 0.665649079873873 and parameters: {'n_estimators': 619}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:32:08,716] Trial 176 finished with value: 0.6658428395750275 and parameters: {'n_estimators': 583}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:33:49,912] Trial 177 finished with value: 0.6658003175286413 and parameters: {'n_estimators': 558}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:35:38,919] Trial 178 finished with value: 0.6657752559301464 and parameters: {'n_estimators': 599}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:37:36,044] Trial 179 finished with value: 0.6655472608933725 and parameters: {'n_estimators': 638}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:39:42,853] Trial 180 finished with value: 0.6656581503432639 and parameters: {'n_estimators': 697}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:41:42,644] Trial 181 finished with value: 0.6656576711228247 and parameters: {'n_estimators': 655}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:43:38,208] Trial 182 finished with value: 0.665660011662429 and parameters: {'n_estimators': 626}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:45:29,830] Trial 183 finished with value: 0.665703547935208 and parameters: {'n_estimators': 607}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:47:27,111] Trial 184 finished with value: 0.6655972261520609 and parameters: {'n_estimators': 636}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:49:25,458] Trial 185 finished with value: 0.6657224651236205 and parameters: {'n_estimators': 652}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:51:04,446] Trial 186 finished with value: 0.6660005165427441 and parameters: {'n_estimators': 529}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:53:13,050] Trial 187 finished with value: 0.6654905946073417 and parameters: {'n_estimators': 694}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:55:05,978] Trial 188 finished with value: 0.665649079873873 and parameters: {'n_estimators': 619}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:56:54,551] Trial 189 finished with value: 0.6657752708427033 and parameters: {'n_estimators': 593}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 22:58:56,761] Trial 190 finished with value: 0.6657521240207246 and parameters: {'n_estimators': 661}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:01:06,882] Trial 191 finished with value: 0.6657282187051211 and parameters: {'n_estimators': 718}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:03:04,587] Trial 192 finished with value: 0.6656369646546373 and parameters: {'n_estimators': 635}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:05:07,976] Trial 193 finished with value: 0.6657887168507156 and parameters: {'n_estimators': 662}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:06:58,560] Trial 194 finished with value: 0.6657252264715859 and parameters: {'n_estimators': 608}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:08:56,962] Trial 195 finished with value: 0.6655353036900854 and parameters: {'n_estimators': 642}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:11:03,618] Trial 196 finished with value: 0.66540283266392 and parameters: {'n_estimators': 693}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:12:48,498] Trial 197 finished with value: 0.665707236575979 and parameters: {'n_estimators': 571}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:14:46,415] Trial 198 finished with value: 0.6656209726505221 and parameters: {'n_estimators': 622}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:16:47,942] Trial 199 finished with value: 0.6656743903411746 and parameters: {'n_estimators': 657}. Best is trial 138 with value: 0.6917909965989291.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6918\n",
      "\tBest params:\n",
      "\t\tn_estimators: 635\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.685238    0.711248    0.698779    0.683476\n",
      "1                    TP   27.000000   33.000000   34.000000   38.000000\n",
      "2                    TN  200.000000  197.000000  196.000000  197.000000\n",
      "3                    FP    2.000000    3.000000    4.000000    2.000000\n",
      "4                    FN   39.000000   35.000000   34.000000   31.000000\n",
      "5              Accuracy    0.847015    0.858209    0.858209    0.876866\n",
      "6             Precision    0.931034    0.916667    0.894737    0.950000\n",
      "7           Sensitivity    0.409091    0.485294    0.500000    0.550725\n",
      "8           Specificity    0.990100    0.985000    0.980000    0.989900\n",
      "9              F1 score    0.568421    0.634615    0.641509    0.697248\n",
      "10  F1 score (weighted)    0.823641    0.841646    0.843090    0.864667\n",
      "11     F1 score (macro)    0.737725    0.773326    0.776569    0.809982\n",
      "12    Balanced Accuracy    0.699595    0.735147    0.740000    0.770337\n",
      "13                  MCC    0.553642    0.600128    0.598763    0.663421\n",
      "14                  NPV    0.836800    0.849100    0.852200    0.864000\n",
      "15              ROC_AUC    0.699595    0.735147    0.740000    0.770337\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_rf_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_3_cat = np.where(((y_pred_rf_3 >= 2) | (y_pred_rf_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_rf_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_rf_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_rf_3_cat)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-19 23:18:54,981] Trial 200 finished with value: 0.6688858324073903 and parameters: {'n_estimators': 590}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:21:06,051] Trial 201 finished with value: 0.6686380763890893 and parameters: {'n_estimators': 679}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:23:06,583] Trial 202 finished with value: 0.6681812934233813 and parameters: {'n_estimators': 632}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:25:18,805] Trial 203 finished with value: 0.6686581840550924 and parameters: {'n_estimators': 686}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:27:27,469] Trial 204 finished with value: 0.6684103620659279 and parameters: {'n_estimators': 662}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:29:29,710] Trial 205 finished with value: 0.6682702804447114 and parameters: {'n_estimators': 644}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:31:28,636] Trial 206 finished with value: 0.668578574612689 and parameters: {'n_estimators': 612}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:33:35,069] Trial 207 finished with value: 0.6684306761367174 and parameters: {'n_estimators': 663}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:35:33,119] Trial 208 finished with value: 0.6685506424417834 and parameters: {'n_estimators': 604}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:37:49,906] Trial 209 finished with value: 0.6682720862641696 and parameters: {'n_estimators': 707}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:39:52,302] Trial 210 finished with value: 0.6681423001664742 and parameters: {'n_estimators': 629}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:42:02,889] Trial 211 finished with value: 0.6686033312618298 and parameters: {'n_estimators': 682}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:44:15,872] Trial 212 finished with value: 0.6684845482696823 and parameters: {'n_estimators': 673}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:46:27,967] Trial 213 finished with value: 0.6682814975158742 and parameters: {'n_estimators': 649}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:48:35,607] Trial 214 finished with value: 0.6684357915212509 and parameters: {'n_estimators': 621}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:50:45,736] Trial 215 finished with value: 0.6682465508621117 and parameters: {'n_estimators': 637}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:53:12,914] Trial 216 finished with value: 0.6680740403378649 and parameters: {'n_estimators': 734}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:53:51,235] Trial 217 finished with value: 0.6663827244514698 and parameters: {'n_estimators': 220}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:55:37,231] Trial 218 finished with value: 0.6684388157235431 and parameters: {'n_estimators': 664}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:57:11,028] Trial 219 finished with value: 0.6688622579843738 and parameters: {'n_estimators': 586}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-19 23:59:02,459] Trial 220 finished with value: 0.6686246931341376 and parameters: {'n_estimators': 690}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:00:51,921] Trial 221 finished with value: 0.6685551558935059 and parameters: {'n_estimators': 676}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:02:32,200] Trial 222 finished with value: 0.668215533444105 and parameters: {'n_estimators': 645}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:04:11,318] Trial 223 finished with value: 0.6684616933459185 and parameters: {'n_estimators': 615}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:05:57,240] Trial 224 finished with value: 0.6683243398620776 and parameters: {'n_estimators': 659}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:07:51,839] Trial 225 finished with value: 0.6683028632579319 and parameters: {'n_estimators': 706}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:09:34,274] Trial 226 finished with value: 0.6681812934233813 and parameters: {'n_estimators': 632}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:11:09,534] Trial 227 finished with value: 0.6686484979250074 and parameters: {'n_estimators': 603}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:12:59,467] Trial 228 finished with value: 0.668526716658896 and parameters: {'n_estimators': 674}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:14:40,254] Trial 229 finished with value: 0.6682552288256443 and parameters: {'n_estimators': 636}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:16:25,343] Trial 230 finished with value: 0.6683240557352933 and parameters: {'n_estimators': 650}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:18:14,745] Trial 231 finished with value: 0.668622537392466 and parameters: {'n_estimators': 681}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:20:04,945] Trial 232 finished with value: 0.6684367230318122 and parameters: {'n_estimators': 669}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:21:43,837] Trial 233 finished with value: 0.6684748516925629 and parameters: {'n_estimators': 618}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:23:35,444] Trial 234 finished with value: 0.6684674539250104 and parameters: {'n_estimators': 697}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:25:22,024] Trial 235 finished with value: 0.6683094917275537 and parameters: {'n_estimators': 653}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:26:56,929] Trial 236 finished with value: 0.6688938411057118 and parameters: {'n_estimators': 598}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:28:54,541] Trial 237 finished with value: 0.6681790416660908 and parameters: {'n_estimators': 717}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:30:38,467] Trial 238 finished with value: 0.6682028671576353 and parameters: {'n_estimators': 627}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:32:24,264] Trial 239 finished with value: 0.6684845482696822 and parameters: {'n_estimators': 673}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:34:08,101] Trial 240 finished with value: 0.6681995970196575 and parameters: {'n_estimators': 647}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:35:57,942] Trial 241 finished with value: 0.6686132730841944 and parameters: {'n_estimators': 689}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:37:46,278] Trial 242 finished with value: 0.6684556013924843 and parameters: {'n_estimators': 670}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:39:24,349] Trial 243 finished with value: 0.6684920184141063 and parameters: {'n_estimators': 617}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:41:05,862] Trial 244 finished with value: 0.6682652196729293 and parameters: {'n_estimators': 640}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:42:55,685] Trial 245 finished with value: 0.668622537392466 and parameters: {'n_estimators': 681}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:44:44,769] Trial 246 finished with value: 0.6683243398620776 and parameters: {'n_estimators': 659}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:46:26,037] Trial 247 finished with value: 0.6681423001664742 and parameters: {'n_estimators': 629}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:48:01,318] Trial 248 finished with value: 0.6688705020165541 and parameters: {'n_estimators': 600}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:49:54,884] Trial 249 finished with value: 0.668400604848298 and parameters: {'n_estimators': 701}. Best is trial 138 with value: 0.6917909965989291.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6918\n",
      "\tBest params:\n",
      "\t\tn_estimators: 635\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77894dbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685238    0.711248    0.698779    0.683476   \n",
      "1                    TP   27.000000   33.000000   34.000000   38.000000   \n",
      "2                    TN  200.000000  197.000000  196.000000  197.000000   \n",
      "3                    FP    2.000000    3.000000    4.000000    2.000000   \n",
      "4                    FN   39.000000   35.000000   34.000000   31.000000   \n",
      "5              Accuracy    0.847015    0.858209    0.858209    0.876866   \n",
      "6             Precision    0.931034    0.916667    0.894737    0.950000   \n",
      "7           Sensitivity    0.409091    0.485294    0.500000    0.550725   \n",
      "8           Specificity    0.990100    0.985000    0.980000    0.989900   \n",
      "9              F1 score    0.568421    0.634615    0.641509    0.697248   \n",
      "10  F1 score (weighted)    0.823641    0.841646    0.843090    0.864667   \n",
      "11     F1 score (macro)    0.737725    0.773326    0.776569    0.809982   \n",
      "12    Balanced Accuracy    0.699595    0.735147    0.740000    0.770337   \n",
      "13                  MCC    0.553642    0.600128    0.598763    0.663421   \n",
      "14                  NPV    0.836800    0.849100    0.852200    0.864000   \n",
      "15              ROC_AUC    0.699595    0.735147    0.740000    0.770337   \n",
      "\n",
      "          Set4  \n",
      "0     0.682063  \n",
      "1    37.000000  \n",
      "2   199.000000  \n",
      "3     3.000000  \n",
      "4    29.000000  \n",
      "5     0.880597  \n",
      "6     0.925000  \n",
      "7     0.560606  \n",
      "8     0.985100  \n",
      "9     0.698113  \n",
      "10    0.869563  \n",
      "11    0.811847  \n",
      "12    0.772877  \n",
      "13    0.659854  \n",
      "14    0.872800  \n",
      "15    0.772877  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_rf_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_4_cat = np.where(((y_pred_rf_4 >= 2) | (y_pred_rf_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_rf_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_rf_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_rf_4_cat)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 00:51:44,730] Trial 250 finished with value: 0.673427110779109 and parameters: {'n_estimators': 653}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:53:13,694] Trial 251 finished with value: 0.6729466936100653 and parameters: {'n_estimators': 583}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:54:48,186] Trial 252 finished with value: 0.6731924623368302 and parameters: {'n_estimators': 614}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:56:27,106] Trial 253 finished with value: 0.6735359759760335 and parameters: {'n_estimators': 638}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:58:05,433] Trial 254 finished with value: 0.6732420252982763 and parameters: {'n_estimators': 677}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 00:59:47,534] Trial 255 finished with value: 0.6732706163849348 and parameters: {'n_estimators': 659}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:01:36,009] Trial 256 finished with value: 0.6728663423996017 and parameters: {'n_estimators': 727}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:03:10,023] Trial 257 finished with value: 0.673294016639045 and parameters: {'n_estimators': 626}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:04:53,776] Trial 258 finished with value: 0.6732392066262788 and parameters: {'n_estimators': 689}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:06:24,506] Trial 259 finished with value: 0.6732158908657055 and parameters: {'n_estimators': 609}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:07:57,075] Trial 260 finished with value: 0.6735156185792126 and parameters: {'n_estimators': 648}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:09:19,568] Trial 261 finished with value: 0.6732984802846155 and parameters: {'n_estimators': 573}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:10:55,755] Trial 262 finished with value: 0.673219573358471 and parameters: {'n_estimators': 673}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:12:28,837] Trial 263 finished with value: 0.6734481060528904 and parameters: {'n_estimators': 634}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:14:14,946] Trial 264 finished with value: 0.6731405188581039 and parameters: {'n_estimators': 708}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:15:52,936] Trial 265 finished with value: 0.6732319712974522 and parameters: {'n_estimators': 663}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:17:22,908] Trial 266 finished with value: 0.6729745561428929 and parameters: {'n_estimators': 597}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:18:57,274] Trial 267 finished with value: 0.6732624771888259 and parameters: {'n_estimators': 623}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:20:32,683] Trial 268 finished with value: 0.6734515312228181 and parameters: {'n_estimators': 642}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:22:15,322] Trial 269 finished with value: 0.6733000824309514 and parameters: {'n_estimators': 688}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:23:46,821] Trial 270 finished with value: 0.673097397584921 and parameters: {'n_estimators': 606}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:25:25,606] Trial 271 finished with value: 0.6732455687236667 and parameters: {'n_estimators': 666}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:27:02,453] Trial 272 finished with value: 0.6734519483738878 and parameters: {'n_estimators': 647}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:28:35,148] Trial 273 finished with value: 0.673246948091105 and parameters: {'n_estimators': 618}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:30:05,003] Trial 274 finished with value: 0.6729249886396658 and parameters: {'n_estimators': 585}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:31:49,071] Trial 275 finished with value: 0.6732493054782862 and parameters: {'n_estimators': 694}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:33:24,807] Trial 276 finished with value: 0.6735359759760335 and parameters: {'n_estimators': 638}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:35:01,765] Trial 277 finished with value: 0.6734015389711077 and parameters: {'n_estimators': 657}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:36:43,670] Trial 278 finished with value: 0.6732420252982763 and parameters: {'n_estimators': 677}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:38:16,205] Trial 279 finished with value: 0.6732517553175349 and parameters: {'n_estimators': 625}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:40:01,312] Trial 280 finished with value: 0.6730565229901165 and parameters: {'n_estimators': 712}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:41:30,500] Trial 281 finished with value: 0.6730742532580712 and parameters: {'n_estimators': 599}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:43:10,126] Trial 282 finished with value: 0.6734056221130483 and parameters: {'n_estimators': 655}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:44:34,605] Trial 283 finished with value: 0.6733949622900889 and parameters: {'n_estimators': 564}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:46:10,897] Trial 284 finished with value: 0.6735134893640051 and parameters: {'n_estimators': 637}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:47:42,819] Trial 285 finished with value: 0.6731785927260864 and parameters: {'n_estimators': 612}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:49:24,235] Trial 286 finished with value: 0.673198992802758 and parameters: {'n_estimators': 675}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:50:57,321] Trial 287 finished with value: 0.673282328387321 and parameters: {'n_estimators': 627}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:52:36,632] Trial 288 finished with value: 0.6732706163849348 and parameters: {'n_estimators': 659}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:54:17,442] Trial 289 finished with value: 0.6731863773554364 and parameters: {'n_estimators': 683}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:55:53,013] Trial 290 finished with value: 0.673462641828899 and parameters: {'n_estimators': 644}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:57:36,152] Trial 291 finished with value: 0.673149205906299 and parameters: {'n_estimators': 702}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 01:59:04,427] Trial 292 finished with value: 0.6729249886396658 and parameters: {'n_estimators': 585}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:00:35,022] Trial 293 finished with value: 0.6731924623368302 and parameters: {'n_estimators': 614}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:02:14,611] Trial 294 finished with value: 0.6732455687236667 and parameters: {'n_estimators': 666}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:03:48,076] Trial 295 finished with value: 0.673311479829496 and parameters: {'n_estimators': 631}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:05:40,846] Trial 296 finished with value: 0.6727206241770215 and parameters: {'n_estimators': 749}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:07:17,876] Trial 297 finished with value: 0.6734433816896757 and parameters: {'n_estimators': 649}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:08:45,754] Trial 298 finished with value: 0.6731305222730626 and parameters: {'n_estimators': 600}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:10:28,424] Trial 299 finished with value: 0.6732392066262788 and parameters: {'n_estimators': 689}. Best is trial 138 with value: 0.6917909965989291.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6918\n",
      "\tBest params:\n",
      "\t\tn_estimators: 635\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685238    0.711248    0.698779    0.683476   \n",
      "1                    TP   27.000000   33.000000   34.000000   38.000000   \n",
      "2                    TN  200.000000  197.000000  196.000000  197.000000   \n",
      "3                    FP    2.000000    3.000000    4.000000    2.000000   \n",
      "4                    FN   39.000000   35.000000   34.000000   31.000000   \n",
      "5              Accuracy    0.847015    0.858209    0.858209    0.876866   \n",
      "6             Precision    0.931034    0.916667    0.894737    0.950000   \n",
      "7           Sensitivity    0.409091    0.485294    0.500000    0.550725   \n",
      "8           Specificity    0.990100    0.985000    0.980000    0.989900   \n",
      "9              F1 score    0.568421    0.634615    0.641509    0.697248   \n",
      "10  F1 score (weighted)    0.823641    0.841646    0.843090    0.864667   \n",
      "11     F1 score (macro)    0.737725    0.773326    0.776569    0.809982   \n",
      "12    Balanced Accuracy    0.699595    0.735147    0.740000    0.770337   \n",
      "13                  MCC    0.553642    0.600128    0.598763    0.663421   \n",
      "14                  NPV    0.836800    0.849100    0.852200    0.864000   \n",
      "15              ROC_AUC    0.699595    0.735147    0.740000    0.770337   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.682063    0.698584  \n",
      "1    37.000000   38.000000  \n",
      "2   199.000000  198.000000  \n",
      "3     3.000000    2.000000  \n",
      "4    29.000000   30.000000  \n",
      "5     0.880597    0.880597  \n",
      "6     0.925000    0.950000  \n",
      "7     0.560606    0.558824  \n",
      "8     0.985100    0.990000  \n",
      "9     0.698113    0.703704  \n",
      "10    0.869563    0.869025  \n",
      "11    0.811847    0.814469  \n",
      "12    0.772877    0.774412  \n",
      "13    0.659854    0.670201  \n",
      "14    0.872800    0.868400  \n",
      "15    0.772877    0.774412  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_rf_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_5_cat = np.where(((y_pred_rf_5 >= 2) | (y_pred_rf_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_rf_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_rf_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_rf_5_cat)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 02:12:16,614] Trial 300 finished with value: 0.6613557957946947 and parameters: {'n_estimators': 623}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:14:01,682] Trial 301 finished with value: 0.6617003563375847 and parameters: {'n_estimators': 668}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:15:52,246] Trial 302 finished with value: 0.6617359341987348 and parameters: {'n_estimators': 721}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:17:31,600] Trial 303 finished with value: 0.6613298460677043 and parameters: {'n_estimators': 642}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:19:07,856] Trial 304 finished with value: 0.6612739355686938 and parameters: {'n_estimators': 609}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:20:50,386] Trial 305 finished with value: 0.6615514495025021 and parameters: {'n_estimators': 661}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:22:38,326] Trial 306 finished with value: 0.6618497144904449 and parameters: {'n_estimators': 697}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:24:01,957] Trial 307 finished with value: 0.6613263468848107 and parameters: {'n_estimators': 541}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:25:41,003] Trial 308 finished with value: 0.6613858395529582 and parameters: {'n_estimators': 633}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:27:12,788] Trial 309 finished with value: 0.6612545284395732 and parameters: {'n_estimators': 585}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:29:00,963] Trial 310 finished with value: 0.6615756461922991 and parameters: {'n_estimators': 680}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:30:40,267] Trial 311 finished with value: 0.6613333915088957 and parameters: {'n_estimators': 647}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:32:14,432] Trial 312 finished with value: 0.6614444138937456 and parameters: {'n_estimators': 617}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:33:59,539] Trial 313 finished with value: 0.6615021194032631 and parameters: {'n_estimators': 658}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:35:33,815] Trial 314 finished with value: 0.6612618797681548 and parameters: {'n_estimators': 600}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:37:12,341] Trial 315 finished with value: 0.6613343916649932 and parameters: {'n_estimators': 636}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:38:58,545] Trial 316 finished with value: 0.6615180898813242 and parameters: {'n_estimators': 675}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:40:35,878] Trial 317 finished with value: 0.6613569234180846 and parameters: {'n_estimators': 622}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:42:22,502] Trial 318 finished with value: 0.6617982888090064 and parameters: {'n_estimators': 696}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:44:04,042] Trial 319 finished with value: 0.6614926231365355 and parameters: {'n_estimators': 654}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:45:32,788] Trial 320 finished with value: 0.6613065585513755 and parameters: {'n_estimators': 568}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:47:04,448] Trial 321 finished with value: 0.6612036476922691 and parameters: {'n_estimators': 599}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:48:43,058] Trial 322 finished with value: 0.6614913871432859 and parameters: {'n_estimators': 638}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:50:26,874] Trial 323 finished with value: 0.6616980729920076 and parameters: {'n_estimators': 669}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:52:04,301] Trial 324 finished with value: 0.6614444138937456 and parameters: {'n_estimators': 617}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:53:55,256] Trial 325 finished with value: 0.6617773375960323 and parameters: {'n_estimators': 711}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:55:49,426] Trial 326 finished with value: 0.6618743203719497 and parameters: {'n_estimators': 734}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:57:28,519] Trial 327 finished with value: 0.6612311192805166 and parameters: {'n_estimators': 651}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 02:59:17,240] Trial 328 finished with value: 0.6617604423704697 and parameters: {'n_estimators': 686}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:00:54,520] Trial 329 finished with value: 0.6613076489435196 and parameters: {'n_estimators': 629}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:02:24,905] Trial 330 finished with value: 0.6612767146092665 and parameters: {'n_estimators': 584}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:04:09,733] Trial 331 finished with value: 0.6616980729920077 and parameters: {'n_estimators': 669}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:05:45,436] Trial 332 finished with value: 0.66134304370849 and parameters: {'n_estimators': 612}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:07:26,383] Trial 333 finished with value: 0.6612932737786912 and parameters: {'n_estimators': 648}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:09:04,008] Trial 334 finished with value: 0.6613287942406898 and parameters: {'n_estimators': 631}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:10:49,167] Trial 335 finished with value: 0.6617604423704697 and parameters: {'n_estimators': 686}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:12:31,795] Trial 336 finished with value: 0.6615430496880808 and parameters: {'n_estimators': 659}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:14:05,379] Trial 337 finished with value: 0.6612770831849872 and parameters: {'n_estimators': 598}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:15:54,224] Trial 338 finished with value: 0.661779274550509 and parameters: {'n_estimators': 708}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:17:33,201] Trial 339 finished with value: 0.6613582292469666 and parameters: {'n_estimators': 637}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:19:18,231] Trial 340 finished with value: 0.6615454010260978 and parameters: {'n_estimators': 677}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:20:53,812] Trial 341 finished with value: 0.6613531268231274 and parameters: {'n_estimators': 615}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:22:35,422] Trial 342 finished with value: 0.6612932737786912 and parameters: {'n_estimators': 648}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:24:17,698] Trial 343 finished with value: 0.6616980729920077 and parameters: {'n_estimators': 669}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:25:54,025] Trial 344 finished with value: 0.6613273399784039 and parameters: {'n_estimators': 625}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:27:26,491] Trial 345 finished with value: 0.6611238524613684 and parameters: {'n_estimators': 593}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:29:12,831] Trial 346 finished with value: 0.6617388999725309 and parameters: {'n_estimators': 691}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:30:54,998] Trial 347 finished with value: 0.6613132574190649 and parameters: {'n_estimators': 652}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:32:27,405] Trial 348 finished with value: 0.661315432407851 and parameters: {'n_estimators': 606}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:33:06,892] Trial 349 finished with value: 0.6591684741810113 and parameters: {'n_estimators': 257}. Best is trial 138 with value: 0.6917909965989291.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6918\n",
      "\tBest params:\n",
      "\t\tn_estimators: 635\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685238    0.711248    0.698779    0.683476   \n",
      "1                    TP   27.000000   33.000000   34.000000   38.000000   \n",
      "2                    TN  200.000000  197.000000  196.000000  197.000000   \n",
      "3                    FP    2.000000    3.000000    4.000000    2.000000   \n",
      "4                    FN   39.000000   35.000000   34.000000   31.000000   \n",
      "5              Accuracy    0.847015    0.858209    0.858209    0.876866   \n",
      "6             Precision    0.931034    0.916667    0.894737    0.950000   \n",
      "7           Sensitivity    0.409091    0.485294    0.500000    0.550725   \n",
      "8           Specificity    0.990100    0.985000    0.980000    0.989900   \n",
      "9              F1 score    0.568421    0.634615    0.641509    0.697248   \n",
      "10  F1 score (weighted)    0.823641    0.841646    0.843090    0.864667   \n",
      "11     F1 score (macro)    0.737725    0.773326    0.776569    0.809982   \n",
      "12    Balanced Accuracy    0.699595    0.735147    0.740000    0.770337   \n",
      "13                  MCC    0.553642    0.600128    0.598763    0.663421   \n",
      "14                  NPV    0.836800    0.849100    0.852200    0.864000   \n",
      "15              ROC_AUC    0.699595    0.735147    0.740000    0.770337   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.682063    0.698584    0.735695  \n",
      "1    37.000000   38.000000   36.000000  \n",
      "2   199.000000  198.000000  199.000000  \n",
      "3     3.000000    2.000000    1.000000  \n",
      "4    29.000000   30.000000   32.000000  \n",
      "5     0.880597    0.880597    0.876866  \n",
      "6     0.925000    0.950000    0.972973  \n",
      "7     0.560606    0.558824    0.529412  \n",
      "8     0.985100    0.990000    0.995000  \n",
      "9     0.698113    0.703704    0.685714  \n",
      "10    0.869563    0.869025    0.863117  \n",
      "11    0.811847    0.814469    0.804574  \n",
      "12    0.772877    0.774412    0.762206  \n",
      "13    0.659854    0.670201    0.661508  \n",
      "14    0.872800    0.868400    0.861500  \n",
      "15    0.772877    0.774412    0.762206  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_rf_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_6_cat = np.where(((y_pred_rf_6 >= 2) | (y_pred_rf_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_rf_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_rf_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_rf_6_cat)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 03:34:54,088] Trial 350 finished with value: 0.6883368685069984 and parameters: {'n_estimators': 637}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:36:29,852] Trial 351 finished with value: 0.6884707209994941 and parameters: {'n_estimators': 666}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:38:18,847] Trial 352 finished with value: 0.6883047854349572 and parameters: {'n_estimators': 722}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:39:43,870] Trial 353 finished with value: 0.6879468009786953 and parameters: {'n_estimators': 570}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:41:16,864] Trial 354 finished with value: 0.6882802829001705 and parameters: {'n_estimators': 621}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:41:34,782] Trial 355 finished with value: 0.6841190436007629 and parameters: {'n_estimators': 116}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:43:20,068] Trial 356 finished with value: 0.6883682891780586 and parameters: {'n_estimators': 698}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:44:57,201] Trial 357 finished with value: 0.688516257633114 and parameters: {'n_estimators': 648}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:46:38,537] Trial 358 finished with value: 0.688432241447361 and parameters: {'n_estimators': 676}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:48:13,048] Trial 359 finished with value: 0.6884799147240894 and parameters: {'n_estimators': 631}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:49:43,785] Trial 360 finished with value: 0.6881264249094308 and parameters: {'n_estimators': 608}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:51:21,670] Trial 361 finished with value: 0.6884711265079956 and parameters: {'n_estimators': 659}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:52:52,385] Trial 362 finished with value: 0.6881509402948209 and parameters: {'n_estimators': 592}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:54:27,259] Trial 363 finished with value: 0.6884260636258622 and parameters: {'n_estimators': 643}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:56:08,381] Trial 364 finished with value: 0.6884307640365055 and parameters: {'n_estimators': 683}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:57:42,421] Trial 365 finished with value: 0.68845807153298 and parameters: {'n_estimators': 628}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 03:59:26,477] Trial 366 finished with value: 0.6882600163482262 and parameters: {'n_estimators': 705}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:01:07,057] Trial 367 finished with value: 0.6884931137762544 and parameters: {'n_estimators': 662}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:02:38,413] Trial 368 finished with value: 0.6881444610047054 and parameters: {'n_estimators': 610}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:04:04,397] Trial 369 finished with value: 0.6879874904710856 and parameters: {'n_estimators': 576}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:05:40,459] Trial 370 finished with value: 0.6884406682725246 and parameters: {'n_estimators': 642}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:07:23,113] Trial 371 finished with value: 0.6883970446235368 and parameters: {'n_estimators': 675}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:08:58,069] Trial 372 finished with value: 0.6883389013040057 and parameters: {'n_estimators': 624}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:10:37,304] Trial 373 finished with value: 0.6883992369552112 and parameters: {'n_estimators': 658}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:12:06,034] Trial 374 finished with value: 0.6881824725368502 and parameters: {'n_estimators': 602}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:13:29,116] Trial 375 finished with value: 0.6876136138377753 and parameters: {'n_estimators': 554}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:15:05,386] Trial 376 finished with value: 0.6883851684332931 and parameters: {'n_estimators': 641}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:16:59,141] Trial 377 finished with value: 0.6881740755371617 and parameters: {'n_estimators': 750}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:18:41,811] Trial 378 finished with value: 0.6884245184493103 and parameters: {'n_estimators': 682}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:20:14,347] Trial 379 finished with value: 0.688210821727098 and parameters: {'n_estimators': 619}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:21:59,662] Trial 380 finished with value: 0.6883593217123251 and parameters: {'n_estimators': 699}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:23:39,193] Trial 381 finished with value: 0.688422670858998 and parameters: {'n_estimators': 655}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:25:15,828] Trial 382 finished with value: 0.6885125479443485 and parameters: {'n_estimators': 633}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:26:42,681] Trial 383 finished with value: 0.6879407953435972 and parameters: {'n_estimators': 585}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:28:24,897] Trial 384 finished with value: 0.6884651633114505 and parameters: {'n_estimators': 669}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:29:57,016] Trial 385 finished with value: 0.6881954223890906 and parameters: {'n_estimators': 616}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:31:44,043] Trial 386 finished with value: 0.6881585850075509 and parameters: {'n_estimators': 714}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:33:27,288] Trial 387 finished with value: 0.6883948640052635 and parameters: {'n_estimators': 689}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:35:06,338] Trial 388 finished with value: 0.688516257633114 and parameters: {'n_estimators': 648}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:36:24,339] Trial 389 finished with value: 0.6872717825643189 and parameters: {'n_estimators': 518}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:37:56,862] Trial 390 finished with value: 0.6881136940408835 and parameters: {'n_estimators': 600}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:39:38,771] Trial 391 finished with value: 0.6884848727424894 and parameters: {'n_estimators': 667}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:41:13,333] Trial 392 finished with value: 0.6884799147240893 and parameters: {'n_estimators': 631}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:42:50,820] Trial 393 finished with value: 0.688516257633114 and parameters: {'n_estimators': 648}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:44:23,642] Trial 394 finished with value: 0.6881248926154608 and parameters: {'n_estimators': 617}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:46:06,058] Trial 395 finished with value: 0.688425255775958 and parameters: {'n_estimators': 685}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:47:50,122] Trial 396 finished with value: 0.6884126284557701 and parameters: {'n_estimators': 664}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:49:39,330] Trial 397 finished with value: 0.6883981184394501 and parameters: {'n_estimators': 728}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:51:07,362] Trial 398 finished with value: 0.6881509402948209 and parameters: {'n_estimators': 592}. Best is trial 138 with value: 0.6917909965989291.\n",
      "[I 2023-12-20 04:52:42,908] Trial 399 finished with value: 0.6884784889694273 and parameters: {'n_estimators': 635}. Best is trial 138 with value: 0.6917909965989291.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.6918\n",
      "\tBest params:\n",
      "\t\tn_estimators: 635\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685238    0.711248    0.698779    0.683476   \n",
      "1                    TP   27.000000   33.000000   34.000000   38.000000   \n",
      "2                    TN  200.000000  197.000000  196.000000  197.000000   \n",
      "3                    FP    2.000000    3.000000    4.000000    2.000000   \n",
      "4                    FN   39.000000   35.000000   34.000000   31.000000   \n",
      "5              Accuracy    0.847015    0.858209    0.858209    0.876866   \n",
      "6             Precision    0.931034    0.916667    0.894737    0.950000   \n",
      "7           Sensitivity    0.409091    0.485294    0.500000    0.550725   \n",
      "8           Specificity    0.990100    0.985000    0.980000    0.989900   \n",
      "9              F1 score    0.568421    0.634615    0.641509    0.697248   \n",
      "10  F1 score (weighted)    0.823641    0.841646    0.843090    0.864667   \n",
      "11     F1 score (macro)    0.737725    0.773326    0.776569    0.809982   \n",
      "12    Balanced Accuracy    0.699595    0.735147    0.740000    0.770337   \n",
      "13                  MCC    0.553642    0.600128    0.598763    0.663421   \n",
      "14                  NPV    0.836800    0.849100    0.852200    0.864000   \n",
      "15              ROC_AUC    0.699595    0.735147    0.740000    0.770337   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.682063    0.698584    0.735695    0.615020  \n",
      "1    37.000000   38.000000   36.000000   33.000000  \n",
      "2   199.000000  198.000000  199.000000  195.000000  \n",
      "3     3.000000    2.000000    1.000000    6.000000  \n",
      "4    29.000000   30.000000   32.000000   34.000000  \n",
      "5     0.880597    0.880597    0.876866    0.850746  \n",
      "6     0.925000    0.950000    0.972973    0.846154  \n",
      "7     0.560606    0.558824    0.529412    0.492537  \n",
      "8     0.985100    0.990000    0.995000    0.970100  \n",
      "9     0.698113    0.703704    0.685714    0.622642  \n",
      "10    0.869563    0.869025    0.863117    0.835893  \n",
      "11    0.811847    0.814469    0.804574    0.764809  \n",
      "12    0.772877    0.774412    0.762206    0.731343  \n",
      "13    0.659854    0.670201    0.661508    0.568162  \n",
      "14    0.872800    0.868400    0.861500    0.851500  \n",
      "15    0.772877    0.774412    0.762206    0.731343  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_rf_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_7_cat = np.where(((y_pred_rf_7 >= 2) | (y_pred_rf_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_rf_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_rf_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_rf_7_cat)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 04:54:24,254] Trial 400 finished with value: 0.6945503000970982 and parameters: {'n_estimators': 609}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 04:55:50,063] Trial 401 finished with value: 0.6941508161353108 and parameters: {'n_estimators': 581}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 04:57:15,346] Trial 402 finished with value: 0.6940371292169263 and parameters: {'n_estimators': 560}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 04:58:33,421] Trial 403 finished with value: 0.6937995596188818 and parameters: {'n_estimators': 536}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 04:59:54,290] Trial 404 finished with value: 0.6937882072599083 and parameters: {'n_estimators': 543}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:01:13,757] Trial 405 finished with value: 0.6939887552919694 and parameters: {'n_estimators': 529}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:02:30,897] Trial 406 finished with value: 0.6942849170111446 and parameters: {'n_estimators': 521}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:03:43,280] Trial 407 finished with value: 0.6939685050456393 and parameters: {'n_estimators': 483}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:04:57,318] Trial 408 finished with value: 0.6940882993661115 and parameters: {'n_estimators': 497}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:06:10,334] Trial 409 finished with value: 0.6938629474779782 and parameters: {'n_estimators': 486}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:07:27,610] Trial 410 finished with value: 0.6939857246725227 and parameters: {'n_estimators': 527}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:08:40,370] Trial 411 finished with value: 0.693886158613657 and parameters: {'n_estimators': 485}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:09:52,366] Trial 412 finished with value: 0.6940446765952563 and parameters: {'n_estimators': 481}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:11:05,154] Trial 413 finished with value: 0.6939685050456393 and parameters: {'n_estimators': 483}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:12:15,934] Trial 414 finished with value: 0.6938629474779782 and parameters: {'n_estimators': 486}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:13:27,592] Trial 415 finished with value: 0.69408732343141 and parameters: {'n_estimators': 477}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:14:40,780] Trial 416 finished with value: 0.6938750883700394 and parameters: {'n_estimators': 487}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:15:54,037] Trial 417 finished with value: 0.6938629474779782 and parameters: {'n_estimators': 486}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:17:05,603] Trial 418 finished with value: 0.693886158613657 and parameters: {'n_estimators': 485}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:18:17,907] Trial 419 finished with value: 0.6939833867059145 and parameters: {'n_estimators': 491}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:19:28,194] Trial 420 finished with value: 0.6939171278702523 and parameters: {'n_estimators': 484}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:20:41,396] Trial 421 finished with value: 0.693862947477978 and parameters: {'n_estimators': 486}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:21:53,392] Trial 422 finished with value: 0.6940606023917174 and parameters: {'n_estimators': 479}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:23:06,494] Trial 423 finished with value: 0.6939685050456393 and parameters: {'n_estimators': 483}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:24:17,938] Trial 424 finished with value: 0.6939856084856543 and parameters: {'n_estimators': 482}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:25:29,275] Trial 425 finished with value: 0.69408732343141 and parameters: {'n_estimators': 477}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:26:41,854] Trial 426 finished with value: 0.6938750883700395 and parameters: {'n_estimators': 487}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:27:53,082] Trial 427 finished with value: 0.6939171278702523 and parameters: {'n_estimators': 484}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:29:01,597] Trial 428 finished with value: 0.694229250786301 and parameters: {'n_estimators': 450}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:30:06,811] Trial 429 finished with value: 0.6941540050819555 and parameters: {'n_estimators': 436}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:31:14,556] Trial 430 finished with value: 0.6942508836288168 and parameters: {'n_estimators': 442}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:32:22,415] Trial 431 finished with value: 0.6942513541614647 and parameters: {'n_estimators': 448}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:33:29,114] Trial 432 finished with value: 0.6941989251889693 and parameters: {'n_estimators': 449}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:34:33,833] Trial 433 finished with value: 0.6942513541614644 and parameters: {'n_estimators': 448}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:35:43,581] Trial 434 finished with value: 0.6941750007262387 and parameters: {'n_estimators': 458}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:36:53,337] Trial 435 finished with value: 0.6942642656208303 and parameters: {'n_estimators': 452}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:38:01,293] Trial 436 finished with value: 0.6941989251889693 and parameters: {'n_estimators': 449}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:39:09,303] Trial 437 finished with value: 0.694312636069365 and parameters: {'n_estimators': 445}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:40:18,190] Trial 438 finished with value: 0.694229250786301 and parameters: {'n_estimators': 450}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:41:26,712] Trial 439 finished with value: 0.6942513541614647 and parameters: {'n_estimators': 448}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:42:34,361] Trial 440 finished with value: 0.6943206687699216 and parameters: {'n_estimators': 443}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:43:43,757] Trial 441 finished with value: 0.6941989251889693 and parameters: {'n_estimators': 449}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:44:55,201] Trial 442 finished with value: 0.6941840136698781 and parameters: {'n_estimators': 454}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:46:10,688] Trial 443 finished with value: 0.6943430636220468 and parameters: {'n_estimators': 444}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:47:27,041] Trial 444 finished with value: 0.6942513541614647 and parameters: {'n_estimators': 448}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:48:51,545] Trial 445 finished with value: 0.6943430636220468 and parameters: {'n_estimators': 444}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:50:10,098] Trial 446 finished with value: 0.694270031014167 and parameters: {'n_estimators': 446}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:51:35,265] Trial 447 finished with value: 0.694229250786301 and parameters: {'n_estimators': 450}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:53:06,998] Trial 448 finished with value: 0.694229250786301 and parameters: {'n_estimators': 450}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:54:30,631] Trial 449 finished with value: 0.6943206687699217 and parameters: {'n_estimators': 443}. Best is trial 400 with value: 0.6945503000970982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.6946\n",
      "\tBest params:\n",
      "\t\tn_estimators: 609\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685238    0.711248    0.698779    0.683476   \n",
      "1                    TP   27.000000   33.000000   34.000000   38.000000   \n",
      "2                    TN  200.000000  197.000000  196.000000  197.000000   \n",
      "3                    FP    2.000000    3.000000    4.000000    2.000000   \n",
      "4                    FN   39.000000   35.000000   34.000000   31.000000   \n",
      "5              Accuracy    0.847015    0.858209    0.858209    0.876866   \n",
      "6             Precision    0.931034    0.916667    0.894737    0.950000   \n",
      "7           Sensitivity    0.409091    0.485294    0.500000    0.550725   \n",
      "8           Specificity    0.990100    0.985000    0.980000    0.989900   \n",
      "9              F1 score    0.568421    0.634615    0.641509    0.697248   \n",
      "10  F1 score (weighted)    0.823641    0.841646    0.843090    0.864667   \n",
      "11     F1 score (macro)    0.737725    0.773326    0.776569    0.809982   \n",
      "12    Balanced Accuracy    0.699595    0.735147    0.740000    0.770337   \n",
      "13                  MCC    0.553642    0.600128    0.598763    0.663421   \n",
      "14                  NPV    0.836800    0.849100    0.852200    0.864000   \n",
      "15              ROC_AUC    0.699595    0.735147    0.740000    0.770337   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.682063    0.698584    0.735695    0.615020    0.640357  \n",
      "1    37.000000   38.000000   36.000000   33.000000   33.000000  \n",
      "2   199.000000  198.000000  199.000000  195.000000  198.000000  \n",
      "3     3.000000    2.000000    1.000000    6.000000    2.000000  \n",
      "4    29.000000   30.000000   32.000000   34.000000   35.000000  \n",
      "5     0.880597    0.880597    0.876866    0.850746    0.861940  \n",
      "6     0.925000    0.950000    0.972973    0.846154    0.942857  \n",
      "7     0.560606    0.558824    0.529412    0.492537    0.485294  \n",
      "8     0.985100    0.990000    0.995000    0.970100    0.990000  \n",
      "9     0.698113    0.703704    0.685714    0.622642    0.640777  \n",
      "10    0.869563    0.869025    0.863117    0.835893    0.845085  \n",
      "11    0.811847    0.814469    0.804574    0.764809    0.777663  \n",
      "12    0.772877    0.774412    0.762206    0.731343    0.737647  \n",
      "13    0.659854    0.670201    0.661508    0.568162    0.613790  \n",
      "14    0.872800    0.868400    0.861500    0.851500    0.849800  \n",
      "15    0.772877    0.774412    0.762206    0.731343    0.737647  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_rf_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_8_cat = np.where(((y_pred_rf_8 >= 2) | (y_pred_rf_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_rf_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_rf_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_rf_8_cat)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 05:56:02,243] Trial 450 finished with value: 0.6838757614033291 and parameters: {'n_estimators': 444}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:57:08,420] Trial 451 finished with value: 0.6834990705207773 and parameters: {'n_estimators': 405}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:58:05,439] Trial 452 finished with value: 0.6836806101202907 and parameters: {'n_estimators': 447}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:58:59,411] Trial 453 finished with value: 0.6837510438119085 and parameters: {'n_estimators': 425}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 05:59:57,384] Trial 454 finished with value: 0.6836162303012268 and parameters: {'n_estimators': 454}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:00:49,268] Trial 455 finished with value: 0.6833540530807533 and parameters: {'n_estimators': 403}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:01:44,261] Trial 456 finished with value: 0.6836674115099542 and parameters: {'n_estimators': 430}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:02:32,438] Trial 457 finished with value: 0.6830906813053157 and parameters: {'n_estimators': 380}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:03:27,765] Trial 458 finished with value: 0.6838889984455812 and parameters: {'n_estimators': 434}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:04:25,862] Trial 459 finished with value: 0.6835099061586787 and parameters: {'n_estimators': 456}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:05:18,465] Trial 460 finished with value: 0.6837532286914568 and parameters: {'n_estimators': 414}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:06:17,597] Trial 461 finished with value: 0.6834586885423256 and parameters: {'n_estimators': 460}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:07:13,811] Trial 462 finished with value: 0.6840833991362384 and parameters: {'n_estimators': 438}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:08:12,795] Trial 463 finished with value: 0.6834704445756213 and parameters: {'n_estimators': 459}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:09:05,858] Trial 464 finished with value: 0.6839790527784395 and parameters: {'n_estimators': 422}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:10:01,421] Trial 465 finished with value: 0.6840334028750521 and parameters: {'n_estimators': 441}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:10:58,841] Trial 466 finished with value: 0.683521230857234 and parameters: {'n_estimators': 450}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:11:51,829] Trial 467 finished with value: 0.6836674081862242 and parameters: {'n_estimators': 416}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:12:50,498] Trial 468 finished with value: 0.6834586885423256 and parameters: {'n_estimators': 460}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:13:40,808] Trial 469 finished with value: 0.6833251863041025 and parameters: {'n_estimators': 399}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:14:36,204] Trial 470 finished with value: 0.6838897601666452 and parameters: {'n_estimators': 435}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:15:25,212] Trial 471 finished with value: 0.6830988869651236 and parameters: {'n_estimators': 379}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:16:23,304] Trial 472 finished with value: 0.6834586885423255 and parameters: {'n_estimators': 460}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:17:19,274] Trial 473 finished with value: 0.6836708646602631 and parameters: {'n_estimators': 432}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:18:17,664] Trial 474 finished with value: 0.6834586885423255 and parameters: {'n_estimators': 460}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:19:10,675] Trial 475 finished with value: 0.6836674081862242 and parameters: {'n_estimators': 416}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:20:07,812] Trial 476 finished with value: 0.6839515838676034 and parameters: {'n_estimators': 443}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:20:58,300] Trial 477 finished with value: 0.6833350862332336 and parameters: {'n_estimators': 397}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:21:53,120] Trial 478 finished with value: 0.6836437711118205 and parameters: {'n_estimators': 427}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:22:51,606] Trial 479 finished with value: 0.6834499641077831 and parameters: {'n_estimators': 461}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:23:56,001] Trial 480 finished with value: 0.6835950885985355 and parameters: {'n_estimators': 510}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:24:52,257] Trial 481 finished with value: 0.6838757614033291 and parameters: {'n_estimators': 444}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:25:50,251] Trial 482 finished with value: 0.6834919497763637 and parameters: {'n_estimators': 463}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:26:44,354] Trial 483 finished with value: 0.6836069103816627 and parameters: {'n_estimators': 418}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:27:39,712] Trial 484 finished with value: 0.6840593359525244 and parameters: {'n_estimators': 439}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:28:36,129] Trial 485 finished with value: 0.6837795287982538 and parameters: {'n_estimators': 446}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:29:26,373] Trial 486 finished with value: 0.6833287633723731 and parameters: {'n_estimators': 393}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:30:20,789] Trial 487 finished with value: 0.6838412532826328 and parameters: {'n_estimators': 421}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:31:19,496] Trial 488 finished with value: 0.6835750290002314 and parameters: {'n_estimators': 457}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:32:18,409] Trial 489 finished with value: 0.6835640248303478 and parameters: {'n_estimators': 464}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:33:12,675] Trial 490 finished with value: 0.6836708646602631 and parameters: {'n_estimators': 432}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:34:09,426] Trial 491 finished with value: 0.6839834258708439 and parameters: {'n_estimators': 442}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:35:02,368] Trial 492 finished with value: 0.6836491570658382 and parameters: {'n_estimators': 412}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:35:49,065] Trial 493 finished with value: 0.6831203067438884 and parameters: {'n_estimators': 365}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:36:48,836] Trial 494 finished with value: 0.6835482471749968 and parameters: {'n_estimators': 468}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:37:52,353] Trial 495 finished with value: 0.6836724174525296 and parameters: {'n_estimators': 508}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:38:47,579] Trial 496 finished with value: 0.6837138129654606 and parameters: {'n_estimators': 431}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:39:44,147] Trial 497 finished with value: 0.6836806101202907 and parameters: {'n_estimators': 447}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:40:43,533] Trial 498 finished with value: 0.6836389767379056 and parameters: {'n_estimators': 466}. Best is trial 400 with value: 0.6945503000970982.\n",
      "[I 2023-12-20 06:41:36,047] Trial 499 finished with value: 0.6837429242217478 and parameters: {'n_estimators': 413}. Best is trial 400 with value: 0.6945503000970982.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6946\n",
      "\tBest params:\n",
      "\t\tn_estimators: 609\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFRegressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.685238    0.711248    0.698779    0.683476   \n",
      "1                    TP   27.000000   33.000000   34.000000   38.000000   \n",
      "2                    TN  200.000000  197.000000  196.000000  197.000000   \n",
      "3                    FP    2.000000    3.000000    4.000000    2.000000   \n",
      "4                    FN   39.000000   35.000000   34.000000   31.000000   \n",
      "5              Accuracy    0.847015    0.858209    0.858209    0.876866   \n",
      "6             Precision    0.931034    0.916667    0.894737    0.950000   \n",
      "7           Sensitivity    0.409091    0.485294    0.500000    0.550725   \n",
      "8           Specificity    0.990100    0.985000    0.980000    0.989900   \n",
      "9              F1 score    0.568421    0.634615    0.641509    0.697248   \n",
      "10  F1 score (weighted)    0.823641    0.841646    0.843090    0.864667   \n",
      "11     F1 score (macro)    0.737725    0.773326    0.776569    0.809982   \n",
      "12    Balanced Accuracy    0.699595    0.735147    0.740000    0.770337   \n",
      "13                  MCC    0.553642    0.600128    0.598763    0.663421   \n",
      "14                  NPV    0.836800    0.849100    0.852200    0.864000   \n",
      "15              ROC_AUC    0.699595    0.735147    0.740000    0.770337   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.682063    0.698584    0.735695    0.615020    0.640357    0.673751  \n",
      "1    37.000000   38.000000   36.000000   33.000000   33.000000   30.000000  \n",
      "2   199.000000  198.000000  199.000000  195.000000  198.000000  199.000000  \n",
      "3     3.000000    2.000000    1.000000    6.000000    2.000000    2.000000  \n",
      "4    29.000000   30.000000   32.000000   34.000000   35.000000   37.000000  \n",
      "5     0.880597    0.880597    0.876866    0.850746    0.861940    0.854478  \n",
      "6     0.925000    0.950000    0.972973    0.846154    0.942857    0.937500  \n",
      "7     0.560606    0.558824    0.529412    0.492537    0.485294    0.447761  \n",
      "8     0.985100    0.990000    0.995000    0.970100    0.990000    0.990000  \n",
      "9     0.698113    0.703704    0.685714    0.622642    0.640777    0.606061  \n",
      "10    0.869563    0.869025    0.863117    0.835893    0.845085    0.834582  \n",
      "11    0.811847    0.814469    0.804574    0.764809    0.777663    0.758408  \n",
      "12    0.772877    0.774412    0.762206    0.731343    0.737647    0.718905  \n",
      "13    0.659854    0.670201    0.661508    0.568162    0.613790    0.584643  \n",
      "14    0.872800    0.868400    0.861500    0.851500    0.849800    0.843200  \n",
      "15    0.772877    0.774412    0.762206    0.731343    0.737647    0.718905  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_rf_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_rf_9_cat = np.where(((y_pred_rf_9 >= 2) | (y_pred_rf_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_rf_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_rf_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_rf_9_cat)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.6946\n",
      "\tBest params:\n",
      "\t\tn_estimators: 609\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.689422</td>\n",
       "      <td>0.074949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>17.300000</td>\n",
       "      <td>2.406011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>98.600000</td>\n",
       "      <td>1.173788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>1.100505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>2.884826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.865576</td>\n",
       "      <td>0.022493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.901852</td>\n",
       "      <td>0.051374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.518983</td>\n",
       "      <td>0.078566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.981110</td>\n",
       "      <td>0.010904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.655728</td>\n",
       "      <td>0.068184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.851272</td>\n",
       "      <td>0.027828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.786085</td>\n",
       "      <td>0.040672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.750046</td>\n",
       "      <td>0.039611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.616224</td>\n",
       "      <td>0.067803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.860060</td>\n",
       "      <td>0.022127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.750046</td>\n",
       "      <td>0.039611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.689422     0.074949\n",
       "1                    TP        17.300000     2.406011\n",
       "2                    TN        98.600000     1.173788\n",
       "3                    FP         1.900000     1.100505\n",
       "4                    FN        16.100000     2.884826\n",
       "5              Accuracy         0.865576     0.022493\n",
       "6             Precision         0.901852     0.051374\n",
       "7           Sensitivity         0.518983     0.078566\n",
       "8           Specificity         0.981110     0.010904\n",
       "9              F1 score         0.655728     0.068184\n",
       "10  F1 score (weighted)         0.851272     0.027828\n",
       "11     F1 score (macro)         0.786085     0.040672\n",
       "12    Balanced Accuracy         0.750046     0.039611\n",
       "13                  MCC         0.616224     0.067803\n",
       "14                  NPV         0.860060     0.022127\n",
       "15              ROC_AUC         0.750046     0.039611"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.685238</td>\n",
       "      <td>0.711248</td>\n",
       "      <td>0.698779</td>\n",
       "      <td>0.683476</td>\n",
       "      <td>0.682063</td>\n",
       "      <td>0.698584</td>\n",
       "      <td>0.735695</td>\n",
       "      <td>0.615020</td>\n",
       "      <td>0.640357</td>\n",
       "      <td>0.673751</td>\n",
       "      <td>0.682421</td>\n",
       "      <td>0.034310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>33.900000</td>\n",
       "      <td>3.541814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>197.800000</td>\n",
       "      <td>1.549193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>1.418136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.600000</td>\n",
       "      <td>3.134042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.847015</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.854478</td>\n",
       "      <td>0.864552</td>\n",
       "      <td>0.012932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.926692</td>\n",
       "      <td>0.035334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.550725</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.501954</td>\n",
       "      <td>0.049458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990100</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.986520</td>\n",
       "      <td>0.007063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.641509</td>\n",
       "      <td>0.697248</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.640777</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.649880</td>\n",
       "      <td>0.045258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.823641</td>\n",
       "      <td>0.841646</td>\n",
       "      <td>0.843090</td>\n",
       "      <td>0.864667</td>\n",
       "      <td>0.869563</td>\n",
       "      <td>0.869025</td>\n",
       "      <td>0.863117</td>\n",
       "      <td>0.835893</td>\n",
       "      <td>0.845085</td>\n",
       "      <td>0.834582</td>\n",
       "      <td>0.849031</td>\n",
       "      <td>0.016315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.737725</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>0.776569</td>\n",
       "      <td>0.809982</td>\n",
       "      <td>0.811847</td>\n",
       "      <td>0.814469</td>\n",
       "      <td>0.804574</td>\n",
       "      <td>0.764809</td>\n",
       "      <td>0.777663</td>\n",
       "      <td>0.758408</td>\n",
       "      <td>0.782937</td>\n",
       "      <td>0.026172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.699595</td>\n",
       "      <td>0.735147</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.770337</td>\n",
       "      <td>0.772877</td>\n",
       "      <td>0.774412</td>\n",
       "      <td>0.762206</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.737647</td>\n",
       "      <td>0.718905</td>\n",
       "      <td>0.744247</td>\n",
       "      <td>0.025121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.553642</td>\n",
       "      <td>0.600128</td>\n",
       "      <td>0.598763</td>\n",
       "      <td>0.663421</td>\n",
       "      <td>0.659854</td>\n",
       "      <td>0.670201</td>\n",
       "      <td>0.661508</td>\n",
       "      <td>0.568162</td>\n",
       "      <td>0.613790</td>\n",
       "      <td>0.584643</td>\n",
       "      <td>0.617411</td>\n",
       "      <td>0.043301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.836800</td>\n",
       "      <td>0.849100</td>\n",
       "      <td>0.852200</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.872800</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>0.851500</td>\n",
       "      <td>0.849800</td>\n",
       "      <td>0.843200</td>\n",
       "      <td>0.854930</td>\n",
       "      <td>0.011412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.699595</td>\n",
       "      <td>0.735147</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.770337</td>\n",
       "      <td>0.772877</td>\n",
       "      <td>0.774412</td>\n",
       "      <td>0.762206</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.737647</td>\n",
       "      <td>0.718905</td>\n",
       "      <td>0.744247</td>\n",
       "      <td>0.025121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.685238    0.711248    0.698779    0.683476   \n",
       "1                    TP   27.000000   33.000000   34.000000   38.000000   \n",
       "2                    TN  200.000000  197.000000  196.000000  197.000000   \n",
       "3                    FP    2.000000    3.000000    4.000000    2.000000   \n",
       "4                    FN   39.000000   35.000000   34.000000   31.000000   \n",
       "5              Accuracy    0.847015    0.858209    0.858209    0.876866   \n",
       "6             Precision    0.931034    0.916667    0.894737    0.950000   \n",
       "7           Sensitivity    0.409091    0.485294    0.500000    0.550725   \n",
       "8           Specificity    0.990100    0.985000    0.980000    0.989900   \n",
       "9              F1 score    0.568421    0.634615    0.641509    0.697248   \n",
       "10  F1 score (weighted)    0.823641    0.841646    0.843090    0.864667   \n",
       "11     F1 score (macro)    0.737725    0.773326    0.776569    0.809982   \n",
       "12    Balanced Accuracy    0.699595    0.735147    0.740000    0.770337   \n",
       "13                  MCC    0.553642    0.600128    0.598763    0.663421   \n",
       "14                  NPV    0.836800    0.849100    0.852200    0.864000   \n",
       "15              ROC_AUC    0.699595    0.735147    0.740000    0.770337   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.682063    0.698584    0.735695    0.615020    0.640357    0.673751   \n",
       "1    37.000000   38.000000   36.000000   33.000000   33.000000   30.000000   \n",
       "2   199.000000  198.000000  199.000000  195.000000  198.000000  199.000000   \n",
       "3     3.000000    2.000000    1.000000    6.000000    2.000000    2.000000   \n",
       "4    29.000000   30.000000   32.000000   34.000000   35.000000   37.000000   \n",
       "5     0.880597    0.880597    0.876866    0.850746    0.861940    0.854478   \n",
       "6     0.925000    0.950000    0.972973    0.846154    0.942857    0.937500   \n",
       "7     0.560606    0.558824    0.529412    0.492537    0.485294    0.447761   \n",
       "8     0.985100    0.990000    0.995000    0.970100    0.990000    0.990000   \n",
       "9     0.698113    0.703704    0.685714    0.622642    0.640777    0.606061   \n",
       "10    0.869563    0.869025    0.863117    0.835893    0.845085    0.834582   \n",
       "11    0.811847    0.814469    0.804574    0.764809    0.777663    0.758408   \n",
       "12    0.772877    0.774412    0.762206    0.731343    0.737647    0.718905   \n",
       "13    0.659854    0.670201    0.661508    0.568162    0.613790    0.584643   \n",
       "14    0.872800    0.868400    0.861500    0.851500    0.849800    0.843200   \n",
       "15    0.772877    0.774412    0.762206    0.731343    0.737647    0.718905   \n",
       "\n",
       "           ave       std  \n",
       "0     0.682421  0.034310  \n",
       "1    33.900000  3.541814  \n",
       "2   197.800000  1.549193  \n",
       "3     2.700000  1.418136  \n",
       "4    33.600000  3.134042  \n",
       "5     0.864552  0.012932  \n",
       "6     0.926692  0.035334  \n",
       "7     0.501954  0.049458  \n",
       "8     0.986520  0.007063  \n",
       "9     0.649880  0.045258  \n",
       "10    0.849031  0.016315  \n",
       "11    0.782937  0.026172  \n",
       "12    0.744247  0.025121  \n",
       "13    0.617411  0.043301  \n",
       "14    0.854930  0.011412  \n",
       "15    0.744247  0.025121  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.702569</td>\n",
       "      <td>0.055436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.870351</td>\n",
       "      <td>0.023735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.923268</td>\n",
       "      <td>0.050407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.523376</td>\n",
       "      <td>0.093296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.985484</td>\n",
       "      <td>0.009427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.663628</td>\n",
       "      <td>0.078671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.855867</td>\n",
       "      <td>0.029471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.791607</td>\n",
       "      <td>0.046016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.754429</td>\n",
       "      <td>0.046836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.630638</td>\n",
       "      <td>0.075878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.862270</td>\n",
       "      <td>0.023236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.754429</td>\n",
       "      <td>0.046836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.702569     0.055436\n",
       "1              Accuracy         0.870351     0.023735\n",
       "2             Precision         0.923268     0.050407\n",
       "3           Sensitivity         0.523376     0.093296\n",
       "4           Specificity         0.985484     0.009427\n",
       "5              F1 score         0.663628     0.078671\n",
       "6   F1 score (weighted)         0.855867     0.029471\n",
       "7      F1 score (macro)         0.791607     0.046016\n",
       "8     Balanced Accuracy         0.754429     0.046836\n",
       "9                   MCC         0.630638     0.075878\n",
       "10                  NPV         0.862270     0.023236\n",
       "11              ROC_AUC         0.754429     0.046836"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestRegressor(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=4, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_rf_cat = np.where(((y_pred_optimized_rf >= 2) | (y_pred_optimized_rf <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_rf_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_rf))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_rf_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_rf_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_rf_cat))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "rf_run0 = data_rf[['y_test_idx0', 'y_test0', 'y_pred_rf0']]\n",
    "rf_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "rf_run0.reset_index(inplace=True, drop=True)\n",
    "rf_run1 = data_rf[['y_test_idx1', 'y_test1', 'y_pred_rf1']]\n",
    "rf_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "rf_run1.reset_index(inplace=True, drop=True)\n",
    "rf_run2 = data_rf[['y_test_idx2', 'y_test2', 'y_pred_rf2']]\n",
    "rf_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "rf_run2.reset_index(inplace=True, drop=True)\n",
    "rf_run3 = data_rf[['y_test_idx3', 'y_test3', 'y_pred_rf3']]\n",
    "rf_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "rf_run3.reset_index(inplace=True, drop=True)\n",
    "rf_run4 = data_rf[['y_test_idx4', 'y_test4', 'y_pred_rf4']]\n",
    "rf_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "rf_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "rf_5preds = pd.concat([chembl_id, rf_run0, rf_run1, rf_run2, rf_run3, rf_run4], axis=1)\n",
    "rf_5preds = rf_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_rf0', 'y_pred_rf1', 'y_pred_rf2', 'y_pred_rf3', 'y_pred_rf4']]\n",
    "rf_5preds['y_pred_rf_ave'] = rf_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "rf_5preds['y_pred_rf_std'] = rf_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfc78124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (RF)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(rf_5preds['y_test0'], rf_5preds['y_pred_rf_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model r2_score 0.7009 with a standard deviation of 0.0764\n",
      "RF optimized model r2_score 0.7014 with a standard deviation of 0.0709\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"RF baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_reg, \"OUTPUT/rf_reg.joblib\")\n",
    "#joblib.dump(optimized_rf, \"OUTPUT/optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.680184     0.062393\n",
      "1                    TP        18.600000     2.756810\n",
      "2                    TN        98.500000     1.354006\n",
      "3                    FP         2.000000     1.154701\n",
      "4                    FN        14.800000     3.293090\n",
      "5              Accuracy         0.874554     0.027573\n",
      "6             Precision         0.902831     0.053697\n",
      "7           Sensitivity         0.558220     0.091265\n",
      "8           Specificity         0.980100     0.011452\n",
      "9              F1 score         0.686608     0.077634\n",
      "10  F1 score (weighted)         0.862746     0.032640\n",
      "11     F1 score (macro)         0.804073     0.047063\n",
      "12    Balanced Accuracy         0.769159     0.047279\n",
      "13                  MCC         0.643892     0.081040\n",
      "14                  NPV         0.869900     0.026197\n",
      "15              ROC_AUC         0.769159     0.047279\n",
      "CPU times: user 6.68 s, sys: 84 ms, total: 6.77 s\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_reg = lgbm.LGBMRegressor(\n",
    "        objective=\"regression\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=4,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_reg.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"rmse\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_reg.predict(X_test) \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=4,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=4,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "         # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 06:53:35,389] A new study created in memory with name: lgbmRegressor\n",
      "[I 2023-12-20 06:53:36,941] Trial 0 finished with value: 0.6734305039855422 and parameters: {'n_estimators': 342, 'learning_rate': 0.1427494247912503, 'max_depth': 9, 'max_bin': 298, 'num_leaves': 557}. Best is trial 0 with value: 0.6734305039855422.\n",
      "[I 2023-12-20 06:53:38,171] Trial 1 finished with value: 0.6675669632972875 and parameters: {'n_estimators': 134, 'learning_rate': 0.11232944732508023, 'max_depth': 8, 'max_bin': 199, 'num_leaves': 224}. Best is trial 0 with value: 0.6734305039855422.\n",
      "[I 2023-12-20 06:53:40,052] Trial 2 finished with value: 0.6711336450284485 and parameters: {'n_estimators': 540, 'learning_rate': 0.08619065143833218, 'max_depth': 8, 'max_bin': 295, 'num_leaves': 267}. Best is trial 0 with value: 0.6734305039855422.\n",
      "[I 2023-12-20 06:53:41,141] Trial 3 finished with value: 0.6573893745063785 and parameters: {'n_estimators': 105, 'learning_rate': 0.07160765992923751, 'max_depth': 8, 'max_bin': 258, 'num_leaves': 597}. Best is trial 0 with value: 0.6734305039855422.\n",
      "[I 2023-12-20 06:53:42,656] Trial 4 finished with value: 0.6825627854753764 and parameters: {'n_estimators': 377, 'learning_rate': 0.15404745651877808, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 344}. Best is trial 4 with value: 0.6825627854753764.\n",
      "[I 2023-12-20 06:53:44,241] Trial 5 finished with value: 0.6330588986844315 and parameters: {'n_estimators': 322, 'learning_rate': 0.01855030727333373, 'max_depth': 5, 'max_bin': 158, 'num_leaves': 190}. Best is trial 4 with value: 0.6825627854753764.\n",
      "[I 2023-12-20 06:53:45,092] Trial 6 finished with value: 0.6566106961673045 and parameters: {'n_estimators': 194, 'learning_rate': 0.1300236107146907, 'max_depth': 4, 'max_bin': 244, 'num_leaves': 595}. Best is trial 4 with value: 0.6825627854753764.\n",
      "[I 2023-12-20 06:53:46,436] Trial 7 finished with value: 0.6893429214347069 and parameters: {'n_estimators': 308, 'learning_rate': 0.18451558747223668, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 670}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:47,607] Trial 8 finished with value: 0.6810803307807051 and parameters: {'n_estimators': 464, 'learning_rate': 0.17196366616350345, 'max_depth': 8, 'max_bin': 225, 'num_leaves': 325}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:48,864] Trial 9 finished with value: 0.6849616993427088 and parameters: {'n_estimators': 655, 'learning_rate': 0.18444235951634266, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 423}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:50,251] Trial 10 finished with value: 0.6763943620231592 and parameters: {'n_estimators': 802, 'learning_rate': 0.19481664060928272, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 731}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:51,621] Trial 11 finished with value: 0.6721165855169475 and parameters: {'n_estimators': 651, 'learning_rate': 0.19910328198781835, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 464}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:53,052] Trial 12 finished with value: 0.6813337697809007 and parameters: {'n_estimators': 719, 'learning_rate': 0.1674614097878997, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 64}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:54,131] Trial 13 finished with value: 0.6675687544864416 and parameters: {'n_estimators': 864, 'learning_rate': 0.17761658622477572, 'max_depth': 6, 'max_bin': 265, 'num_leaves': 746}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:55,669] Trial 14 finished with value: 0.6784670635649906 and parameters: {'n_estimators': 585, 'learning_rate': 0.1361966101416987, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 462}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:57,181] Trial 15 finished with value: 0.6761910409212863 and parameters: {'n_estimators': 255, 'learning_rate': 0.18987463251227418, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 452}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:58,776] Trial 16 finished with value: 0.6827898019458452 and parameters: {'n_estimators': 478, 'learning_rate': 0.1595842174111791, 'max_depth': 10, 'max_bin': 280, 'num_leaves': 629}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:53:59,612] Trial 17 finished with value: 0.6524185621651432 and parameters: {'n_estimators': 721, 'learning_rate': 0.19901847901268932, 'max_depth': 3, 'max_bin': 191, 'num_leaves': 126}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:01,145] Trial 18 finished with value: 0.6694075805716994 and parameters: {'n_estimators': 420, 'learning_rate': 0.1725798932906366, 'max_depth': 11, 'max_bin': 244, 'num_leaves': 659}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:02,385] Trial 19 finished with value: 0.6740251689929888 and parameters: {'n_estimators': 580, 'learning_rate': 0.1235641673123224, 'max_depth': 6, 'max_bin': 279, 'num_leaves': 531}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:04,083] Trial 20 finished with value: 0.6866139186938247 and parameters: {'n_estimators': 242, 'learning_rate': 0.15108593460261419, 'max_depth': 11, 'max_bin': 172, 'num_leaves': 390}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:05,516] Trial 21 finished with value: 0.6778206833186238 and parameters: {'n_estimators': 220, 'learning_rate': 0.1571639683644829, 'max_depth': 11, 'max_bin': 151, 'num_leaves': 397}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:07,098] Trial 22 finished with value: 0.687246439019232 and parameters: {'n_estimators': 289, 'learning_rate': 0.18214561010372998, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 399}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:07,860] Trial 23 finished with value: 0.6645684243234495 and parameters: {'n_estimators': 60, 'learning_rate': 0.1490485461687574, 'max_depth': 11, 'max_bin': 173, 'num_leaves': 321}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:09,161] Trial 24 finished with value: 0.677209904224481 and parameters: {'n_estimators': 281, 'learning_rate': 0.17552550895644958, 'max_depth': 9, 'max_bin': 217, 'num_leaves': 527}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:10,566] Trial 25 finished with value: 0.6837229397611825 and parameters: {'n_estimators': 168, 'learning_rate': 0.14623058247546006, 'max_depth': 12, 'max_bin': 173, 'num_leaves': 258}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:11,780] Trial 26 finished with value: 0.6641285058555959 and parameters: {'n_estimators': 284, 'learning_rate': 0.18329229000394925, 'max_depth': 10, 'max_bin': 253, 'num_leaves': 677}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:13,098] Trial 27 finished with value: 0.6815920197441632 and parameters: {'n_estimators': 377, 'learning_rate': 0.1568911235161486, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 371}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:14,328] Trial 28 finished with value: 0.6761560764669813 and parameters: {'n_estimators': 225, 'learning_rate': 0.16780757175666552, 'max_depth': 9, 'max_bin': 224, 'num_leaves': 152}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:15,407] Trial 29 finished with value: 0.6660372531698804 and parameters: {'n_estimators': 355, 'learning_rate': 0.13652794797910178, 'max_depth': 7, 'max_bin': 272, 'num_leaves': 509}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:16,758] Trial 30 finished with value: 0.6789073570971704 and parameters: {'n_estimators': 308, 'learning_rate': 0.1856835986059139, 'max_depth': 12, 'max_bin': 193, 'num_leaves': 287}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:18,084] Trial 31 finished with value: 0.6759644584719611 and parameters: {'n_estimators': 455, 'learning_rate': 0.18335380935382725, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 408}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:19,203] Trial 32 finished with value: 0.6746172604521002 and parameters: {'n_estimators': 147, 'learning_rate': 0.18680064317402034, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 421}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:20,290] Trial 33 finished with value: 0.6713799426877336 and parameters: {'n_estimators': 97, 'learning_rate': 0.16286848190473305, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 576}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:21,546] Trial 34 finished with value: 0.6813335785813885 and parameters: {'n_estimators': 507, 'learning_rate': 0.19977084576875032, 'max_depth': 10, 'max_bin': 295, 'num_leaves': 363}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:23,070] Trial 35 finished with value: 0.677539399218118 and parameters: {'n_estimators': 381, 'learning_rate': 0.14544677026739442, 'max_depth': 11, 'max_bin': 256, 'num_leaves': 495}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:24,795] Trial 36 finished with value: 0.6830411426947455 and parameters: {'n_estimators': 426, 'learning_rate': 0.11419139199238224, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 300}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:26,052] Trial 37 finished with value: 0.6824757886206188 and parameters: {'n_estimators': 258, 'learning_rate': 0.1787790916819927, 'max_depth': 9, 'max_bin': 217, 'num_leaves': 260}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:27,351] Trial 38 finished with value: 0.6802300227196056 and parameters: {'n_estimators': 557, 'learning_rate': 0.16530949268943806, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 210}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:28,763] Trial 39 finished with value: 0.6741673982685759 and parameters: {'n_estimators': 327, 'learning_rate': 0.18985763113392012, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 435}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:30,471] Trial 40 finished with value: 0.6780816078309589 and parameters: {'n_estimators': 669, 'learning_rate': 0.09409831747819482, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 552}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:31,939] Trial 41 finished with value: 0.674613550710023 and parameters: {'n_estimators': 166, 'learning_rate': 0.1486480620707051, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 247}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:33,229] Trial 42 finished with value: 0.67914818374666 and parameters: {'n_estimators': 199, 'learning_rate': 0.14205649679792234, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 42}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:34,341] Trial 43 finished with value: 0.6743136033718774 and parameters: {'n_estimators': 119, 'learning_rate': 0.17086358242160876, 'max_depth': 11, 'max_bin': 184, 'num_leaves': 355}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:35,546] Trial 44 finished with value: 0.6798389958961354 and parameters: {'n_estimators': 195, 'learning_rate': 0.15382451300869657, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 699}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:36,449] Trial 45 finished with value: 0.6805780335334572 and parameters: {'n_estimators': 163, 'learning_rate': 0.1628174926889419, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 149}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:37,593] Trial 46 finished with value: 0.6741164022535517 and parameters: {'n_estimators': 244, 'learning_rate': 0.17904512323560232, 'max_depth': 11, 'max_bin': 178, 'num_leaves': 627}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:38,191] Trial 47 finished with value: 0.6541497627034503 and parameters: {'n_estimators': 53, 'learning_rate': 0.19074157647966283, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 487}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:39,326] Trial 48 finished with value: 0.6698047139883696 and parameters: {'n_estimators': 808, 'learning_rate': 0.12860100524503235, 'max_depth': 8, 'max_bin': 260, 'num_leaves': 307}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:40,387] Trial 49 finished with value: 0.685142621022633 and parameters: {'n_estimators': 285, 'learning_rate': 0.17253495828004808, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 218}. Best is trial 7 with value: 0.6893429214347069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6893\n",
      "\tBest params:\n",
      "\t\tn_estimators: 308\n",
      "\t\tlearning_rate: 0.18451558747223668\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 248\n",
      "\t\tnum_leaves: 670\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.708945\n",
      "1                    TP   34.000000\n",
      "2                    TN  201.000000\n",
      "3                    FP    1.000000\n",
      "4                    FN   32.000000\n",
      "5              Accuracy    0.876866\n",
      "6             Precision    0.971429\n",
      "7           Sensitivity    0.515152\n",
      "8           Specificity    0.995000\n",
      "9              F1 score    0.673267\n",
      "10  F1 score (weighted)    0.862356\n",
      "11     F1 score (macro)    0.798703\n",
      "12    Balanced Accuracy    0.755101\n",
      "13                  MCC    0.652344\n",
      "14                  NPV    0.862700\n",
      "15              ROC_AUC    0.755101\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_lgbm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_0_cat = np.where(((y_pred_lgbm_0 >= 2) | (y_pred_lgbm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_lgbm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_lgbm_0_cat)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 06:54:41,302] Trial 50 finished with value: 0.6438247368040639 and parameters: {'n_estimators': 407, 'learning_rate': 0.17435532434802736, 'max_depth': 4, 'max_bin': 251, 'num_leaves': 94}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:42,417] Trial 51 finished with value: 0.6744248657321634 and parameters: {'n_estimators': 318, 'learning_rate': 0.1930800282943822, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 210}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:43,526] Trial 52 finished with value: 0.6702450985897658 and parameters: {'n_estimators': 301, 'learning_rate': 0.17051807936351754, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 182}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:44,582] Trial 53 finished with value: 0.6623906258287502 and parameters: {'n_estimators': 351, 'learning_rate': 0.17885589081001474, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 235}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:45,641] Trial 54 finished with value: 0.6663425882354014 and parameters: {'n_estimators': 625, 'learning_rate': 0.15482370085293087, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 280}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:46,804] Trial 55 finished with value: 0.6634603562054906 and parameters: {'n_estimators': 895, 'learning_rate': 0.19980908573661685, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 381}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:48,028] Trial 56 finished with value: 0.668668031154112 and parameters: {'n_estimators': 275, 'learning_rate': 0.16830135295973234, 'max_depth': 12, 'max_bin': 248, 'num_leaves': 443}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:49,181] Trial 57 finished with value: 0.6741644890378119 and parameters: {'n_estimators': 233, 'learning_rate': 0.18453092929324721, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 349}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:50,312] Trial 58 finished with value: 0.668556990007162 and parameters: {'n_estimators': 182, 'learning_rate': 0.16094670906445618, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 177}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:51,254] Trial 59 finished with value: 0.6589212918436897 and parameters: {'n_estimators': 730, 'learning_rate': 0.19417752576040948, 'max_depth': 7, 'max_bin': 186, 'num_leaves': 329}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:52,117] Trial 60 finished with value: 0.6608234004625255 and parameters: {'n_estimators': 107, 'learning_rate': 0.15353129654300604, 'max_depth': 10, 'max_bin': 169, 'num_leaves': 474}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:53,515] Trial 61 finished with value: 0.6732083451327935 and parameters: {'n_estimators': 417, 'learning_rate': 0.11470824643938268, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 289}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:54,797] Trial 62 finished with value: 0.6710064613172508 and parameters: {'n_estimators': 348, 'learning_rate': 0.1351868073140409, 'max_depth': 11, 'max_bin': 234, 'num_leaves': 314}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:56,086] Trial 63 finished with value: 0.668919027472082 and parameters: {'n_estimators': 443, 'learning_rate': 0.1430001545140394, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 395}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:57,233] Trial 64 finished with value: 0.6629410068995417 and parameters: {'n_estimators': 490, 'learning_rate': 0.1758159299103426, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 230}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:54:58,652] Trial 65 finished with value: 0.6632039972224978 and parameters: {'n_estimators': 513, 'learning_rate': 0.11886948691211463, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 333}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:00,070] Trial 66 finished with value: 0.6659126980976491 and parameters: {'n_estimators': 391, 'learning_rate': 0.10219842669357838, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 273}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:01,097] Trial 67 finished with value: 0.6638943552150918 and parameters: {'n_estimators': 214, 'learning_rate': 0.16660412236543937, 'max_depth': 11, 'max_bin': 249, 'num_leaves': 418}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:02,149] Trial 68 finished with value: 0.6679419349335646 and parameters: {'n_estimators': 258, 'learning_rate': 0.1847642992063858, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 256}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:03,345] Trial 69 finished with value: 0.669913659096177 and parameters: {'n_estimators': 291, 'learning_rate': 0.15001096900018565, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 299}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:04,313] Trial 70 finished with value: 0.6714862885462972 and parameters: {'n_estimators': 140, 'learning_rate': 0.15809327911979584, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 204}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:05,518] Trial 71 finished with value: 0.6701510604958686 and parameters: {'n_estimators': 536, 'learning_rate': 0.16080965392357693, 'max_depth': 10, 'max_bin': 284, 'num_leaves': 641}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:06,990] Trial 72 finished with value: 0.6733478718776675 and parameters: {'n_estimators': 441, 'learning_rate': 0.1725383606270315, 'max_depth': 11, 'max_bin': 293, 'num_leaves': 724}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:08,249] Trial 73 finished with value: 0.6696888578990683 and parameters: {'n_estimators': 373, 'learning_rate': 0.17792668923434452, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 593}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:09,398] Trial 74 finished with value: 0.6650734471660208 and parameters: {'n_estimators': 678, 'learning_rate': 0.1654812145575098, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 680}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:10,627] Trial 75 finished with value: 0.6634451825200074 and parameters: {'n_estimators': 613, 'learning_rate': 0.13831697681224547, 'max_depth': 12, 'max_bin': 177, 'num_leaves': 626}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:11,189] Trial 76 finished with value: 0.6416900939454843 and parameters: {'n_estimators': 79, 'learning_rate': 0.18146412317767147, 'max_depth': 6, 'max_bin': 270, 'num_leaves': 541}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:12,198] Trial 77 finished with value: 0.6591499711981876 and parameters: {'n_estimators': 332, 'learning_rate': 0.19034604208897105, 'max_depth': 10, 'max_bin': 258, 'num_leaves': 373}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:13,270] Trial 78 finished with value: 0.6621374136059737 and parameters: {'n_estimators': 274, 'learning_rate': 0.1474472116298868, 'max_depth': 9, 'max_bin': 299, 'num_leaves': 710}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:14,560] Trial 79 finished with value: 0.661597774246675 and parameters: {'n_estimators': 471, 'learning_rate': 0.1306363150176318, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 580}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:15,897] Trial 80 finished with value: 0.6676615291386039 and parameters: {'n_estimators': 771, 'learning_rate': 0.1590028686396523, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 748}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:17,195] Trial 81 finished with value: 0.6694909555288941 and parameters: {'n_estimators': 402, 'learning_rate': 0.15338874047115778, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 350}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:18,360] Trial 82 finished with value: 0.670113318491072 and parameters: {'n_estimators': 365, 'learning_rate': 0.17305488471049693, 'max_depth': 12, 'max_bin': 246, 'num_leaves': 430}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:19,694] Trial 83 finished with value: 0.6659531391713689 and parameters: {'n_estimators': 316, 'learning_rate': 0.14327670710547982, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 399}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:20,859] Trial 84 finished with value: 0.6697794516940863 and parameters: {'n_estimators': 241, 'learning_rate': 0.16275806306289722, 'max_depth': 11, 'max_bin': 220, 'num_leaves': 337}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:22,092] Trial 85 finished with value: 0.668503464658498 and parameters: {'n_estimators': 339, 'learning_rate': 0.18183816148945772, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 668}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:23,206] Trial 86 finished with value: 0.6591094569008895 and parameters: {'n_estimators': 210, 'learning_rate': 0.1880403188474228, 'max_depth': 11, 'max_bin': 239, 'num_leaves': 455}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:24,416] Trial 87 finished with value: 0.6660954250843566 and parameters: {'n_estimators': 299, 'learning_rate': 0.14937008027978926, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 510}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:25,924] Trial 88 finished with value: 0.6678747711760147 and parameters: {'n_estimators': 174, 'learning_rate': 0.16805314920687558, 'max_depth': 12, 'max_bin': 254, 'num_leaves': 149}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:27,248] Trial 89 finished with value: 0.6657182550237446 and parameters: {'n_estimators': 269, 'learning_rate': 0.19393939349354491, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 245}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:28,599] Trial 90 finished with value: 0.6725029024380811 and parameters: {'n_estimators': 428, 'learning_rate': 0.1743723025960131, 'max_depth': 10, 'max_bin': 243, 'num_leaves': 265}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:29,937] Trial 91 finished with value: 0.6603847842692412 and parameters: {'n_estimators': 254, 'learning_rate': 0.17995151691745193, 'max_depth': 9, 'max_bin': 227, 'num_leaves': 386}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:31,258] Trial 92 finished with value: 0.6697001684004565 and parameters: {'n_estimators': 316, 'learning_rate': 0.18591919599588108, 'max_depth': 9, 'max_bin': 193, 'num_leaves': 220}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:32,712] Trial 93 finished with value: 0.6607365970666655 and parameters: {'n_estimators': 230, 'learning_rate': 0.1690769279677247, 'max_depth': 10, 'max_bin': 154, 'num_leaves': 304}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:33,773] Trial 94 finished with value: 0.6610812795931704 and parameters: {'n_estimators': 193, 'learning_rate': 0.17686006190774436, 'max_depth': 8, 'max_bin': 261, 'num_leaves': 171}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:35,298] Trial 95 finished with value: 0.665262278649773 and parameters: {'n_estimators': 390, 'learning_rate': 0.1583795068626258, 'max_depth': 12, 'max_bin': 198, 'num_leaves': 196}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:36,680] Trial 96 finished with value: 0.6706883298305226 and parameters: {'n_estimators': 352, 'learning_rate': 0.1648596439842493, 'max_depth': 11, 'max_bin': 232, 'num_leaves': 264}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:38,082] Trial 97 finished with value: 0.6610330484455543 and parameters: {'n_estimators': 284, 'learning_rate': 0.19762499969366132, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 295}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:38,818] Trial 98 finished with value: 0.6304042070269608 and parameters: {'n_estimators': 260, 'learning_rate': 0.15315350000196218, 'max_depth': 3, 'max_bin': 220, 'num_leaves': 316}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:40,121] Trial 99 finished with value: 0.6639729177052855 and parameters: {'n_estimators': 156, 'learning_rate': 0.17211908314245702, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 242}. Best is trial 7 with value: 0.6893429214347069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6893\n",
      "\tBest params:\n",
      "\t\tn_estimators: 308\n",
      "\t\tlearning_rate: 0.18451558747223668\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 248\n",
      "\t\tnum_leaves: 670\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.708945    0.673766\n",
      "1                    TP   34.000000   33.000000\n",
      "2                    TN  201.000000  194.000000\n",
      "3                    FP    1.000000    6.000000\n",
      "4                    FN   32.000000   35.000000\n",
      "5              Accuracy    0.876866    0.847015\n",
      "6             Precision    0.971429    0.846154\n",
      "7           Sensitivity    0.515152    0.485294\n",
      "8           Specificity    0.995000    0.970000\n",
      "9              F1 score    0.673267    0.616822\n",
      "10  F1 score (weighted)    0.862356    0.831454\n",
      "11     F1 score (macro)    0.798703    0.760626\n",
      "12    Balanced Accuracy    0.755101    0.727647\n",
      "13                  MCC    0.652344    0.561838\n",
      "14                  NPV    0.862700    0.847200\n",
      "15              ROC_AUC    0.755101    0.727647\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_lgbm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_1_cat = np.where(((y_pred_lgbm_1 >= 2) | (y_pred_lgbm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_lgbm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_lgbm_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 06:55:41,887] Trial 100 finished with value: 0.6727781048046029 and parameters: {'n_estimators': 569, 'learning_rate': 0.18858329986849878, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 120}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:43,301] Trial 101 finished with value: 0.6754858004712467 and parameters: {'n_estimators': 365, 'learning_rate': 0.1561278007080272, 'max_depth': 10, 'max_bin': 184, 'num_leaves': 368}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:44,832] Trial 102 finished with value: 0.6738684377473042 and parameters: {'n_estimators': 456, 'learning_rate': 0.16277610011180804, 'max_depth': 12, 'max_bin': 174, 'num_leaves': 412}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:46,340] Trial 103 finished with value: 0.6772091624570119 and parameters: {'n_estimators': 300, 'learning_rate': 0.14548735743936436, 'max_depth': 9, 'max_bin': 206, 'num_leaves': 340}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:47,725] Trial 104 finished with value: 0.6771250540140836 and parameters: {'n_estimators': 329, 'learning_rate': 0.1823628010984012, 'max_depth': 11, 'max_bin': 225, 'num_leaves': 365}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:49,777] Trial 105 finished with value: 0.6772999608349624 and parameters: {'n_estimators': 493, 'learning_rate': 0.13951892243822792, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 644}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:51,199] Trial 106 finished with value: 0.6701992528530867 and parameters: {'n_estimators': 429, 'learning_rate': 0.16975432982923166, 'max_depth': 8, 'max_bin': 165, 'num_leaves': 693}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:52,426] Trial 107 finished with value: 0.6777312256476964 and parameters: {'n_estimators': 126, 'learning_rate': 0.1787327492529594, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 283}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:54,023] Trial 108 finished with value: 0.6814573166375507 and parameters: {'n_estimators': 379, 'learning_rate': 0.157878064785383, 'max_depth': 11, 'max_bin': 189, 'num_leaves': 223}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:55,425] Trial 109 finished with value: 0.6784706159655991 and parameters: {'n_estimators': 413, 'learning_rate': 0.1515551181185504, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 324}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:56,846] Trial 110 finished with value: 0.6835444607494306 and parameters: {'n_estimators': 820, 'learning_rate': 0.1756731523749619, 'max_depth': 10, 'max_bin': 277, 'num_leaves': 400}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:57,961] Trial 111 finished with value: 0.6641101284949682 and parameters: {'n_estimators': 830, 'learning_rate': 0.17574592390709246, 'max_depth': 7, 'max_bin': 281, 'num_leaves': 383}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:55:59,681] Trial 112 finished with value: 0.6835358311859173 and parameters: {'n_estimators': 755, 'learning_rate': 0.16629994623811403, 'max_depth': 10, 'max_bin': 287, 'num_leaves': 443}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:00,888] Trial 113 finished with value: 0.6728501895043407 and parameters: {'n_estimators': 775, 'learning_rate': 0.16416726353256905, 'max_depth': 10, 'max_bin': 284, 'num_leaves': 446}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:02,495] Trial 114 finished with value: 0.6815248057097657 and parameters: {'n_estimators': 847, 'learning_rate': 0.18494032971462163, 'max_depth': 9, 'max_bin': 276, 'num_leaves': 476}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:04,179] Trial 115 finished with value: 0.6783075748335422 and parameters: {'n_estimators': 891, 'learning_rate': 0.17182389935900919, 'max_depth': 10, 'max_bin': 290, 'num_leaves': 427}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:05,804] Trial 116 finished with value: 0.6762998845015145 and parameters: {'n_estimators': 747, 'learning_rate': 0.19209720311997758, 'max_depth': 10, 'max_bin': 286, 'num_leaves': 414}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:07,876] Trial 117 finished with value: 0.6842185450954504 and parameters: {'n_estimators': 678, 'learning_rate': 0.16738366849041514, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 398}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:09,497] Trial 118 finished with value: 0.6706378109357063 and parameters: {'n_estimators': 658, 'learning_rate': 0.16656038554549304, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 404}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:11,100] Trial 119 finished with value: 0.6849631452861625 and parameters: {'n_estimators': 681, 'learning_rate': 0.16052382581505437, 'max_depth': 12, 'max_bin': 266, 'num_leaves': 435}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:12,554] Trial 120 finished with value: 0.677123874684997 and parameters: {'n_estimators': 713, 'learning_rate': 0.161380968188273, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 465}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:14,296] Trial 121 finished with value: 0.6777831923247255 and parameters: {'n_estimators': 635, 'learning_rate': 0.14957471021169033, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 444}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:16,345] Trial 122 finished with value: 0.6881429743624368 and parameters: {'n_estimators': 695, 'learning_rate': 0.1575087365309976, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 436}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:18,263] Trial 123 finished with value: 0.6809921294056311 and parameters: {'n_estimators': 673, 'learning_rate': 0.15567245457054846, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 496}. Best is trial 7 with value: 0.6893429214347069.\n",
      "[I 2023-12-20 06:56:20,015] Trial 124 finished with value: 0.6894670842022781 and parameters: {'n_estimators': 703, 'learning_rate': 0.16975367736859273, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 431}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:21,854] Trial 125 finished with value: 0.682096429795954 and parameters: {'n_estimators': 688, 'learning_rate': 0.17522892230896533, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 435}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:23,457] Trial 126 finished with value: 0.6811489388679703 and parameters: {'n_estimators': 736, 'learning_rate': 0.169367509774628, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 458}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:25,235] Trial 127 finished with value: 0.6812463042749864 and parameters: {'n_estimators': 783, 'learning_rate': 0.1818583802795143, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 401}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:26,872] Trial 128 finished with value: 0.678267044017305 and parameters: {'n_estimators': 714, 'learning_rate': 0.16022009884709726, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 426}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:28,419] Trial 129 finished with value: 0.6741725684135981 and parameters: {'n_estimators': 692, 'learning_rate': 0.16658236895765874, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 485}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:30,089] Trial 130 finished with value: 0.6785376745768905 and parameters: {'n_estimators': 752, 'learning_rate': 0.17602516471076507, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 516}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:31,910] Trial 131 finished with value: 0.6877694683480529 and parameters: {'n_estimators': 602, 'learning_rate': 0.17125531532032306, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 389}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:33,795] Trial 132 finished with value: 0.6827587392499908 and parameters: {'n_estimators': 609, 'learning_rate': 0.1724202466144739, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 378}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:35,347] Trial 133 finished with value: 0.6780039890399602 and parameters: {'n_estimators': 700, 'learning_rate': 0.1870277579238753, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 415}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:36,749] Trial 134 finished with value: 0.6721041843044919 and parameters: {'n_estimators': 641, 'learning_rate': 0.1638571240808277, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 402}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:37,820] Trial 135 finished with value: 0.671259233787572 and parameters: {'n_estimators': 601, 'learning_rate': 0.18004574842958504, 'max_depth': 4, 'max_bin': 270, 'num_leaves': 357}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:38,844] Trial 136 finished with value: 0.671066417454999 and parameters: {'n_estimators': 658, 'learning_rate': 0.16939670129778506, 'max_depth': 5, 'max_bin': 289, 'num_leaves': 389}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:40,968] Trial 137 finished with value: 0.6770597463854193 and parameters: {'n_estimators': 704, 'learning_rate': 0.1454939328606805, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 439}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:42,769] Trial 138 finished with value: 0.680350687247313 and parameters: {'n_estimators': 725, 'learning_rate': 0.15700189158052694, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 464}. Best is trial 124 with value: 0.6894670842022781.\n",
      "[I 2023-12-20 06:56:44,942] Trial 139 finished with value: 0.6933131199103784 and parameters: {'n_estimators': 681, 'learning_rate': 0.17725588980199594, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 424}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:56:46,488] Trial 140 finished with value: 0.6861095497488731 and parameters: {'n_estimators': 817, 'learning_rate': 0.18307673327863094, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 424}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:56:48,267] Trial 141 finished with value: 0.6784406288581358 and parameters: {'n_estimators': 806, 'learning_rate': 0.18256921313451377, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 388}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:56:49,898] Trial 142 finished with value: 0.6772433945872249 and parameters: {'n_estimators': 791, 'learning_rate': 0.17647225925039903, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 423}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:56:51,601] Trial 143 finished with value: 0.6826457663989267 and parameters: {'n_estimators': 828, 'learning_rate': 0.18726977042775578, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 437}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:56:53,424] Trial 144 finished with value: 0.6841861616394735 and parameters: {'n_estimators': 675, 'learning_rate': 0.17915748494155695, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 452}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:56:55,166] Trial 145 finished with value: 0.6799986412418162 and parameters: {'n_estimators': 666, 'learning_rate': 0.1925279823963576, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 416}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:56:57,172] Trial 146 finished with value: 0.6749129876586988 and parameters: {'n_estimators': 643, 'learning_rate': 0.18010108821825624, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 455}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:56:59,102] Trial 147 finished with value: 0.6832202192426363 and parameters: {'n_estimators': 588, 'learning_rate': 0.17262732349654014, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 475}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:00,929] Trial 148 finished with value: 0.6727921371330032 and parameters: {'n_estimators': 620, 'learning_rate': 0.18499803062746423, 'max_depth': 11, 'max_bin': 266, 'num_leaves': 398}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:02,471] Trial 149 finished with value: 0.6761497654772748 and parameters: {'n_estimators': 680, 'learning_rate': 0.18911937028592757, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 374}. Best is trial 139 with value: 0.6933131199103784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6933\n",
      "\tBest params:\n",
      "\t\tn_estimators: 681\n",
      "\t\tlearning_rate: 0.17725588980199594\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 280\n",
      "\t\tnum_leaves: 424\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.708945    0.673766    0.671030\n",
      "1                    TP   34.000000   33.000000   36.000000\n",
      "2                    TN  201.000000  194.000000  197.000000\n",
      "3                    FP    1.000000    6.000000    3.000000\n",
      "4                    FN   32.000000   35.000000   32.000000\n",
      "5              Accuracy    0.876866    0.847015    0.869403\n",
      "6             Precision    0.971429    0.846154    0.923077\n",
      "7           Sensitivity    0.515152    0.485294    0.529412\n",
      "8           Specificity    0.995000    0.970000    0.985000\n",
      "9              F1 score    0.673267    0.616822    0.672897\n",
      "10  F1 score (weighted)    0.862356    0.831454    0.856119\n",
      "11     F1 score (macro)    0.798703    0.760626    0.795656\n",
      "12    Balanced Accuracy    0.755101    0.727647    0.757206\n",
      "13                  MCC    0.652344    0.561838    0.634790\n",
      "14                  NPV    0.862700    0.847200    0.860300\n",
      "15              ROC_AUC    0.755101    0.727647    0.757206\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_lgbm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_2_cat = np.where(((y_pred_lgbm_2 >= 2) | (y_pred_lgbm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_lgbm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_lgbm_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 06:57:04,294] Trial 150 finished with value: 0.653295317297223 and parameters: {'n_estimators': 652, 'learning_rate': 0.1787421830186268, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 417}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:05,810] Trial 151 finished with value: 0.6539290925198226 and parameters: {'n_estimators': 738, 'learning_rate': 0.16828519324125227, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 453}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:07,593] Trial 152 finished with value: 0.6533749367848658 and parameters: {'n_estimators': 760, 'learning_rate': 0.1741920182545224, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 434}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:09,149] Trial 153 finished with value: 0.6464991697364864 and parameters: {'n_estimators': 862, 'learning_rate': 0.18203664089543545, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 491}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:10,547] Trial 154 finished with value: 0.6562431748248393 and parameters: {'n_estimators': 687, 'learning_rate': 0.1651301082488439, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 407}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:11,993] Trial 155 finished with value: 0.6531495335843049 and parameters: {'n_estimators': 711, 'learning_rate': 0.16135960312628186, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 391}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:13,351] Trial 156 finished with value: 0.6559671282273859 and parameters: {'n_estimators': 823, 'learning_rate': 0.17024450032721605, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 564}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:14,647] Trial 157 finished with value: 0.6473503524712331 and parameters: {'n_estimators': 668, 'learning_rate': 0.17657204284315056, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 430}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:15,612] Trial 158 finished with value: 0.646569204847672 and parameters: {'n_estimators': 768, 'learning_rate': 0.19355737889116229, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 448}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:16,684] Trial 159 finished with value: 0.6571177894568312 and parameters: {'n_estimators': 629, 'learning_rate': 0.19756627751390582, 'max_depth': 12, 'max_bin': 178, 'num_leaves': 470}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:17,857] Trial 160 finished with value: 0.6483020839009699 and parameters: {'n_estimators': 724, 'learning_rate': 0.18417887493781243, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 355}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:18,937] Trial 161 finished with value: 0.6467342476944956 and parameters: {'n_estimators': 589, 'learning_rate': 0.17206552107471207, 'max_depth': 12, 'max_bin': 291, 'num_leaves': 419}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:19,998] Trial 162 finished with value: 0.649206715514343 and parameters: {'n_estimators': 525, 'learning_rate': 0.17428020202679917, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 455}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:21,294] Trial 163 finished with value: 0.6599800253369759 and parameters: {'n_estimators': 692, 'learning_rate': 0.16672514194355184, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 482}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:22,477] Trial 164 finished with value: 0.6483543645990295 and parameters: {'n_estimators': 591, 'learning_rate': 0.17859188782479357, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 403}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:23,587] Trial 165 finished with value: 0.6554641743968121 and parameters: {'n_estimators': 652, 'learning_rate': 0.17051070967626436, 'max_depth': 12, 'max_bin': 299, 'num_leaves': 439}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:24,639] Trial 166 finished with value: 0.6545331140273792 and parameters: {'n_estimators': 704, 'learning_rate': 0.16274038223532417, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 466}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:25,985] Trial 167 finished with value: 0.6523013522875402 and parameters: {'n_estimators': 565, 'learning_rate': 0.15085693595756577, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 613}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:27,456] Trial 168 finished with value: 0.6584679298003003 and parameters: {'n_estimators': 800, 'learning_rate': 0.15823818942562407, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 378}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:29,025] Trial 169 finished with value: 0.6580140397057623 and parameters: {'n_estimators': 679, 'learning_rate': 0.1726749414880919, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 427}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:30,818] Trial 170 finished with value: 0.6464796177724699 and parameters: {'n_estimators': 630, 'learning_rate': 0.1901195834492109, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 502}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:32,793] Trial 171 finished with value: 0.6585863213673514 and parameters: {'n_estimators': 275, 'learning_rate': 0.12928452807820967, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 404}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:34,640] Trial 172 finished with value: 0.6548850973625338 and parameters: {'n_estimators': 669, 'learning_rate': 0.10006336431275417, 'max_depth': 12, 'max_bin': 233, 'num_leaves': 365}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:36,703] Trial 173 finished with value: 0.6577604834344852 and parameters: {'n_estimators': 847, 'learning_rate': 0.10837711160421018, 'max_depth': 12, 'max_bin': 236, 'num_leaves': 447}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:38,273] Trial 174 finished with value: 0.6532954191586835 and parameters: {'n_estimators': 735, 'learning_rate': 0.1792415213727421, 'max_depth': 12, 'max_bin': 252, 'num_leaves': 419}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:40,079] Trial 175 finished with value: 0.6546753177956088 and parameters: {'n_estimators': 242, 'learning_rate': 0.0755620373096482, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 207}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:41,516] Trial 176 finished with value: 0.6496237476458081 and parameters: {'n_estimators': 290, 'learning_rate': 0.15550724095664067, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 529}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:43,279] Trial 177 finished with value: 0.6537663080276812 and parameters: {'n_estimators': 698, 'learning_rate': 0.1354554693314115, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 394}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:44,921] Trial 178 finished with value: 0.6449745646576812 and parameters: {'n_estimators': 646, 'learning_rate': 0.1422471439420075, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 474}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:46,795] Trial 179 finished with value: 0.6587774567225131 and parameters: {'n_estimators': 225, 'learning_rate': 0.16607423418002187, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 433}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:48,386] Trial 180 finished with value: 0.6482651639999331 and parameters: {'n_estimators': 753, 'learning_rate': 0.11892890159380766, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 387}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:49,977] Trial 181 finished with value: 0.6613563807842604 and parameters: {'n_estimators': 537, 'learning_rate': 0.16155311989912485, 'max_depth': 10, 'max_bin': 283, 'num_leaves': 235}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:51,458] Trial 182 finished with value: 0.6513673495811695 and parameters: {'n_estimators': 305, 'learning_rate': 0.1548740855201158, 'max_depth': 10, 'max_bin': 282, 'num_leaves': 651}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:52,753] Trial 183 finished with value: 0.6510347615414206 and parameters: {'n_estimators': 549, 'learning_rate': 0.15154509392273863, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 256}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:54,496] Trial 184 finished with value: 0.6522474255641212 and parameters: {'n_estimators': 499, 'learning_rate': 0.17483752849833534, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 678}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:56,223] Trial 185 finished with value: 0.6454195905313327 and parameters: {'n_estimators': 610, 'learning_rate': 0.18421531536901684, 'max_depth': 11, 'max_bin': 284, 'num_leaves': 720}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:57,521] Trial 186 finished with value: 0.6546536202194291 and parameters: {'n_estimators': 468, 'learning_rate': 0.16803779197008376, 'max_depth': 10, 'max_bin': 276, 'num_leaves': 695}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:57:58,940] Trial 187 finished with value: 0.647462239019348 and parameters: {'n_estimators': 186, 'learning_rate': 0.14800064871022797, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 417}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:00,411] Trial 188 finished with value: 0.6585388320143706 and parameters: {'n_estimators': 680, 'learning_rate': 0.16009777636273353, 'max_depth': 12, 'max_bin': 175, 'num_leaves': 440}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:01,941] Trial 189 finished with value: 0.6456656356521887 and parameters: {'n_estimators': 205, 'learning_rate': 0.17745817208286224, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 408}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:03,623] Trial 190 finished with value: 0.6553072216870233 and parameters: {'n_estimators': 712, 'learning_rate': 0.12409702133158529, 'max_depth': 11, 'max_bin': 262, 'num_leaves': 456}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:05,229] Trial 191 finished with value: 0.6510237615936549 and parameters: {'n_estimators': 573, 'learning_rate': 0.1714360858779552, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 378}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:06,743] Trial 192 finished with value: 0.6504950331749851 and parameters: {'n_estimators': 602, 'learning_rate': 0.17302331748729438, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 374}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:08,178] Trial 193 finished with value: 0.6413935286761424 and parameters: {'n_estimators': 619, 'learning_rate': 0.18117343026691565, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 342}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:09,592] Trial 194 finished with value: 0.6545164208665757 and parameters: {'n_estimators': 659, 'learning_rate': 0.1642751232821232, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 392}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:10,601] Trial 195 finished with value: 0.6449941839432327 and parameters: {'n_estimators': 82, 'learning_rate': 0.1688315913683748, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 424}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:12,200] Trial 196 finished with value: 0.6541642370198993 and parameters: {'n_estimators': 691, 'learning_rate': 0.18581702941434955, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 407}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:13,776] Trial 197 finished with value: 0.6553681541976708 and parameters: {'n_estimators': 662, 'learning_rate': 0.09291741441303972, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 275}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:14,966] Trial 198 finished with value: 0.6416383303500766 and parameters: {'n_estimators': 815, 'learning_rate': 0.17627999075830578, 'max_depth': 9, 'max_bin': 289, 'num_leaves': 357}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:16,270] Trial 199 finished with value: 0.6532822164368285 and parameters: {'n_estimators': 636, 'learning_rate': 0.15942055251405154, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 185}. Best is trial 139 with value: 0.6933131199103784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6933\n",
      "\tBest params:\n",
      "\t\tn_estimators: 681\n",
      "\t\tlearning_rate: 0.17725588980199594\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 280\n",
      "\t\tnum_leaves: 424\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.708945    0.673766    0.671030    0.711722\n",
      "1                    TP   34.000000   33.000000   36.000000   41.000000\n",
      "2                    TN  201.000000  194.000000  197.000000  197.000000\n",
      "3                    FP    1.000000    6.000000    3.000000    2.000000\n",
      "4                    FN   32.000000   35.000000   32.000000   28.000000\n",
      "5              Accuracy    0.876866    0.847015    0.869403    0.888060\n",
      "6             Precision    0.971429    0.846154    0.923077    0.953488\n",
      "7           Sensitivity    0.515152    0.485294    0.529412    0.594203\n",
      "8           Specificity    0.995000    0.970000    0.985000    0.989900\n",
      "9              F1 score    0.673267    0.616822    0.672897    0.732143\n",
      "10  F1 score (weighted)    0.862356    0.831454    0.856119    0.878499\n",
      "11     F1 score (macro)    0.798703    0.760626    0.795656    0.830694\n",
      "12    Balanced Accuracy    0.755101    0.727647    0.757206    0.792076\n",
      "13                  MCC    0.652344    0.561838    0.634790    0.695908\n",
      "14                  NPV    0.862700    0.847200    0.860300    0.875600\n",
      "15              ROC_AUC    0.755101    0.727647    0.757206    0.792076\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_lgbm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_3_cat = np.where(((y_pred_lgbm_3 >= 2) | (y_pred_lgbm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_lgbm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_lgbm_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 06:58:18,397] Trial 200 finished with value: 0.6530129660690668 and parameters: {'n_estimators': 877, 'learning_rate': 0.17296830980293912, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 621}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:20,214] Trial 201 finished with value: 0.6516357981296726 and parameters: {'n_estimators': 801, 'learning_rate': 0.18544930235591048, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 438}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:21,901] Trial 202 finished with value: 0.6553404435722461 and parameters: {'n_estimators': 822, 'learning_rate': 0.1875801806874714, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 428}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:23,341] Trial 203 finished with value: 0.6602833113160246 and parameters: {'n_estimators': 843, 'learning_rate': 0.18009644956500628, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 445}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:24,812] Trial 204 finished with value: 0.6560033581638102 and parameters: {'n_estimators': 839, 'learning_rate': 0.1668381897491273, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 410}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:26,154] Trial 205 finished with value: 0.6561041483618679 and parameters: {'n_estimators': 784, 'learning_rate': 0.19030666530832882, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 394}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:27,487] Trial 206 finished with value: 0.6493511935133107 and parameters: {'n_estimators': 581, 'learning_rate': 0.18326779557951128, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 466}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:28,793] Trial 207 finished with value: 0.6537609324532168 and parameters: {'n_estimators': 252, 'learning_rate': 0.1769690953848448, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 425}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:30,334] Trial 208 finished with value: 0.6523690543107904 and parameters: {'n_estimators': 333, 'learning_rate': 0.17078443953409728, 'max_depth': 10, 'max_bin': 236, 'num_leaves': 450}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:32,021] Trial 209 finished with value: 0.6495386841951672 and parameters: {'n_estimators': 866, 'learning_rate': 0.14028836012302137, 'max_depth': 11, 'max_bin': 246, 'num_leaves': 374}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:33,723] Trial 210 finished with value: 0.651535281048264 and parameters: {'n_estimators': 681, 'learning_rate': 0.1322200365825601, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 437}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:35,402] Trial 211 finished with value: 0.6487503135485111 and parameters: {'n_estimators': 390, 'learning_rate': 0.15339698077264774, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 346}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:36,935] Trial 212 finished with value: 0.6502285707171471 and parameters: {'n_estimators': 447, 'learning_rate': 0.16359596498219464, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 382}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:38,805] Trial 213 finished with value: 0.6548613793781574 and parameters: {'n_estimators': 361, 'learning_rate': 0.14524171085048732, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 416}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:40,344] Trial 214 finished with value: 0.651808568471586 and parameters: {'n_estimators': 311, 'learning_rate': 0.1580092217551773, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 313}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:41,799] Trial 215 finished with value: 0.6563315922797853 and parameters: {'n_estimators': 480, 'learning_rate': 0.1489420800571123, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 401}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:43,386] Trial 216 finished with value: 0.6429333600000463 and parameters: {'n_estimators': 402, 'learning_rate': 0.18102370662549344, 'max_depth': 12, 'max_bin': 181, 'num_leaves': 293}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:45,143] Trial 217 finished with value: 0.6549780255665414 and parameters: {'n_estimators': 281, 'learning_rate': 0.10892318831413007, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 326}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:46,748] Trial 218 finished with value: 0.6561686791939887 and parameters: {'n_estimators': 724, 'learning_rate': 0.15533245759373712, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 395}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:48,426] Trial 219 finished with value: 0.6518813206176625 and parameters: {'n_estimators': 640, 'learning_rate': 0.1736810697613558, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 371}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:50,232] Trial 220 finished with value: 0.6461104266474854 and parameters: {'n_estimators': 426, 'learning_rate': 0.16444013269667396, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 433}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:51,383] Trial 221 finished with value: 0.6430397613984347 and parameters: {'n_estimators': 279, 'learning_rate': 0.1878076496737095, 'max_depth': 9, 'max_bin': 277, 'num_leaves': 280}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:53,043] Trial 222 finished with value: 0.6485957051927199 and parameters: {'n_estimators': 266, 'learning_rate': 0.18072489539153044, 'max_depth': 10, 'max_bin': 281, 'num_leaves': 735}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:54,521] Trial 223 finished with value: 0.6515449184668978 and parameters: {'n_estimators': 240, 'learning_rate': 0.1762173088690495, 'max_depth': 10, 'max_bin': 233, 'num_leaves': 256}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:55,654] Trial 224 finished with value: 0.6499175044907688 and parameters: {'n_estimators': 704, 'learning_rate': 0.16994185737865308, 'max_depth': 9, 'max_bin': 228, 'num_leaves': 223}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:56,837] Trial 225 finished with value: 0.6483396567309051 and parameters: {'n_estimators': 293, 'learning_rate': 0.16124548144964668, 'max_depth': 8, 'max_bin': 250, 'num_leaves': 666}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:58:59,056] Trial 226 finished with value: 0.6443132132068687 and parameters: {'n_estimators': 256, 'learning_rate': 0.02643438707327192, 'max_depth': 11, 'max_bin': 290, 'num_leaves': 419}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:00,245] Trial 227 finished with value: 0.6424501021830217 and parameters: {'n_estimators': 666, 'learning_rate': 0.19528390129998677, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 240}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:01,512] Trial 228 finished with value: 0.6489735142489569 and parameters: {'n_estimators': 141, 'learning_rate': 0.1781420386411011, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 264}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:02,919] Trial 229 finished with value: 0.6537251537366314 and parameters: {'n_estimators': 321, 'learning_rate': 0.16761402943631196, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 300}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:04,374] Trial 230 finished with value: 0.6435142473246339 and parameters: {'n_estimators': 689, 'learning_rate': 0.18398751376036274, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 597}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:05,982] Trial 231 finished with value: 0.6581658754783055 and parameters: {'n_estimators': 675, 'learning_rate': 0.17382640025302762, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 441}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:07,523] Trial 232 finished with value: 0.6585014934005018 and parameters: {'n_estimators': 690, 'learning_rate': 0.17752827951966646, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 429}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:09,038] Trial 233 finished with value: 0.6564114596124101 and parameters: {'n_estimators': 711, 'learning_rate': 0.17373313797651693, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 459}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:10,842] Trial 234 finished with value: 0.6582035391971874 and parameters: {'n_estimators': 611, 'learning_rate': 0.16960894718776517, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 410}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:12,735] Trial 235 finished with value: 0.6540078739431597 and parameters: {'n_estimators': 653, 'learning_rate': 0.18653584947602989, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 447}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:14,546] Trial 236 finished with value: 0.6471037964129516 and parameters: {'n_estimators': 832, 'learning_rate': 0.1508309302082571, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 386}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:16,237] Trial 237 finished with value: 0.6580136630907363 and parameters: {'n_estimators': 741, 'learning_rate': 0.12577036053623947, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 167}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:17,740] Trial 238 finished with value: 0.6492485795097992 and parameters: {'n_estimators': 702, 'learning_rate': 0.18080768041394346, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 430}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:19,108] Trial 239 finished with value: 0.65833766539802 and parameters: {'n_estimators': 217, 'learning_rate': 0.15788800036101627, 'max_depth': 10, 'max_bin': 283, 'num_leaves': 406}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:20,806] Trial 240 finished with value: 0.654162930473954 and parameters: {'n_estimators': 669, 'learning_rate': 0.13688737172501098, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 414}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:22,563] Trial 241 finished with value: 0.653642679084091 and parameters: {'n_estimators': 347, 'learning_rate': 0.15339644849302977, 'max_depth': 11, 'max_bin': 217, 'num_leaves': 386}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:24,360] Trial 242 finished with value: 0.6575642947378687 and parameters: {'n_estimators': 436, 'learning_rate': 0.16362896524322357, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 359}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:26,174] Trial 243 finished with value: 0.6530712171773791 and parameters: {'n_estimators': 597, 'learning_rate': 0.11982354588579458, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 364}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:27,984] Trial 244 finished with value: 0.6480127962082192 and parameters: {'n_estimators': 386, 'learning_rate': 0.14424409771322894, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 398}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:29,375] Trial 245 finished with value: 0.6470295030606557 and parameters: {'n_estimators': 519, 'learning_rate': 0.16026200015485065, 'max_depth': 12, 'max_bin': 194, 'num_leaves': 442}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:30,816] Trial 246 finished with value: 0.6591170142712455 and parameters: {'n_estimators': 304, 'learning_rate': 0.1663044342023861, 'max_depth': 10, 'max_bin': 239, 'num_leaves': 340}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:32,253] Trial 247 finished with value: 0.6564940866807305 and parameters: {'n_estimators': 266, 'learning_rate': 0.1755527173597961, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 378}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:33,363] Trial 248 finished with value: 0.6378941682121739 and parameters: {'n_estimators': 410, 'learning_rate': 0.15500831768966056, 'max_depth': 7, 'max_bin': 230, 'num_leaves': 474}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:34,913] Trial 249 finished with value: 0.6533452598995602 and parameters: {'n_estimators': 803, 'learning_rate': 0.13267285916248076, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 424}. Best is trial 139 with value: 0.6933131199103784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6933\n",
      "\tBest params:\n",
      "\t\tn_estimators: 681\n",
      "\t\tlearning_rate: 0.17725588980199594\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 280\n",
      "\t\tnum_leaves: 424\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.708945    0.673766    0.671030    0.711722   \n",
      "1                    TP   34.000000   33.000000   36.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  197.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    3.000000    2.000000   \n",
      "4                    FN   32.000000   35.000000   32.000000   28.000000   \n",
      "5              Accuracy    0.876866    0.847015    0.869403    0.888060   \n",
      "6             Precision    0.971429    0.846154    0.923077    0.953488   \n",
      "7           Sensitivity    0.515152    0.485294    0.529412    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.985000    0.989900   \n",
      "9              F1 score    0.673267    0.616822    0.672897    0.732143   \n",
      "10  F1 score (weighted)    0.862356    0.831454    0.856119    0.878499   \n",
      "11     F1 score (macro)    0.798703    0.760626    0.795656    0.830694   \n",
      "12    Balanced Accuracy    0.755101    0.727647    0.757206    0.792076   \n",
      "13                  MCC    0.652344    0.561838    0.634790    0.695908   \n",
      "14                  NPV    0.862700    0.847200    0.860300    0.875600   \n",
      "15              ROC_AUC    0.755101    0.727647    0.757206    0.792076   \n",
      "\n",
      "          Set4  \n",
      "0     0.685058  \n",
      "1    40.000000  \n",
      "2   197.000000  \n",
      "3     5.000000  \n",
      "4    26.000000  \n",
      "5     0.884328  \n",
      "6     0.888889  \n",
      "7     0.606061  \n",
      "8     0.975200  \n",
      "9     0.720721  \n",
      "10    0.876244  \n",
      "11    0.823890  \n",
      "12    0.790654  \n",
      "13    0.670032  \n",
      "14    0.883400  \n",
      "15    0.790654  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_lgbm_4)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_4_cat = np.where(((y_pred_lgbm_4 >= 2) | (y_pred_lgbm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_lgbm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_lgbm_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 06:59:36,792] Trial 250 finished with value: 0.6512107110803834 and parameters: {'n_estimators': 624, 'learning_rate': 0.17071130668317863, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 456}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:38,010] Trial 251 finished with value: 0.6519867825571541 and parameters: {'n_estimators': 169, 'learning_rate': 0.14831311823230267, 'max_depth': 11, 'max_bin': 243, 'num_leaves': 213}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:39,101] Trial 252 finished with value: 0.6392264373216946 and parameters: {'n_estimators': 724, 'learning_rate': 0.18327399933294614, 'max_depth': 9, 'max_bin': 273, 'num_leaves': 399}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:40,360] Trial 253 finished with value: 0.6452919132670323 and parameters: {'n_estimators': 688, 'learning_rate': 0.17910663185794948, 'max_depth': 12, 'max_bin': 264, 'num_leaves': 417}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:41,632] Trial 254 finished with value: 0.6441134374116848 and parameters: {'n_estimators': 369, 'learning_rate': 0.18882295261950202, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 546}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:43,275] Trial 255 finished with value: 0.6472375363900473 and parameters: {'n_estimators': 649, 'learning_rate': 0.1598769389335981, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 435}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:44,417] Trial 256 finished with value: 0.6489564452480503 and parameters: {'n_estimators': 105, 'learning_rate': 0.16719192996644586, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 382}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:45,925] Trial 257 finished with value: 0.6509618922273711 and parameters: {'n_estimators': 676, 'learning_rate': 0.1726832587832017, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 370}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:47,245] Trial 258 finished with value: 0.6516194842072127 and parameters: {'n_estimators': 771, 'learning_rate': 0.17587197425238643, 'max_depth': 11, 'max_bin': 270, 'num_leaves': 329}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:48,569] Trial 259 finished with value: 0.6484879490826951 and parameters: {'n_estimators': 852, 'learning_rate': 0.15659320224437825, 'max_depth': 10, 'max_bin': 234, 'num_leaves': 195}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:49,811] Trial 260 finished with value: 0.647581216546299 and parameters: {'n_estimators': 818, 'learning_rate': 0.19092902813326296, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 406}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:51,014] Trial 261 finished with value: 0.6441708199393043 and parameters: {'n_estimators': 343, 'learning_rate': 0.1638942479759345, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 448}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:51,897] Trial 262 finished with value: 0.6403783229958963 and parameters: {'n_estimators': 555, 'learning_rate': 0.18203265072879612, 'max_depth': 5, 'max_bin': 288, 'num_leaves': 430}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:53,448] Trial 263 finished with value: 0.6520950675313848 and parameters: {'n_estimators': 283, 'learning_rate': 0.14098306674670558, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 355}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:54,911] Trial 264 finished with value: 0.6424857056258914 and parameters: {'n_estimators': 699, 'learning_rate': 0.17311466359996502, 'max_depth': 10, 'max_bin': 284, 'num_leaves': 395}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:56,558] Trial 265 finished with value: 0.6553000104544593 and parameters: {'n_estimators': 234, 'learning_rate': 0.178425604207725, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 244}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 06:59:58,904] Trial 266 finished with value: 0.6460489106061367 and parameters: {'n_estimators': 789, 'learning_rate': 0.11041828621535367, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 463}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:00,813] Trial 267 finished with value: 0.6553735912222338 and parameters: {'n_estimators': 662, 'learning_rate': 0.1529124826331619, 'max_depth': 12, 'max_bin': 168, 'num_leaves': 313}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:02,667] Trial 268 finished with value: 0.6555243192655762 and parameters: {'n_estimators': 314, 'learning_rate': 0.16924894398893234, 'max_depth': 12, 'max_bin': 176, 'num_leaves': 415}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:04,204] Trial 269 finished with value: 0.652664924349921 and parameters: {'n_estimators': 466, 'learning_rate': 0.11703538317098748, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 278}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:05,636] Trial 270 finished with value: 0.6498565767583934 and parameters: {'n_estimators': 632, 'learning_rate': 0.18485065776528925, 'max_depth': 12, 'max_bin': 247, 'num_leaves': 707}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:07,115] Trial 271 finished with value: 0.64272714223202 and parameters: {'n_estimators': 718, 'learning_rate': 0.1600919288243398, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 433}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:08,682] Trial 272 finished with value: 0.6526016300342171 and parameters: {'n_estimators': 687, 'learning_rate': 0.10386350288409531, 'max_depth': 8, 'max_bin': 232, 'num_leaves': 391}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:10,458] Trial 273 finished with value: 0.6463773890743105 and parameters: {'n_estimators': 291, 'learning_rate': 0.14602280183474245, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 648}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:12,369] Trial 274 finished with value: 0.645340476165475 and parameters: {'n_estimators': 650, 'learning_rate': 0.1278473517620567, 'max_depth': 9, 'max_bin': 172, 'num_leaves': 447}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:13,991] Trial 275 finished with value: 0.6503648715318084 and parameters: {'n_estimators': 584, 'learning_rate': 0.1647847421137434, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 408}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:15,731] Trial 276 finished with value: 0.6598649700016659 and parameters: {'n_estimators': 264, 'learning_rate': 0.12320646740643322, 'max_depth': 12, 'max_bin': 238, 'num_leaves': 483}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:17,240] Trial 277 finished with value: 0.6462001259611523 and parameters: {'n_estimators': 671, 'learning_rate': 0.1757916640517663, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 634}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:18,610] Trial 278 finished with value: 0.6474186893332061 and parameters: {'n_estimators': 332, 'learning_rate': 0.17112157376763323, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 370}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:20,310] Trial 279 finished with value: 0.6477618950012861 and parameters: {'n_estimators': 757, 'learning_rate': 0.18076339418821216, 'max_depth': 10, 'max_bin': 295, 'num_leaves': 427}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:21,601] Trial 280 finished with value: 0.6501628706275016 and parameters: {'n_estimators': 406, 'learning_rate': 0.1513836265311282, 'max_depth': 12, 'max_bin': 286, 'num_leaves': 59}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:23,210] Trial 281 finished with value: 0.6407935866322634 and parameters: {'n_estimators': 614, 'learning_rate': 0.15649754070726052, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 383}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:24,801] Trial 282 finished with value: 0.6534112550202575 and parameters: {'n_estimators': 707, 'learning_rate': 0.16225342573277707, 'max_depth': 12, 'max_bin': 181, 'num_leaves': 261}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:26,505] Trial 283 finished with value: 0.6480396080761611 and parameters: {'n_estimators': 869, 'learning_rate': 0.09697763080694605, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 291}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:28,280] Trial 284 finished with value: 0.6524005973966636 and parameters: {'n_estimators': 376, 'learning_rate': 0.11251249276224308, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 420}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:29,593] Trial 285 finished with value: 0.6441152816684278 and parameters: {'n_estimators': 245, 'learning_rate': 0.18661410952294735, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 442}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:30,850] Trial 286 finished with value: 0.6461284600114956 and parameters: {'n_estimators': 819, 'learning_rate': 0.19572001055954702, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 462}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:32,767] Trial 287 finished with value: 0.6512803179339488 and parameters: {'n_estimators': 676, 'learning_rate': 0.08369778365338476, 'max_depth': 12, 'max_bin': 242, 'num_leaves': 684}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:33,987] Trial 288 finished with value: 0.6420953424307503 and parameters: {'n_estimators': 733, 'learning_rate': 0.16755535333830096, 'max_depth': 10, 'max_bin': 285, 'num_leaves': 408}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:35,263] Trial 289 finished with value: 0.6411877138028588 and parameters: {'n_estimators': 454, 'learning_rate': 0.17769788466613354, 'max_depth': 12, 'max_bin': 265, 'num_leaves': 569}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:37,111] Trial 290 finished with value: 0.6514874199644515 and parameters: {'n_estimators': 692, 'learning_rate': 0.1733663230709745, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 362}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:38,859] Trial 291 finished with value: 0.6528033951089485 and parameters: {'n_estimators': 834, 'learning_rate': 0.13418202353017522, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 231}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:40,033] Trial 292 finished with value: 0.6418804803845202 and parameters: {'n_estimators': 199, 'learning_rate': 0.19206235192256374, 'max_depth': 12, 'max_bin': 261, 'num_leaves': 394}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:41,337] Trial 293 finished with value: 0.645032296408145 and parameters: {'n_estimators': 298, 'learning_rate': 0.18428633315485995, 'max_depth': 11, 'max_bin': 229, 'num_leaves': 423}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:42,900] Trial 294 finished with value: 0.6561025749646812 and parameters: {'n_estimators': 650, 'learning_rate': 0.16941641346633776, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 351}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:44,761] Trial 295 finished with value: 0.6555037195138189 and parameters: {'n_estimators': 272, 'learning_rate': 0.1501090082057071, 'max_depth': 9, 'max_bin': 235, 'num_leaves': 450}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:46,879] Trial 296 finished with value: 0.6615254163168076 and parameters: {'n_estimators': 492, 'learning_rate': 0.18021975057698889, 'max_depth': 12, 'max_bin': 292, 'num_leaves': 407}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:48,878] Trial 297 finished with value: 0.6564539178085248 and parameters: {'n_estimators': 635, 'learning_rate': 0.1407478308652028, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 437}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:50,253] Trial 298 finished with value: 0.6464292904772962 and parameters: {'n_estimators': 602, 'learning_rate': 0.15779809991425048, 'max_depth': 7, 'max_bin': 209, 'num_leaves': 383}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:51,769] Trial 299 finished with value: 0.6518869756109995 and parameters: {'n_estimators': 569, 'learning_rate': 0.1626342183463703, 'max_depth': 11, 'max_bin': 163, 'num_leaves': 341}. Best is trial 139 with value: 0.6933131199103784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6933\n",
      "\tBest params:\n",
      "\t\tn_estimators: 681\n",
      "\t\tlearning_rate: 0.17725588980199594\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 280\n",
      "\t\tnum_leaves: 424\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.708945    0.673766    0.671030    0.711722   \n",
      "1                    TP   34.000000   33.000000   36.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  197.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    3.000000    2.000000   \n",
      "4                    FN   32.000000   35.000000   32.000000   28.000000   \n",
      "5              Accuracy    0.876866    0.847015    0.869403    0.888060   \n",
      "6             Precision    0.971429    0.846154    0.923077    0.953488   \n",
      "7           Sensitivity    0.515152    0.485294    0.529412    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.985000    0.989900   \n",
      "9              F1 score    0.673267    0.616822    0.672897    0.732143   \n",
      "10  F1 score (weighted)    0.862356    0.831454    0.856119    0.878499   \n",
      "11     F1 score (macro)    0.798703    0.760626    0.795656    0.830694   \n",
      "12    Balanced Accuracy    0.755101    0.727647    0.757206    0.792076   \n",
      "13                  MCC    0.652344    0.561838    0.634790    0.695908   \n",
      "14                  NPV    0.862700    0.847200    0.860300    0.875600   \n",
      "15              ROC_AUC    0.755101    0.727647    0.757206    0.792076   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.685058    0.678225  \n",
      "1    40.000000   44.000000  \n",
      "2   197.000000  195.000000  \n",
      "3     5.000000    5.000000  \n",
      "4    26.000000   24.000000  \n",
      "5     0.884328    0.891791  \n",
      "6     0.888889    0.897959  \n",
      "7     0.606061    0.647059  \n",
      "8     0.975200    0.975000  \n",
      "9     0.720721    0.752137  \n",
      "10    0.876244    0.885458  \n",
      "11    0.823890    0.841462  \n",
      "12    0.790654    0.811029  \n",
      "13    0.670032    0.700295  \n",
      "14    0.883400    0.890400  \n",
      "15    0.790654    0.811029  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_lgbm_5)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_5_cat = np.where(((y_pred_lgbm_5 >= 2) | (y_pred_lgbm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_lgbm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_lgbm_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:00:53,948] Trial 300 finished with value: 0.65641461596413 and parameters: {'n_estimators': 422, 'learning_rate': 0.11477168844126676, 'max_depth': 10, 'max_bin': 186, 'num_leaves': 585}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:55,824] Trial 301 finished with value: 0.6485595460466584 and parameters: {'n_estimators': 703, 'learning_rate': 0.174814670885208, 'max_depth': 12, 'max_bin': 232, 'num_leaves': 474}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:57,596] Trial 302 finished with value: 0.654924038409926 and parameters: {'n_estimators': 323, 'learning_rate': 0.18756713373762163, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 608}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:00:58,660] Trial 303 finished with value: 0.63936803190994 and parameters: {'n_estimators': 669, 'learning_rate': 0.16725884213765008, 'max_depth': 6, 'max_bin': 272, 'num_leaves': 398}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:00,265] Trial 304 finished with value: 0.6512904742444409 and parameters: {'n_estimators': 353, 'learning_rate': 0.18217118780949493, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 420}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:01,986] Trial 305 finished with value: 0.6470425035516085 and parameters: {'n_estimators': 798, 'learning_rate': 0.1994073153296116, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 456}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:03,000] Trial 306 finished with value: 0.6393909390497245 and parameters: {'n_estimators': 125, 'learning_rate': 0.17749742592326595, 'max_depth': 8, 'max_bin': 287, 'num_leaves': 374}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:04,956] Trial 307 finished with value: 0.6499924641384233 and parameters: {'n_estimators': 716, 'learning_rate': 0.10474436235640837, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 437}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:06,568] Trial 308 finished with value: 0.6490320712746676 and parameters: {'n_estimators': 254, 'learning_rate': 0.1711048601064202, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 322}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:08,285] Trial 309 finished with value: 0.654061572953018 and parameters: {'n_estimators': 393, 'learning_rate': 0.1541977190724151, 'max_depth': 10, 'max_bin': 237, 'num_leaves': 413}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:10,016] Trial 310 finished with value: 0.6538549461814298 and parameters: {'n_estimators': 687, 'learning_rate': 0.14712025331009632, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 254}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:11,574] Trial 311 finished with value: 0.6504229151915205 and parameters: {'n_estimators': 157, 'learning_rate': 0.164377239772942, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 393}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:13,068] Trial 312 finished with value: 0.6495354453621112 and parameters: {'n_estimators': 854, 'learning_rate': 0.15912316910524127, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 431}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:14,785] Trial 313 finished with value: 0.6491403939313943 and parameters: {'n_estimators': 887, 'learning_rate': 0.17380504628090887, 'max_depth': 11, 'max_bin': 276, 'num_leaves': 667}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:16,613] Trial 314 finished with value: 0.6480334386906702 and parameters: {'n_estimators': 774, 'learning_rate': 0.19133498694599127, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 364}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:18,112] Trial 315 finished with value: 0.6500539705125993 and parameters: {'n_estimators': 679, 'learning_rate': 0.17967885199628483, 'max_depth': 12, 'max_bin': 249, 'num_leaves': 122}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:19,811] Trial 316 finished with value: 0.6561879456296087 and parameters: {'n_estimators': 747, 'learning_rate': 0.12706655957124843, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 308}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:21,285] Trial 317 finished with value: 0.6479694376218198 and parameters: {'n_estimators': 663, 'learning_rate': 0.18369978843139892, 'max_depth': 10, 'max_bin': 271, 'num_leaves': 405}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:22,838] Trial 318 finished with value: 0.6543320827098541 and parameters: {'n_estimators': 642, 'learning_rate': 0.16520604305204697, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 215}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:24,753] Trial 319 finished with value: 0.6510268415563171 and parameters: {'n_estimators': 224, 'learning_rate': 0.06155573926482367, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 497}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:26,173] Trial 320 finished with value: 0.6564885606528235 and parameters: {'n_estimators': 593, 'learning_rate': 0.15302479864850024, 'max_depth': 12, 'max_bin': 167, 'num_leaves': 272}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:27,666] Trial 321 finished with value: 0.651102838648534 and parameters: {'n_estimators': 548, 'learning_rate': 0.17603099970161368, 'max_depth': 12, 'max_bin': 245, 'num_leaves': 420}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:29,171] Trial 322 finished with value: 0.6481325441715813 and parameters: {'n_estimators': 621, 'learning_rate': 0.14339676420621728, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 383}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:30,929] Trial 323 finished with value: 0.652468029404141 and parameters: {'n_estimators': 812, 'learning_rate': 0.13853882255070404, 'max_depth': 12, 'max_bin': 273, 'num_leaves': 464}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:32,642] Trial 324 finished with value: 0.6532041937166511 and parameters: {'n_estimators': 699, 'learning_rate': 0.13159620362419505, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 438}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:34,318] Trial 325 finished with value: 0.6547649545111371 and parameters: {'n_estimators': 305, 'learning_rate': 0.17145665324506526, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 452}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:35,828] Trial 326 finished with value: 0.6475029055985153 and parameters: {'n_estimators': 286, 'learning_rate': 0.1873206810257534, 'max_depth': 10, 'max_bin': 150, 'num_leaves': 399}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:37,660] Trial 327 finished with value: 0.6533806006151728 and parameters: {'n_estimators': 722, 'learning_rate': 0.16775118156583163, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 425}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:39,749] Trial 328 finished with value: 0.6571061066377257 and parameters: {'n_estimators': 507, 'learning_rate': 0.16117940767624236, 'max_depth': 11, 'max_bin': 240, 'num_leaves': 378}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:41,624] Trial 329 finished with value: 0.6524418753830131 and parameters: {'n_estimators': 368, 'learning_rate': 0.17857037332997472, 'max_depth': 12, 'max_bin': 293, 'num_leaves': 288}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:43,229] Trial 330 finished with value: 0.654304772297577 and parameters: {'n_estimators': 657, 'learning_rate': 0.17153803001210813, 'max_depth': 9, 'max_bin': 232, 'num_leaves': 516}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:44,084] Trial 331 finished with value: 0.6107394034352056 and parameters: {'n_estimators': 532, 'learning_rate': 0.15643788172244066, 'max_depth': 3, 'max_bin': 266, 'num_leaves': 446}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:45,898] Trial 332 finished with value: 0.6521703352090376 and parameters: {'n_estimators': 438, 'learning_rate': 0.18312638867847375, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 411}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:47,582] Trial 333 finished with value: 0.6447196253149338 and parameters: {'n_estimators': 833, 'learning_rate': 0.16697112035083397, 'max_depth': 12, 'max_bin': 178, 'num_leaves': 347}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:48,473] Trial 334 finished with value: 0.6347866804872492 and parameters: {'n_estimators': 73, 'learning_rate': 0.14832163405731763, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 245}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:49,986] Trial 335 finished with value: 0.6486254597093704 and parameters: {'n_estimators': 264, 'learning_rate': 0.16086240814543823, 'max_depth': 11, 'max_bin': 277, 'num_leaves': 429}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:51,494] Trial 336 finished with value: 0.6488172069384512 and parameters: {'n_estimators': 477, 'learning_rate': 0.19518766770748136, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 391}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:53,250] Trial 337 finished with value: 0.6502260652152104 and parameters: {'n_estimators': 684, 'learning_rate': 0.17636079366417534, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 365}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:55,127] Trial 338 finished with value: 0.6608142967445179 and parameters: {'n_estimators': 334, 'learning_rate': 0.15105960114418562, 'max_depth': 12, 'max_bin': 235, 'num_leaves': 477}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:56,939] Trial 339 finished with value: 0.6495017442779274 and parameters: {'n_estimators': 707, 'learning_rate': 0.18097159519028908, 'max_depth': 12, 'max_bin': 272, 'num_leaves': 415}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:01:58,395] Trial 340 finished with value: 0.651797755399152 and parameters: {'n_estimators': 179, 'learning_rate': 0.1882087831136243, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 225}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:02:00,103] Trial 341 finished with value: 0.6538925249057173 and parameters: {'n_estimators': 733, 'learning_rate': 0.17081905057370997, 'max_depth': 10, 'max_bin': 254, 'num_leaves': 204}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:02:01,613] Trial 342 finished with value: 0.6543505523518771 and parameters: {'n_estimators': 630, 'learning_rate': 0.16387828716884473, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 394}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:02:02,733] Trial 343 finished with value: 0.6477651402011675 and parameters: {'n_estimators': 789, 'learning_rate': 0.17462627386409474, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 330}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:02:04,041] Trial 344 finished with value: 0.6669772501964701 and parameters: {'n_estimators': 416, 'learning_rate': 0.15704422586548727, 'max_depth': 12, 'max_bin': 296, 'num_leaves': 167}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:02:05,523] Trial 345 finished with value: 0.6601315885797817 and parameters: {'n_estimators': 241, 'learning_rate': 0.1168551801592501, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 456}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:02:07,015] Trial 346 finished with value: 0.6538648498950885 and parameters: {'n_estimators': 282, 'learning_rate': 0.09107201833644997, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 440}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:02:08,655] Trial 347 finished with value: 0.6494759476052756 and parameters: {'n_estimators': 667, 'learning_rate': 0.13601886277233727, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 423}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:02:10,379] Trial 348 finished with value: 0.6538193008627745 and parameters: {'n_estimators': 610, 'learning_rate': 0.12379633795475956, 'max_depth': 12, 'max_bin': 239, 'num_leaves': 405}. Best is trial 139 with value: 0.6933131199103784.\n",
      "[I 2023-12-20 07:02:11,976] Trial 349 finished with value: 0.6568680422953099 and parameters: {'n_estimators': 311, 'learning_rate': 0.1794790773183262, 'max_depth': 10, 'max_bin': 159, 'num_leaves': 736}. Best is trial 139 with value: 0.6933131199103784.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.693313\n",
      "\tBest params:\n",
      "\t\tn_estimators: 681\n",
      "\t\tlearning_rate: 0.17725588980199594\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 280\n",
      "\t\tnum_leaves: 424\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.708945    0.673766    0.671030    0.711722   \n",
      "1                    TP   34.000000   33.000000   36.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  197.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    3.000000    2.000000   \n",
      "4                    FN   32.000000   35.000000   32.000000   28.000000   \n",
      "5              Accuracy    0.876866    0.847015    0.869403    0.888060   \n",
      "6             Precision    0.971429    0.846154    0.923077    0.953488   \n",
      "7           Sensitivity    0.515152    0.485294    0.529412    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.985000    0.989900   \n",
      "9              F1 score    0.673267    0.616822    0.672897    0.732143   \n",
      "10  F1 score (weighted)    0.862356    0.831454    0.856119    0.878499   \n",
      "11     F1 score (macro)    0.798703    0.760626    0.795656    0.830694   \n",
      "12    Balanced Accuracy    0.755101    0.727647    0.757206    0.792076   \n",
      "13                  MCC    0.652344    0.561838    0.634790    0.695908   \n",
      "14                  NPV    0.862700    0.847200    0.860300    0.875600   \n",
      "15              ROC_AUC    0.755101    0.727647    0.757206    0.792076   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.685058    0.678225    0.715178  \n",
      "1    40.000000   44.000000   38.000000  \n",
      "2   197.000000  195.000000  199.000000  \n",
      "3     5.000000    5.000000    1.000000  \n",
      "4    26.000000   24.000000   30.000000  \n",
      "5     0.884328    0.891791    0.884328  \n",
      "6     0.888889    0.897959    0.974359  \n",
      "7     0.606061    0.647059    0.558824  \n",
      "8     0.975200    0.975000    0.995000  \n",
      "9     0.720721    0.752137    0.710280  \n",
      "10    0.876244    0.885458    0.872563  \n",
      "11    0.823890    0.841462    0.819010  \n",
      "12    0.790654    0.811029    0.776912  \n",
      "13    0.670032    0.700295    0.683425  \n",
      "14    0.883400    0.890400    0.869000  \n",
      "15    0.790654    0.811029    0.776912  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_lgbm_6)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_6_cat = np.where(((y_pred_lgbm_6 >= 2) | (y_pred_lgbm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_lgbm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_lgbm_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:02:14,000] Trial 350 finished with value: 0.6959005951249126 and parameters: {'n_estimators': 695, 'learning_rate': 0.18355222483593647, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 377}. Best is trial 350 with value: 0.6959005951249126.\n",
      "[I 2023-12-20 07:02:16,402] Trial 351 finished with value: 0.7011130985935945 and parameters: {'n_estimators': 704, 'learning_rate': 0.18207322485580765, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 433}. Best is trial 351 with value: 0.7011130985935945.\n",
      "[I 2023-12-20 07:02:18,422] Trial 352 finished with value: 0.7012570192905636 and parameters: {'n_estimators': 720, 'learning_rate': 0.19142464906098378, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 403}. Best is trial 352 with value: 0.7012570192905636.\n",
      "[I 2023-12-20 07:02:20,722] Trial 353 finished with value: 0.704754290920893 and parameters: {'n_estimators': 716, 'learning_rate': 0.19360057162963062, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 387}. Best is trial 353 with value: 0.704754290920893.\n",
      "[I 2023-12-20 07:02:22,625] Trial 354 finished with value: 0.7004169310833566 and parameters: {'n_estimators': 726, 'learning_rate': 0.1910374854975247, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 404}. Best is trial 353 with value: 0.704754290920893.\n",
      "[I 2023-12-20 07:02:24,951] Trial 355 finished with value: 0.7056690365805496 and parameters: {'n_estimators': 741, 'learning_rate': 0.19351748913464237, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 383}. Best is trial 355 with value: 0.7056690365805496.\n",
      "[I 2023-12-20 07:02:27,117] Trial 356 finished with value: 0.7035465646120648 and parameters: {'n_estimators': 747, 'learning_rate': 0.1962587618018148, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 401}. Best is trial 355 with value: 0.7056690365805496.\n",
      "[I 2023-12-20 07:02:28,977] Trial 357 finished with value: 0.6998712433989073 and parameters: {'n_estimators': 751, 'learning_rate': 0.1956579302280151, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 389}. Best is trial 355 with value: 0.7056690365805496.\n",
      "[I 2023-12-20 07:02:31,106] Trial 358 finished with value: 0.7021517385576913 and parameters: {'n_estimators': 755, 'learning_rate': 0.19770300991207898, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 394}. Best is trial 355 with value: 0.7056690365805496.\n",
      "[I 2023-12-20 07:02:33,369] Trial 359 finished with value: 0.7044766167020191 and parameters: {'n_estimators': 734, 'learning_rate': 0.197113348969346, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 387}. Best is trial 355 with value: 0.7056690365805496.\n",
      "[I 2023-12-20 07:02:35,639] Trial 360 finished with value: 0.7098134088196765 and parameters: {'n_estimators': 742, 'learning_rate': 0.1999021593197947, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 385}. Best is trial 360 with value: 0.7098134088196765.\n",
      "[I 2023-12-20 07:02:37,680] Trial 361 finished with value: 0.7116037570153535 and parameters: {'n_estimators': 745, 'learning_rate': 0.1996657861663137, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 392}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:39,010] Trial 362 finished with value: 0.6938482641089843 and parameters: {'n_estimators': 745, 'learning_rate': 0.19886189062908574, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 382}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:40,338] Trial 363 finished with value: 0.7027814514436905 and parameters: {'n_estimators': 748, 'learning_rate': 0.19747423645450665, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 378}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:42,180] Trial 364 finished with value: 0.7000585086918729 and parameters: {'n_estimators': 744, 'learning_rate': 0.19858989526069443, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 376}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:44,102] Trial 365 finished with value: 0.709497877535064 and parameters: {'n_estimators': 754, 'learning_rate': 0.19806052643259808, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 375}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:46,042] Trial 366 finished with value: 0.7083812680377429 and parameters: {'n_estimators': 761, 'learning_rate': 0.19803693222812918, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 370}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:47,948] Trial 367 finished with value: 0.704320478007671 and parameters: {'n_estimators': 763, 'learning_rate': 0.1997947436877472, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 369}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:49,582] Trial 368 finished with value: 0.6969584430644111 and parameters: {'n_estimators': 756, 'learning_rate': 0.19875669279764624, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 360}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:51,931] Trial 369 finished with value: 0.7091369773935708 and parameters: {'n_estimators': 761, 'learning_rate': 0.1992099883572867, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 361}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:54,227] Trial 370 finished with value: 0.7076796655214708 and parameters: {'n_estimators': 743, 'learning_rate': 0.19981587389723623, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 364}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:55,446] Trial 371 finished with value: 0.6812452783395864 and parameters: {'n_estimators': 759, 'learning_rate': 0.1991447769994594, 'max_depth': 5, 'max_bin': 222, 'num_leaves': 356}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:57,577] Trial 372 finished with value: 0.7055273094102307 and parameters: {'n_estimators': 746, 'learning_rate': 0.1997408964876252, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 365}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:02:59,509] Trial 373 finished with value: 0.7107194231006344 and parameters: {'n_estimators': 745, 'learning_rate': 0.1998878603377706, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 368}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:03:01,341] Trial 374 finished with value: 0.7057000894102279 and parameters: {'n_estimators': 750, 'learning_rate': 0.19913285761781674, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 362}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:03:03,143] Trial 375 finished with value: 0.7053459059776515 and parameters: {'n_estimators': 743, 'learning_rate': 0.1998115522219472, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 362}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:03:05,135] Trial 376 finished with value: 0.7025046587277075 and parameters: {'n_estimators': 748, 'learning_rate': 0.1982847055684336, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 350}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:03:07,386] Trial 377 finished with value: 0.7062253388161468 and parameters: {'n_estimators': 766, 'learning_rate': 0.19682415028360647, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 349}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:03:10,011] Trial 378 finished with value: 0.7062539107943674 and parameters: {'n_estimators': 765, 'learning_rate': 0.19997180333315479, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 349}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:03:11,134] Trial 379 finished with value: 0.6738999099884866 and parameters: {'n_estimators': 769, 'learning_rate': 0.1951963409267841, 'max_depth': 4, 'max_bin': 219, 'num_leaves': 343}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:03:12,948] Trial 380 finished with value: 0.7002179190055083 and parameters: {'n_estimators': 743, 'learning_rate': 0.1956763785949941, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 345}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:03:14,716] Trial 381 finished with value: 0.6952405992707907 and parameters: {'n_estimators': 774, 'learning_rate': 0.19587872559027733, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 346}. Best is trial 361 with value: 0.7116037570153535.\n",
      "[I 2023-12-20 07:03:16,720] Trial 382 finished with value: 0.7127149507043422 and parameters: {'n_estimators': 742, 'learning_rate': 0.19954488235092985, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 336}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:18,976] Trial 383 finished with value: 0.7061114784140468 and parameters: {'n_estimators': 736, 'learning_rate': 0.1999755759140814, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 333}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:20,827] Trial 384 finished with value: 0.7049837596424534 and parameters: {'n_estimators': 771, 'learning_rate': 0.1991558663413361, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 357}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:22,875] Trial 385 finished with value: 0.7083112716624178 and parameters: {'n_estimators': 768, 'learning_rate': 0.1996128249886598, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 337}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:24,890] Trial 386 finished with value: 0.7093376469647913 and parameters: {'n_estimators': 766, 'learning_rate': 0.19990869200238703, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 327}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:26,646] Trial 387 finished with value: 0.7037280077617984 and parameters: {'n_estimators': 766, 'learning_rate': 0.19980209763487583, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 330}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:28,512] Trial 388 finished with value: 0.7099216267654881 and parameters: {'n_estimators': 779, 'learning_rate': 0.19952152627307584, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 323}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:30,208] Trial 389 finished with value: 0.7098349322120843 and parameters: {'n_estimators': 777, 'learning_rate': 0.1994510750909973, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 329}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:32,049] Trial 390 finished with value: 0.707965591316454 and parameters: {'n_estimators': 773, 'learning_rate': 0.1996948852692628, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 328}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:33,390] Trial 391 finished with value: 0.6960477529790488 and parameters: {'n_estimators': 775, 'learning_rate': 0.19894546196987972, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 319}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:35,168] Trial 392 finished with value: 0.706389142874208 and parameters: {'n_estimators': 770, 'learning_rate': 0.19938821598974021, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 333}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:36,655] Trial 393 finished with value: 0.6983721188535629 and parameters: {'n_estimators': 789, 'learning_rate': 0.19373981973051313, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 329}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:38,767] Trial 394 finished with value: 0.7056688690834175 and parameters: {'n_estimators': 781, 'learning_rate': 0.1935186764252925, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 335}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:41,271] Trial 395 finished with value: 0.7077785096714297 and parameters: {'n_estimators': 790, 'learning_rate': 0.19353064223654587, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 314}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:43,071] Trial 396 finished with value: 0.6977678732150682 and parameters: {'n_estimators': 780, 'learning_rate': 0.19372548347762064, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 312}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:44,972] Trial 397 finished with value: 0.7107469838277114 and parameters: {'n_estimators': 784, 'learning_rate': 0.19984573473206166, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 334}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:46,819] Trial 398 finished with value: 0.6983704523540587 and parameters: {'n_estimators': 786, 'learning_rate': 0.1938395949305829, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 336}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:48,736] Trial 399 finished with value: 0.6988613834736854 and parameters: {'n_estimators': 774, 'learning_rate': 0.19340997407779567, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 321}. Best is trial 382 with value: 0.7127149507043422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7127150\n",
      "\tBest params:\n",
      "\t\tn_estimators: 742\n",
      "\t\tlearning_rate: 0.19954488235092985\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 221\n",
      "\t\tnum_leaves: 336\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.708945    0.673766    0.671030    0.711722   \n",
      "1                    TP   34.000000   33.000000   36.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  197.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    3.000000    2.000000   \n",
      "4                    FN   32.000000   35.000000   32.000000   28.000000   \n",
      "5              Accuracy    0.876866    0.847015    0.869403    0.888060   \n",
      "6             Precision    0.971429    0.846154    0.923077    0.953488   \n",
      "7           Sensitivity    0.515152    0.485294    0.529412    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.985000    0.989900   \n",
      "9              F1 score    0.673267    0.616822    0.672897    0.732143   \n",
      "10  F1 score (weighted)    0.862356    0.831454    0.856119    0.878499   \n",
      "11     F1 score (macro)    0.798703    0.760626    0.795656    0.830694   \n",
      "12    Balanced Accuracy    0.755101    0.727647    0.757206    0.792076   \n",
      "13                  MCC    0.652344    0.561838    0.634790    0.695908   \n",
      "14                  NPV    0.862700    0.847200    0.860300    0.875600   \n",
      "15              ROC_AUC    0.755101    0.727647    0.757206    0.792076   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.685058    0.678225    0.715178    0.590449  \n",
      "1    40.000000   44.000000   38.000000   34.000000  \n",
      "2   197.000000  195.000000  199.000000  194.000000  \n",
      "3     5.000000    5.000000    1.000000    7.000000  \n",
      "4    26.000000   24.000000   30.000000   33.000000  \n",
      "5     0.884328    0.891791    0.884328    0.850746  \n",
      "6     0.888889    0.897959    0.974359    0.829268  \n",
      "7     0.606061    0.647059    0.558824    0.507463  \n",
      "8     0.975200    0.975000    0.995000    0.965200  \n",
      "9     0.720721    0.752137    0.710280    0.629630  \n",
      "10    0.876244    0.885458    0.872563    0.837314  \n",
      "11    0.823890    0.841462    0.819010    0.768086  \n",
      "12    0.790654    0.811029    0.776912    0.736318  \n",
      "13    0.670032    0.700295    0.683425    0.568536  \n",
      "14    0.883400    0.890400    0.869000    0.854600  \n",
      "15    0.790654    0.811029    0.776912    0.736318  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_lgbm_7)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_7_cat = np.where(((y_pred_lgbm_7 >= 2) | (y_pred_lgbm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_lgbm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_lgbm_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:03:50,012] Trial 400 finished with value: 0.6846438987392776 and parameters: {'n_estimators': 796, 'learning_rate': 0.19943313759511688, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 326}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:50,988] Trial 401 finished with value: 0.6812610523043222 and parameters: {'n_estimators': 766, 'learning_rate': 0.19995552643251827, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 304}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:52,086] Trial 402 finished with value: 0.6907844765627401 and parameters: {'n_estimators': 785, 'learning_rate': 0.19418964451030837, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 336}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:53,208] Trial 403 finished with value: 0.6833201806913073 and parameters: {'n_estimators': 765, 'learning_rate': 0.19977519657008733, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 337}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:54,372] Trial 404 finished with value: 0.6847178220320835 and parameters: {'n_estimators': 793, 'learning_rate': 0.19176468153686776, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 356}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:55,405] Trial 405 finished with value: 0.6842245242764089 and parameters: {'n_estimators': 774, 'learning_rate': 0.1998929904613871, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 317}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:56,478] Trial 406 finished with value: 0.6915039736016241 and parameters: {'n_estimators': 805, 'learning_rate': 0.19528092114904977, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 350}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:57,813] Trial 407 finished with value: 0.6826815946935219 and parameters: {'n_estimators': 767, 'learning_rate': 0.19468412402241939, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 334}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:03:58,963] Trial 408 finished with value: 0.6809212983489447 and parameters: {'n_estimators': 735, 'learning_rate': 0.19997066190892457, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 306}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:00,179] Trial 409 finished with value: 0.6812998467248054 and parameters: {'n_estimators': 759, 'learning_rate': 0.19187252366458799, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 353}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:01,460] Trial 410 finished with value: 0.6802323330710112 and parameters: {'n_estimators': 795, 'learning_rate': 0.19546423261830342, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 336}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:02,673] Trial 411 finished with value: 0.685849056274878 and parameters: {'n_estimators': 785, 'learning_rate': 0.19107031291142873, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 316}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:03,690] Trial 412 finished with value: 0.6809215048265364 and parameters: {'n_estimators': 740, 'learning_rate': 0.19996899230430057, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 357}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:04,666] Trial 413 finished with value: 0.6897331610673423 and parameters: {'n_estimators': 767, 'learning_rate': 0.1950677368345993, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 340}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:05,791] Trial 414 finished with value: 0.6874681412091123 and parameters: {'n_estimators': 755, 'learning_rate': 0.19540006947137334, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 323}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:06,890] Trial 415 finished with value: 0.6877269355715171 and parameters: {'n_estimators': 780, 'learning_rate': 0.19087893538712797, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 362}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:07,843] Trial 416 finished with value: 0.6809226660546536 and parameters: {'n_estimators': 736, 'learning_rate': 0.19995960391254286, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 299}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:08,901] Trial 417 finished with value: 0.6898760144162811 and parameters: {'n_estimators': 757, 'learning_rate': 0.19539251856893775, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 348}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:09,874] Trial 418 finished with value: 0.6709717239855963 and parameters: {'n_estimators': 807, 'learning_rate': 0.19032616200034724, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 330}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:10,989] Trial 419 finished with value: 0.6867194179006427 and parameters: {'n_estimators': 737, 'learning_rate': 0.19597088870383245, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 361}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:11,999] Trial 420 finished with value: 0.6765139981928099 and parameters: {'n_estimators': 779, 'learning_rate': 0.19221049168838183, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 344}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:12,901] Trial 421 finished with value: 0.6872421692807252 and parameters: {'n_estimators': 760, 'learning_rate': 0.19631291944159218, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 317}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:13,835] Trial 422 finished with value: 0.6863978794413562 and parameters: {'n_estimators': 778, 'learning_rate': 0.19626031631523444, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 333}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:14,880] Trial 423 finished with value: 0.6810288127089238 and parameters: {'n_estimators': 730, 'learning_rate': 0.19156560596276812, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 363}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:16,200] Trial 424 finished with value: 0.6890460934896663 and parameters: {'n_estimators': 800, 'learning_rate': 0.19934998103756674, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 348}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:17,389] Trial 425 finished with value: 0.680580948964495 and parameters: {'n_estimators': 746, 'learning_rate': 0.19986103413284367, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 328}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:18,644] Trial 426 finished with value: 0.6816329555615699 and parameters: {'n_estimators': 761, 'learning_rate': 0.19335479389567065, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 311}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:20,069] Trial 427 finished with value: 0.6832204976589866 and parameters: {'n_estimators': 786, 'learning_rate': 0.19980105570193674, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 366}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:21,803] Trial 428 finished with value: 0.6840267279207976 and parameters: {'n_estimators': 744, 'learning_rate': 0.1899315389045917, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 347}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:23,070] Trial 429 finished with value: 0.679971906172132 and parameters: {'n_estimators': 769, 'learning_rate': 0.19559540731990563, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 296}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:24,510] Trial 430 finished with value: 0.6794034742542354 and parameters: {'n_estimators': 804, 'learning_rate': 0.19580912720805552, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 337}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:25,947] Trial 431 finished with value: 0.6810813681855186 and parameters: {'n_estimators': 759, 'learning_rate': 0.1897652804835344, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 357}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:27,347] Trial 432 finished with value: 0.6854302497814565 and parameters: {'n_estimators': 732, 'learning_rate': 0.195993546445797, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 320}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:28,598] Trial 433 finished with value: 0.6825826064614944 and parameters: {'n_estimators': 780, 'learning_rate': 0.1999974814823073, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 349}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:29,781] Trial 434 finished with value: 0.679437687184651 and parameters: {'n_estimators': 749, 'learning_rate': 0.1926896017716359, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 368}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:31,229] Trial 435 finished with value: 0.6854303930627049 and parameters: {'n_estimators': 727, 'learning_rate': 0.1959923797123787, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 303}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:32,633] Trial 436 finished with value: 0.678986935755531 and parameters: {'n_estimators': 767, 'learning_rate': 0.18909229288626897, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 325}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:34,051] Trial 437 finished with value: 0.683156455704055 and parameters: {'n_estimators': 793, 'learning_rate': 0.19320349719731741, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 337}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:35,380] Trial 438 finished with value: 0.6790626924092014 and parameters: {'n_estimators': 753, 'learning_rate': 0.19958996907011192, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 364}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:36,652] Trial 439 finished with value: 0.6809668506420623 and parameters: {'n_estimators': 774, 'learning_rate': 0.19998078217510226, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 355}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:38,065] Trial 440 finished with value: 0.6789696305016565 and parameters: {'n_estimators': 808, 'learning_rate': 0.19580368596853123, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 337}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:39,344] Trial 441 finished with value: 0.6800445839447311 and parameters: {'n_estimators': 731, 'learning_rate': 0.1917084141822471, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 315}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:40,722] Trial 442 finished with value: 0.6812630975912961 and parameters: {'n_estimators': 750, 'learning_rate': 0.1999403931685181, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 370}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:42,088] Trial 443 finished with value: 0.682072494020596 and parameters: {'n_estimators': 787, 'learning_rate': 0.18876145857187288, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 347}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:43,306] Trial 444 finished with value: 0.6795444688912383 and parameters: {'n_estimators': 762, 'learning_rate': 0.19549057899102087, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 325}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:44,634] Trial 445 finished with value: 0.6885100524219762 and parameters: {'n_estimators': 737, 'learning_rate': 0.19297432452514243, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 354}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:45,893] Trial 446 finished with value: 0.6871381363752651 and parameters: {'n_estimators': 777, 'learning_rate': 0.19620056906673453, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 368}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:47,209] Trial 447 finished with value: 0.6843287590833401 and parameters: {'n_estimators': 723, 'learning_rate': 0.19595716955067496, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 309}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:48,446] Trial 448 finished with value: 0.6780831487339352 and parameters: {'n_estimators': 795, 'learning_rate': 0.18924878671215875, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 342}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:49,282] Trial 449 finished with value: 0.6646173149910627 and parameters: {'n_estimators': 754, 'learning_rate': 0.19269719797546864, 'max_depth': 4, 'max_bin': 216, 'num_leaves': 328}. Best is trial 382 with value: 0.7127149507043422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.71271495\n",
      "\tBest params:\n",
      "\t\tn_estimators: 742\n",
      "\t\tlearning_rate: 0.19954488235092985\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 221\n",
      "\t\tnum_leaves: 336\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.708945    0.673766    0.671030    0.711722   \n",
      "1                    TP   34.000000   33.000000   36.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  197.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    3.000000    2.000000   \n",
      "4                    FN   32.000000   35.000000   32.000000   28.000000   \n",
      "5              Accuracy    0.876866    0.847015    0.869403    0.888060   \n",
      "6             Precision    0.971429    0.846154    0.923077    0.953488   \n",
      "7           Sensitivity    0.515152    0.485294    0.529412    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.985000    0.989900   \n",
      "9              F1 score    0.673267    0.616822    0.672897    0.732143   \n",
      "10  F1 score (weighted)    0.862356    0.831454    0.856119    0.878499   \n",
      "11     F1 score (macro)    0.798703    0.760626    0.795656    0.830694   \n",
      "12    Balanced Accuracy    0.755101    0.727647    0.757206    0.792076   \n",
      "13                  MCC    0.652344    0.561838    0.634790    0.695908   \n",
      "14                  NPV    0.862700    0.847200    0.860300    0.875600   \n",
      "15              ROC_AUC    0.755101    0.727647    0.757206    0.792076   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.685058    0.678225    0.715178    0.590449    0.617881  \n",
      "1    40.000000   44.000000   38.000000   34.000000   32.000000  \n",
      "2   197.000000  195.000000  199.000000  194.000000  196.000000  \n",
      "3     5.000000    5.000000    1.000000    7.000000    4.000000  \n",
      "4    26.000000   24.000000   30.000000   33.000000   36.000000  \n",
      "5     0.884328    0.891791    0.884328    0.850746    0.850746  \n",
      "6     0.888889    0.897959    0.974359    0.829268    0.888889  \n",
      "7     0.606061    0.647059    0.558824    0.507463    0.470588  \n",
      "8     0.975200    0.975000    0.995000    0.965200    0.980000  \n",
      "9     0.720721    0.752137    0.710280    0.629630    0.615385  \n",
      "10    0.876244    0.885458    0.872563    0.837314    0.833312  \n",
      "11    0.823890    0.841462    0.819010    0.768086    0.761396  \n",
      "12    0.790654    0.811029    0.776912    0.736318    0.725294  \n",
      "13    0.670032    0.700295    0.683425    0.568536    0.574982  \n",
      "14    0.883400    0.890400    0.869000    0.854600    0.844800  \n",
      "15    0.790654    0.811029    0.776912    0.736318    0.725294  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_lgbm_8)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_8_cat = np.where(((y_pred_lgbm_8 >= 2) | (y_pred_lgbm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_lgbm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_lgbm_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:04:51,509] Trial 450 finished with value: 0.6912795796229696 and parameters: {'n_estimators': 815, 'learning_rate': 0.1999973490323622, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 367}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:53,304] Trial 451 finished with value: 0.692167685691987 and parameters: {'n_estimators': 768, 'learning_rate': 0.19997669353858796, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 289}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:55,248] Trial 452 finished with value: 0.6964399410330292 and parameters: {'n_estimators': 742, 'learning_rate': 0.1959107736860247, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 352}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:56,836] Trial 453 finished with value: 0.689743577202414 and parameters: {'n_estimators': 783, 'learning_rate': 0.18941970014475604, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 336}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:04:58,631] Trial 454 finished with value: 0.6813713104381663 and parameters: {'n_estimators': 760, 'learning_rate': 0.19300319819483158, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 366}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:00,648] Trial 455 finished with value: 0.6918235670428668 and parameters: {'n_estimators': 721, 'learning_rate': 0.19637164501176924, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 315}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:02,848] Trial 456 finished with value: 0.696263544255604 and parameters: {'n_estimators': 741, 'learning_rate': 0.19986233566537964, 'max_depth': 11, 'max_bin': 221, 'num_leaves': 343}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:04,894] Trial 457 finished with value: 0.6841436846124018 and parameters: {'n_estimators': 797, 'learning_rate': 0.1920924078040372, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 303}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:06,617] Trial 458 finished with value: 0.6912553932121777 and parameters: {'n_estimators': 773, 'learning_rate': 0.1958614336344971, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 327}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:08,516] Trial 459 finished with value: 0.6890987078306774 and parameters: {'n_estimators': 752, 'learning_rate': 0.19629368105517594, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 373}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:10,094] Trial 460 finished with value: 0.6861427344660511 and parameters: {'n_estimators': 778, 'learning_rate': 0.18769855799806662, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 354}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:11,629] Trial 461 finished with value: 0.6886748220066928 and parameters: {'n_estimators': 730, 'learning_rate': 0.19996673043643237, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 335}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:12,642] Trial 462 finished with value: 0.6618630406377097 and parameters: {'n_estimators': 813, 'learning_rate': 0.1918629397220382, 'max_depth': 6, 'max_bin': 215, 'num_leaves': 353}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:14,450] Trial 463 finished with value: 0.688841678118616 and parameters: {'n_estimators': 763, 'learning_rate': 0.1879914311962947, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 318}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:16,217] Trial 464 finished with value: 0.6908765159066694 and parameters: {'n_estimators': 791, 'learning_rate': 0.19607447878740145, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 375}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:17,887] Trial 465 finished with value: 0.6942080324602142 and parameters: {'n_estimators': 746, 'learning_rate': 0.19992910849130696, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 344}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:19,565] Trial 466 finished with value: 0.6867688337332283 and parameters: {'n_estimators': 765, 'learning_rate': 0.19285897811279712, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 359}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:21,886] Trial 467 finished with value: 0.6953512229995752 and parameters: {'n_estimators': 741, 'learning_rate': 0.19610013560156758, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 304}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:23,863] Trial 468 finished with value: 0.6766946216780231 and parameters: {'n_estimators': 720, 'learning_rate': 0.192471510400709, 'max_depth': 11, 'max_bin': 223, 'num_leaves': 327}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:25,747] Trial 469 finished with value: 0.6876641397977976 and parameters: {'n_estimators': 782, 'learning_rate': 0.18772176839174334, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 375}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:27,403] Trial 470 finished with value: 0.6958607464221174 and parameters: {'n_estimators': 799, 'learning_rate': 0.19595405538949245, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 359}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:28,920] Trial 471 finished with value: 0.6830153394656395 and parameters: {'n_estimators': 755, 'learning_rate': 0.19685759176700615, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 337}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:30,450] Trial 472 finished with value: 0.6811299089834988 and parameters: {'n_estimators': 771, 'learning_rate': 0.19268818409058366, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 346}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:32,217] Trial 473 finished with value: 0.6928611419483357 and parameters: {'n_estimators': 729, 'learning_rate': 0.18937724041644077, 'max_depth': 12, 'max_bin': 229, 'num_leaves': 374}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:34,172] Trial 474 finished with value: 0.6843934047005267 and parameters: {'n_estimators': 751, 'learning_rate': 0.19652151132734744, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 320}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:35,856] Trial 475 finished with value: 0.6916214278211914 and parameters: {'n_estimators': 822, 'learning_rate': 0.19672543461584116, 'max_depth': 11, 'max_bin': 218, 'num_leaves': 297}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:37,749] Trial 476 finished with value: 0.6848053644716774 and parameters: {'n_estimators': 784, 'learning_rate': 0.19224927342961814, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 361}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:39,208] Trial 477 finished with value: 0.6793398404654465 and parameters: {'n_estimators': 763, 'learning_rate': 0.19690034841718804, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 335}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:41,159] Trial 478 finished with value: 0.6884785686820825 and parameters: {'n_estimators': 741, 'learning_rate': 0.19931689572258618, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 348}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:42,751] Trial 479 finished with value: 0.6808625565778218 and parameters: {'n_estimators': 719, 'learning_rate': 0.1884383683776756, 'max_depth': 12, 'max_bin': 216, 'num_leaves': 374}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:45,311] Trial 480 finished with value: 0.692023895173864 and parameters: {'n_estimators': 800, 'learning_rate': 0.05902792686241137, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 317}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:47,106] Trial 481 finished with value: 0.6873570326148071 and parameters: {'n_estimators': 776, 'learning_rate': 0.1936347268061011, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 362}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:49,093] Trial 482 finished with value: 0.6867723790857457 and parameters: {'n_estimators': 756, 'learning_rate': 0.19274580931304838, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 334}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:51,247] Trial 483 finished with value: 0.6843132596853312 and parameters: {'n_estimators': 742, 'learning_rate': 0.19960426729579592, 'max_depth': 12, 'max_bin': 231, 'num_leaves': 308}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:52,898] Trial 484 finished with value: 0.6835314345117227 and parameters: {'n_estimators': 786, 'learning_rate': 0.19999362148365057, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 379}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:54,831] Trial 485 finished with value: 0.6866740879699135 and parameters: {'n_estimators': 769, 'learning_rate': 0.19036841734654858, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 349}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:56,751] Trial 486 finished with value: 0.6890495026794475 and parameters: {'n_estimators': 728, 'learning_rate': 0.1068672097068065, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 329}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:58,006] Trial 487 finished with value: 0.6930498659875223 and parameters: {'n_estimators': 813, 'learning_rate': 0.19994722278300747, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 363}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:05:59,164] Trial 488 finished with value: 0.6909391313430858 and parameters: {'n_estimators': 750, 'learning_rate': 0.19501898792157296, 'max_depth': 11, 'max_bin': 207, 'num_leaves': 285}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:00,669] Trial 489 finished with value: 0.6864779595899815 and parameters: {'n_estimators': 792, 'learning_rate': 0.1999881392605374, 'max_depth': 12, 'max_bin': 222, 'num_leaves': 346}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:02,254] Trial 490 finished with value: 0.6907316523336292 and parameters: {'n_estimators': 771, 'learning_rate': 0.18628388750324193, 'max_depth': 12, 'max_bin': 219, 'num_leaves': 376}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:03,981] Trial 491 finished with value: 0.6902627231631144 and parameters: {'n_estimators': 719, 'learning_rate': 0.19343677051554595, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 328}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:05,739] Trial 492 finished with value: 0.6875874816153351 and parameters: {'n_estimators': 758, 'learning_rate': 0.19525254322251986, 'max_depth': 12, 'max_bin': 213, 'num_leaves': 357}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:10,811] Trial 493 finished with value: 0.6548703504005251 and parameters: {'n_estimators': 737, 'learning_rate': 0.005325244944305252, 'max_depth': 12, 'max_bin': 223, 'num_leaves': 309}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:12,041] Trial 494 finished with value: 0.6837545715433442 and parameters: {'n_estimators': 806, 'learning_rate': 0.18928721855825822, 'max_depth': 12, 'max_bin': 217, 'num_leaves': 342}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:13,839] Trial 495 finished with value: 0.6890181748331937 and parameters: {'n_estimators': 776, 'learning_rate': 0.11304819475295858, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 381}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:15,442] Trial 496 finished with value: 0.6916579377263488 and parameters: {'n_estimators': 752, 'learning_rate': 0.0971600314515987, 'max_depth': 11, 'max_bin': 230, 'num_leaves': 322}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:16,950] Trial 497 finished with value: 0.6919795746741232 and parameters: {'n_estimators': 792, 'learning_rate': 0.19635146756407187, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 366}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:18,475] Trial 498 finished with value: 0.6880008718576558 and parameters: {'n_estimators': 736, 'learning_rate': 0.19284965768063242, 'max_depth': 12, 'max_bin': 215, 'num_leaves': 344}. Best is trial 382 with value: 0.7127149507043422.\n",
      "[I 2023-12-20 07:06:19,784] Trial 499 finished with value: 0.6941465968851585 and parameters: {'n_estimators': 768, 'learning_rate': 0.1961689421252684, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 354}. Best is trial 382 with value: 0.7127149507043422.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.712714951\n",
      "\tBest params:\n",
      "\t\tn_estimators: 742\n",
      "\t\tlearning_rate: 0.19954488235092985\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 221\n",
      "\t\tnum_leaves: 336\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"lgbmRegressor_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.708945    0.673766    0.671030    0.711722   \n",
      "1                    TP   34.000000   33.000000   36.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  197.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    3.000000    2.000000   \n",
      "4                    FN   32.000000   35.000000   32.000000   28.000000   \n",
      "5              Accuracy    0.876866    0.847015    0.869403    0.888060   \n",
      "6             Precision    0.971429    0.846154    0.923077    0.953488   \n",
      "7           Sensitivity    0.515152    0.485294    0.529412    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.985000    0.989900   \n",
      "9              F1 score    0.673267    0.616822    0.672897    0.732143   \n",
      "10  F1 score (weighted)    0.862356    0.831454    0.856119    0.878499   \n",
      "11     F1 score (macro)    0.798703    0.760626    0.795656    0.830694   \n",
      "12    Balanced Accuracy    0.755101    0.727647    0.757206    0.792076   \n",
      "13                  MCC    0.652344    0.561838    0.634790    0.695908   \n",
      "14                  NPV    0.862700    0.847200    0.860300    0.875600   \n",
      "15              ROC_AUC    0.755101    0.727647    0.757206    0.792076   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.685058    0.678225    0.715178    0.590449    0.617881    0.629364  \n",
      "1    40.000000   44.000000   38.000000   34.000000   32.000000   32.000000  \n",
      "2   197.000000  195.000000  199.000000  194.000000  196.000000  199.000000  \n",
      "3     5.000000    5.000000    1.000000    7.000000    4.000000    2.000000  \n",
      "4    26.000000   24.000000   30.000000   33.000000   36.000000   35.000000  \n",
      "5     0.884328    0.891791    0.884328    0.850746    0.850746    0.861940  \n",
      "6     0.888889    0.897959    0.974359    0.829268    0.888889    0.941176  \n",
      "7     0.606061    0.647059    0.558824    0.507463    0.470588    0.477612  \n",
      "8     0.975200    0.975000    0.995000    0.965200    0.980000    0.990000  \n",
      "9     0.720721    0.752137    0.710280    0.629630    0.615385    0.633663  \n",
      "10    0.876244    0.885458    0.872563    0.837314    0.833312    0.844623  \n",
      "11    0.823890    0.841462    0.819010    0.768086    0.761396    0.774303  \n",
      "12    0.790654    0.811029    0.776912    0.736318    0.725294    0.733831  \n",
      "13    0.670032    0.700295    0.683425    0.568536    0.574982    0.608443  \n",
      "14    0.883400    0.890400    0.869000    0.854600    0.844800    0.850400  \n",
      "15    0.790654    0.811029    0.776912    0.736318    0.725294    0.733831  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_lgbm_9)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_lgbm_9_cat = np.where(((y_pred_lgbm_9 >= 2) | (y_pred_lgbm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_lgbm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_lgbm_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAokAAAHJCAYAAADpZ6PQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB160lEQVR4nO3deXhM5/8+8HsmmWyyyr5HSGJL7FuoqJ2qUMRObB871VaJFkFLo2qrnSL2JUUsJdSulFgjKnZCZEVkl0lyfn/4Zb7GmSCTycb9ui6XzDnPec77PHNibmcbiSAIAoiIiIiI3iAt7QKIiIiIqOxhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEUkkikUAikbyzjYuLCyQSCR4+fFgyRVGZ06JFi/fuJyXF398fEokE69evL+1Sil1ZGnci+ngxJBIRERGRCEMiEREREYkwJJLGvHjxAgYGBqhcuTIEQVDZplOnTpBIJLh06RIA4OHDh5BIJPD390dUVBS6dOmCihUrokKFCmjWrBkOHz5c4Pq2bt2Kzz//HGZmZtDT00O1atXw008/4dWrV6K2EokELVq0wNOnTzFo0CDY2tpCS0tLcWoy/1Tl/fv3MX/+fFStWhV6enpwcHDAhAkTkJKSIurz+PHj+N///ofq1avD2NgY+vr6qFGjBqZPn47MzExR+8DAQEgkEpw4cQIbNmxAgwYNUKFCBbi4uCjarF+/Ht26dYOrqyv09fVhbGyMpk2bYsOGDSrHIP+0o1wux8yZM1G5cmXo6enBw8MDq1evVrRbunQpatasCX19fTg4OCAwMBB5eXkq+zx//jy6d+8OGxsb6OjowNHREcOHD8fTp08VbfLft5MnTyrGN/9PixYtlPp78uQJxowZA1dXV+jq6sLc3BydO3dGeHi4WmNUWJocI3X316ysLMyZMweenp4wMDCAsbExPvvsM2zbtk3U9u11dO/eHZaWlpBKpVi/fv0HjXtR9s2QkBA0bNgQBgYGqFixInr27IknT56o3K7nz5/jhx9+QM2aNWFgYAATExPUqlULkydPRnp6uqhtQEAAqlWrBn19fZiYmKBVq1Yqx+zVq1dYsGAB6tSpAzMzMxgYGMDR0RFffvkljhw5orIWItI87dIugD4eZmZm6NWrF9atW4e///4bbdq0UZr/+PFjHDx4EPXq1UO9evWU5j148ABNmjRBzZo1MXz4cMTGxmL79u3o0KEDtmzZgp49eyq1HzJkCNauXQtHR0d069YNJiYm+PfffzF16lQcPXoUhw8fhkwmU1rm2bNnaNKkCYyMjNC9e3cIggArKyulNhMmTMCpU6fg5+cHX19fhIWFYeHChTh9+jTOnDkDPT09RdugoCBERUXB29sbX3zxBTIzM/HPP/9g5syZOH78OI4dOwZtbfGv2Lx58/D333/jyy+/RMuWLZGcnKyYN3LkSFSvXh3NmzeHra0tkpKScODAAQwcOBBRUVGYPXu2yrHv1asXzp8/j44dO0ImkyEkJAT/+9//oKOjg4sXL2LLli3o1KkTWrdujX379mHGjBnQ19fHpEmTlPpZt24dhg0bBj09PXTu3BkODg64c+cO1qxZg3379uHff/+Fk5MTTE1NMX36dKxfvx6PHj3C9OnTFX28GeguX76Mtm3b4vnz52jXrh2++uorJCUlYc+ePWjWrBl2796Njh07FmqM1KWpMQIKt79mZ2ejbdu2OH36NKpXr47Ro0cjIyMDO3fuRO/evXHlyhUEBQWJ1nH37l00btwYHh4e6NevH9LS0uDp6flB467uvrls2TLs3bsXnTt3ho+PD86fP48dO3bg6tWriIiIgK6urtIYfP7553j06BHq1auHkSNHIi8vD7du3cKCBQswYsQIVKhQAQDw6NEjtGjRAg8fPkTz5s3RoUMHpKWlYf/+/Wjfvj1WrFiB//3vf4q+BwwYgB07dqBmzZoYMGAA9PX18fTpU5w5cwZhYWGif1uIqJgIRCoAEAAI06dPL/CPiYmJAEB48OCBYrmLFy8KAIRu3bqJ+pw6daoAQFi1apVi2oMHDxTr+u6775Tah4eHC9ra2oKpqanw8uVLxfR169YJAITu3bsLmZmZSstMnz5dACAsWLBA5fb0799fkMvlotoGDhwoABDMzc2Fhw8fKqbn5uYKX331lQBAmDlzptIy9+7dE/Ly8kR9BQQECACErVu3qqzNwMBAuHz5smg5QRCEu3fviqZlZWUJLVq0ELS1tYXHjx8rzfPx8REACPXr1xdevHihVJtMJhNMTEwEFxcX4cmTJ4p5ycnJgoWFhWBhYaE0Frdu3RJkMpng5uYmPH36VGk9R48eFaRSqeDr66ty/arI5XKhcuXKgp6ennD69GmleTExMYKdnZ1gbW2t9B5+yBgVJP89XLduncoaNTFG6uyvP//8swBA6NSpk1JfcXFxgqOjowBAaXzeXEdAQIDKbX3XuOdvmzr7ppGRkRAREaE0r3fv3gIAYdu2bUrTvb29BQDC7NmzRetJTExUel99fHwEiUQi7NixQ6ndixcvhFq1agl6enpCbGysIAivx14ikQj16tUTcnJyRH0nJSUVuN1EpFkMiaRS/ofUh/x5MyQKgiA0aNBAkMlkQlxcnGJaTk6OYGdnJxgZGQlpaWmK6fkfiCYmJkJKSoqojvwP/vXr1yum1a5dW5DJZEof+G+ux9zcXKhfv75oe3R0dIT4+HiV25u/nreDoCC8/sCVSqWCi4uLymXflpSUJAAQBg0apDQ9/4N4/PjxH9TPm0JCQgQAQnBwsNL0/LBw9OhR0TKff/65AED4448/RPMGDRokAFAKxF9//bUAQDhw4IDKGrp06SJIpVKlAPSusLJnzx4BgDBx4kSV8xcuXCgAEPbv36+YVpQxel9I1MQYqbO/Vq5cWZBIJMKtW7dE7VetWiXaV/LXYW1tLWRlZanc1veFxIK8b9/88ccfRcscO3ZMACB8++23imn5/xmsXbu2kJub+851Xr16VQAg9OjRQ+X8/P1kyZIlgiAIQkpKigBA8Pb2Vhl0iajk8HQzvZNQwLWFwOvTW48ePRJNHzVqFAYNGoS1a9ciICAAALBv3z48ffoUI0eOVJyCelPdunVhZGQkmt6iRQsEBwfjypUrGDhwIDIyMnDt2jVYWFhg4cKFKuvS1dVFVFSUynrfPr38Nh8fH9E0V1dXODo64uHDh0hOToapqSkAID09HYsWLcLu3btx+/ZtpKamKo1XTEyMynU0atSowPVHR0cjKCgIR48eRXR0tOj6sYL6fPv0PQDY2dm9d96TJ0/g7OwMADh37hwA4MSJE7hw4YJomYSEBOTl5eHOnTsq+3xbfn8PHz5EYGCgaP6dO3cAAFFRUfjiiy+U5r1rjNSliTHK96H7a2pqKu7duwcHBwe4u7uL2rdu3RrA69Pyb6tVq5bS6d3CUHffrF+/vmiao6MjgNfXHOf7999/AQDt2rWDVPruS9vz94Pk5GSV+0FiYiIAKH5njYyM8OWXX2Lfvn2oU6cOunXrhmbNmqFRo0YwMDB457qISLMYEknjevbsiW+//RZr1qzB5MmTIZFIsHLlSgDAiBEjVC5jbW2tcrqNjQ0A4OXLlwBef1AJgoDExETMmDGjUHXl9/Uu76rj0aNHePnyJUxNTSGXy9GyZUtcuHABNWvWRM+ePWFpaam4DnLGjBkqb6B5Vx33799Hw4YN8eLFC3z22Wdo27YtTExMoKWlhYcPHyI4OLjAPk1MTETT8q85e9c8uVyumPbs2TMAwK+//qpyHfnS0tLeOf/t/nbu3Fno/j7kvSosTYxRvg/dX/P/Lmh7bG1tldqp6quwirJvvmsccnNzFdPyrxG1t7d/bz35+8GRI0feedPJm/vB9u3bERQUhC1btmDatGkAAD09Pfj5+WHevHmwtLR873qJqOgYEknj9PX14e/vj/nz5+PIkSNwd3fH4cOH0bhxY3h5ealcJj4+XuX0uLg4AP/34ZX/d506dVQefXmXD3n4cHx8PDw8PN5bR2hoKC5cuICBAweKHt4cGxv7zgBbUB3z58/Hs2fPsG7dOvj7+yvN27p1K4KDg99bf1Hkb9vLly9hbGyssf5CQ0PRuXPnQi1b1h8UXdj9NX/622JjY5XavUndMSjKvvmh8o+mF3RE8k3527Zo0SKMGzfug/rX19dHYGAgAgMD8fjxY5w6dQrr16/Hhg0b8PDhQ8Xd3URUvPgIHCoWI0eOVBxBXL16NfLy8jB8+PAC21++fBmpqami6SdOnADwOhQCgKGhIWrUqIEbN27g+fPnGq9b1YfP/fv38fjxY7i4uCg+HO/evQsA6Nat2wf18SGKo8/CaNy4MQDg9OnTH7yMlpYWAOWjTEXpr7z40P3VyMgIlStXRkxMjOL0+puOHz8O4PXp68J417iXxH6U/94eOXLknZekvNlW3f3A0dERffv2RVhYGNzc3HDq1Kli+d0nIjGGRCoWVapUQZs2bbB3716sWrUKpqamosfYvOnly5eYOXOm0rSLFy9i8+bNMDExQdeuXRXTv/nmG2RnZ2Pw4MEqH43y4sWLQh9lzLdo0SKl6yzz8vIwceJE5OXlYdCgQYrp+Y8byf+Qz3f//n2Vj0z5EAX1GRYWhjVr1qjVZ2GMGTMGMpkMEyZMwO3bt0Xzs7OzRR/05ubmAF4/3uhtvr6+qFy5MpYuXYq//vpL5TrPnTuHjIwMDVRfsgqzvw4ePBiCIGDixIlKoS4pKQmzZs1StCmMd417ceybb6tXrx68vb1x+fJlzJs3TzT/2bNnyMrKAvD6OsfPPvsMu3btwtq1a1X2d/36dSQkJAB4fY3i+fPnRW3S09ORmpoKLS0tlY/vISLN428aFZuRI0fi8OHDSEpKwrhx46Cvr19g2+bNm2PNmjU4f/48mjZtqnjuXF5eHlauXKl0+nPw4MG4dOkSli1bhsqVK6Ndu3ZwcnLC8+fP8eDBA5w6dQqDBg3CihUrCl1zs2bNULt2bfTs2RMmJiYICwvDtWvXUK9ePXz//feKdl9++SWqVKmCBQsWIDIyEnXq1EF0dDT279+PL774AtHR0YVe96hRo7Bu3Tr4+fmhW7dusLe3R2RkJA4dOgQ/Pz9s37690H0WRtWqVbF27VoMHjwYNWrUQPv27eHu7g65XI7o6GicPn0alpaWSjcFtWrVCjt37sRXX32FDh06QF9fH87Ozujfvz9kMhl27dqFdu3a4YsvvoC3tzdq164NAwMDPH78GOHh4bh//z5iY2PL3Q0Jhdlfv/vuOxw8eBChoaGoVasWOnbsqHhOYkJCAr7//ns0a9asUOt/17gXx76pyqZNm9CiRQt8//332LFjB3x8fCAIAu7cuYPDhw8jKipKEVi3bNmCli1bYsiQIVi8eDEaNWoEU1NTPHnyBBEREYiMjMS5c+dgZWWFmJgYNG7cGNWqVUPdunXh6OiIlJQU7N+/H3FxcRgzZoxGLocgog9QindWUxmG//94m3dxdnZW+QicfDk5OYKFhYUAQLhx44bKNvmP+xg4cKBw8+ZNoXPnzoKpqamgr68veHt7C4cOHSpw/fv27RO++OILwdLSUpDJZIK1tbXQoEED4YcffhBu3rwp2h4fH58C+8p/dMm9e/eEefPmCR4eHoKurq5gZ2cnjB8/XumxL/mio6OFPn36CHZ2doKenp5QvXp1ISgoSJDL5SrXl/+YkePHjxdYxz///CN8/vnngqmpqWBoaCg0bdpU2L17t3D8+HHFcyvf9K5HoeRvk6r35121RERECAMHDhScnJwEHR0dwczMTKhRo4bwv//9T/QYmZycHCEgIECoVKmSoK2trXK74+PjhUmTJgk1atQQ9PX1hQoVKghVqlQRunXrJmzcuFHp2YEfMkYFed8jcN61zIeOkbr7a2ZmpvDzzz8LNWrUEPT09BTv7ZYtW0Rt31xHQd437prcN99VT1JSkvD9998L7u7ugq6urmBiYiLUqlVLmDJlipCenq7UNiUlRfj555+FunXrChUqVBD09PQEFxcXoWPHjsLKlSsVj8Z68eKFMGPGDOHzzz8X7OzsBB0dHcHGxkbw8fERtmzZwsfiEJUgiSC854ISIjXdu3cPbm5uaNasGU6dOqWyzcOHD1GpUiWVF9mXJH9/fwQHB+PBgwdF+go4+riVlf2ViKgk8JpEKja//vorBEHAmDFjSrsUIiIiKiRek0ga9ejRI2zcuBF37tzBxo0bUadOHXTv3r20yyIiIqJCYkgkjXrw4AGmTp2KChUqoF27dli+fPl7v5GBiIiIyh5ek0hEREREIjzEQ0REREQiDIlEREREJMKQSEREREQiDIlEREREJMK7m0nkxYsXyMnJKe0yyj1LS0skJiaWdhkfBY6lZnAcNYdjqTkcy6LT1taGmZmZ5vvVeI9U7uXk5EAul5d2GeWaRCIB8Hos+QCBouFYagbHUXM4lprDsSzbeLqZiIiIiEQYEomIiIhIhCGRiIiIiEQYEomIiIhIhCGRiIiIiEQYEomIiIhIhCGRiIiIiEQYEomIiIhIhCGRiIiIiEQYEomIiIhIhCGRiIiIiEQYEomIiIhIhCGRiIiIiEQYEomIiIhIRCIIglDaRVDZ0mf1BUTFpZV2GURERMVm/5CqpV2CxshkMlhaWmq8Xx5JJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiIiEmFIJCIiIiIRhkQiIiL6pK1fvx6NGzeGq6sr2rdvj/PnzxfY9uzZs7C3txf9uXv3rsr2oaGhsLe3x+DBg4ur/GLDkFiMTpw4AX9//xJZ19KlSzF37twSWRcREdHHIjQ0FIGBgRg3bhzCwsLQsGFD9OvXDzExMe9c7tSpU7hy5YriT6VKlURtnjx5gpkzZ6JRo0bFVX6xYkgsZxISEuDn54eHDx+WdilERETl3urVq9GrVy/06dMHbm5umDlzJuzs7LBhw4Z3LmdhYQErKyvFHy0tLaX5ubm5GDNmDL777js4OTkV5yYUG4ZEIiIi+iRlZ2cjIiICPj4+StN9fHxw8eLFdy7brl071KlTB35+fvjnn39E8xcsWABzc3P07t1bozWXJO3SLkBdgYGBcHJyglQqxcmTJ6GtrY2ePXuiWbNmWLt2Lf7991+YmJhg8ODBqFOnDvLy8rBy5UpERkYiOTkZFhYWaNeuHTp27Ajg9Y4yefJkeHh4YPjw4QBeH7WbOHEi+vfvj9atW7+3phMnTmD79u1ITU1FrVq1ULVqVVGbixcvYufOnXjy5AnMzMzg4+ODr776SvE/ED8/PwwdOhQXL17EjRs3YGpqin79+qFJkyYAgDFjxgAAvv/+ewBA9erVERgYqOh/79692L9/P3JycuDt7Q1/f39oa5fbt5mIiKjYPH/+HLm5ubCwsFCabmFhgYSEBJXLWFlZYe7cufDy8sKrV6/w559/omfPnggJCUHjxo0BAOHh4di6dSuOHDlS7NtQnMp1ejh58iQ6d+6M2bNn4+zZs1i9ejXCw8PRoEEDdO3aFQcOHMCSJUuwbNkyaGlpwdzcHBMmTICxsTFu3bqFVatWwdTUFN7e3tDR0cG4ceMwZcoU1KlTB/Xr18fvv/+OGjVqfFBAvHPnDpYvX47evXujYcOGuHr1Knbu3KnU5urVq/j9998xaNAgVKtWDfHx8Vi5ciUAoEePHop227dvR58+feDv749Tp05h0aJFcHR0hIODA2bPno0pU6Zg6tSpcHR0VAqAN27cgJmZGaZPn464uDgsXLgQLi4uBdYvl8shl8sVryUSCfT19Qv1HhAREZVHEokEEokEACCVShU/q5r/Jjc3N7i5uSleN2jQAE+fPsWKFSvQpEkTpKWlYezYsZg3bx7Mzc0Vfb35d3FsS3Eo1yHR2dkZ3bp1AwB07doVe/bsgZGRkSIUde/eHYcPH8ajR4/g7u4OPz8/xbJWVla4desWzp07B29vbwCAi4sLevXqpTjiGB8fj4kTJ35QLX/99Rdq1aqFLl26AADs7Oxw+/ZtXL16VdFm9+7d6NKlC1q0aAEAsLa2Rs+ePbF582alkNi4cWO0atUKANCrVy9cv34dhw4dwtChQ2FsbAwAMDIygqmpqVINhoaGGDJkCKRSKezt7VGnTh1ERkYWGBJ3796NkJAQxetKlSohKCjog7aXiIioPLO1tYW5uTm0tLSQk5MDW1tbxbzMzEzY29srTXuXFi1aYNOmTbC1tcXVq1fx+PFjDBw4UDE/Ly8PAODo6Ihbt26hcuXKmt2YYlKuQ+KbF4JKpVIYGRkpTTMxMQEApKSkAAAOHz6MY8eOITExEdnZ2cjJyYGLi4tSn506dUJ4eDgOHTqEKVOmKELZ+8TExKBhw4ZK09zd3ZVC4v3793H37l3s2rVLMS0vLw9yuRyvXr2Crq6uYrk3ubm54dGjR++twcHBAVLp/11mamZmhujo6ALbd+3aFZ06dVK8Lq7/iRAREZU1sbGxAAAvLy+EhoYqThUDwMGDB9GuXTtFm/c5d+4czM3NERsbCxMTExw7dkxpflBQENLT0zFz5kxoa2t/cL8fSiaTiU6Za0K5DolvX2snkUiU7i7KDz15eXk4e/YsgoODMWDAALi7u0NfXx979+7FnTt3lPpISUnB06dPIZVKERsbi9q1a39QLYIgvLdNXl4e/Pz8VN4KL5PJPmg97/L2nVUSieSddclkMo2sl4iIqLzJ/3wcNmwYxo8fDy8vL9SrVw+bNm1CTEwM+vfvD0EQMGfOHMTGxmLx4sUAXt8N7ejoCHd3d8jlcuzatQsHDhzA6tWrIQgCdHV14eHhobSu/ANO+dM/JDOosy2aVq5DYmFERUXBw8MD7dq1U0yLj48XtVu+fDmcnJzQqlUrLF++HJ6ennBwcHhv/w4ODqLAefv2baXXrq6uePr0KWxsbN7Z1507d5TutLpz547i+Uv5wTj/0DURERGpz9fXFy9evMCCBQuQkJAADw8PbNy4UfHZHx8fj6dPnyray+VyzJo1C3FxcdDT04O7uzs2bNiguEzsY/LJhEQbGxucPHkSV69ehZWVFU6dOoW7d+/CyspK0ebQoUO4ffs2fv31V1hYWODKlStYvHgxZs+e/d47hDt06ICpU6ciNDQUDRo0QEREBK5du6bUplu3bggKCoK5uTmaNGkCiUSC6OhoREdHo1evXop2586dg6urK6pWrYozZ87g7t27GDlyJIDXp9B1dHRw9epVVKxYETo6OjAwMNDgSBEREX1a/P39C/zyi4ULFyq9HjVqFEaNGlWo/t/uo7z4ZJ6T2KZNGzRq1AgLFy7EDz/8gLS0NKWjijExMdi0aROGDBmiOK8/ZMgQpKenY9u2be/t393dHcOHD8ehQ4fw/fff49q1a/jqq6+U2tSuXRuTJk3C9evXERAQgB9++AH79+8XXUfg5+eHs2fPYuLEiTh58iTGjRun+B+NlpYWBg0ahCNHjmD48OH8lhUiIiIqFhKhuE5kk1r8/Pzw3XffiW6CKUl9Vl9AVFxaqa2fiIiouO0fIn6WcXklk8lgaWmp8X4/mSOJRERERPThPplrEotq9uzZuHnzpsp5Xbt2FZ1aJiIiIirPGBI/0IgRI5Cdna1ynqGhocbWs2PHDo31RURERKQuhsQPVLFixdIugYiIiKjE8JpEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhKRCIIglHYRVLYkJiZCLpeXdhnlmkQiga2tLWJjY8FfsaLhWGoGx1FzOJaaw7HUDJlMBktLS433yyOJRERERCTCkEhEREREIgyJRERERCTCkEhEREREIgyJRERERCTCkEhEREREIgyJRERERCTCkEhEREREIgyJRERERCTCkEhEREREIgyJRERERCTCkEhEREREIgyJRERERCSiXdoFUNkzfs8DRMWllXYZH4Gbpbbm/UOqltq6iYjo48AjiUREREQkwpBIRERERCIMiUREREQkwpBIRERERCIMiUREREQkwpBIRERERCIMiUREREQkwpBIRERERCIMiUREREQkwpBIRERERCIMiUREREQkwpBIRERERCIMiUREREQkwpBIRERERCIMiUREREQkwpBIRERERCIMiUREREQkwpBIRERERCIMiUREREQkwpBIRERERCIMiUREREQkwpBI9JFbv349GjduDFdXV7Rv3x7nz58vsG18fDxGjx6Nzz77DA4ODpg2bdo7+w4NDYW9vT0GDx6s6bKJiKiUMSQWYPTo0Thw4EBpl0FUJKGhoQgMDMS4ceMQFhaGhg0bol+/foiJiVHZPjs7G+bm5hg3bhyqV6/+zr6fPHmCmTNnolGjRsVROhERlbJPPiSeOHEC/v7+oulz5sxB69ati339DKNUnFavXo1evXqhT58+cHNzw8yZM2FnZ4cNGzaobO/o6IiZM2eiR48eMDY2LrDf3NxcjBkzBt999x2cnJyKq3wiIipFn3xILIixsTF0dXVLu4wPlpOTU9olUBmTnZ2NiIgI+Pj4KE338fHBxYsXi9T3ggULYG5ujt69exepHyIiKru0S7uAfIGBgXBycoKOjg6OHj0KbW1ttGnTBn5+fu9dNiMjAxs3bkR4eDjkcjlcXV0xcOBAuLi4AAAePnyI4OBg3Lt3DxKJBDY2Nvjf//6HrKwsLFu2DAAU6+nevTv8/PwwevRodOzYEV988YVi/rBhw3Dp0iVERkbC0tISI0eOhLGxMVasWIF79+7ByckJY8eOhY2NDQAgLi4OGzZswJ07d5CVlQUHBwf07t0bXl5eim1OTExEcHAwgoODAQA7duwAAPz777/YsWMH4uLiYGZmhvbt2+PLL79UbPPo0aPRsmVLxMXF4cKFC2jQoAFGjBiB4OBgnD9/Hunp6TA1NUXr1q3RtWtXDbxDVN48f/4cubm5sLCwUJpuYWGBhIQEtfsNDw/H1q1bceTIkaKWSEREZViZCYkAcPLkSXTq1AmzZ8/G7du3sWzZMlStWlURqlQRBAFz5syBoaEhAgICYGBggCNHjmDWrFlYtGgRDA0N8fvvv8PFxQVDhw6FVCrFw4cPoaWlBQ8PD/j7+2P79u1YtGgRAEBPT6/Adf35558YMGAABgwYgM2bN2PRokWwtrZGly5dYGFhgeXLl2Pt2rWYMmUKACArKwt16tRBr169IJPJcPLkSQQFBWHRokWwsLDAd999h4kTJ6JVq1ZKp7bv37+PBQsWoEePHvD29sbt27exZs0aGBkZoUWLFop2e/fuRbdu3dCtWzcAwF9//YWLFy9iwoQJsLCwwLNnz5CUlFTg9sjlcsjlcsVriUQCfX39d79JVC5IJBJIJBIAgFQqVfysav6H9gMAaWlpGDt2LObNmwdzc3NFmzf/1rTi7v9TwXHUHI6l5nAsNaO4xq9MhURnZ2f06NEDAGBra4tDhw7h+vXr7wyJN27cQHR0NNasWQOZTAYAGDBgAMLDw/Hvv/+idevWSEpKwpdffgl7e3tF3/kMDAwgkUhgamr63vpatGgBb29vAICvry9+/PFHdOvWDbVr1wYAdOzYUXFkEgBcXFwURzMBoFevXrhw4QIuXryI9u3bw9DQEFKpFPr6+krr379/Pzw9PdG9e3cAgJ2dHZ48eYK9e/cqhcSaNWuic+fOitdJSUmwtbVF1apVIZFIYGlp+c7t2b17N0JCQhSvK1WqhKCgoPeOA5V9tra2MDc3h5aWFnJycpT2+czMTNjb2ytNU0VHRwcVKlRQanf16lU8fvwYAwcOVEzLy8sD8Pp6xlu3bqFy5coa3prX8o/QU9FwHDWHY6k5HMuyqUyFxLcvgDczM8PLly/fucz9+/eRlZUlegRHdnY24uLiAABffPEFVq5cidOnT8PT0xONGzdWa4d0dnZW/Jwf6t6s2cTEBHK5HBkZGTAwMEBWVhZCQkJw6dIlvHjxArm5ucjOzn7n0T0AiImJQf369ZWmeXh44MCBA8jLy4NU+vpS0rc/jFu0aIGffvoJX3/9NWrVqoV69eqhVq1aBa6na9eu6NSpk+I1/yf38YiNjQUAeHl5ITQ0FI0bN1bMO3jwINq1a6doU5Ds7Gykp6crtTMxMcGxY8eU2gUFBSE9PR0zZ86Etrb2e/strPxLROLi4iAIgkb7/pRwHDWHY6k5HEvNkMlkokuLNKFMhURtbXE579tp8vLyYGZmhsDAQNE8AwMDAK+vJ2zWrBkuX76Mq1evYseOHfj666/RsGHDQtWnpaX1zprzQ1Z+zZs2bcK1a9fQv39/2NjYQEdHB7/99tt7bzIRBEEU2FSNw9s31ri6umLJkiW4evUqIiIisGDBAnh6euLbb79VuR6ZTKY4+kofl/z9ZdiwYRg/fjy8vLxQr149bNq0CTExMejfv7/iUo3Y2FgsXrxYsWxkZCQAID09Hc+ePcP169eho6MDd3d36OrqwsPDQ2ld+XdB508vrn/oBUHgh4gGcBw1h2OpORzLoimusVMrJGZnZ+PUqVOoWrUqHBwcNF1Tobi6uiI5ORlSqRRWVlYFtrOzs4OdnR06deqEhQsX4vjx42jYsCG0tbUVp8s07ebNm/Dx8VGE0aysLCQmJiq1UbV+BwcHREVFKU27ffs27OzsFEcRC2JgYABvb294e3ujcePGmD17NtLS0mBoaKiBLaLyxtfXFy9evMCCBQuQkJAADw8PbNy4UfF7Gx8fj6dPnyot065dO8XPERER2L17NxwcHN75EG4iIvr4qBUSdXR0sG7dOvzwww+arqfQPD094e7ujl9//RV9+/aFnZ0dXrx4gStXrqBBgwZwdHTExo0b0bhxY1hZWeHZs2e4d++e4gHAlpaWyMrKwvXr1+Hs7AxdXV2NPfrGxsYGFy5cUJw63r59uyjtW1pa4ubNm2jatCm0tbVhbGyMTp06ISAgACEhIYobVw4dOoShQ4e+c3379++HmZkZXFxcIJFI8O+//8LU1FRxRJU+Tf7+/iqfBQoACxcuFE0r6EHbBVHVBxERlX9qn262srJCcnKyBktRj0QiQUBAALZu3Yrly5cjJSUFpqamqFatGkxMTCCVSpGamoolS5bg5cuXMDIyQqNGjRSPvPHw8ECbNm2wcOFCpKamKh6BowkDBw7E8uXL8eOPP8LIyAi+vr7IzMxUauPn54fVq1dj7NixkMvl2LFjB1xdXTFhwgTs2LEDf/75J8zMzODn56d004oqenp6CA0NRWxsLKRSKapUqYKAgID3Hn0kIiIieptEUPNE9pEjR3DkyBEEBgbySNVHps/qC4iKSyvtMqgI9g+pWtolaIxEIoGtrS1iY2N5zVIRcBw1h2OpORxLzZDJZO99ook61D6S+PjxY6SmpmL06NGoWbMmzMzMlOZLJBIMGjSoyAUSERERUclTOySGhYUpfr5w4YLKNpoIiadPn8aqVatUzrO0tMT8+fOLvA4iIiIiUqZ2SNy+fbsm6yhQ/fr14ebmpnKeqkfSEBEREVHRlannJKqir6/Pr4ojIiIiKmFFDolXr17Ff//9h5SUFHTv3h0WFha4e/curKysFA/ZJSIiIqLyRe2Q+OrVK8ydO1fx7QwA0LZtW1hYWGDfvn0wNzfHgAEDNFIkEREREZUstR+gt3XrVty/fx/ffvstgoODlebVqlUL169fL3JxRERERFQ61D6S+O+//6Jnz55o2LCh6GvlLCwskJSUVOTiiIiIiKh0qH0kMSUlpcDvbZZIJMjOzla7KCIiIiIqXWqHxIoVKyI6OlrlvEePHsHKykrtooiIiIiodKkdEhs2bIjdu3fjwYMHimkSiQSJiYk4cOAAmjRpopECiYiIiKjkqX1NYo8ePRAZGYkpU6bA0dERALBs2TLEx8fDzs4OXbp00VSNRERERFTC1A6J+vr6+Omnn/DXX3/h8uXLsLGxga6uLrp06YIvvvgCOjo6mqyTiIiIiEpQkR6mraOjgy5duvCoIREREdFHRu1rEseMGYOHDx+qnBcdHY0xY8ao2zURERERlTK1Q2JiYiJycnJUzpPL5UhMTFS7KCIiIiIqXWqHxHeJj4+Hvr5+cXRNRERERCWgUNcknjhxAidPnlS8XrNmjSgMZmdn49GjR6hevbpmKiQiIiKiEleokJidnY2UlBTF6/T0dMjlcqU2MpkM3t7e8PPz00yFRERERFTiChUS27Zti7Zt2wIARo8ejW+//RYuLi7FURcRERERlSK1H4GzdOlSTdZBRERERGVIkZ6TKJfLceLECdy4cQOpqakYOnQobG1tER4eDicnJ1hbW2uqTipBi7pUEl1GQIUjkUhga2uL2NhYCIJQ2uUQEREVmtohMSUlBTNmzMCTJ09gamqK5ORkZGZmAgDCw8Nx7do1DB06VGOFEhEREVHJUfsROJs2bUJGRgbmzJmDZcuWKc2rUaMG/vvvvyIXR0RERESlQ+2QePnyZfj5+cHV1RUSiURpnrm5OZ49e1bk4oiIiIiodKgdEjMzM2FpaalyXk5ODvLy8tQuioiIiIhKl9oh0crKCrdv31Y57+7du7Czs1O7KCIiIiIqXWqHxGbNmiE0NBTh4eGKuzclEgnu3r2LgwcP4rPPPtNYkURERERUstS+u9nX1xe3bt3CvHnzUKFCBQDAzz//jNTUVNSuXRsdO3bUWJFEREREVLLUDona2toICAjA2bNncfnyZbx8+RJGRkaoV68evL29IZWqfZCSiIiIiEpZkR6mLZFI0LRpUzRt2lRT9RARERFRGcDDfUREREQkovaRxLy8PBw8eBBnzpxBYmKiyq9xCw4OLlJxRERERFQ61A6Jmzdvxv79++Hi4gIvLy9oaxfpzDURERERlSFqJ7szZ87A19cXffr00WQ9RERERFQGqB0Ss7Oz4eXlpclaqIwYv+cBouLSSruMj8DNEl3b/iFVS3R9RET0cVP7xhUvLy/cuXNHk7UQERERURmh9pHEQYMG4ZdffoGuri7q1q0LQ0NDURtV04iIiIio7FM7JBoYGMDOzg7BwcEF3sW8fft2tQsjIiIiotKjdkhctWoVzp07hwYNGsDe3p53NxMRERF9RNROduHh4ejduzc6d+6syXqIiIiIqAxQ+8YVbW1tVKpUSZO1EBEREVEZoXZIbNiwIa5du6bJWoiIiIiojFD7dHPTpk2xcuVK5OTkFHh3s6ura5GKIyIiIqLSoXZInDVrFgDg4MGDOHjwoMo2vLuZiIiIqHxSOySOHDlSk3UQERERURmidkhs0aKFBssgIiIiorJE7RtXiIiIiOjjVaQnYKelpeHMmTN48uQJsrOzleZJJBKekiYiIiIqp9QOiUlJSQgICMCrV6/w6tUrGBsbIy0tDXl5eahQoQIMDAw0WScRERERlSC1Tzdv3rwZDg4OWL16NQAgICAAGzduxKBBgyCTyTB58mSNFUlEREREJUvtkHj79m20bdsWMplMMU1bWxvt27dHy5YtsWnTJo0USEREREQlT+2Q+PLlS5iZmUEqlUIqlSIjI0Mxr3r16oiKitJIgURERERU8tQOiSYmJkhLSwMAWFpa4v79+4p5iYmJ0NLSKnp1RERERFQq1L5xxc3NDQ8ePED9+vXRsGFDhISEQC6XQ1tbG3v37kWNGjU0WScRERERlSC1Q2Lnzp2RkJAAAOjevTtiYmKwY8cOAEC1atUwaNAgzVRIRERERCVO7ZDo6uoKV1dXAICenh4mTZqEjIwMSCQS6Ovra6xAIiIiIip5al2TmJ2djeHDh+PixYtK0w0MDBgQicqA9evXo3HjxnB1dUX79u1x/vz5AtvGx8dj9OjR+Oyzz+Dg4IBp06aJ2vz111/o0KEDqlWrhipVqqBNmzYICQkpzk0gIqJSplZI1NHRQXZ2NvT09DRdDwE4ceIE/P3939lmx44dmDhxYskUROVKaGgoAgMDMW7cOISFhaFhw4bo168fYmJiVLbPzs6Gubk5xo0bh+rVq6tsY2pqinHjxmHv3r34+++/0bNnT3zzzTc4ceJEMW4JERGVJrXvbvb09ERERIQma6FC6Ny5s8ojPkSrV69Gr1690KdPH7i5uWHmzJmws7PDhg0bVLZ3dHTEzJkz0aNHDxgbG6ts4+3tjQ4dOsDNzQ0uLi4YOnQoqlWrhgsXLhTnphARUSlSOyR27doVZ8+eRUhICKKjo5Gamoq0tDSlP1R89PT0YGRkVNplUBmTnZ2NiIgI+Pj4KE338fERXR6iLkEQcPr0ady7dw+NGzfWSJ9ERFT2qH3jSv7X7u3cuRM7d+5U2Wb79u3qdl+iAgMD4eTkBKlUipMnT0JbWxs9e/ZEs2bNsHbtWvz7778wMTHB4MGDUadOHeTl5WHlypWIjIxEcnIyLCws0K5dO3Ts2BHA6w/qyZMnw8PDA8OHDwcAJCQkYOLEiejfvz9at279QXVduHABmzdvRlJSEqpWrYqRI0fCwsICwOvTzeHh4fj1118BAEuXLkV6ejqqVq2K/fv3IycnB97e3vD394e2ttpvM5Uzz58/R25urmI/yWdhYaF4GoG6UlJSUK9ePWRnZ0NLSwuzZ89G8+bNi9QnERGVXWqnh27dukEikWiyllJ18uRJdO7cGbNnz8bZs2exevVqhIeHo0GDBujatSsOHDiAJUuWYNmyZdDS0oK5uTkmTJgAY2Nj3Lp1C6tWrYKpqSm8vb2ho6ODcePGYcqUKahTpw7q16+P33//HTVq1PjggPjq1Svs3r0bo0ePhra2NtasWYNFixZh1qxZBS5z48YNmJmZYfr06YiLi8PChQvh4uJS4DrlcjnkcrniNe9ML98kEonid1IqlYp+P9+c/6H9vMnIyAhHjhxBeno6zpw5gxkzZsDZ2Rne3t6a2YB31PPm36QejqPmcCw1h2OpGcU1fmqHRD8/P03WUeqcnZ3RrVs3AK9Ppe/ZswdGRkaKgNW9e3ccPnwYjx49gru7u9L2W1lZ4datWzh37pziA9PFxQW9evVSHHGMj48v1I0mubm5GDx4MNzc3AAAo0ePxoQJE3D37l1UqVJF5TKGhoYYMmQIpFIp7O3tUadOHURGRhYYEnfv3q10h2qlSpUQFBT0wTVS2WJrawtzc3NoaWkhJycHtra2inmZmZmwt7dXmqaKjo4OKlSoUGA7e3t7AECbNm0QExODVatWKX5vipuNjU2JrOdjx3HUHI6l5nAsyyaeh/z/nJycFD9LpVIYGRkpTTMxMQHw+pQbABw+fBjHjh1DYmIisrOzkZOTAxcXF6U+O3XqhPDwcBw6dAhTpkwp8KYAVbS0tFC5cmXFa3t7e1SoUAFPnjwpMCQ6ODhAKv2/y0zNzMwQHR1d4Dq6du2KTp06KV7zf3LlW2xsLADAy8sLoaGhStcLHjx4EO3atVO0KUh2djbS09Pf2w4A0tPTkZqa+kFti0IikcDGxgZxcXEQBKFY1/Ux4zhqDsdScziWmiGTyUSXGWlCkUJiXl4erly5gpiYGGRnZ4vmd+/evSjdl6i3r9uTSCRK3z+dH6Dy8vJw9uxZBAcHY8CAAXB3d4e+vj727t2LO3fuKPWRkpKCp0+fQiqVIjY2FrVr1y5yne8Kcm9/X7ZEInnnL51MJoNMJityTVQ25L/Xw4YNw/jx4+Hl5YV69eph06ZNiImJQf/+/SEIAubMmYPY2FgsXrxYsWxkZCSA18Hv2bNnuH79OnR0dODu7g4A+P3331GrVi04OztDLpfj6NGjCAkJwZw5c0rsH3ZBEPghogEcR83hWGoOx7Joimvs1A6JqampmDZtGp4+fVpgm/IUEgsjKioKHh4eaNeunWJafHy8qN3y5cvh5OSEVq1aYfny5fD09ISDg8MHrSM3Nxf3799XHDV8+vQp0tPTFaf7iAri6+uLFy9eYMGCBUhISICHhwc2btyo2Pfi4+NFv7dv7ssRERHYvXs3HBwcFA/hzsjIQEBAAOLi4qCnp4fKlStj8eLF8PX1LbkNIyKiEqV2SNy6dSt0dHSwdOlSjB49Gj///DMMDQ1x5MgRXL58GVOnTtVknWWKjY0NTp48iatXr8LKygqnTp3C3bt3YWVlpWhz6NAh3L59G7/++issLCxw5coVLF68GLNnz/6gu421tLSwdu1aDBo0SPGzm5tbgaeaid7k7+9f4APZFy5cKJpW0IO2802aNAmTJk3SQGVERFReqP2cxMjISHzxxReoWLHi646kUtjY2KB///7w9PQs8MG9H4M2bdqgUaNGWLhwIX744QekpaUpHYmJiYnBpk2bMGTIEMU1AkOGDEF6ejq2bdv2QevQ1dWFr68vFi9ejB9//BE6Ojr4+uuvi2NziIiIiEQkgponsvv27YupU6eiatWq6NWrF6ZNm6b4Sq9r165h8eLF+OOPPzRaLJWMPqsvICqOD0Mvb/YPqVraJRQLiUQCW1tbxMbG8pqlIuA4ag7HUnM4lpohk8lgaWmp8X7VPpJobGyMjIwMAK/von38+LFiXlpaGnJzc4teHRERERGVCrWvSaxUqRIeP36MunXrok6dOggJCYG+vj60tbWxdetWxfP9SGz27Nm4efOmynldu3bFV199VcIVERERESlTOyS2b99ecUdvr169cOfOHSxduhQAYG1tjUGDBmmmwo/QiBEjVD4yCHj9QGwiIiKi0qZ2SPTy8lL8bGxsjLlz5ypOOdvb24ue2Uf/J/9mHyIiIqKySmPfuCKRSJS+oYSIiIiIyq8ihcSMjAyEhYXhxo0bSE1NhZGREWrUqIG2bduiQoUKmqqRiIiIiEqY2iExISEBM2bMQFJSEiwsLGBqaorY2Fhcv34dR44cwfTp02Ftba3JWomIiIiohKgdEtetW4fs7GzMmjVL8f2uAHDr1i3MmzcP69ev5zc0EBEREZVTRfrGld69eysFRADw8PBAr169EBkZWeTiiIiIiKh0qB0SZTIZzM3NVc6zsLCATCZTuygiIiIiKl1qh8T69evj3LlzKuedO3cOdevWVbsoIiIiIipdal+T2KxZM6xYsQLz589Hs2bNYGpqiuTkZJw+fRr379/HiBEjcP/+fUV7V1dXjRRMRERERMVP7ZD4888/AwCePXuG8+fPi+b/9NNPSq+3b9+u7qqIiIiIqISpHRJHjhypyTqIiIiIqAxRKyTm5eXB3d0dJiYmfGg2ERER0UdIrRtXBEHAN998g9u3b2u6HiIiIiIqA9QKiVpaWjA1NYUgCJquh4iIiIjKALUfgePt7Y2TJ09qshYiIiIiKiPUvnHFxcUF586dw4wZM9CoUSOYmppCIpEotWnUqFGRCyQiIiKikqd2SFy6dCkA4Pnz5/jvv/9UtuFjb4iIiIjKJ7VD4vTp0zVZBxERERGVIWqHxOrVq2uyDipDFnWpBLlcXtpllGsSiQS2traIjY3lDV5ERFQuqR0S82VkZOD27dtITU1FnTp1YGhoqIm6iIiIiKgUFSkkhoSEIDQ0FNnZ2QCAOXPmwNDQEDNnzoSXlxe6dOmiiRqJiIiIqISp/QicsLAwhISE4PPPP8fkyZOV5tWtWxeXL18ucnFEREREVDrUPpJ46NAhdOrUCf369UNeXp7SvPxrsYiIiIiofFL7SGJCQgJq1aqlcp6+vj4yMjLULoqIiIiISpfaIdHAwAAvX75UOS8hIQHGxsZqF0VEREREpUvtkFizZk2EhoYiKytLMU0ikSA3NxdHjhwp8CgjEREREZV9al+T2LNnTwQEBOCbb75Bw4YNAby+TvHhw4dISkrChAkTNFYkEREREZUstY8k2tjYYNasWbC3t0dYWBgA4NSpUzAyMsKMGTNgYWGhsSKJiIiIqGQV6TmJDg4O+OGHHyCXy5GamgpDQ0Po6OhoqjYiIiIiKiVqH0l8k7a2NvT19SGTyTTRHRERERGVsiIdSbxz5w527NiB//77Dzk5OdDW1kb16tXRo0cPuLu7a6pGIiIiIiphah9JjIyMxPTp03H//n00bdoUvr6+aNq0Ke7fv4/AwEBcv35dk3USERERUQlS+0ji5s2bUalSJUydOhV6enqK6ZmZmZg5cya2bNmCOXPmaKRIKlnj9zxAVFxaaZfxEbiJA0OrlXYRREREalH7SGJ0dDQ6d+6sFBCB19+24uvri+jo6CIXR0RERESlQ+2QaGJiAolEorpTqZTfuEJERERUjqkdElu3bo0DBw4gJydHaXpOTg4OHDiA1q1bF7k4IiIiIiodal+TqK2tjcTERIwdOxYNGzaEqakpkpOTceHCBUilUshkMuzfv1/RvlOnThopmIiIiIiKX5FuXMl36NChd84HGBKJiIiIyhO1Q+KSJUs0WQcRERERlSFqh0RLS0tN1kFEREREZYjaN6788ssvuHr1qgZLISIiIqKyQu0jiTExMZgzZw5sbGzQrl07tGjRAgYGBpqsjYiIiIhKidoh8ffff8fly5cRFhaG4OBgbNu2Dc2aNUP79u3h5OSkyRqJiIiIqISpHRIBoG7duqhbty7i4uIQFhaGEydO4OjRo6hWrRrat2+Phg0bQipV+4w2EREREZWSIoXEfDY2Nhg4cCC6deuG+fPn48aNG7h58yYqVqyIzp07o3379gV+OwsRERERlT0aCYnPnj3DkSNHcPToUaSkpKB27drw9vZGeHg41q9fj6dPn2LIkCGaWBURERERlYAihcTIyEgcOnQIly5dgo6ODnx8fNChQwfY2toCAHx8fPDXX39h586dDIlERERE5YjaIXHChAl4+vQprKys0K9fP3z++ecq726uUqUKMjIyilQkEREREZUstUNixYoV0bdvX9SrV++d1xu6urry21mIiIiIyhm1Q+LUqVM/bAXa2vx2FiIiIqJyplAhccyYMR/cViKR4Pfffy90QURERERU+goVEh0cHETTrly5gqpVq0JfX19jRRERERFR6SpUSJw8ebLS69zcXPTp0wcDBw6Eq6urRgsjIiIiotJTpK9D4QOyiYiIiD5O/M48ohKyfv16NG7cGK6urmjfvj3Onz9fYNv4+HiMHj0an332GRwcHDBt2jSV7Q4cOIAWLVqgUqVKaNGiBQ4ePFhc5RMR0Sem1ENiYGAg1q9fX9plYMeOHZg4cWJpl0EfqdDQUAQGBmLcuHEICwtDw4YN0a9fP8TExKhsn52dDXNzc4wbNw7Vq1dX2ebixYsYOXIkunXrhiNHjqBbt24YMWIELl++XJybQkREn4hSD4llRefOnQs8WlPWLF26FHPnzi3tMqgQVq9ejV69eqFPnz5wc3PDzJkzYWdnhw0bNqhs7+joiJkzZ6JHjx4wNjZW2WbNmjVo3rw5xo4diypVqmDs2LFo1qwZ1qxZU5ybQkREn4hC3bhy//59pdd5eXkAgKdPn6psXxZuZsnJyYG29vs3U09PD3p6eiVQUcE+tFYqX7KzsxEREYHRo0crTffx8cHFixfV7vfSpUsYNmyYqE+GRCIi0oRCJZKAgACV0wt6HuL27dsLVUxOTg62bduG06dPIyMjA46Ojujbty9q1KgBAEhNTcUff/yBqKgopKWlwdraGl27dkWzZs0UfQQGBsLR0RHa2to4deoUHBwc4OfnhxkzZmDq1KnYvHkznjx5AhcXF4waNQp2dnYAXp9uDg8Px6+//grg9dG69PR0VK1aFfv370dOTg68vb3h7++vCHIvXrzAihUrEBkZCVNTU/Tu3Rtbt25Fx44d8cUXX7x3e/38/DB06FBcvXoV169fx5dffonu3btj5cqViIyMRHJyMiwsLNCuXTt07NhRUefJkycVywPA9OnTUaNGDTx//hzBwcGIiIiARCJB1apV4e/vDysrq0K9D6RZz58/R25uLiwsLJSmW1hYICEhQe1+ExMTRQ+qt7S0RGJiotp9EhER5StUSBw5cmRx1QEAWLZsGRITE/H111/DzMwMFy5cwOzZszFv3jzY2tpCLpfD1dUVXbp0gb6+Pi5fvowlS5bA2toabm5uin5OnjyJtm3bYtasWRAEAcnJyQCAbdu2YcCAATA2Nsbq1auxfPlyzJo1q8B6bty4ATMzM0yfPh1xcXFYuHAhXFxc0Lp1awDAkiVLkJqaisDAQGhpaWHDhg14+fJlobZ5586d6N27NwYOHAipVIq8vDyYm5tjwoQJMDY2xq1bt7Bq1SqYmprC29sbnTt3RkxMDDIzMzFq1CgAgKGhIV69eoUZM2agatWqmDFjBqRSKXbt2qUYP1VHKOVyOeRyueK1RCLh8y41TCKRKJ4CIJVKRU8EeHP+h/bzvukf2md5kb8tH9M2lQaOo+ZwLDWHY6kZxTV+hQqJLVq0KJYiACAuLg7//PMPli9fjooVKwJ4fZ3gtWvXcPz4cfTp0wcVK1ZE586dFct06NABV69exblz55RCoo2NDfr166d4nR8Se/XqpbgJwNfXF7/88guys7Oho6OjsiZDQ0MMGTIEUqkU9vb2qFOnDiIjI9G6dWvExMTg+vXrmDNnDipXrgwAGDFiBMaNG1eo7W7atClatmypNC3/CCEAWFlZ4datWzh37hy8vb2hp6cHHR0dyOVymJqaKtqdOnUKEokEI0aMUOwso0aNgr+/P27cuIFatWqJ1r17926EhIQoXleqVAlBQUGFqp/ezcbGBhUrVoSWlhZycnJga2urmJeZmQl7e3ulaaro6OigQoUKonY2NjZ49eqV0vTs7GxYW1u/t8/yyMbGprRL+ChwHDWHY6k5HMuyqcxcAPfgwQMIgoDx48crTc/JyYGhoSGA19dA7tmzB2fPnsXz588hl8uRk5MDXV1dpWUKuhbS2dlZ8bOZmRkAICUlRXQaMJ+DgwOkUqnSMtHR0QBeX4eppaWFSpUqKebb2NigQoUKH7rJAKAImG86fPgwjh07hsTERGRnZyMnJwcuLi7v7Of+/fuIi4vDgAEDlKbL5XLEx8erXKZr167o1KmT4jX/J6d5cXFxEAQBXl5eCA0NRePGjRXzDh48iHbt2iE2NvadfWRnZyM9PV3Urnbt2ti/fz969eqlmLZv3z7UqVPnvX2WJxKJBDY2NoqxJPVwHDWHY6k5HEvNkMlkBWaZoigzIVEQBEilUgQFBSkFMwCKG0r27duHAwcOYODAgXBycoKenh7Wr1+PnJwcle3fpqWlpfg5PxDl33zzvvb5y+TvxJramd8OuGfPnkVwcDAGDBgAd3d36OvrY+/evbhz5847+xEEAa6uriqPZBZ0d6xMJoNMJlO/eHovQRAgCAKGDRuG8ePHw8vLC/Xq1cOmTZsQExOD/v37QxAEzJkzB7GxsVi8eLFi2cjISABAeno6nj17huvXr0NHRwfu7u4AgCFDhqBbt25YsmQJ2rVrh7CwMJw+fRq7d+/+KP+xzR9LKhqOo+ZwLDWHY1k0xTV2ZSYkuri4IC8vDy9fvkS1atVUtrl58ybq16+P5s2bA3gd8GJjY2Fvb1+SpQIA7O3tkZubi4cPHyqOXMbFxSE9Pb1I/UZFRcHDwwPt2rVTTHv7SKC2trYo3FaqVAlnz56FsbExDAwMilQDaZ6vry9evHiBBQsWICEhAR4eHti4caPi+9Dj4+NFTwl4cx+IiIjA7t274eDgoHgId4MGDbBs2TLMnTsXv/76K5ydnbF8+XLUrVu35DaMiIg+WmUmJNrZ2aFZs2ZYsmQJBgwYgEqVKiElJQWRkZFwcnJC3bp1YWNjg/Pnz+PWrVuoUKEC9u/fj+Tk5FILiZ6enli5ciWGDRumuHFFR0enSKdtbWxscPLkSVy9ehVWVlY4deoU7t69q3SHsqWlJa5du4anT5/C0NAQBgYG+Oyzz7Bv3z78+uuv8PPzg7m5OZKSknD+/Hl07twZ5ubmmthsKgJ/f3/4+/urnLdw4ULRtIIetP2mTp06KV0yQEREpCllJiQCr2+02LVrFzZs2IDnz5/DyMgI7u7uiiMj3bt3R0JCAn7++Wfo6uqiVatWaNCgATIyMkql3jFjxmDFihWYPn264hE4T548KdIp3DZt2uDhw4dYuHAhJBIJmjZtinbt2uHKlSuKNq1bt8Z///2HyZMnIysrS/EInBkzZmDTpk2YN28esrKyULFiRdSsWZN3LBMREVGhSQReBKAxz549w8iRIzF16lR4enqWdjlq67P6AqLi0kq7jI/CgaHVeJ1NEUkkEtja2iI2NpZjWQQcR83hWGoOx1IzZDKZ6Lm5mlCmjiSWN5GRkcjKyoKTkxNevHiBTZs2wdLSssBrKomIiIjKC4bEIsjJycHWrVsRHx8PfX19uLu7Y9y4cdDW1sbp06exatUqlctZWlpi/vz5JVwtERER0YdjSCyC2rVro3bt2irn1a9fX+kB3296+9E6RERERGUNQ2Ix0dfX5w0jREREVG5J39+EiIiIiD41DIlEREREJMKQSEREREQiDIlEREREJMKQSEREREQiDIlEREREJMKQSEREREQiDIlEREREJMKQSEREREQiDIlEREREJMKQSEREREQiDIlEREREJMKQSEREREQiDIlEREREJMKQSEREREQiDIlEREREJMKQSEREREQi2qVdAJU9i7pUglwuL+0yyjWJRAJbW1vExsaWdilERERq4ZFEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhLRLu0CqOwZv+cBouLSSruMMm//kKqlXQIREVGx4ZFEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCQiIiIiEYZEIiIiIhJhSCTSgPXr16Nx48ZwdXVF+/btcf78+Xe2P3fuHNq3bw9XV1c0adIEGzZsUJovl8uxYMECeHt7w9XVFa1bt8bx48eLcxOIiIiUMCSqKTAwEOvXry/tMqgMCA0NRWBgIMaNG4ewsDA0bNgQffv2RXR0tMr20dHR6N+/Pxo2bIiwsDCMHTsW06ZNw4EDBxRt5s6di02bNmHWrFk4fvw4+vfvj6FDhyIyMrKkNouIiD5xDIlERbR69Wr06tULffr0gZubG2bOnAk7OzssX75cZfuNGzfC3t4eM2fOhJubG/r06YOePXtixYoVijZ//vknxo4di1atWsHZ2RkDBw6Ej48PVq5cWVKbRUREnziGRKIiyM7ORkREBHx8fJSm+/j44OzZsyqXuXTpkqh9ixYtEBERAblcDgB49eoVdHV1ldro6enhwoULGqyeiIioYNqlXcCHCAwMhJOTE3R0dHD06FFoa2ujTZs28PPzQ0JCAsaMGYO5c+fCxcUFAJCeno5BgwZh+vTpqFGjBm7cuIEZM2ZgypQp2LJlC2JiYuDu7o6vv/4a9+/fx4YNG/D8+XPUqVMHI0eOFH04f4icnBxs27YNp0+fRkZGBhwdHdG3b1/UqFEDAJCamoo//vgDUVFRSEtLg7W1Nbp27YpmzZoBAI4cOYKQkBAsX74cUun/ZfegoCBUqFABY8aMAQBcvHgRO3fuxJMnT2BmZgYfHx989dVX0NLSAgDs2LEDx48fx8uXL2FkZIRGjRph8ODBRRl+eofnz58jNzcXFhYWStMtLS1x+vRplcskJCSI2ltYWCAnJwfPnz+HtbU1WrRogVWrVqFRo0ZwcXHBmTNnEBYWhry8vGLbFiIiojeVi5AIACdPnkSnTp0we/Zs3L59G8uWLUPVqlVhY2PzwX3s3LkTgwcPhq6uLhYsWIAFCxZAJpNh3LhxyMrKwrx583Dw4EF06dKl0PUtW7YMiYmJ+Prrr2FmZoYLFy5g9uzZmDdvHmxtbSGXy+Hq6oouXbpAX18fly9fxpIlS2BtbQ03Nzc0adIE69atw40bN+Dp6QkASEtLw7Vr1zBp0iQAwNWrV/H7779j0KBBqFatGuLj4xWnH3v06IF///0XBw4cwNdffw1HR0ckJyfj4cOHBdYsl8sVR64AQCKRQF9fv9Db/qmSSCSQSCQAAKlUqvgZAARBUJr/9nJvt8+XP33WrFn47rvv4OPjA4lEAmdnZ/Ts2RPbt29XudzHLH97P7Xt1jSOo+ZwLDWHY6kZxTV+5SYkOjs7o0ePHgAAW1tbHDp0CNevXy9USOzVqxeqVq0KAGjZsiW2bNmC33//HdbW1gCARo0a4caNG4UOiXFxcfjnn3+wfPlyVKxYEQDQuXNnXLt2DcePH0efPn1QsWJFdO7cWbFMhw4dcPXqVZw7dw5ubm4wNDRE7dq1cebMGUVI/Pfff2FoaKh4vXv3bnTp0gUtWrQAAFhbW6Nnz57YvHkzevTogaSkJJiamsLT0xPa2tqwsLBAlSpVCqx79+7dCAkJUbyuVKkSgoKCCrXtnzJbW1uYm5tDS0sLOTk5sLW1VczLzMyEtbW1yv3T3t4e6enpSu3z8vKgra2N6tWrQyaTKfbxrKwsPHv2DHZ2dpg8eTJcXV2VlvuUFOZ3nQrGcdQcjqXmcCzLpnITEp2cnJRem5mZ4eXLl4Xqw9nZWfGziYkJdHV1FQERAExNTXHv3r1C1/bgwQMIgoDx48crTc/JyYGhoSGA1yFgz549OHv2LJ4/fw65XI6cnBylU9vNmjXDqlWrMHToUMhkMpw+fRre3t6K08/379/H3bt3sWvXLsUyeXl5kMvlePXqFRo3bowDBw5g7NixqFWrFurWrYt69eopTkW/rWvXrujUqZPiNf8nVzixsbEAAC8vL4SGhqJx48aKeQcPHkS3bt0QFxcHQRCUlvP09MTBgwcxefJkxbQ9e/agVq1aSEpKEq1HKpXi8ePH2LFjB7788kvFej8VEokENjY2KseSPhzHUXM4lprDsdQMmUwmuoxJE8pNSNTWFpcqCIIiQL25c+Xm5qrs482wJJFIVIYnda75yq8jKChI6XpC4PXNBgCwb98+HDhwAAMHDoSTkxP09PSwfv165OTkKNrWr18fK1euxOXLl1G5cmVERUVh4MCBSrX5+fmhUaNGohryd5BFixYhIiICERERWLNmDfbu3YvAwECV4yeTySCTyQq9vfRa/j43bNgwjB8/Hl5eXqhXrx42bdqEmJgYjBgxAoIgYPbs2YiNjcXixYsBAP3798e6deswffp09O3bF5cuXcLWrVuxdOlSRZ+XL19GXFwcatSogbi4OPz222/Iy8vDyJEjP9l/SAVB+GS3XZM4jprDsdQcjmXRFNfYlZuQWBBjY2MAwIsXL1CpUiUAeOd1eMXBxcUFeXl5ePnyJapVq6ayzc2bN1G/fn00b94cwOvAFxsbC3t7e0UbHR0dNGzYEKdPn0ZcXBxsbW3h6uqqmO/q6oqnT5++87C8jo4O6tevj/r166N9+/b4+uuvER0drdQPaZavry9evHiBBQsWICEhAR4eHti0aROcnZ0RGxuL+Ph4PH36VNHeyckJGzduRGBgIIKDg2FtbY2ZM2fiiy++ULR59eoV5s6di+joaBgYGKBly5ZYvHgxTExMSmMTiYjoE1TuQ6KOjg7c3NwQGhoKKysrpKSkYNu2bSVag52dHZo1a4YlS5ZgwIABqFSpElJSUhAZGQknJyfUrVsXNjY2OH/+PG7duoUKFSpg//79SE5OVgqJAPDZZ58hKCgIT548wWeffaY0r1u3bggKCoK5uTmaNGkCiUSC6OhoREdHo1evXjhx4gTy8vJQpUoV6Orq4tSpU9DR0YGlpWVJDscnyd/fH/7+/orXb566X7hwoah9kyZNEBYWVmB/TZo0wYkTJzRYIRERUeGU+5AIACNHjsTy5csxefJk2NnZoV+/fvjpp59KtIZRo0Zh165disfpGBkZwd3dHXXr1gUAdO/eHQkJCfj555+hq6uLVq1aoUGDBsjIyFDqp2bNmjA0NMTTp08Vj8fJV7t2bUyaNAl//vkn9u7dCy0tLdjb26Nly5YAAAMDA4SGhiI4OBh5eXlwcnLCpEmTYGRkVDKDQERERB8NicCLAOgtfVZfQFRcWmmXUebtH1K1wHkSiQS2traIjY3ldTZFxLHUDI6j5nAsNYdjqRkymaxYzhryG1eIiIiISOSjON2saUlJSZgwYUKB8xcsWFAst5oTERERlRUMiSqYmZnh119/fed8IiIioo8ZQ6IKWlpafPo7ERERfdJ4TSIRERERiTAkEhEREZEITzcTERGpISsrC69evSrtMsq9zMxMZGdnl3YZZZ5EIoGhoaHSlzUUN4ZEIiKiQoqLi0Nubi6MjIxK9EP7YySTySCXy0u7jDIvOzsbaWlpJfoFGTzdTEREVEgZGRkwMDBgQKQSo6OjU+IPHGdIJCIiIiIRhkQiIiIiEmFIJCIiIiWNGjXC6tWri9ymqLZv345q1aoV6zo0obzUWVgMiURERJ+ImJgYfPvtt6hbty5cXFzQsGFDTJs2Dc+fPy90X3/99Rf69eunsdpUhc7OnTvj9OnTGlvH2w4cOABHR0fExMSonN+8eXNMnTq12NZf1vHuZiIiIg3p9EdUia1r/5CqhWr/6NEjdO7cGa6urli6dCmcnJxw69Yt/PTTTzh27Bj27dtXqK+dNTc3L2zJhaavrw99ff1i679t27YwMzPDjh07MGHCBKV54eHhuHfvHpYvX15s6y/reCSRiIjoE/DDDz9AJpNhy5YtaNKkCezt7dGyZUts27YNcXFxCAoKUmqflpaG0aNHw83NDXXr1sXatWuV5r995C8lJQXff/89vLy84OHhgR49euDGjRtKyxw+fBgdOnSAq6sratasiaFDhwIAunfvjidPniAwMBD29vawt7cHoHwa9+7du7C3t8fdu3eV+ly5ciUaNWqkuPP39u3b6N+/P9zc3FCrVi2MHTu2wCOlMpkM3bp1w86dO0V3Dm/btg1eXl6oUaMGVq5ciVatWqFKlSqoX78+AgICkJ6eXuBYf/311xg8eLDStGnTpqF79+6K14IgYNmyZWjSpAkqV66M1q1bY//+/QX2WRoYEomIiD5yL168wIkTJzBw4EDRkTkrKyt89dVX2Ldvn1JQWrFiBapVq4ZDhw5hzJgxCAwMxKlTp1T2LwgCBgwYgISEBGzcuBEHDx6Ep6cnevbsiRcvXgAA/v77bwwdOhStWrVCWFgYtm/fDi8vLwDA6tWrYWtri++++w5XrlzBlStXROuoUqUKvLy8sGvXLqXpe/bsQZcuXSCRSBAfH49u3bqhevXqOHjwIDZv3oykpCQMHz68wLHp3bs3Hj16hHPnzimmZWRkYN++fejVqxcAQCqVYubMmTh27BgWLlyIf/75Bz/99NO7hvy9goKCsH37dsyZMwfHjh3DsGHDMG7cOKU6ShtPNxMREX3kHjx4AEEQ4ObmpnJ+lSpVkJycjGfPnsHCwgIA0KBBA4wZMwYAULlyZYSHh2P16tVo3ry5aPl//vkHUVFRuHbtGnR1dQG8PnIWFhaGAwcOoF+/fli8eDF8fX3x3XffKZarUaMGAMDMzAxaWlowNDSElZVVgdvRtWtXrF+/Ht9//z0A4N69e4iIiMCiRYsAABs2bICnpycCAgIUy/z2229o0KAB7t27h8qVK4v6dHd3R506dbB9+3Z4e3sDAPbt24fc3Fx06dIFADBs2DBFeycnJ0ycOBEBAQGYM2dOgbW+S0ZGBlavXo3t27ejfv36AABnZ2eEh4dj06ZNaNKkiVr9ahpDIhER0Scu/wjimw8Hr1evnlKbevXqYc2aNSqXv379OtLT01GzZk2l6VlZWXj06BEA4MaNG+jbt2+R6vT19cVPP/2ES5cuoV69eti9ezdq1KgBd3d3AEBERATOnj2rMgw/evRIZUgEXh9NnD59On7++WcYGhpi27Zt6NixI0xMTAC8DsG///477ty5g9TUVOTm5iIrK0vxUPXCun37NrKystC7d2+l6XK5XDSGpYkhkYiI6CPn4uICiUSC27dvo3379qL59+7dg6mpKSpWrPjOfgr6hpm8vDxYWVkhJCRENC8/aOnp6alRuTJra2t4e3tjz549qFevHvbs2aN0h7UgCGjTpg2mTJmictmC+Pr6IjAwEHv37kWTJk1w4cIFxRHPJ0+eYMCAAejXrx8mTpwIU1NThIeH49tvvy3w6wSlUqnoGsecnBzFz3l5eQBeH/m0sbFRaqejo/OeUSg5DIlEREQfuYoVK6J58+YIDg7GsGHDlK5LTEhIwK5du9C9e3elEHj58mWlPi5fvowqVaqo7N/T0xOJiYnQ1taGo6OjyjbVqlXDmTNn0LNnT5XzZTIZcnNz37stXbt2xezZs+Hr64tHjx7B19dXMa9mzZr466+/4OjoCG3tD484hoaG6NSpE7Zv345Hjx7B2dlZcer52rVryMnJwfTp0yGVvr6VY9++fe/sz9zcHLdu3VKaduPGDchkMgCvT3Hr6uoiJiamzJxaVoU3rhAREX0CfvrpJ2RnZ6Nv3774999/ERMTg+PHj6N3796wsbHBpEmTlNqHh4dj2bJluHfvHtavX4/9+/djyJAhKvv+7LPPUK9ePQwePBgnTpzA48ePER4ejqCgIFy7dg0A8M0332DPnj2YN28e7ty5g5s3b2LZsmWKPhwdHXH+/HnExsa+87mNHTt2RFpaGgICAuDt7Q1bW1vFPH9/fyQnJ2PUqFG4cuUKHj16hJMnT+Kbb755bwDt3bs3Ll68iI0bN6Jnz56KwOzs7IycnBysXbsWjx49QkhICDZu3PjOvpo2bYpr165h586duH//PubNm6cUGg0NDTF8+HAEBgZix44dePjwISIjI7F+/Xrs2LHjnX2XJB5JJJFFXSoVeAidiIjKJ1dXVxw8eBC//fYbRo4ciRcvXsDS0hLt27fHhAkTRM9IHD58OCIiIjB//nwYGhpi2rRpaNGihcq+JRIJNm7ciKCgIHz77bd49uwZLC0t0bhxY8WNMN7e3li5ciUWLlyIpUuXwtDQEI0bN1b08d1332HSpElo2rQpXr16VeADro2MjBSPi5k/f77SPBsbG+zZswezZ89G37598erVKzg4OKBFixaKo4AFadiwISpXrowHDx6gR48eiuk1a9bE9OnTsWzZMsyZMweNGzdGQEAAxo8fX2BfLVq0wNdff42ff/4Zr169Qs+ePdG9e3dERf3fczS///57WFhYYMmSJYiOjoaxsTE8PT0xduzYd9ZZkiTC2yfN6ZOXmJjIkFhEEokEtra2iI2NFV2XQoXDsdQMjqPmSCQSZGZmauQau/KsTp06mDhxIvr06VOkfmQyGT9zPlBKSgqMjY1F02UyGSwtLTW+Ph5JJCIiog+WmZmJ8PBwJCYmKu4qpo8Tr0kkIiKiD7Zp0yaMHDkSQ4cOVTzjjz5OPJJIREREH2zYsGFKD5emjxePJBIRERGRCEMiEREREYkwJBIRERVSQd88QvQxYUgkIiIqJIlEovhqNaKSUBqPrmJIJCIiKiRra2ukpqYyKFKJycjIgK6ubomuk3c3ExERFZK+vj4qVKiAtLS00i6l3NPR0UF2dnZpl1GmCYIAbW1thkQiIqLyQCaTqfz2C/pw/Cagso2nm4mIiIhIhCGRiIiIiEQYEomIiIhIhCGRiIiIiER44wqJaGtzt9AUjqXmcCw1g+OoORxLzeFYFk1xjZ9E4O1E9P/J5XLIZLLSLoOIiIjUoOnPcZ5uJgW5XI5FixYhMzOztEsp9zIzMzFp0iSOpQZwLDWD46g5HEvN4VhqRmZmJhYtWgS5XK7RfhkSSck///zDZ1VpgCAIePDgAcdSAziWmsFx1ByOpeZwLDVDEAT8888/Gu+XIZGIiIiIRBgSiYiIiEiEIZEUZDIZunfvzptXNIBjqTkcS83gOGoOx1JzOJaaUVzjyLubiYiIiEiERxKJiIiISIQhkYiIiIhEGBKJiIiISIQhkYiIiIhE+GWJn5iwsDDs3bsXycnJcHBwgL+/P6pVq1Zg+//++w/BwcF48uQJzMzM0LlzZ7Rt27YEKy67CjOWL168wIYNG3D//n3ExcWhQ4cO8Pf3L9mCy6jCjOP58+dx+PBhPHz4EDk5OXBwcECPHj1Qu3btki26jCrMWEZFRWHz5s2IiYnBq1evYGlpidatW6NTp04lXHXZVNh/K/NFRUUhMDAQjo6O+PXXX0ug0rKtMON448YNzJgxQzR9wYIFsLe3L+5Sy7zC7pNyuRwhISE4ffo0kpOTYW5ujq5du6Jly5YfvE6GxE/I2bNnsX79egwdOhQeHh74+++/MXv2bCxYsAAWFhai9gkJCZgzZw5atWqFsWPH4tatW1izZg2MjY3RuHHjUtiCsqOwYymXy2FsbIyvvvoKBw4cKIWKy6bCjuPNmzfh5eWF3r17o0KFCjh+/DiCgoIwe/ZsVKpUqRS2oOwo7Fjq6uqiXbt2cHZ2hq6uLqKiorB69Wro6emhdevWpbAFZUdhxzJfRkYGli5dCk9PTyQnJ5dcwWWUuuO4cOFCGBgYKF4bGxuXRLllmjpjuWDBArx8+RIjRoyAjY0NUlJSkJubW6j18nTzJ2T//v1o2bIlWrVqpfhfiIWFBQ4fPqyy/eHDh2FhYQF/f384ODigVatW+Pzzz7Fv374SrrzsKexYWllZYdCgQfDx8VH6x+9TV9hx9Pf3h6+vL6pUqQJbW1v06dMHtra2uHTpUglXXvYUdiwrVaqEZs2awdHREVZWVmjevDlq1aqFmzdvlnDlZU9hxzLfqlWr0LRpU7i5uZVQpWWbuuNoYmICU1NTxR+plFGlsGN59epV/PfffwgICICXlxesrKxQpUoVeHh4FGq9HPlPRE5ODu7fv49atWopTffy8sKtW7dULnPnzh14eXkpTatduzbu37+PnJycYqu1rFNnLElME+OYl5eHzMxMGBoaFkeJ5YYmxvLBgwe4desWqlevXhwllhvqjuXx48cRHx+PHj16FHeJ5UJR9snvv/8e//vf/zBz5kxERkYWZ5nlgjpjefHiRVSuXBmhoaEYPnw4xo8fjw0bNiA7O7tQ6+bp5k9ESkoK8vLyYGJiojTdxMSkwNMiycnJKtvn5uYiNTUVZmZmxVVumabOWJKYJsZx//79ePXqFZo0aVIMFZYfRRnLESNGKE5D9ejRA61atSrGSss+dcYyNjYWW7ZswYwZM6ClpVUCVZZ96oyjmZkZ/ve//8HV1RU5OTk4deoUZs2ahenTp3/S/3lRZyzj4+MRFRUFmUyGiRMnIiUlBX/88QfS0tIwatSoD143Q+InRiKRfNC0gublf0HPu5b5VBR2LEk1dcfxzJkz2LlzJyZOnCj6x/NTpc5Yzpw5E1lZWbh9+za2bNkCGxsbNGvWrLhKLDc+dCzz8vKwePFi9OjRA3Z2diVRWrlSmH3Szs5OaQzd3d2RlJSEffv2fdIhMV9hxjL/s3rcuHGKS5zkcjnmz5+PoUOHQkdH54PWyZD4iTA2NoZUKhX9r+Ply5cFfsCampqK2qekpEBLS+uTPr2nzliSWFHG8ezZs1ixYgW++eYb0SURn6KijKWVlRUAwMnJCS9fvsTOnTs/6ZBY2LHMzMzEvXv38ODBA6xduxbA6w9oQRDQq1cv/Pjjj6hZs2ZJlF6maOrfSXd3d5w+fVrD1ZUv6n5+V6xYUekaeHt7ewiCgGfPnsHW1vaD1s1rEj8R2tracHV1RUREhNL0iIiIAi9kdXNzE7W/du0aXF1doa396f7/Qp2xJDF1x/HMmTNYunQpxo0bh7p16xZ3meWCpvZJQRA+6euNgcKPpb6+PubNm4e5c+cq/rRp0wZ2dnaYO3cuqlSpUlKllyma2icfPHgAU1NTDVdXvqgzllWrVsWLFy+QlZWlmBYbGwuJRAJzc/MPXjdD4iekU6dOOHr0KI4dO4YnT55g/fr1SEpKQps2bQAAW7ZswZIlSxTt27Zti6SkJMVzEo8dO4Zjx47hyy+/LK1NKDMKO5YA8PDhQzx8+BBZWVlISUnBw4cP8eTJk9Iov8wo7DjmB8QBAwbA3d0dycnJSE5ORkZGRmltQplR2LE8dOgQLl68iNjYWMTGxuL48ePYt28fPvvss9LahDKjMGMplUrh5OSk9MfY2BgymQxOTk7Q09MrzU0pVYXdJw8cOIALFy4gNjYWjx8/xpYtW3D+/Hm0b9++tDahzCjsWDZr1gxGRkZYtmwZnjx5gv/++w+bNm3C559//sGnmgGebv6keHt7IzU1FX/++SdevHgBR0dHBAQEwNLSEsDrBz4nJSUp2ltZWSEgIADBwcEICwuDmZkZBg0a9Mk/IxEo/FgCr+/Yy3f//n2cOXMGlpaWWLp0aYnWXpYUdhz//vtv5Obm4o8//sAff/yhmO7j44PRo0eXeP1lSWHHUhAEbN26FQkJCZBKpbCxsUHfvn0/+WckAur9fpNYYccxJycHGzduxPPnz6GjowNHR0dMnjyZZwxQ+LHU09PDjz/+iLVr12Ly5MkwMjJCkyZN0KtXr0KtVyLkX91IRERERPT/8XQzEREREYkwJBIRERGRCEMiEREREYkwJBIRERGRCEMiEREREYkwJBIRERGRCEMiEREREYkwJBKRyIkTJ+Dn54d79+6pnP/LL7988g+vLi/CwsJw4sSJEl1nYGAgvv322xJdpya9evUKO3bswI0bN0q7FKJSxZBIRPQRO3z4cImHxPLu1atXCAkJYUikTx5DIhF9dHJycpCbm1ti63v16lWJrassEAQB2dnZpV2Gxn2s20WkLn53MxEV2cyZM/H8+XMsWLAAEolEMV0QBIwbNw52dnYICAhAQkICxowZg759+yI3NxdHjhxBSkoKHB0d0bdvX3h6eir1Gxsbix07duD69evIyMiAtbU12rVrh/bt2yva3LhxAzNmzMCYMWPw8OFD/PPPP0hOTsb8+fNx584dLFu2DD/++CPOnDmD8PBw5OTkoEaNGhg0aBCsra0V/URERODQoUO4f/8+UlNTUbFiRXh6eqJXr14wNjZWtNuxYwdCQkLwyy+/YPfu3YiMjIRMJsOqVatw79497Nu3D3fu3EFycjJMTU3h5uaGvn37Kr5jFXh9On/ZsmWYNm0azpw5gwsXLiA3NxcNGjTA0KFDkZWVhbVr1yIiIgI6Ojpo1qwZ+vTpA23t//snOycnB6GhoTh9+jQSEhKgr6+PevXqoV+/fop6R48ejcTERACAn58fACh9X3hGRgZCQkJw/vx5PH/+HMbGxorvd9XT01Osy8/PD+3atYOjoyMOHjyIuLg4DBo0CG3btv3gfSS/D1dXV+zZswdJSUlwdHTE4MGD4ebmhn379iEsLAwpKSmoUqUKhg8fDhsbG8XygYGBSE1NxdChQ7Fp0yY8fPgQhoaG+Pzzz+Hn5wep9P+OeaSlpWHbtm0IDw9HSkoKzM3N0bRpU3Tv3h0ymey927VmzRoAQEhICEJCQgD83/eDx8XFYdeuXYiKisLz589RoUIFVKpUCX369IGTk5Novxw3bhweP36MEydOICsrC1WqVMGQIUNgZ2enND5Xr17F3r17ce/ePeTm5sLS0hLNmzdH165dFW3u3buHkJAQREVFITs7G/b29ujSpQu8vb0/+H0gKgyGRCIqUF5ensojcm9/5XvHjh0xd+5cXL9+HV5eXorpV65cQXx8PAYNGqTU/tChQ7C0tIS/vz8EQUBoaChmz56NGTNmwN3dHQDw5MkT/Pjjj7CwsMCAAQNgamqKq1evYt26dUhNTUWPHj2U+tyyZQvc3d0xbNgwSKVSmJiYKOYtX74cXl5eGD9+PJKSkrB9+3YEBgZi3rx5qFChAgAgLi4O7u7uaNmyJQwMDJCYmIj9+/dj2rRpmDdvnlJAA4DffvsN3t7eaNOmjeJIYmJiIuzs7ODt7Q1DQ0MkJyfj8OHDCAgIwPz585XCJgCsWLECDRs2xNdff40HDx5g69atyM3NxdOnT9GoUSO0bt0a169fR2hoKCpWrIhOnTop3pe5c+fi5s2b8PX1hbu7O5KSkrBjxw4EBgbil19+gY6ODr777jvMnz8fBgYGGDJkCAAoQtKrV68QGBiIZ8+eoWvXrnB2dsbjx4+xY8cOREdHY+rUqUqBPzw8HFFRUejWrRtMTU2VxvdDXb58GQ8fPkTfvn0BAJs3b8Yvv/wCHx8fxMfHY8iQIcjIyEBwcDB+++03zJ07V6mG5ORkLFy4EF26dIGfnx8uX76MXbt2IT09XbF92dnZmDFjBuLi4uDn5wdnZ2fcvHkTe/bswcOHDxEQEKBU09vbZWhoiClTpmD27Nlo2bIlWrZsCQCK9+758+cwNDREnz59YGxsjLS0NJw8eRJTpkzB3LlzReFv69at8PDwwPDhw5GZmYnNmzcjKCgICxYsUATbY8eOYeXKlahevTqGDRsGExMTxMbGIjo6WtFPZGQkZs+eDTc3NwwbNgwGBgY4e/YsFi5ciOzsbLRo0aLQ7wfR+zAkElGBfvjhhwLnvXlkrG7durC2tsahQ4eUQmJYWBisra1Rp04dpWXz8vLw448/QkdHBwBQq1YtjB49Gtu3b8fUqVMBAMHBwdDX18fMmTNhYGAAAPDy8kJOTg727NmDDh06wNDQUNGntbU1vvnmG5W1Vq5cGSNHjlS8dnR0xNSpUxEWFoavvvoKAJSOigmCAA8PD9SoUQOjRo3C1atXUb9+faU+fXx8FEfn8jVu3BiNGzdW2s66deti2LBhOHPmDDp27KjUvm7duhgwYIBi227fvo1//vkHAwYMUARCLy8vXLt2DadPn1ZMO3fuHK5evYpvv/0WjRo1UvTn7OyMgIAAnDhxAm3btkWlSpWgo6MDfX19RfjOd/DgQTx69AizZ89G5cqVAQCenp6oWLEi5s+fj6tXryq9b1lZWZg3b57SmBeWXC7HDz/8oDhKKZFI8Ouvv+LGjRsICgpSBMKUlBSsX78ejx8/Vjo6l5qaiu+//17xXtSqVQvZ2dk4fPgwfH19YWFhgZMnT+LRo0eYMGECmjRpohhDPT09bN68GREREUr7qKrtSklJAQBUrFhRNG7Vq1dH9erVFa/z3+Nvv/0WR44cwcCBA5XaOzg4YNy4cYrXUqkUCxYswN27d+Hu7o6srCwEBwfDw8MD06ZNU4zB20fV//jjDzg6OmLatGnQ0tICANSuXRspKSnYunUrmjdvrnQ0lUgTGBKJqEBjxoyBvb29aHpwcDCePXumeC2VStGuXTts2rQJSUlJsLCwQFxcHK5evYr+/fsrHQ0CgEaNGikCIgDFqdJ//vkHeXl5yMnJQWRkJNq0aQNdXV2lo5l16tTBoUOHcOfOHaUQ82ZYeluzZs2UXnt4eMDS0hI3btxQhMSXL19i+/btuHLlCp4/f650tPTJkyeikKhqfVlZWYrTt4mJicjLy1PMi4mJEbWvV6+e0mt7e3uEh4ejbt26oukRERGK15cuXUKFChVQr149pbFxcXGBqakpbty48d5TwZcuXYKTkxNcXFyU+qhduzYkEglu3LihNL41a9YsUkAEgBo1aiidxs7ft/LX+fb0xMREpZCor68veh+aNWuGo0eP4r///kPz5s0RGRkJXV1dpbAOAC1atMDmzZtFR7sLu125ubmK0/xxcXFKY6fqPX67XmdnZwBAUlIS3N3dcevWLWRmZqJt27ai35N8cXFxiImJQf/+/RU15Ktbty4uX76Mp0+fwsHB4YO3g+hDMCQSUYHs7e0VR5neZGBgoBQSAaBly5bYsWMHDh8+jD59+iAsLAw6Ojr4/PPPRcubmpqqnJaTk4OsrCxkZWUhNzcXhw4dwqFDh1TWlpqaqvTazMyswO0oaH35feTl5eGnn37Cixcv0K1bNzg5OUFXVxeCIOCHH35QeTODqvUtWrQIkZGR6NatGypXrgx9fX1IJBLMmTNHZR9vh5P8U9qqpr+5/MuXL5Geno4+ffqo3N63x0aVly9fIi4uDr179/6gPlSNYWEVZnuB10ce36TqFHd+XWlpaYq/TU1NRYHLxMQEWlpaRd6u4OBghIWFwdfXF9WrV4ehoSEkEglWrFih8j02MjJSuW35bfOPWpqbmxe4zuTkZADAxo0bsXHjRpVtPuQ9JyoshkQi0ggDAwP4+Pjg2LFj6Ny5M06cOIGmTZsqrvl7U/6H3tvTtLW1oaenBy0tLUilUjRv3hzt2rVTuT4rKyul1wUdhXnX+vJvjHj8+DEePXqEUaNGKV3bFRcXV2Cfb8vIyMDly5fRvXt3dOnSRTFdLpcrAoymGBkZwcjICFOmTFE5X19f/4P60NHRUToN//b8N71rfEvKy5cvRdPy39v8oGloaIg7d+5AEASlml++fInc3FzRdaGF3a7Tp0/Dx8dHFNBTU1NV7uvvk1/P2//pUtWmS5cuBR4xf/taSCJNYEgkIo3p0KEDDh8+jN9++w3p6elKdyG/6fz58+jXr5/ilHNmZiYuXbqEatWqQSqVQldXFzVq1MCDBw/g7OwsummksM6cOaN0+vHWrVtITExU3JSQHxTevPMVAI4cOVKo9QiCIOrj6NGjSqedNaFevXo4e/Ys8vLy4Obm9s62bx+FfLOP3bt3w8jISBS4y6rMzExcvHhR6RTumTNnIJFIFNcJenp64ty5cwgPD0fDhg0V7U6ePAng9enl98l/D1WNm0QiEe2Ply9fxvPnz5Xuxv5QHh4eMDAwwJEjR9C0aVOVodXOzg62trZ49OhRgUePiYoDQyIRaYydnR1q166NK1euoGrVqnBxcVHZTiqV4qeffkKnTp2Ql5eH0NBQZGZmKt2xPGjQIEydOhXTpk1D27ZtYWlpiczMTMTFxeHSpUuYPn36B9d17949rFixAo0bN8azZ8+wbds2VKxYUXGU0s7ODtbW1tiyZQsEQYChoSEuXbqkdB3g+xgYGKBatWrYu3cvjIyMYGlpif/++w/Hjx9X6wjTuzRt2hRnzpzBnDlz0LFjR1SpUgVaWlp49uwZbty4gQYNGigCkpOTE86ePYuzZ8/CysoKOjo6cHJyQseOHXH+/HlMnz4dX3zxBZycnCAIApKSknDt2jV8+eWX7w2gJc3IyAirV69GUlISbG1tceXKFRw9ehRt27aFhYUFAKB58+YICwvD0qVLkZCQACcnJ0RFRWH37t2oU6eO0vWIBdHX14elpSUuXrwIT09PGBoaKsJ03bp1cfLkSdjb28PZ2Rn379/H3r1733m6+F309PQwYMAArFixArNmzUKrVq1gYmKCuLg4PHr0SHHX9rBhwzBnzhz8/PPP8PHxQcWKFZGWloaYmBg8ePCgwJu2iIqCIZGINKpJkya4cuVKgUcRAaB9+/aQy+VYt24dXr58CUdHR0yePBlVq1ZVtHFwcEBQUBD+/PNPbNu2DS9fvkSFChVga2srulv6fUaOHIlTp05h0aJFkMvliuck5p+i1NbWxqRJk7B+/XqsXr0aUqkUnp6emDp1KkaNGvXB6xk/fjzWrVuHTZs2IS8vDx4eHvjxxx/xyy+/FKre95FKpfj+++/x119/4dSpU9i9eze0tLRgbm6OatWqKd3s4efnh+TkZKxcuRKZmZmK5yTq6elhxowZ2LNnD/7++28kJCRAR0cHFhYW8PT0VLp7vawwNTXFkCFDsHHjRkRHR8PQ0BBdu3ZVustcR0cH06dPx9atW7Fv3z6kpKSgYsWK+PLLL0WPTXqXESNGYNOmTZg7dy7kcrniOYmDBg2CtrY29uzZg6ysLFSqVAnfffcdtm3bpvZ2tWzZEmZmZggNDcWKFSsAvH56gI+Pj6JNzZo1MXv2bOzatQvBwcFIS0uDkZERHBwcFHdxE2maRHj7gWdEREUwb9483LlzB0uXLhWdlst/mHa/fv3QuXPnYq8l/6HVc+bMUXkDDpUf+Q/T/u2330q7FKJPBo8kElGRyeVyPHjwAHfv3kV4eDgGDBhQ5OsIiYiodPFfcSIqshcvXuDHH3+Evr4+WrdujQ4dOpR2SUREVEQ83UxEREREIvwOHyIiIiISYUgkIiIiIhGGRCIiIiISYUgkIiIiIhGGRCIiIiISYUgkIiIiIhGGRCIiIiISYUgkIiIiIhGGRCIiIiIS+X9s+JxbURdl2QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.681501     0.061641\n",
      "1                    TP        18.700000     1.766981\n",
      "2                    TN        98.900000     1.370320\n",
      "3                    FP         1.600000     1.074968\n",
      "4                    FN        14.700000     2.110819\n",
      "5              Accuracy         0.878280     0.016430\n",
      "6             Precision         0.923565     0.049191\n",
      "7           Sensitivity         0.560458     0.058211\n",
      "8           Specificity         0.984070     0.010732\n",
      "9              F1 score         0.695569     0.045761\n",
      "10  F1 score (weighted)         0.866879     0.019367\n",
      "11     F1 score (macro)         0.809732     0.027676\n",
      "12    Balanced Accuracy         0.772263     0.028684\n",
      "13                  MCC         0.657042     0.048323\n",
      "14                  NPV         0.870850     0.016625\n",
      "15              ROC_AUC         0.772263     0.028684\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.708945</td>\n",
       "      <td>0.673766</td>\n",
       "      <td>0.671030</td>\n",
       "      <td>0.711722</td>\n",
       "      <td>0.685058</td>\n",
       "      <td>0.678225</td>\n",
       "      <td>0.715178</td>\n",
       "      <td>0.590449</td>\n",
       "      <td>0.617881</td>\n",
       "      <td>0.629364</td>\n",
       "      <td>0.668162</td>\n",
       "      <td>0.042518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>36.400000</td>\n",
       "      <td>4.168666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>196.900000</td>\n",
       "      <td>2.282786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>2.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>4.040077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.847015</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.870522</td>\n",
       "      <td>0.016967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.829268</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.911469</td>\n",
       "      <td>0.050065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.515152</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.539167</td>\n",
       "      <td>0.060187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.975200</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.982030</td>\n",
       "      <td>0.010535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.673267</td>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.710280</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>0.675695</td>\n",
       "      <td>0.050836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.862356</td>\n",
       "      <td>0.831454</td>\n",
       "      <td>0.856119</td>\n",
       "      <td>0.878499</td>\n",
       "      <td>0.876244</td>\n",
       "      <td>0.885458</td>\n",
       "      <td>0.872563</td>\n",
       "      <td>0.837314</td>\n",
       "      <td>0.833312</td>\n",
       "      <td>0.844623</td>\n",
       "      <td>0.857794</td>\n",
       "      <td>0.020172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.798703</td>\n",
       "      <td>0.760626</td>\n",
       "      <td>0.795656</td>\n",
       "      <td>0.830694</td>\n",
       "      <td>0.823890</td>\n",
       "      <td>0.841462</td>\n",
       "      <td>0.819010</td>\n",
       "      <td>0.768086</td>\n",
       "      <td>0.761396</td>\n",
       "      <td>0.774303</td>\n",
       "      <td>0.797382</td>\n",
       "      <td>0.030304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.755101</td>\n",
       "      <td>0.727647</td>\n",
       "      <td>0.757206</td>\n",
       "      <td>0.792076</td>\n",
       "      <td>0.790654</td>\n",
       "      <td>0.811029</td>\n",
       "      <td>0.776912</td>\n",
       "      <td>0.736318</td>\n",
       "      <td>0.725294</td>\n",
       "      <td>0.733831</td>\n",
       "      <td>0.760607</td>\n",
       "      <td>0.030521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.652344</td>\n",
       "      <td>0.561838</td>\n",
       "      <td>0.634790</td>\n",
       "      <td>0.695908</td>\n",
       "      <td>0.670032</td>\n",
       "      <td>0.700295</td>\n",
       "      <td>0.683425</td>\n",
       "      <td>0.568536</td>\n",
       "      <td>0.574982</td>\n",
       "      <td>0.608443</td>\n",
       "      <td>0.635059</td>\n",
       "      <td>0.053670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>0.847200</td>\n",
       "      <td>0.860300</td>\n",
       "      <td>0.875600</td>\n",
       "      <td>0.883400</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.869000</td>\n",
       "      <td>0.854600</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.850400</td>\n",
       "      <td>0.863840</td>\n",
       "      <td>0.015536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.755101</td>\n",
       "      <td>0.727647</td>\n",
       "      <td>0.757206</td>\n",
       "      <td>0.792076</td>\n",
       "      <td>0.790654</td>\n",
       "      <td>0.811029</td>\n",
       "      <td>0.776912</td>\n",
       "      <td>0.736318</td>\n",
       "      <td>0.725294</td>\n",
       "      <td>0.733831</td>\n",
       "      <td>0.760607</td>\n",
       "      <td>0.030521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.708945    0.673766    0.671030    0.711722   \n",
       "1                    TP   34.000000   33.000000   36.000000   41.000000   \n",
       "2                    TN  201.000000  194.000000  197.000000  197.000000   \n",
       "3                    FP    1.000000    6.000000    3.000000    2.000000   \n",
       "4                    FN   32.000000   35.000000   32.000000   28.000000   \n",
       "5              Accuracy    0.876866    0.847015    0.869403    0.888060   \n",
       "6             Precision    0.971429    0.846154    0.923077    0.953488   \n",
       "7           Sensitivity    0.515152    0.485294    0.529412    0.594203   \n",
       "8           Specificity    0.995000    0.970000    0.985000    0.989900   \n",
       "9              F1 score    0.673267    0.616822    0.672897    0.732143   \n",
       "10  F1 score (weighted)    0.862356    0.831454    0.856119    0.878499   \n",
       "11     F1 score (macro)    0.798703    0.760626    0.795656    0.830694   \n",
       "12    Balanced Accuracy    0.755101    0.727647    0.757206    0.792076   \n",
       "13                  MCC    0.652344    0.561838    0.634790    0.695908   \n",
       "14                  NPV    0.862700    0.847200    0.860300    0.875600   \n",
       "15              ROC_AUC    0.755101    0.727647    0.757206    0.792076   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.685058    0.678225    0.715178    0.590449    0.617881    0.629364   \n",
       "1    40.000000   44.000000   38.000000   34.000000   32.000000   32.000000   \n",
       "2   197.000000  195.000000  199.000000  194.000000  196.000000  199.000000   \n",
       "3     5.000000    5.000000    1.000000    7.000000    4.000000    2.000000   \n",
       "4    26.000000   24.000000   30.000000   33.000000   36.000000   35.000000   \n",
       "5     0.884328    0.891791    0.884328    0.850746    0.850746    0.861940   \n",
       "6     0.888889    0.897959    0.974359    0.829268    0.888889    0.941176   \n",
       "7     0.606061    0.647059    0.558824    0.507463    0.470588    0.477612   \n",
       "8     0.975200    0.975000    0.995000    0.965200    0.980000    0.990000   \n",
       "9     0.720721    0.752137    0.710280    0.629630    0.615385    0.633663   \n",
       "10    0.876244    0.885458    0.872563    0.837314    0.833312    0.844623   \n",
       "11    0.823890    0.841462    0.819010    0.768086    0.761396    0.774303   \n",
       "12    0.790654    0.811029    0.776912    0.736318    0.725294    0.733831   \n",
       "13    0.670032    0.700295    0.683425    0.568536    0.574982    0.608443   \n",
       "14    0.883400    0.890400    0.869000    0.854600    0.844800    0.850400   \n",
       "15    0.790654    0.811029    0.776912    0.736318    0.725294    0.733831   \n",
       "\n",
       "           ave       std  \n",
       "0     0.668162  0.042518  \n",
       "1    36.400000  4.168666  \n",
       "2   196.900000  2.282786  \n",
       "3     3.600000  2.118700  \n",
       "4    31.100000  4.040077  \n",
       "5     0.870522  0.016967  \n",
       "6     0.911469  0.050065  \n",
       "7     0.539167  0.060187  \n",
       "8     0.982030  0.010535  \n",
       "9     0.675695  0.050836  \n",
       "10    0.857794  0.020172  \n",
       "11    0.797382  0.030304  \n",
       "12    0.760607  0.030521  \n",
       "13    0.635059  0.053670  \n",
       "14    0.863840  0.015536  \n",
       "15    0.760607  0.030521  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.690248</td>\n",
       "      <td>0.050656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.880367</td>\n",
       "      <td>0.025403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.917352</td>\n",
       "      <td>0.051558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.571807</td>\n",
       "      <td>0.090913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.982888</td>\n",
       "      <td>0.010603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.700870</td>\n",
       "      <td>0.075780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.869235</td>\n",
       "      <td>0.030165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.813006</td>\n",
       "      <td>0.045281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.777347</td>\n",
       "      <td>0.046663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.661346</td>\n",
       "      <td>0.077509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.874104</td>\n",
       "      <td>0.024082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.777347</td>\n",
       "      <td>0.046663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.690248     0.050656\n",
       "1              Accuracy         0.880367     0.025403\n",
       "2             Precision         0.917352     0.051558\n",
       "3           Sensitivity         0.571807     0.090913\n",
       "4           Specificity         0.982888     0.010603\n",
       "5              F1 score         0.700870     0.075780\n",
       "6   F1 score (weighted)         0.869235     0.030165\n",
       "7      F1 score (macro)         0.813006     0.045281\n",
       "8     Balanced Accuracy         0.777347     0.046663\n",
       "9                   MCC         0.661346     0.077509\n",
       "10                  NPV         0.874104     0.024082\n",
       "11              ROC_AUC         0.777347     0.046663"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMRegressor(objective=\"regression\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"rmse\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_lgbm_cat = np.where(((y_pred_optimized_lgbm >= 2) | (y_pred_optimized_lgbm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_lgbm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_lgbm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_lgbm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_lgbm_cat))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "lgbm_run0 = data_lgbm[['y_test_idx0', 'y_test0', 'y_pred_lgbm0']]\n",
    "lgbm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "lgbm_run0.reset_index(inplace=True, drop=True)\n",
    "lgbm_run1 = data_lgbm[['y_test_idx1', 'y_test1', 'y_pred_lgbm1']]\n",
    "lgbm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "lgbm_run1.reset_index(inplace=True, drop=True)\n",
    "lgbm_run2 = data_lgbm[['y_test_idx2', 'y_test2', 'y_pred_lgbm2']]\n",
    "lgbm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "lgbm_run2.reset_index(inplace=True, drop=True)\n",
    "lgbm_run3 = data_lgbm[['y_test_idx3', 'y_test3', 'y_pred_lgbm3']]\n",
    "lgbm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "lgbm_run3.reset_index(inplace=True, drop=True)\n",
    "lgbm_run4 = data_lgbm[['y_test_idx4', 'y_test4', 'y_pred_lgbm4']]\n",
    "lgbm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "lgbm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "lgbm_5preds = pd.concat([chembl_id, lgbm_run0, lgbm_run1, lgbm_run2, lgbm_run3, lgbm_run4], axis=1)\n",
    "lgbm_5preds = lgbm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_lgbm0', 'y_pred_lgbm1', 'y_pred_lgbm2', 'y_pred_lgbm3', 'y_pred_lgbm4']]\n",
    "lgbm_5preds['y_pred_lgbm_ave'] = lgbm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "lgbm_5preds['y_pred_lgbm_std'] = lgbm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "\n",
    "mat_met_optimized_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db4ac315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(lgbm_5preds['y_test0'], lgbm_5preds['y_pred_lgbm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model r2_score 0.6997 with a standard deviation of 0.0751\n",
      "LightGBM optimized model r2_score 0.6750 with a standard deviation of 0.0749\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#r2_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"r2\")\n",
    "r2_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"r2\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)r2_score %0.4f with a standard deviation of %0.4f\" % (r2_cv_lgbm_opt_testSet.mean(), r2_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(r2_cv_lgbm_opt), np.std(r2_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm.joblib']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_reg, \"OUTPUT/lgbm_reg.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"OUTPUT/optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm.joblib\") \n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.681838     0.065479\n",
      "1                    TP        18.100000     2.131770\n",
      "2                    TN        98.300000     1.159502\n",
      "3                    FP         2.200000     0.788811\n",
      "4                    FN        15.300000     2.451757\n",
      "5              Accuracy         0.869313     0.018640\n",
      "6             Precision         0.892240     0.036624\n",
      "7           Sensitivity         0.542528     0.067596\n",
      "8           Specificity         0.978100     0.007890\n",
      "9              F1 score         0.672573     0.055212\n",
      "10  F1 score (weighted)         0.856938     0.022794\n",
      "11     F1 score (macro)         0.795447     0.033019\n",
      "12    Balanced Accuracy         0.760313     0.033615\n",
      "13                  MCC         0.627426     0.054655\n",
      "14                  NPV         0.865650     0.019057\n",
      "15              ROC_AUC         0.760313     0.033615\n",
      "CPU times: user 1h 59min 57s, sys: 7.17 s, total: 2h 4s\n",
      "Wall time: 5min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_reg.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores),np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=4, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=4, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"rmse\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # convert to categorical values\n",
    "        y_test_cat = np.where( ((y_test>=2) | (y_test<= -2.0)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred>=2) | (y_pred<= -2.0)), 1, 0)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:12:31,987] A new study created in memory with name: XGBRegressor\n",
      "[I 2023-12-20 07:12:37,990] Trial 0 finished with value: 0.6528073047900718 and parameters: {'n_estimators': 150, 'eta': 0.0263599150207785, 'max_depth': 12, 'alpha': 0.2099, 'lambda': 32.135516587182025, 'max_bin': 478}. Best is trial 0 with value: 0.6528073047900718.\n",
      "[I 2023-12-20 07:12:44,674] Trial 1 finished with value: 0.6975041423961056 and parameters: {'n_estimators': 616, 'eta': 0.06763019192216786, 'max_depth': 5, 'alpha': 0.8944000000000001, 'lambda': 28.163801578421065, 'max_bin': 263}. Best is trial 1 with value: 0.6975041423961056.\n",
      "[I 2023-12-20 07:12:50,960] Trial 2 finished with value: 0.23323082193586017 and parameters: {'n_estimators': 320, 'eta': 0.0012158881245137375, 'max_depth': 6, 'alpha': 0.47740000000000005, 'lambda': 16.0128738402708, 'max_bin': 485}. Best is trial 1 with value: 0.6975041423961056.\n",
      "[I 2023-12-20 07:13:06,121] Trial 3 finished with value: 0.6992726153741023 and parameters: {'n_estimators': 850, 'eta': 0.016049650650614585, 'max_depth': 11, 'alpha': 0.0679, 'lambda': 1.7403192415311808, 'max_bin': 318}. Best is trial 3 with value: 0.6992726153741023.\n",
      "[I 2023-12-20 07:13:15,584] Trial 4 finished with value: 0.7015790398793545 and parameters: {'n_estimators': 765, 'eta': 0.019070245543092532, 'max_depth': 6, 'alpha': 0.1688, 'lambda': 1.5052230343631, 'max_bin': 378}. Best is trial 4 with value: 0.7015790398793545.\n",
      "[I 2023-12-20 07:13:24,115] Trial 5 finished with value: 0.6412374726193171 and parameters: {'n_estimators': 255, 'eta': 0.012849270943763706, 'max_depth': 10, 'alpha': 0.8005, 'lambda': 22.353193173682527, 'max_bin': 478}. Best is trial 4 with value: 0.7015790398793545.\n",
      "[I 2023-12-20 07:13:30,224] Trial 6 finished with value: 0.5554812103016662 and parameters: {'n_estimators': 213, 'eta': 0.009743262367110636, 'max_depth': 9, 'alpha': 0.9307000000000001, 'lambda': 34.0304516310637, 'max_bin': 294}. Best is trial 4 with value: 0.7015790398793545.\n",
      "[I 2023-12-20 07:13:34,273] Trial 7 finished with value: 0.6978307199485272 and parameters: {'n_estimators': 405, 'eta': 0.07634546508915242, 'max_depth': 9, 'alpha': 0.8051, 'lambda': 5.17788691879165, 'max_bin': 492}. Best is trial 4 with value: 0.7015790398793545.\n",
      "[I 2023-12-20 07:13:38,025] Trial 8 finished with value: 0.6971674908339989 and parameters: {'n_estimators': 753, 'eta': 0.09672025288897013, 'max_depth': 6, 'alpha': 0.29610000000000003, 'lambda': 6.051472242336496, 'max_bin': 254}. Best is trial 4 with value: 0.7015790398793545.\n",
      "[I 2023-12-20 07:13:45,445] Trial 9 finished with value: 0.7011991760209891 and parameters: {'n_estimators': 770, 'eta': 0.051546311073930755, 'max_depth': 9, 'alpha': 0.47490000000000004, 'lambda': 18.096008128544252, 'max_bin': 363}. Best is trial 4 with value: 0.7015790398793545.\n",
      "[I 2023-12-20 07:13:52,757] Trial 10 finished with value: 0.7000841601850334 and parameters: {'n_estimators': 561, 'eta': 0.03412658197168328, 'max_depth': 7, 'alpha': 0.024, 'lambda': 10.065588571315965, 'max_bin': 407}. Best is trial 4 with value: 0.7015790398793545.\n",
      "[I 2023-12-20 07:14:02,099] Trial 11 finished with value: 0.7002690956168063 and parameters: {'n_estimators': 732, 'eta': 0.04456021528060209, 'max_depth': 8, 'alpha': 0.5575, 'lambda': 14.045026526721502, 'max_bin': 375}. Best is trial 4 with value: 0.7015790398793545.\n",
      "[I 2023-12-20 07:14:11,321] Trial 12 finished with value: 0.7018948021046592 and parameters: {'n_estimators': 872, 'eta': 0.05295700799666592, 'max_depth': 8, 'alpha': 0.4041, 'lambda': 21.142698065699935, 'max_bin': 375}. Best is trial 12 with value: 0.7018948021046592.\n",
      "[I 2023-12-20 07:14:24,336] Trial 13 finished with value: 0.7003424716579562 and parameters: {'n_estimators': 899, 'eta': 0.029930274903746727, 'max_depth': 7, 'alpha': 0.2635, 'lambda': 24.64104899229369, 'max_bin': 423}. Best is trial 12 with value: 0.7018948021046592.\n",
      "[I 2023-12-20 07:14:30,140] Trial 14 finished with value: 0.6957858654582614 and parameters: {'n_estimators': 633, 'eta': 0.04499362949308905, 'max_depth': 5, 'alpha': 0.38270000000000004, 'lambda': 12.194797557029219, 'max_bin': 332}. Best is trial 12 with value: 0.7018948021046592.\n",
      "[I 2023-12-20 07:14:39,027] Trial 15 finished with value: 0.7034597123881214 and parameters: {'n_estimators': 544, 'eta': 0.05677614844374113, 'max_depth': 7, 'alpha': 0.5945, 'lambda': 39.18282610340614, 'max_bin': 427}. Best is trial 15 with value: 0.7034597123881214.\n",
      "[I 2023-12-20 07:14:47,594] Trial 16 finished with value: 0.7050080631336485 and parameters: {'n_estimators': 472, 'eta': 0.06054444329357659, 'max_depth': 8, 'alpha': 0.6321, 'lambda': 39.55508038872104, 'max_bin': 436}. Best is trial 16 with value: 0.7050080631336485.\n",
      "[I 2023-12-20 07:14:54,850] Trial 17 finished with value: 0.7029087676999886 and parameters: {'n_estimators': 465, 'eta': 0.06432662577922646, 'max_depth': 7, 'alpha': 0.6309, 'lambda': 39.52033368156299, 'max_bin': 439}. Best is trial 16 with value: 0.7050080631336485.\n",
      "[I 2023-12-20 07:14:56,455] Trial 18 finished with value: 0.6562030355881013 and parameters: {'n_estimators': 57, 'eta': 0.08238103937367056, 'max_depth': 10, 'alpha': 0.6765, 'lambda': 39.85675794548234, 'max_bin': 443}. Best is trial 16 with value: 0.7050080631336485.\n",
      "[I 2023-12-20 07:15:03,735] Trial 19 finished with value: 0.7017127243920497 and parameters: {'n_estimators': 389, 'eta': 0.061901228424341995, 'max_depth': 8, 'alpha': 0.6912, 'lambda': 34.91701987021714, 'max_bin': 409}. Best is trial 16 with value: 0.7050080631336485.\n",
      "[I 2023-12-20 07:15:17,251] Trial 20 finished with value: 0.6991824039566131 and parameters: {'n_estimators': 564, 'eta': 0.03685509609853218, 'max_depth': 10, 'alpha': 0.5674, 'lambda': 28.772503961297254, 'max_bin': 449}. Best is trial 16 with value: 0.7050080631336485.\n",
      "[I 2023-12-20 07:15:25,556] Trial 21 finished with value: 0.7024453600082842 and parameters: {'n_estimators': 473, 'eta': 0.06052997460182919, 'max_depth': 7, 'alpha': 0.6623, 'lambda': 39.94075800037846, 'max_bin': 451}. Best is trial 16 with value: 0.7050080631336485.\n",
      "[I 2023-12-20 07:15:33,038] Trial 22 finished with value: 0.7048392893932196 and parameters: {'n_estimators': 478, 'eta': 0.06987667058587385, 'max_depth': 7, 'alpha': 0.5969, 'lambda': 36.78188421120411, 'max_bin': 427}. Best is trial 16 with value: 0.7050080631336485.\n",
      "[I 2023-12-20 07:15:41,186] Trial 23 finished with value: 0.7082329914489204 and parameters: {'n_estimators': 524, 'eta': 0.07101769028875411, 'max_depth': 8, 'alpha': 0.7864, 'lambda': 35.72460692646191, 'max_bin': 399}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:15:47,633] Trial 24 finished with value: 0.7028605914994418 and parameters: {'n_estimators': 383, 'eta': 0.07324899023998535, 'max_depth': 8, 'alpha': 0.7733, 'lambda': 34.587811218094245, 'max_bin': 397}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:15:55,003] Trial 25 finished with value: 0.7064428162049347 and parameters: {'n_estimators': 657, 'eta': 0.08232611239586768, 'max_depth': 9, 'alpha': 0.7514000000000001, 'lambda': 35.205720759963256, 'max_bin': 346}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:16:02,040] Trial 26 finished with value: 0.6998620445084797 and parameters: {'n_estimators': 668, 'eta': 0.08310633734900251, 'max_depth': 9, 'alpha': 0.8756, 'lambda': 31.655034362637128, 'max_bin': 345}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:16:09,190] Trial 27 finished with value: 0.6975219006033455 and parameters: {'n_estimators': 686, 'eta': 0.09076089581696667, 'max_depth': 11, 'alpha': 0.7331000000000001, 'lambda': 37.39144805252599, 'max_bin': 304}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:16:18,646] Trial 28 finished with value: 0.702461502725271 and parameters: {'n_estimators': 504, 'eta': 0.07851825261051154, 'max_depth': 10, 'alpha': 0.9546, 'lambda': 36.250386559440344, 'max_bin': 353}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:16:27,049] Trial 29 finished with value: 0.6977498508345691 and parameters: {'n_estimators': 311, 'eta': 0.06943352362966784, 'max_depth': 12, 'alpha': 0.8528, 'lambda': 31.88248407275763, 'max_bin': 467}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:16:32,398] Trial 30 finished with value: 0.69793698986105 and parameters: {'n_estimators': 591, 'eta': 0.08918457720049379, 'max_depth': 8, 'alpha': 0.9888, 'lambda': 29.823643598219018, 'max_bin': 394}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:16:39,626] Trial 31 finished with value: 0.7038744220880451 and parameters: {'n_estimators': 440, 'eta': 0.0715318855860666, 'max_depth': 9, 'alpha': 0.74, 'lambda': 35.5396310479609, 'max_bin': 415}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:16:47,354] Trial 32 finished with value: 0.7029869310671292 and parameters: {'n_estimators': 520, 'eta': 0.0661799011282944, 'max_depth': 8, 'alpha': 0.53, 'lambda': 37.241186462535154, 'max_bin': 463}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:16:53,665] Trial 33 finished with value: 0.7001650087628846 and parameters: {'n_estimators': 612, 'eta': 0.07060705882895746, 'max_depth': 6, 'alpha': 0.6243000000000001, 'lambda': 33.07110870623443, 'max_bin': 392}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:16:59,110] Trial 34 finished with value: 0.7000988958305152 and parameters: {'n_estimators': 313, 'eta': 0.05845071609453241, 'max_depth': 7, 'alpha': 0.7184, 'lambda': 26.35562833743021, 'max_bin': 429}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:17:06,116] Trial 35 finished with value: 0.7053440620145782 and parameters: {'n_estimators': 424, 'eta': 0.0650907093570569, 'max_depth': 8, 'alpha': 0.8397, 'lambda': 32.090670630192086, 'max_bin': 282}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:17:13,093] Trial 36 finished with value: 0.6994653260774323 and parameters: {'n_estimators': 375, 'eta': 0.06366750807373253, 'max_depth': 9, 'alpha': 0.8293, 'lambda': 30.523316456468077, 'max_bin': 291}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:17:20,398] Trial 37 finished with value: 0.7052457241009796 and parameters: {'n_estimators': 667, 'eta': 0.07707926881825632, 'max_depth': 8, 'alpha': 0.9, 'lambda': 33.290117171720496, 'max_bin': 273}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:17:27,986] Trial 38 finished with value: 0.699478262955028 and parameters: {'n_estimators': 818, 'eta': 0.07940055362441886, 'max_depth': 10, 'alpha': 0.9187000000000001, 'lambda': 32.78214069519127, 'max_bin': 276}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:17:35,040] Trial 39 finished with value: 0.6995944552667156 and parameters: {'n_estimators': 652, 'eta': 0.07446846411310327, 'max_depth': 9, 'alpha': 0.991, 'lambda': 27.664250505477, 'max_bin': 270}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:17:41,659] Trial 40 finished with value: 0.6984991938867143 and parameters: {'n_estimators': 710, 'eta': 0.08672179970140673, 'max_depth': 11, 'alpha': 0.89, 'lambda': 31.16466170756144, 'max_bin': 316}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:17:47,592] Trial 41 finished with value: 0.70297728436208 and parameters: {'n_estimators': 428, 'eta': 0.07794941174402903, 'max_depth': 8, 'alpha': 0.7775000000000001, 'lambda': 33.979164779393685, 'max_bin': 256}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:17:56,142] Trial 42 finished with value: 0.7042637493098467 and parameters: {'n_estimators': 805, 'eta': 0.064519245779689, 'max_depth': 8, 'alpha': 0.8260000000000001, 'lambda': 33.22682246508504, 'max_bin': 287}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:18:02,110] Trial 43 finished with value: 0.7015898025717047 and parameters: {'n_estimators': 584, 'eta': 0.09951872223738335, 'max_depth': 9, 'alpha': 0.7836000000000001, 'lambda': 36.621478951045006, 'max_bin': 312}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:18:09,272] Trial 44 finished with value: 0.7080927786244527 and parameters: {'n_estimators': 520, 'eta': 0.07513765798284061, 'max_depth': 8, 'alpha': 0.8628, 'lambda': 38.077080594942935, 'max_bin': 338}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:18:15,012] Trial 45 finished with value: 0.7000954480608181 and parameters: {'n_estimators': 522, 'eta': 0.08364588110351101, 'max_depth': 6, 'alpha': 0.9410000000000001, 'lambda': 29.88539800120382, 'max_bin': 332}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:18:22,596] Trial 46 finished with value: 0.7029500594789837 and parameters: {'n_estimators': 622, 'eta': 0.0742300535182091, 'max_depth': 9, 'alpha': 0.9134, 'lambda': 34.38842791222688, 'max_bin': 331}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:18:28,656] Trial 47 finished with value: 0.7054789214203417 and parameters: {'n_estimators': 270, 'eta': 0.09196916184132445, 'max_depth': 8, 'alpha': 0.8606, 'lambda': 37.44298350041872, 'max_bin': 362}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:18:32,130] Trial 48 finished with value: 0.6993290000707479 and parameters: {'n_estimators': 202, 'eta': 0.09099039987612698, 'max_depth': 7, 'alpha': 0.8493, 'lambda': 37.710553560014965, 'max_bin': 364}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:18:37,631] Trial 49 finished with value: 0.6994706649843917 and parameters: {'n_estimators': 251, 'eta': 0.06791777192054475, 'max_depth': 9, 'alpha': 0.7572, 'lambda': 37.975170546339484, 'max_bin': 362}. Best is trial 23 with value: 0.7082329914489204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.7082\n",
      "\tBest params:\n",
      "\t\tn_estimators: 524\n",
      "\t\teta: 0.07101769028875411\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7864\n",
      "\t\tlambda: 35.72460692646191\n",
      "\t\tmax_bin: 399\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBRegressor\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.689126\n",
      "1                    TP   32.000000\n",
      "2                    TN  201.000000\n",
      "3                    FP    1.000000\n",
      "4                    FN   34.000000\n",
      "5              Accuracy    0.869403\n",
      "6             Precision    0.969697\n",
      "7           Sensitivity    0.484848\n",
      "8           Specificity    0.995000\n",
      "9              F1 score    0.646465\n",
      "10  F1 score (weighted)    0.852568\n",
      "11     F1 score (macro)    0.783187\n",
      "12    Balanced Accuracy    0.739949\n",
      "13                  MCC    0.629225\n",
      "14                  NPV    0.855300\n",
      "15              ROC_AUC    0.739949\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_xgb_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_xgb_0_cat = np.where(((y_pred_xgb_0 >= 2) | (y_pred_xgb_0 <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_xgb_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_xgb_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:18:44,132] Trial 50 finished with value: 0.6969686373285711 and parameters: {'n_estimators': 341, 'eta': 0.08510079657711592, 'max_depth': 8, 'alpha': 0.8151, 'lambda': 35.59722262768717, 'max_bin': 383}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:18:46,926] Trial 51 finished with value: 0.6785405114003986 and parameters: {'n_estimators': 143, 'eta': 0.09358755848054703, 'max_depth': 8, 'alpha': 0.8826, 'lambda': 32.310048175408035, 'max_bin': 342}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:18:55,743] Trial 52 finished with value: 0.6987337427240905 and parameters: {'n_estimators': 695, 'eta': 0.07973642394300584, 'max_depth': 8, 'alpha': 0.8508, 'lambda': 35.34757937065807, 'max_bin': 278}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:19:04,931] Trial 53 finished with value: 0.6961457534582907 and parameters: {'n_estimators': 750, 'eta': 0.07576768645533094, 'max_depth': 8, 'alpha': 0.9463, 'lambda': 38.18328292017482, 'max_bin': 302}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:19:11,493] Trial 54 finished with value: 0.6966751358708693 and parameters: {'n_estimators': 275, 'eta': 0.08761273466862422, 'max_depth': 9, 'alpha': 0.7039000000000001, 'lambda': 33.7951145324962, 'max_bin': 324}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:19:17,015] Trial 55 finished with value: 0.6914087328807815 and parameters: {'n_estimators': 351, 'eta': 0.08124328749893207, 'max_depth': 7, 'alpha': 0.1292, 'lambda': 38.540647974977745, 'max_bin': 352}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:19:22,754] Trial 56 finished with value: 0.6962355386881065 and parameters: {'n_estimators': 422, 'eta': 0.09416892257079382, 'max_depth': 7, 'alpha': 0.7936000000000001, 'lambda': 36.250804295810454, 'max_bin': 384}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:19:30,270] Trial 57 finished with value: 0.6972035416605247 and parameters: {'n_estimators': 547, 'eta': 0.07633485892156974, 'max_depth': 8, 'alpha': 0.893, 'lambda': 38.645307730249044, 'max_bin': 253}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:19:40,649] Trial 58 finished with value: 0.6981278954094465 and parameters: {'n_estimators': 652, 'eta': 0.08589425747488272, 'max_depth': 9, 'alpha': 0.8569, 'lambda': 34.77391218475064, 'max_bin': 369}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:19:43,811] Trial 59 finished with value: 0.6831124730223656 and parameters: {'n_estimators': 148, 'eta': 0.06741588176522029, 'max_depth': 8, 'alpha': 0.39830000000000004, 'lambda': 32.70202968644189, 'max_bin': 341}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:19:53,264] Trial 60 finished with value: 0.6992466086968852 and parameters: {'n_estimators': 592, 'eta': 0.0725636924695708, 'max_depth': 10, 'alpha': 0.6622, 'lambda': 31.25775862704642, 'max_bin': 300}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:20:02,000] Trial 61 finished with value: 0.6910360860971964 and parameters: {'n_estimators': 490, 'eta': 0.05486909331830554, 'max_depth': 7, 'alpha': 0.4605, 'lambda': 39.70508555214953, 'max_bin': 401}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:20:10,567] Trial 62 finished with value: 0.6941171257545578 and parameters: {'n_estimators': 467, 'eta': 0.06228337744328221, 'max_depth': 8, 'alpha': 0.7575000000000001, 'lambda': 36.885547858063106, 'max_bin': 357}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:20:18,087] Trial 63 finished with value: 0.6961491752773739 and parameters: {'n_estimators': 519, 'eta': 0.08158761539943125, 'max_depth': 8, 'alpha': 0.8055, 'lambda': 39.03768973553045, 'max_bin': 267}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:20:26,824] Trial 64 finished with value: 0.6944125426954209 and parameters: {'n_estimators': 446, 'eta': 0.06049993444589295, 'max_depth': 8, 'alpha': 0.6458, 'lambda': 35.79119955794343, 'max_bin': 498}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:20:28,604] Trial 65 finished with value: 0.6395920011834837 and parameters: {'n_estimators': 88, 'eta': 0.06738904682135244, 'max_depth': 7, 'alpha': 0.9703, 'lambda': 37.67133674563354, 'max_bin': 282}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:20:37,670] Trial 66 finished with value: 0.7033766514539244 and parameters: {'n_estimators': 559, 'eta': 0.07729607234854356, 'max_depth': 9, 'alpha': 0.7018, 'lambda': 33.55196277655511, 'max_bin': 373}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:20:46,981] Trial 67 finished with value: 0.6976790383706525 and parameters: {'n_estimators': 732, 'eta': 0.07107843278429432, 'max_depth': 8, 'alpha': 0.9192, 'lambda': 34.97691241196383, 'max_bin': 325}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:20:56,321] Trial 68 finished with value: 0.6982699483960598 and parameters: {'n_estimators': 409, 'eta': 0.05614555844865309, 'max_depth': 9, 'alpha': 0.7339, 'lambda': 36.773319597311435, 'max_bin': 385}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:21:03,945] Trial 69 finished with value: 0.6958701814295172 and parameters: {'n_estimators': 504, 'eta': 0.07352013451015665, 'max_depth': 8, 'alpha': 0.6153000000000001, 'lambda': 39.516171238832214, 'max_bin': 412}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:21:07,251] Trial 70 finished with value: 0.668739239851322 and parameters: {'n_estimators': 198, 'eta': 0.051284767910210693, 'max_depth': 7, 'alpha': 0.8297, 'lambda': 37.97002479685882, 'max_bin': 476}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:21:14,385] Trial 71 finished with value: 0.6913281945574876 and parameters: {'n_estimators': 447, 'eta': 0.07079677594027993, 'max_depth': 7, 'alpha': 0.5168, 'lambda': 39.994024291692014, 'max_bin': 425}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:21:19,717] Trial 72 finished with value: 0.6892220776178396 and parameters: {'n_estimators': 482, 'eta': 0.06559782700271966, 'max_depth': 6, 'alpha': 0.5759000000000001, 'lambda': 36.24445306739907, 'max_bin': 433}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:21:28,877] Trial 73 finished with value: 0.6998145076710724 and parameters: {'n_estimators': 579, 'eta': 0.06962496959376005, 'max_depth': 8, 'alpha': 0.5926, 'lambda': 34.26619010436291, 'max_bin': 449}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:21:34,450] Trial 74 finished with value: 0.692038652056274 and parameters: {'n_estimators': 397, 'eta': 0.08309973908713744, 'max_depth': 7, 'alpha': 0.4398, 'lambda': 36.508262280698695, 'max_bin': 261}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:21:41,833] Trial 75 finished with value: 0.6934152677660658 and parameters: {'n_estimators': 368, 'eta': 0.05938640191015852, 'max_depth': 8, 'alpha': 0.3473, 'lambda': 38.65374839830691, 'max_bin': 404}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:21:50,392] Trial 76 finished with value: 0.7017470451547065 and parameters: {'n_estimators': 540, 'eta': 0.07904293918386038, 'max_depth': 9, 'alpha': 0.544, 'lambda': 33.510508985494724, 'max_bin': 417}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:22:02,405] Trial 77 finished with value: 0.6979054668035051 and parameters: {'n_estimators': 606, 'eta': 0.06396239646552455, 'max_depth': 10, 'alpha': 0.9053, 'lambda': 37.43170992231689, 'max_bin': 440}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:22:08,829] Trial 78 finished with value: 0.6925612434417593 and parameters: {'n_estimators': 644, 'eta': 0.07521717949302073, 'max_depth': 6, 'alpha': 0.683, 'lambda': 31.93415629018942, 'max_bin': 460}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:22:17,577] Trial 79 finished with value: 0.7004185999489828 and parameters: {'n_estimators': 466, 'eta': 0.06772102632088439, 'max_depth': 9, 'alpha': 0.8685, 'lambda': 35.12432458123934, 'max_bin': 351}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:22:26,921] Trial 80 finished with value: 0.6986317444921935 and parameters: {'n_estimators': 783, 'eta': 0.07277094781904228, 'max_depth': 8, 'alpha': 0.7603000000000001, 'lambda': 36.998220148388285, 'max_bin': 420}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:22:33,071] Trial 81 finished with value: 0.6843385623658476 and parameters: {'n_estimators': 883, 'eta': 0.06284196522953177, 'max_depth': 5, 'alpha': 0.8041, 'lambda': 32.76109160520212, 'max_bin': 285}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:22:41,576] Trial 82 finished with value: 0.6986098160739715 and parameters: {'n_estimators': 865, 'eta': 0.06546143417692932, 'max_depth': 8, 'alpha': 0.8294, 'lambda': 35.64714208447537, 'max_bin': 274}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:22:52,629] Trial 83 finished with value: 0.7017271053677265 and parameters: {'n_estimators': 843, 'eta': 0.06951547994092759, 'max_depth': 8, 'alpha': 0.8355, 'lambda': 33.712056274732696, 'max_bin': 291}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:23:00,831] Trial 84 finished with value: 0.6924138475858108 and parameters: {'n_estimators': 804, 'eta': 0.05965216776534339, 'max_depth': 7, 'alpha': 0.7283000000000001, 'lambda': 30.308141229643617, 'max_bin': 337}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:23:06,784] Trial 85 finished with value: 0.6957089633044925 and parameters: {'n_estimators': 282, 'eta': 0.07692758579072365, 'max_depth': 8, 'alpha': 0.7895000000000001, 'lambda': 34.534657532218375, 'max_bin': 308}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:23:16,134] Trial 86 finished with value: 0.6946074915258426 and parameters: {'n_estimators': 684, 'eta': 0.07993323536346734, 'max_depth': 9, 'alpha': 0.8791, 'lambda': 38.52571649828709, 'max_bin': 298}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:23:25,368] Trial 87 finished with value: 0.6981203764177079 and parameters: {'n_estimators': 497, 'eta': 0.05754971106640997, 'max_depth': 8, 'alpha': 0.9532, 'lambda': 29.286855244273074, 'max_bin': 432}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:23:34,352] Trial 88 finished with value: 0.7006184864945065 and parameters: {'n_estimators': 529, 'eta': 0.06428056042001021, 'max_depth': 8, 'alpha': 0.8637, 'lambda': 31.62630659009269, 'max_bin': 261}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:23:44,149] Trial 89 finished with value: 0.701383331485426 and parameters: {'n_estimators': 729, 'eta': 0.08482862324029986, 'max_depth': 9, 'alpha': 0.49500000000000005, 'lambda': 35.90553161872479, 'max_bin': 285}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:23:52,923] Trial 90 finished with value: 0.6962107550420045 and parameters: {'n_estimators': 565, 'eta': 0.06146080153241853, 'max_depth': 7, 'alpha': 0.9262, 'lambda': 33.1089428118216, 'max_bin': 389}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:24:02,038] Trial 91 finished with value: 0.6967580965668315 and parameters: {'n_estimators': 439, 'eta': 0.0714476375642527, 'max_depth': 9, 'alpha': 0.7285, 'lambda': 37.13451441248481, 'max_bin': 410}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:24:10,851] Trial 92 finished with value: 0.7021643797710512 and parameters: {'n_estimators': 408, 'eta': 0.07410908993810561, 'max_depth': 9, 'alpha': 0.757, 'lambda': 35.31083505938367, 'max_bin': 418}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:24:19,487] Trial 93 finished with value: 0.6968474413703646 and parameters: {'n_estimators': 456, 'eta': 0.06867418695621426, 'max_depth': 8, 'alpha': 0.6654, 'lambda': 39.08803758555519, 'max_bin': 379}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:24:27,746] Trial 94 finished with value: 0.6937202494917166 and parameters: {'n_estimators': 429, 'eta': 0.06584477460538912, 'max_depth': 8, 'alpha': 0.8169000000000001, 'lambda': 37.822107652231544, 'max_bin': 400}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:24:35,342] Trial 95 finished with value: 0.6956571636607066 and parameters: {'n_estimators': 356, 'eta': 0.07590186136260327, 'max_depth': 8, 'alpha': 0.7775000000000001, 'lambda': 36.16187838821947, 'max_bin': 367}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:24:45,217] Trial 96 finished with value: 0.7023567317669706 and parameters: {'n_estimators': 488, 'eta': 0.07215479701415363, 'max_depth': 9, 'alpha': 0.7422000000000001, 'lambda': 34.74787617028422, 'max_bin': 320}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:24:54,660] Trial 97 finished with value: 0.6968205025172424 and parameters: {'n_estimators': 515, 'eta': 0.07723460568531987, 'max_depth': 12, 'alpha': 0.8994000000000001, 'lambda': 30.83745749902667, 'max_bin': 358}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:25:00,961] Trial 98 finished with value: 0.6967509686045695 and parameters: {'n_estimators': 326, 'eta': 0.06942219605138536, 'max_depth': 8, 'alpha': 0.6031000000000001, 'lambda': 32.43225425391747, 'max_bin': 443}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:25:09,892] Trial 99 finished with value: 0.6943699843731689 and parameters: {'n_estimators': 709, 'eta': 0.08096715430802393, 'max_depth': 7, 'alpha': 0.6408, 'lambda': 39.16406846416615, 'max_bin': 350}. Best is trial 23 with value: 0.7082329914489204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.7082\n",
      "\tBest params:\n",
      "\t\tn_estimators: 524\n",
      "\t\teta: 0.07101769028875411\n",
      "\t\tmax_depth: 8\n",
      "\t\talpha: 0.7864\n",
      "\t\tlambda: 35.72460692646191\n",
      "\t\tmax_bin: 399\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.689126    0.712882\n",
      "1                    TP   32.000000   41.000000\n",
      "2                    TN  201.000000  194.000000\n",
      "3                    FP    1.000000    6.000000\n",
      "4                    FN   34.000000   27.000000\n",
      "5              Accuracy    0.869403    0.876866\n",
      "6             Precision    0.969697    0.872340\n",
      "7           Sensitivity    0.484848    0.602941\n",
      "8           Specificity    0.995000    0.970000\n",
      "9              F1 score    0.646465    0.713043\n",
      "10  F1 score (weighted)    0.852568    0.868694\n",
      "11     F1 score (macro)    0.783187    0.817329\n",
      "12    Balanced Accuracy    0.739949    0.786471\n",
      "13                  MCC    0.629225    0.655593\n",
      "14                  NPV    0.855300    0.877800\n",
      "15              ROC_AUC    0.739949    0.786471\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_xgb_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet1 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_1_cat = np.where(((y_pred_xgb_1 >= 2) | (y_pred_xgb_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_xgb_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_xgb_1_cat)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:25:20,801] Trial 100 finished with value: 0.7071717573271215 and parameters: {'n_estimators': 662, 'eta': 0.08280544874678454, 'max_depth': 9, 'alpha': 0.7039000000000001, 'lambda': 34.13882418603396, 'max_bin': 433}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:25:28,845] Trial 101 finished with value: 0.7068741262482416 and parameters: {'n_estimators': 638, 'eta': 0.0894628530209697, 'max_depth': 9, 'alpha': 0.6916, 'lambda': 34.07386170111805, 'max_bin': 436}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:25:39,153] Trial 102 finished with value: 0.6993558260014866 and parameters: {'n_estimators': 634, 'eta': 0.08331259623385129, 'max_depth': 10, 'alpha': 0.7132000000000001, 'lambda': 34.15832949082007, 'max_bin': 457}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:25:48,198] Trial 103 finished with value: 0.7067851721048273 and parameters: {'n_estimators': 665, 'eta': 0.08754393855232949, 'max_depth': 9, 'alpha': 0.6724, 'lambda': 32.169694371351326, 'max_bin': 436}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:25:56,935] Trial 104 finished with value: 0.7053044497191617 and parameters: {'n_estimators': 672, 'eta': 0.08909690418076124, 'max_depth': 9, 'alpha': 0.686, 'lambda': 32.06001987806147, 'max_bin': 454}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:26:06,609] Trial 105 finished with value: 0.7067083503703634 and parameters: {'n_estimators': 674, 'eta': 0.08893642249879087, 'max_depth': 9, 'alpha': 0.6887, 'lambda': 30.861402923267956, 'max_bin': 437}. Best is trial 23 with value: 0.7082329914489204.\n",
      "[I 2023-12-20 07:26:16,581] Trial 106 finished with value: 0.7087486352094894 and parameters: {'n_estimators': 667, 'eta': 0.08793017865098833, 'max_depth': 9, 'alpha': 0.6956, 'lambda': 31.29705291579764, 'max_bin': 468}. Best is trial 106 with value: 0.7087486352094894.\n",
      "[I 2023-12-20 07:26:25,949] Trial 107 finished with value: 0.7075017798251004 and parameters: {'n_estimators': 672, 'eta': 0.08879919344363378, 'max_depth': 9, 'alpha': 0.6950000000000001, 'lambda': 30.886629278881095, 'max_bin': 471}. Best is trial 106 with value: 0.7087486352094894.\n",
      "[I 2023-12-20 07:26:35,012] Trial 108 finished with value: 0.7053539791913057 and parameters: {'n_estimators': 709, 'eta': 0.08750929480736065, 'max_depth': 9, 'alpha': 0.6559, 'lambda': 30.881739028169925, 'max_bin': 475}. Best is trial 106 with value: 0.7087486352094894.\n",
      "[I 2023-12-20 07:26:45,340] Trial 109 finished with value: 0.7049589629284931 and parameters: {'n_estimators': 705, 'eta': 0.0868025752486646, 'max_depth': 10, 'alpha': 0.6588, 'lambda': 28.178695743808944, 'max_bin': 479}. Best is trial 106 with value: 0.7087486352094894.\n",
      "[I 2023-12-20 07:26:54,137] Trial 110 finished with value: 0.7068988769433326 and parameters: {'n_estimators': 622, 'eta': 0.092485473890357, 'max_depth': 9, 'alpha': 0.6925, 'lambda': 29.434348430300613, 'max_bin': 466}. Best is trial 106 with value: 0.7087486352094894.\n",
      "[I 2023-12-20 07:27:02,806] Trial 111 finished with value: 0.7075625823711678 and parameters: {'n_estimators': 611, 'eta': 0.09187316785642116, 'max_depth': 9, 'alpha': 0.7013, 'lambda': 30.686583258040105, 'max_bin': 472}. Best is trial 106 with value: 0.7087486352094894.\n",
      "[I 2023-12-20 07:27:12,160] Trial 112 finished with value: 0.7073715488735448 and parameters: {'n_estimators': 623, 'eta': 0.09229025720279421, 'max_depth': 9, 'alpha': 0.7005, 'lambda': 29.807814337672998, 'max_bin': 469}. Best is trial 106 with value: 0.7087486352094894.\n",
      "[I 2023-12-20 07:27:20,587] Trial 113 finished with value: 0.710476065252818 and parameters: {'n_estimators': 605, 'eta': 0.08934903955009602, 'max_depth': 9, 'alpha': 0.7010000000000001, 'lambda': 29.496290513087406, 'max_bin': 470}. Best is trial 113 with value: 0.710476065252818.\n",
      "[I 2023-12-20 07:27:29,410] Trial 114 finished with value: 0.7090616253773654 and parameters: {'n_estimators': 618, 'eta': 0.09303847215872167, 'max_depth': 9, 'alpha': 0.6910000000000001, 'lambda': 29.75880628447131, 'max_bin': 466}. Best is trial 113 with value: 0.710476065252818.\n",
      "[I 2023-12-20 07:27:37,848] Trial 115 finished with value: 0.7088296456574877 and parameters: {'n_estimators': 614, 'eta': 0.09567622461635208, 'max_depth': 9, 'alpha': 0.7055, 'lambda': 29.315002135654506, 'max_bin': 467}. Best is trial 113 with value: 0.710476065252818.\n",
      "[I 2023-12-20 07:27:46,941] Trial 116 finished with value: 0.7066322089650466 and parameters: {'n_estimators': 620, 'eta': 0.09468224870451163, 'max_depth': 10, 'alpha': 0.7152000000000001, 'lambda': 29.670964114457362, 'max_bin': 468}. Best is trial 113 with value: 0.710476065252818.\n",
      "[I 2023-12-20 07:27:54,585] Trial 117 finished with value: 0.7085043906579054 and parameters: {'n_estimators': 603, 'eta': 0.09179470234221328, 'max_depth': 9, 'alpha': 0.6288, 'lambda': 28.768449218505665, 'max_bin': 486}. Best is trial 113 with value: 0.710476065252818.\n",
      "[I 2023-12-20 07:28:03,147] Trial 118 finished with value: 0.7096103162953831 and parameters: {'n_estimators': 608, 'eta': 0.09697459022070051, 'max_depth': 9, 'alpha': 0.6239, 'lambda': 28.91085215652471, 'max_bin': 488}. Best is trial 113 with value: 0.710476065252818.\n",
      "[I 2023-12-20 07:28:11,676] Trial 119 finished with value: 0.7104341929564479 and parameters: {'n_estimators': 603, 'eta': 0.09668563520708241, 'max_depth': 10, 'alpha': 0.6207, 'lambda': 28.47674423529553, 'max_bin': 490}. Best is trial 113 with value: 0.710476065252818.\n",
      "[I 2023-12-20 07:28:19,891] Trial 120 finished with value: 0.7070875773249263 and parameters: {'n_estimators': 605, 'eta': 0.09680606941325616, 'max_depth': 10, 'alpha': 0.621, 'lambda': 27.345150245575557, 'max_bin': 491}. Best is trial 113 with value: 0.710476065252818.\n",
      "[I 2023-12-20 07:28:29,080] Trial 121 finished with value: 0.7107864949635834 and parameters: {'n_estimators': 570, 'eta': 0.0960461800647675, 'max_depth': 10, 'alpha': 0.6382, 'lambda': 28.76844285630968, 'max_bin': 485}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:28:38,235] Trial 122 finished with value: 0.70549214046568 and parameters: {'n_estimators': 578, 'eta': 0.09646415032204679, 'max_depth': 11, 'alpha': 0.5766, 'lambda': 28.268581450613627, 'max_bin': 487}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:28:46,810] Trial 123 finished with value: 0.708595675262786 and parameters: {'n_estimators': 597, 'eta': 0.09129771008925183, 'max_depth': 10, 'alpha': 0.6435000000000001, 'lambda': 28.914725310999984, 'max_bin': 499}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:28:55,169] Trial 124 finished with value: 0.7032399025003351 and parameters: {'n_estimators': 596, 'eta': 0.09946158927070907, 'max_depth': 10, 'alpha': 0.6237, 'lambda': 28.802507650468815, 'max_bin': 497}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:29:03,818] Trial 125 finished with value: 0.7025874654138915 and parameters: {'n_estimators': 558, 'eta': 0.09155935725249986, 'max_depth': 11, 'alpha': 0.6455000000000001, 'lambda': 26.983724570593456, 'max_bin': 482}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:29:13,044] Trial 126 finished with value: 0.7067596242593206 and parameters: {'n_estimators': 578, 'eta': 0.0959447578281885, 'max_depth': 10, 'alpha': 0.5989, 'lambda': 26.09928631450595, 'max_bin': 488}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:29:21,570] Trial 127 finished with value: 0.7104942507907064 and parameters: {'n_estimators': 538, 'eta': 0.0935518633751383, 'max_depth': 10, 'alpha': 0.5556, 'lambda': 28.784094784599205, 'max_bin': 472}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:29:30,141] Trial 128 finished with value: 0.7063349039982746 and parameters: {'n_estimators': 545, 'eta': 0.09417464835981344, 'max_depth': 10, 'alpha': 0.5524, 'lambda': 28.703576197151534, 'max_bin': 482}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:29:38,464] Trial 129 finished with value: 0.7074265061251148 and parameters: {'n_estimators': 596, 'eta': 0.09790077659648524, 'max_depth': 10, 'alpha': 0.5682, 'lambda': 30.175019512098174, 'max_bin': 495}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:29:46,739] Trial 130 finished with value: 0.709441048330482 and parameters: {'n_estimators': 573, 'eta': 0.09418776824507963, 'max_depth': 10, 'alpha': 0.5375, 'lambda': 28.018752570769877, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:29:56,912] Trial 131 finished with value: 0.7080499519965657 and parameters: {'n_estimators': 607, 'eta': 0.09418929075849271, 'max_depth': 10, 'alpha': 0.5182, 'lambda': 29.075901379634256, 'max_bin': 474}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:30:05,949] Trial 132 finished with value: 0.705975657232008 and parameters: {'n_estimators': 567, 'eta': 0.09477474110965596, 'max_depth': 10, 'alpha': 0.5205000000000001, 'lambda': 28.492496715115255, 'max_bin': 491}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:30:14,961] Trial 133 finished with value: 0.7051035385246289 and parameters: {'n_estimators': 546, 'eta': 0.0985182676701272, 'max_depth': 11, 'alpha': 0.5478000000000001, 'lambda': 27.844243786931553, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:30:23,594] Trial 134 finished with value: 0.7043630054053418 and parameters: {'n_estimators': 531, 'eta': 0.09730138728122598, 'max_depth': 10, 'alpha': 0.5795, 'lambda': 26.524238616027677, 'max_bin': 484}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:30:32,744] Trial 135 finished with value: 0.7083241710989819 and parameters: {'n_estimators': 587, 'eta': 0.09355387579960821, 'max_depth': 10, 'alpha': 0.4772, 'lambda': 29.21900476262399, 'max_bin': 480}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:30:41,307] Trial 136 finished with value: 0.7067459035140207 and parameters: {'n_estimators': 582, 'eta': 0.0906566744953614, 'max_depth': 10, 'alpha': 0.4682, 'lambda': 27.610153825231787, 'max_bin': 480}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:30:50,483] Trial 137 finished with value: 0.7083297303839077 and parameters: {'n_estimators': 639, 'eta': 0.09997347990287664, 'max_depth': 10, 'alpha': 0.4898, 'lambda': 25.534866089271958, 'max_bin': 493}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:30:58,883] Trial 138 finished with value: 0.7026805570282562 and parameters: {'n_estimators': 644, 'eta': 0.09981589810149703, 'max_depth': 10, 'alpha': 0.448, 'lambda': 26.111804787793005, 'max_bin': 461}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:31:07,482] Trial 139 finished with value: 0.705843703961053 and parameters: {'n_estimators': 631, 'eta': 0.09625560307424412, 'max_depth': 11, 'alpha': 0.4894, 'lambda': 25.18025882474766, 'max_bin': 493}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:31:16,733] Trial 140 finished with value: 0.7082101149358234 and parameters: {'n_estimators': 565, 'eta': 0.09332033883920154, 'max_depth': 10, 'alpha': 0.42310000000000003, 'lambda': 28.959347748957, 'max_bin': 489}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:31:25,965] Trial 141 finished with value: 0.7073289053421263 and parameters: {'n_estimators': 564, 'eta': 0.09325748144372493, 'max_depth': 10, 'alpha': 0.4269, 'lambda': 29.433049707003242, 'max_bin': 488}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:31:33,890] Trial 142 finished with value: 0.7040816596483125 and parameters: {'n_estimators': 588, 'eta': 0.09788835517326505, 'max_depth': 10, 'alpha': 0.3902, 'lambda': 28.181606839969927, 'max_bin': 486}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:31:42,900] Trial 143 finished with value: 0.7048307915793856 and parameters: {'n_estimators': 600, 'eta': 0.09497726637075578, 'max_depth': 10, 'alpha': 0.34890000000000004, 'lambda': 27.049249435473193, 'max_bin': 477}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:31:52,672] Trial 144 finished with value: 0.7035581262598585 and parameters: {'n_estimators': 651, 'eta': 0.0904397852408002, 'max_depth': 10, 'alpha': 0.4161, 'lambda': 30.005595785835215, 'max_bin': 494}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:32:01,611] Trial 145 finished with value: 0.704475650073758 and parameters: {'n_estimators': 573, 'eta': 0.09317099560020792, 'max_depth': 10, 'alpha': 0.6074, 'lambda': 29.063088198016064, 'max_bin': 484}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:32:10,450] Trial 146 finished with value: 0.7053847793410091 and parameters: {'n_estimators': 558, 'eta': 0.09638336079867245, 'max_depth': 10, 'alpha': 0.47800000000000004, 'lambda': 27.584566161705077, 'max_bin': 464}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:32:18,539] Trial 147 finished with value: 0.7033735921040833 and parameters: {'n_estimators': 618, 'eta': 0.0999412500600772, 'max_depth': 10, 'alpha': 0.533, 'lambda': 28.360028381488895, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:32:28,280] Trial 148 finished with value: 0.7024727803019684 and parameters: {'n_estimators': 593, 'eta': 0.09120520102654124, 'max_depth': 11, 'alpha': 0.6333000000000001, 'lambda': 30.242465212417383, 'max_bin': 492}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:32:36,280] Trial 149 finished with value: 0.7072563788966509 and parameters: {'n_estimators': 544, 'eta': 0.09504892967514264, 'max_depth': 10, 'alpha': 0.49660000000000004, 'lambda': 28.970682324528433, 'max_bin': 478}. Best is trial 121 with value: 0.7107864949635834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7108\n",
      "\tBest params:\n",
      "\t\tn_estimators: 570\n",
      "\t\teta: 0.0960461800647675\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.6382\n",
      "\t\tlambda: 28.76844285630968\n",
      "\t\tmax_bin: 485\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.689126    0.712882    0.708425\n",
      "1                    TP   32.000000   41.000000   39.000000\n",
      "2                    TN  201.000000  194.000000  195.000000\n",
      "3                    FP    1.000000    6.000000    5.000000\n",
      "4                    FN   34.000000   27.000000   29.000000\n",
      "5              Accuracy    0.869403    0.876866    0.873134\n",
      "6             Precision    0.969697    0.872340    0.886364\n",
      "7           Sensitivity    0.484848    0.602941    0.573529\n",
      "8           Specificity    0.995000    0.970000    0.975000\n",
      "9              F1 score    0.646465    0.713043    0.696429\n",
      "10  F1 score (weighted)    0.852568    0.868694    0.863132\n",
      "11     F1 score (macro)    0.783187    0.817329    0.808120\n",
      "12    Balanced Accuracy    0.739949    0.786471    0.774265\n",
      "13                  MCC    0.629225    0.655593    0.644346\n",
      "14                  NPV    0.855300    0.877800    0.870500\n",
      "15              ROC_AUC    0.739949    0.786471    0.774265\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_xgb_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet2 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_2_cat = np.where(((y_pred_xgb_2 >= 2) | (y_pred_xgb_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_xgb_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_xgb_2_cat)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:32:43,342] Trial 150 finished with value: 0.6804864355985524 and parameters: {'n_estimators': 507, 'eta': 0.09780725444590554, 'max_depth': 10, 'alpha': 0.5848, 'lambda': 31.481349598062945, 'max_bin': 489}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:32:50,900] Trial 151 finished with value: 0.6822258066810625 and parameters: {'n_estimators': 532, 'eta': 0.08532891328536724, 'max_depth': 10, 'alpha': 0.6472, 'lambda': 26.767193198515102, 'max_bin': 496}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:32:57,810] Trial 152 finished with value: 0.6851963576605897 and parameters: {'n_estimators': 636, 'eta': 0.09286240519435611, 'max_depth': 9, 'alpha': 0.5595, 'lambda': 27.579397246551544, 'max_bin': 471}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:33:06,148] Trial 153 finished with value: 0.6816788756778215 and parameters: {'n_estimators': 520, 'eta': 0.08978130927995895, 'max_depth': 10, 'alpha': 0.6286, 'lambda': 29.529663238760847, 'max_bin': 482}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:33:12,906] Trial 154 finished with value: 0.6825257274806136 and parameters: {'n_estimators': 613, 'eta': 0.09568799265659045, 'max_depth': 9, 'alpha': 0.4534, 'lambda': 25.621512982183553, 'max_bin': 475}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:33:21,132] Trial 155 finished with value: 0.6839867504781607 and parameters: {'n_estimators': 554, 'eta': 0.09276301092185095, 'max_depth': 9, 'alpha': 0.3694, 'lambda': 28.4270360562683, 'max_bin': 459}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:33:27,661] Trial 156 finished with value: 0.6806758692809903 and parameters: {'n_estimators': 575, 'eta': 0.08625906701844763, 'max_depth': 10, 'alpha': 0.6685, 'lambda': 24.79914099791743, 'max_bin': 451}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:33:33,687] Trial 157 finished with value: 0.6781256504669539 and parameters: {'n_estimators': 592, 'eta': 0.08861209710385898, 'max_depth': 10, 'alpha': 0.5991000000000001, 'lambda': 24.11364728553933, 'max_bin': 484}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:33:40,145] Trial 158 finished with value: 0.680279339879287 and parameters: {'n_estimators': 642, 'eta': 0.09081435289284001, 'max_depth': 9, 'alpha': 0.5031, 'lambda': 30.373578111020638, 'max_bin': 465}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:33:48,240] Trial 159 finished with value: 0.6844199230672607 and parameters: {'n_estimators': 610, 'eta': 0.09796970866313193, 'max_depth': 11, 'alpha': 0.2303, 'lambda': 27.167610499784875, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:33:55,971] Trial 160 finished with value: 0.6800078773461762 and parameters: {'n_estimators': 536, 'eta': 0.09354790137944537, 'max_depth': 9, 'alpha': 0.6688000000000001, 'lambda': 29.463443284267495, 'max_bin': 489}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:34:03,105] Trial 161 finished with value: 0.680690289887907 and parameters: {'n_estimators': 605, 'eta': 0.09424029535154937, 'max_depth': 10, 'alpha': 0.5148, 'lambda': 28.752840349631718, 'max_bin': 474}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:34:11,250] Trial 162 finished with value: 0.6818093486801853 and parameters: {'n_estimators': 630, 'eta': 0.09578611442558581, 'max_depth': 10, 'alpha': 0.5216000000000001, 'lambda': 31.394054463383068, 'max_bin': 479}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:34:18,645] Trial 163 finished with value: 0.6819343989460122 and parameters: {'n_estimators': 580, 'eta': 0.09098603517416172, 'max_depth': 10, 'alpha': 0.5461, 'lambda': 29.07285955918082, 'max_bin': 472}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:34:25,126] Trial 164 finished with value: 0.6840540823517035 and parameters: {'n_estimators': 684, 'eta': 0.09995776263957283, 'max_depth': 10, 'alpha': 0.6197, 'lambda': 27.887193224420955, 'max_bin': 494}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:34:32,424] Trial 165 finished with value: 0.6793035356454207 and parameters: {'n_estimators': 650, 'eta': 0.08816337308492649, 'max_depth': 10, 'alpha': 0.42750000000000005, 'lambda': 30.438168304378916, 'max_bin': 486}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:34:40,365] Trial 166 finished with value: 0.6809756118519399 and parameters: {'n_estimators': 557, 'eta': 0.09486260073862915, 'max_depth': 9, 'alpha': 0.47750000000000004, 'lambda': 29.420324712529762, 'max_bin': 477}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:34:47,798] Trial 167 finished with value: 0.6879915831314651 and parameters: {'n_estimators': 618, 'eta': 0.09741852515128743, 'max_depth': 10, 'alpha': 0.6483, 'lambda': 26.713718128808363, 'max_bin': 468}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:34:56,219] Trial 168 finished with value: 0.6850031676379065 and parameters: {'n_estimators': 570, 'eta': 0.09216709545277546, 'max_depth': 10, 'alpha': 0.6759000000000001, 'lambda': 28.01506406736766, 'max_bin': 491}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:35:03,926] Trial 169 finished with value: 0.679984122997207 and parameters: {'n_estimators': 597, 'eta': 0.08439957553321908, 'max_depth': 9, 'alpha': 0.7296, 'lambda': 31.30216457587632, 'max_bin': 462}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:35:11,481] Trial 170 finished with value: 0.6778725573387124 and parameters: {'n_estimators': 628, 'eta': 0.0897950294374041, 'max_depth': 9, 'alpha': 0.5293, 'lambda': 29.947248072196217, 'max_bin': 481}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:35:16,991] Trial 171 finished with value: 0.6755744974511385 and parameters: {'n_estimators': 611, 'eta': 0.09234908843944652, 'max_depth': 9, 'alpha': 0.7188, 'lambda': 30.68027281165434, 'max_bin': 469}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:35:23,907] Trial 172 finished with value: 0.6854219001862095 and parameters: {'n_estimators': 587, 'eta': 0.09390712851140633, 'max_depth': 9, 'alpha': 0.5705, 'lambda': 28.648791821130448, 'max_bin': 456}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:35:30,036] Trial 173 finished with value: 0.6771075501399159 and parameters: {'n_estimators': 611, 'eta': 0.08666616983762716, 'max_depth': 10, 'alpha': 0.6296, 'lambda': 29.144136220015202, 'max_bin': 472}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:35:37,176] Trial 174 finished with value: 0.6826624927758909 and parameters: {'n_estimators': 597, 'eta': 0.0965424761345056, 'max_depth': 9, 'alpha': 0.762, 'lambda': 29.990627997041745, 'max_bin': 446}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:35:46,017] Trial 175 finished with value: 0.6819354226417385 and parameters: {'n_estimators': 651, 'eta': 0.09008973882695445, 'max_depth': 9, 'alpha': 0.6054, 'lambda': 30.819659751460794, 'max_bin': 495}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:35:54,636] Trial 176 finished with value: 0.6835010402558082 and parameters: {'n_estimators': 508, 'eta': 0.09228357026880717, 'max_depth': 10, 'alpha': 0.7417, 'lambda': 27.363810320492775, 'max_bin': 484}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:36:02,366] Trial 177 finished with value: 0.680939365006014 and parameters: {'n_estimators': 547, 'eta': 0.09792895052214405, 'max_depth': 10, 'alpha': 0.6576000000000001, 'lambda': 28.30785681913192, 'max_bin': 474}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:36:10,726] Trial 178 finished with value: 0.6837557662783988 and parameters: {'n_estimators': 568, 'eta': 0.09537747507440716, 'max_depth': 9, 'alpha': 0.6817000000000001, 'lambda': 31.89235580007181, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:36:18,227] Trial 179 finished with value: 0.6803297946542572 and parameters: {'n_estimators': 630, 'eta': 0.08804111190376185, 'max_depth': 10, 'alpha': 0.7106, 'lambda': 25.946449965026275, 'max_bin': 488}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:36:25,795] Trial 180 finished with value: 0.679087704800606 and parameters: {'n_estimators': 527, 'eta': 0.09354454742884656, 'max_depth': 9, 'alpha': 0.5021, 'lambda': 29.98813386234433, 'max_bin': 479}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:36:32,860] Trial 181 finished with value: 0.6821793902684113 and parameters: {'n_estimators': 697, 'eta': 0.08928618785837582, 'max_depth': 9, 'alpha': 0.7058, 'lambda': 30.749766140132337, 'max_bin': 466}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:36:39,047] Trial 182 finished with value: 0.6818850100578382 and parameters: {'n_estimators': 671, 'eta': 0.09158490283611144, 'max_depth': 9, 'alpha': 0.6888000000000001, 'lambda': 29.070717666021686, 'max_bin': 473}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:36:47,398] Trial 183 finished with value: 0.6850311361599251 and parameters: {'n_estimators': 661, 'eta': 0.08657984930906784, 'max_depth': 9, 'alpha': 0.07490000000000001, 'lambda': 32.663477535911355, 'max_bin': 469}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:36:54,787] Trial 184 finished with value: 0.6829637022161774 and parameters: {'n_estimators': 607, 'eta': 0.0961507801904189, 'max_depth': 9, 'alpha': 0.6457, 'lambda': 31.231594193466005, 'max_bin': 485}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:37:02,086] Trial 185 finished with value: 0.6816325605733644 and parameters: {'n_estimators': 582, 'eta': 0.09852299596386405, 'max_depth': 10, 'alpha': 0.4676, 'lambda': 27.831682019175446, 'max_bin': 492}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:37:10,717] Trial 186 finished with value: 0.6819962407971804 and parameters: {'n_estimators': 625, 'eta': 0.08432126567572706, 'max_depth': 9, 'alpha': 0.5924, 'lambda': 29.857911586401162, 'max_bin': 477}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:37:17,097] Trial 187 finished with value: 0.6804329640647294 and parameters: {'n_estimators': 642, 'eta': 0.09391488910051714, 'max_depth': 10, 'alpha': 0.6808000000000001, 'lambda': 28.74585345306237, 'max_bin': 463}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:37:24,244] Trial 188 finished with value: 0.6810732721797847 and parameters: {'n_estimators': 590, 'eta': 0.0887534565584039, 'max_depth': 10, 'alpha': 0.6165, 'lambda': 30.42754544592259, 'max_bin': 483}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:37:30,799] Trial 189 finished with value: 0.6762887928461285 and parameters: {'n_estimators': 558, 'eta': 0.09106801834671183, 'max_depth': 11, 'alpha': 0.74, 'lambda': 27.106364306222268, 'max_bin': 455}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:37:38,169] Trial 190 finished with value: 0.6843844246198595 and parameters: {'n_estimators': 682, 'eta': 0.09520948161517578, 'max_depth': 9, 'alpha': 0.6634, 'lambda': 31.7503608115415, 'max_bin': 496}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:37:44,157] Trial 191 finished with value: 0.6810208959856846 and parameters: {'n_estimators': 597, 'eta': 0.09831139376756848, 'max_depth': 10, 'alpha': 0.5629000000000001, 'lambda': 29.346151370051956, 'max_bin': 496}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:37:51,254] Trial 192 finished with value: 0.6805359003257905 and parameters: {'n_estimators': 617, 'eta': 0.09725945180598893, 'max_depth': 10, 'alpha': 0.5622, 'lambda': 30.20686683891919, 'max_bin': 490}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:37:58,194] Trial 193 finished with value: 0.6815179979420971 and parameters: {'n_estimators': 579, 'eta': 0.09972731623382562, 'max_depth': 10, 'alpha': 0.5441, 'lambda': 28.234018283546654, 'max_bin': 495}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:38:05,161] Trial 194 finished with value: 0.6775564111497119 and parameters: {'n_estimators': 605, 'eta': 0.09280019341423261, 'max_depth': 10, 'alpha': 0.5812, 'lambda': 31.046433096080897, 'max_bin': 479}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:38:13,240] Trial 195 finished with value: 0.6840249056293068 and parameters: {'n_estimators': 660, 'eta': 0.09606786018595584, 'max_depth': 10, 'alpha': 0.6331, 'lambda': 29.72783107087404, 'max_bin': 487}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:38:19,624] Trial 196 finished with value: 0.6802506288485894 and parameters: {'n_estimators': 568, 'eta': 0.09099976118698512, 'max_depth': 10, 'alpha': 0.5157, 'lambda': 29.058839031697925, 'max_bin': 473}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:38:25,473] Trial 197 finished with value: 0.6776706313900885 and parameters: {'n_estimators': 540, 'eta': 0.09408313633783205, 'max_depth': 9, 'alpha': 0.48700000000000004, 'lambda': 26.553482018028006, 'max_bin': 491}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:38:31,718] Trial 198 finished with value: 0.6773559915772684 and parameters: {'n_estimators': 630, 'eta': 0.09725162147599763, 'max_depth': 10, 'alpha': 0.6970000000000001, 'lambda': 28.223772171816407, 'max_bin': 481}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:38:38,481] Trial 199 finished with value: 0.6788326570017824 and parameters: {'n_estimators': 591, 'eta': 0.08013639533654873, 'max_depth': 9, 'alpha': 0.6106, 'lambda': 23.98748732984158, 'max_bin': 470}. Best is trial 121 with value: 0.7107864949635834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7108\n",
      "\tBest params:\n",
      "\t\tn_estimators: 570\n",
      "\t\teta: 0.0960461800647675\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.6382\n",
      "\t\tlambda: 28.76844285630968\n",
      "\t\tmax_bin: 485\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.689126    0.712882    0.708425    0.727859\n",
      "1                    TP   32.000000   41.000000   39.000000   41.000000\n",
      "2                    TN  201.000000  194.000000  195.000000  197.000000\n",
      "3                    FP    1.000000    6.000000    5.000000    2.000000\n",
      "4                    FN   34.000000   27.000000   29.000000   28.000000\n",
      "5              Accuracy    0.869403    0.876866    0.873134    0.888060\n",
      "6             Precision    0.969697    0.872340    0.886364    0.953488\n",
      "7           Sensitivity    0.484848    0.602941    0.573529    0.594203\n",
      "8           Specificity    0.995000    0.970000    0.975000    0.989900\n",
      "9              F1 score    0.646465    0.713043    0.696429    0.732143\n",
      "10  F1 score (weighted)    0.852568    0.868694    0.863132    0.878499\n",
      "11     F1 score (macro)    0.783187    0.817329    0.808120    0.830694\n",
      "12    Balanced Accuracy    0.739949    0.786471    0.774265    0.792076\n",
      "13                  MCC    0.629225    0.655593    0.644346    0.695908\n",
      "14                  NPV    0.855300    0.877800    0.870500    0.875600\n",
      "15              ROC_AUC    0.739949    0.786471    0.774265    0.792076\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_xgb_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "#Y_trainSet3 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_3_cat = np.where(((y_pred_xgb_3 >= 2) | (y_pred_xgb_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_xgb_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_xgb_3_cat)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:38:47,407] Trial 200 finished with value: 0.6799516286986759 and parameters: {'n_estimators': 605, 'eta': 0.09244694396163632, 'max_depth': 10, 'alpha': 0.7196, 'lambda': 32.47110971968645, 'max_bin': 460}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:38:54,684] Trial 201 finished with value: 0.6800623351913496 and parameters: {'n_estimators': 622, 'eta': 0.0882230391493991, 'max_depth': 9, 'alpha': 0.7001000000000001, 'lambda': 30.227971748168176, 'max_bin': 466}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:39:01,343] Trial 202 finished with value: 0.6758164477228749 and parameters: {'n_estimators': 650, 'eta': 0.09414993052733732, 'max_depth': 9, 'alpha': 0.6519, 'lambda': 29.495352669397715, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:39:09,403] Trial 203 finished with value: 0.6746643166322451 and parameters: {'n_estimators': 619, 'eta': 0.09081810116663064, 'max_depth': 9, 'alpha': 0.5348, 'lambda': 31.085477427419598, 'max_bin': 472}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:39:16,748] Trial 204 finished with value: 0.6793893202200071 and parameters: {'n_estimators': 570, 'eta': 0.08584298075913291, 'max_depth': 9, 'alpha': 0.6696000000000001, 'lambda': 28.756122540676365, 'max_bin': 476}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:39:24,342] Trial 205 finished with value: 0.6802330038288888 and parameters: {'n_estimators': 644, 'eta': 0.08214679360452881, 'max_depth': 9, 'alpha': 0.7669, 'lambda': 27.766788103862602, 'max_bin': 487}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:39:32,861] Trial 206 finished with value: 0.6737992201956361 and parameters: {'n_estimators': 586, 'eta': 0.09244666652266705, 'max_depth': 10, 'alpha': 0.4431, 'lambda': 29.708122743790767, 'max_bin': 467}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:39:41,244] Trial 207 finished with value: 0.674861554945879 and parameters: {'n_estimators': 555, 'eta': 0.09532814485604021, 'max_depth': 10, 'alpha': 0.6346, 'lambda': 30.498175439389726, 'max_bin': 482}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:39:48,183] Trial 208 finished with value: 0.6739049917136211 and parameters: {'n_estimators': 598, 'eta': 0.09801432416109364, 'max_depth': 9, 'alpha': 0.6887, 'lambda': 28.895691817985927, 'max_bin': 495}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:39:55,287] Trial 209 finished with value: 0.6776810415170422 and parameters: {'n_estimators': 634, 'eta': 0.09980199139721903, 'max_depth': 9, 'alpha': 0.7368, 'lambda': 27.42596809140126, 'max_bin': 462}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:40:02,690] Trial 210 finished with value: 0.6760616272716626 and parameters: {'n_estimators': 613, 'eta': 0.08932448175435544, 'max_depth': 10, 'alpha': 0.5889, 'lambda': 31.748570287635886, 'max_bin': 490}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:40:11,063] Trial 211 finished with value: 0.66976833745778 and parameters: {'n_estimators': 569, 'eta': 0.09353837082590291, 'max_depth': 10, 'alpha': 0.3846, 'lambda': 29.395959744645285, 'max_bin': 487}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:40:18,784] Trial 212 finished with value: 0.6749877215744368 and parameters: {'n_estimators': 540, 'eta': 0.0920773511153209, 'max_depth': 10, 'alpha': 0.4243, 'lambda': 29.915153763283772, 'max_bin': 479}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:40:25,772] Trial 213 finished with value: 0.6707280408408535 and parameters: {'n_estimators': 582, 'eta': 0.09603037021559144, 'max_depth': 10, 'alpha': 0.40800000000000003, 'lambda': 21.47106774966202, 'max_bin': 493}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:40:33,709] Trial 214 finished with value: 0.6787244507462741 and parameters: {'n_estimators': 493, 'eta': 0.09415114014366492, 'max_depth': 10, 'alpha': 0.46, 'lambda': 28.694684250923572, 'max_bin': 470}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:40:40,800] Trial 215 finished with value: 0.6791040715654436 and parameters: {'n_estimators': 514, 'eta': 0.09014350188896526, 'max_depth': 10, 'alpha': 0.7171000000000001, 'lambda': 30.75966467314162, 'max_bin': 476}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:40:48,364] Trial 216 finished with value: 0.6780671813484953 and parameters: {'n_estimators': 561, 'eta': 0.0871995080519126, 'max_depth': 10, 'alpha': 0.4919, 'lambda': 28.121829466233624, 'max_bin': 485}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:40:54,382] Trial 217 finished with value: 0.6714184439523003 and parameters: {'n_estimators': 602, 'eta': 0.09762297293511724, 'max_depth': 9, 'alpha': 0.6628000000000001, 'lambda': 19.646064396595733, 'max_bin': 497}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:41:02,385] Trial 218 finished with value: 0.6811420279476159 and parameters: {'n_estimators': 627, 'eta': 0.0783105556534952, 'max_depth': 9, 'alpha': 0.5125000000000001, 'lambda': 33.08039864719712, 'max_bin': 490}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:41:08,817] Trial 219 finished with value: 0.6771696954842548 and parameters: {'n_estimators': 585, 'eta': 0.09339957929074172, 'max_depth': 8, 'alpha': 0.4343, 'lambda': 29.367859860621003, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:41:13,080] Trial 220 finished with value: 0.667658679038347 and parameters: {'n_estimators': 550, 'eta': 0.0956598266370715, 'max_depth': 5, 'alpha': 0.5609000000000001, 'lambda': 30.076715830193088, 'max_bin': 484}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:41:22,961] Trial 221 finished with value: 0.6761981930397117 and parameters: {'n_estimators': 534, 'eta': 0.0917426210131887, 'max_depth': 10, 'alpha': 0.506, 'lambda': 28.878396226303437, 'max_bin': 477}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:41:30,225] Trial 222 finished with value: 0.6735436792018331 and parameters: {'n_estimators': 553, 'eta': 0.09477442804573037, 'max_depth': 10, 'alpha': 0.7932, 'lambda': 29.3211383083099, 'max_bin': 472}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:41:37,976] Trial 223 finished with value: 0.6739228833612486 and parameters: {'n_estimators': 517, 'eta': 0.09649482782984642, 'max_depth': 10, 'alpha': 0.4762, 'lambda': 27.596105672242466, 'max_bin': 479}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:41:48,664] Trial 224 finished with value: 0.6734668290386593 and parameters: {'n_estimators': 571, 'eta': 0.04606493208688184, 'max_depth': 10, 'alpha': 0.5413, 'lambda': 28.405672871940492, 'max_bin': 466}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:41:56,119] Trial 225 finished with value: 0.6777134809398835 and parameters: {'n_estimators': 606, 'eta': 0.08994984886221728, 'max_depth': 10, 'alpha': 0.7060000000000001, 'lambda': 30.446333825553346, 'max_bin': 482}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:42:03,481] Trial 226 finished with value: 0.673652189689769 and parameters: {'n_estimators': 591, 'eta': 0.09275449694339581, 'max_depth': 10, 'alpha': 0.4939, 'lambda': 26.7103042037384, 'max_bin': 489}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:42:12,452] Trial 227 finished with value: 0.6760380443225129 and parameters: {'n_estimators': 616, 'eta': 0.09472331125605012, 'max_depth': 9, 'alpha': 0.6442, 'lambda': 31.347449011502572, 'max_bin': 474}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:42:19,417] Trial 228 finished with value: 0.6689726271838371 and parameters: {'n_estimators': 662, 'eta': 0.09869746750144534, 'max_depth': 10, 'alpha': 0.6112000000000001, 'lambda': 29.692541313636607, 'max_bin': 493}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:42:26,892] Trial 229 finished with value: 0.673778863771483 and parameters: {'n_estimators': 536, 'eta': 0.09115975997325958, 'max_depth': 9, 'alpha': 0.6823, 'lambda': 28.65872171147998, 'max_bin': 459}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:42:36,214] Trial 230 finished with value: 0.6841904118339597 and parameters: {'n_estimators': 641, 'eta': 0.07397317719820506, 'max_depth': 10, 'alpha': 0.3577, 'lambda': 27.744196322853757, 'max_bin': 469}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:42:44,982] Trial 231 finished with value: 0.6853395578355971 and parameters: {'n_estimators': 666, 'eta': 0.08222809655439459, 'max_depth': 9, 'alpha': 0.7042, 'lambda': 33.63573056929477, 'max_bin': 479}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:42:53,280] Trial 232 finished with value: 0.6794029010029304 and parameters: {'n_estimators': 687, 'eta': 0.08468834769726337, 'max_depth': 9, 'alpha': 0.3124, 'lambda': 29.174598363690947, 'max_bin': 486}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:43:01,073] Trial 233 finished with value: 0.6733382453709083 and parameters: {'n_estimators': 629, 'eta': 0.08894620900309493, 'max_depth': 9, 'alpha': 0.7245, 'lambda': 30.527335605862135, 'max_bin': 397}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:43:08,233] Trial 234 finished with value: 0.6775899283778671 and parameters: {'n_estimators': 572, 'eta': 0.08756218094571709, 'max_depth': 9, 'alpha': 0.4541, 'lambda': 32.39784586464429, 'max_bin': 465}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:43:16,066] Trial 235 finished with value: 0.6731031460476464 and parameters: {'n_estimators': 657, 'eta': 0.09359640404950013, 'max_depth': 10, 'alpha': 0.5285000000000001, 'lambda': 36.459001247292946, 'max_bin': 475}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:43:22,338] Trial 236 finished with value: 0.672485021890698 and parameters: {'n_estimators': 597, 'eta': 0.09656570205841118, 'max_depth': 9, 'alpha': 0.7008, 'lambda': 29.89298502652185, 'max_bin': 451}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:43:33,950] Trial 237 finished with value: 0.6761387071754161 and parameters: {'n_estimators': 617, 'eta': 0.07980649194860817, 'max_depth': 12, 'alpha': 0.7481, 'lambda': 35.3893415059695, 'max_bin': 494}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:43:41,464] Trial 238 finished with value: 0.6747113544051582 and parameters: {'n_estimators': 560, 'eta': 0.09199276704462953, 'max_depth': 9, 'alpha': 0.6707000000000001, 'lambda': 31.16356782476358, 'max_bin': 483}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:43:49,126] Trial 239 finished with value: 0.681053361472781 and parameters: {'n_estimators': 584, 'eta': 0.08401536414053361, 'max_depth': 10, 'alpha': 0.6294000000000001, 'lambda': 28.51255969056368, 'max_bin': 470}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:43:53,723] Trial 240 finished with value: 0.6745208784292955 and parameters: {'n_estimators': 641, 'eta': 0.09505185914340776, 'max_depth': 8, 'alpha': 0.405, 'lambda': 14.825664944801158, 'max_bin': 425}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:44:01,260] Trial 241 finished with value: 0.672440097352658 and parameters: {'n_estimators': 604, 'eta': 0.09703246074543682, 'max_depth': 10, 'alpha': 0.6133000000000001, 'lambda': 27.31946004801823, 'max_bin': 490}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:44:08,680] Trial 242 finished with value: 0.6760031322074386 and parameters: {'n_estimators': 607, 'eta': 0.09812393109729074, 'max_depth': 10, 'alpha': 0.5800000000000001, 'lambda': 25.587537898187875, 'max_bin': 496}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:44:18,181] Trial 243 finished with value: 0.6822827648358284 and parameters: {'n_estimators': 623, 'eta': 0.07676735726186047, 'max_depth': 10, 'alpha': 0.6826, 'lambda': 29.23952754526707, 'max_bin': 489}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:44:25,767] Trial 244 finished with value: 0.6714416287247924 and parameters: {'n_estimators': 592, 'eta': 0.09511877573395484, 'max_depth': 10, 'alpha': 0.6217, 'lambda': 28.070626234273938, 'max_bin': 493}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:44:36,274] Trial 245 finished with value: 0.6770336814944888 and parameters: {'n_estimators': 576, 'eta': 0.09078037240508134, 'max_depth': 10, 'alpha': 0.6548, 'lambda': 38.27781239770138, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:44:45,424] Trial 246 finished with value: 0.6774515418519291 and parameters: {'n_estimators': 544, 'eta': 0.05303909017242843, 'max_depth': 9, 'alpha': 0.6454000000000001, 'lambda': 27.092113748743294, 'max_bin': 486}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:44:53,601] Trial 247 finished with value: 0.6769527266612487 and parameters: {'n_estimators': 676, 'eta': 0.09274650231635856, 'max_depth': 10, 'alpha': 0.7210000000000001, 'lambda': 29.685648218778358, 'max_bin': 477}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:45:01,556] Trial 248 finished with value: 0.6762313315541182 and parameters: {'n_estimators': 605, 'eta': 0.09676403610098183, 'max_depth': 10, 'alpha': 0.5564, 'lambda': 28.754871045210198, 'max_bin': 481}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:45:14,485] Trial 249 finished with value: 0.6782415565984596 and parameters: {'n_estimators': 636, 'eta': 0.03846247834608655, 'max_depth': 9, 'alpha': 0.7024, 'lambda': 37.34877602388853, 'max_bin': 389}. Best is trial 121 with value: 0.7107864949635834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7108\n",
      "\tBest params:\n",
      "\t\tn_estimators: 570\n",
      "\t\teta: 0.0960461800647675\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.6382\n",
      "\t\tlambda: 28.76844285630968\n",
      "\t\tmax_bin: 485\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.689126    0.712882    0.708425    0.727859   \n",
      "1                    TP   32.000000   41.000000   39.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  195.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    5.000000    2.000000   \n",
      "4                    FN   34.000000   27.000000   29.000000   28.000000   \n",
      "5              Accuracy    0.869403    0.876866    0.873134    0.888060   \n",
      "6             Precision    0.969697    0.872340    0.886364    0.953488   \n",
      "7           Sensitivity    0.484848    0.602941    0.573529    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.975000    0.989900   \n",
      "9              F1 score    0.646465    0.713043    0.696429    0.732143   \n",
      "10  F1 score (weighted)    0.852568    0.868694    0.863132    0.878499   \n",
      "11     F1 score (macro)    0.783187    0.817329    0.808120    0.830694   \n",
      "12    Balanced Accuracy    0.739949    0.786471    0.774265    0.792076   \n",
      "13                  MCC    0.629225    0.655593    0.644346    0.695908   \n",
      "14                  NPV    0.855300    0.877800    0.870500    0.875600   \n",
      "15              ROC_AUC    0.739949    0.786471    0.774265    0.792076   \n",
      "\n",
      "          Set4  \n",
      "0     0.704049  \n",
      "1    38.000000  \n",
      "2   199.000000  \n",
      "3     3.000000  \n",
      "4    28.000000  \n",
      "5     0.884328  \n",
      "6     0.926829  \n",
      "7     0.575758  \n",
      "8     0.985100  \n",
      "9     0.710280  \n",
      "10    0.874186  \n",
      "11    0.819010  \n",
      "12    0.780453  \n",
      "13    0.671325  \n",
      "14    0.876700  \n",
      "15    0.780453  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_xgb_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "#Y_trainSet4 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_4_cat = np.where(((y_pred_xgb_4 >= 2) | (y_pred_xgb_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_xgb_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_xgb_4_cat)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:45:23,493] Trial 250 finished with value: 0.7002221705303235 and parameters: {'n_estimators': 563, 'eta': 0.09988282532485435, 'max_depth': 10, 'alpha': 0.5914, 'lambda': 34.63832153742462, 'max_bin': 473}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:45:30,440] Trial 251 finished with value: 0.7024031257564064 and parameters: {'n_estimators': 654, 'eta': 0.09382902822163076, 'max_depth': 9, 'alpha': 0.4726, 'lambda': 30.122531025062425, 'max_bin': 492}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:45:37,579] Trial 252 finished with value: 0.7016409992690935 and parameters: {'n_estimators': 583, 'eta': 0.08828834030558391, 'max_depth': 10, 'alpha': 0.5216000000000001, 'lambda': 27.92943426538873, 'max_bin': 462}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:45:43,813] Trial 253 finished with value: 0.6931765604505218 and parameters: {'n_estimators': 618, 'eta': 0.07241183141676551, 'max_depth': 10, 'alpha': 0.6753, 'lambda': 2.107330509267392, 'max_bin': 487}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:45:50,653] Trial 254 finished with value: 0.7033049255295759 and parameters: {'n_estimators': 522, 'eta': 0.08578107516054524, 'max_depth': 9, 'alpha': 0.6345000000000001, 'lambda': 29.043643729360436, 'max_bin': 468}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:45:57,996] Trial 255 finished with value: 0.7031843773938744 and parameters: {'n_estimators': 719, 'eta': 0.09023658157443223, 'max_depth': 8, 'alpha': 0.5045000000000001, 'lambda': 26.277250027400292, 'max_bin': 480}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:46:05,677] Trial 256 finished with value: 0.7017214363152572 and parameters: {'n_estimators': 598, 'eta': 0.09555079675840324, 'max_depth': 10, 'alpha': 0.4319, 'lambda': 31.852285438324476, 'max_bin': 404}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:46:22,027] Trial 257 finished with value: 0.6656086431207304 and parameters: {'n_estimators': 551, 'eta': 0.009835951142459144, 'max_depth': 9, 'alpha': 0.7468, 'lambda': 30.737742327440326, 'max_bin': 497}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:46:28,335] Trial 258 finished with value: 0.7031208565917819 and parameters: {'n_estimators': 497, 'eta': 0.09328767496799707, 'max_depth': 10, 'alpha': 0.6917, 'lambda': 28.303671339658813, 'max_bin': 456}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:46:36,647] Trial 259 finished with value: 0.7014745243731345 and parameters: {'n_estimators': 634, 'eta': 0.09730210859983911, 'max_depth': 9, 'alpha': 0.6004, 'lambda': 29.730628391336232, 'max_bin': 484}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:46:46,905] Trial 260 finished with value: 0.704690499448496 and parameters: {'n_estimators': 695, 'eta': 0.06705013853688001, 'max_depth': 10, 'alpha': 0.6635, 'lambda': 27.490206347493384, 'max_bin': 371}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:46:55,958] Trial 261 finished with value: 0.7018016311187345 and parameters: {'n_estimators': 577, 'eta': 0.08269809615011038, 'max_depth': 10, 'alpha': 0.5722, 'lambda': 35.995825203622736, 'max_bin': 473}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:47:03,405] Trial 262 finished with value: 0.7017934166533651 and parameters: {'n_estimators': 614, 'eta': 0.0919021215972455, 'max_depth': 9, 'alpha': 0.7198, 'lambda': 33.122539523702024, 'max_bin': 491}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:47:16,184] Trial 263 finished with value: 0.6978749087228386 and parameters: {'n_estimators': 595, 'eta': 0.02624498872011964, 'max_depth': 8, 'alpha': 0.5482, 'lambda': 29.06686568361069, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:47:23,545] Trial 264 finished with value: 0.7053160675744057 and parameters: {'n_estimators': 651, 'eta': 0.07559622803029212, 'max_depth': 9, 'alpha': 0.7757000000000001, 'lambda': 23.132252184795203, 'max_bin': 477}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:47:30,315] Trial 265 finished with value: 0.7049033847742303 and parameters: {'n_estimators': 564, 'eta': 0.09503460241785096, 'max_depth': 11, 'alpha': 0.4883, 'lambda': 30.470289225944448, 'max_bin': 486}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:47:39,126] Trial 266 finished with value: 0.7053953720249999 and parameters: {'n_estimators': 528, 'eta': 0.05881703297210146, 'max_depth': 10, 'alpha': 0.6179, 'lambda': 26.680623099829347, 'max_bin': 463}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:47:47,436] Trial 267 finished with value: 0.7042162814374641 and parameters: {'n_estimators': 479, 'eta': 0.0899106615963181, 'max_depth': 10, 'alpha': 0.6474000000000001, 'lambda': 34.20411632042879, 'max_bin': 494}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:47:54,765] Trial 268 finished with value: 0.7078021329578932 and parameters: {'n_estimators': 626, 'eta': 0.07827772086656468, 'max_depth': 9, 'alpha': 0.5338, 'lambda': 31.41585461270943, 'max_bin': 480}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:48:02,511] Trial 269 finished with value: 0.7019738688815498 and parameters: {'n_estimators': 671, 'eta': 0.07631298690728176, 'max_depth': 9, 'alpha': 0.5068, 'lambda': 32.16266817672108, 'max_bin': 470}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:48:12,261] Trial 270 finished with value: 0.7057241397801379 and parameters: {'n_estimators': 628, 'eta': 0.07879473141907255, 'max_depth': 9, 'alpha': 0.5243, 'lambda': 31.614325793806344, 'max_bin': 480}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:48:21,543] Trial 271 finished with value: 0.7022250049387783 and parameters: {'n_estimators': 645, 'eta': 0.08004270959072549, 'max_depth': 9, 'alpha': 0.5547000000000001, 'lambda': 30.829141824318036, 'max_bin': 430}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:48:29,529] Trial 272 finished with value: 0.7030203299740931 and parameters: {'n_estimators': 620, 'eta': 0.08150056224789967, 'max_depth': 9, 'alpha': 0.5391, 'lambda': 29.848568848603787, 'max_bin': 381}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:48:36,752] Trial 273 finished with value: 0.7019859283747388 and parameters: {'n_estimators': 543, 'eta': 0.08815334448556444, 'max_depth': 9, 'alpha': 0.46900000000000003, 'lambda': 31.282428181296993, 'max_bin': 475}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:48:44,534] Trial 274 finished with value: 0.7053270344296931 and parameters: {'n_estimators': 588, 'eta': 0.09235473612476651, 'max_depth': 9, 'alpha': 0.6914, 'lambda': 30.232482795983888, 'max_bin': 468}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:48:52,242] Trial 275 finished with value: 0.7047609596676906 and parameters: {'n_estimators': 604, 'eta': 0.07396526053665133, 'max_depth': 8, 'alpha': 0.44670000000000004, 'lambda': 32.906496527469166, 'max_bin': 484}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:49:00,403] Trial 276 finished with value: 0.704736147306322 and parameters: {'n_estimators': 575, 'eta': 0.08609657230883155, 'max_depth': 9, 'alpha': 0.7259, 'lambda': 29.093971485717923, 'max_bin': 464}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:49:07,150] Trial 277 finished with value: 0.70163807494871 and parameters: {'n_estimators': 664, 'eta': 0.09834014307708262, 'max_depth': 9, 'alpha': 0.48710000000000003, 'lambda': 28.435226767971592, 'max_bin': 477}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:49:15,525] Trial 278 finished with value: 0.7032678760495814 and parameters: {'n_estimators': 624, 'eta': 0.09412862218083057, 'max_depth': 9, 'alpha': 0.6709, 'lambda': 31.330352361971293, 'max_bin': 411}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:49:22,937] Trial 279 finished with value: 0.7033029546024453 and parameters: {'n_estimators': 550, 'eta': 0.09153406233546155, 'max_depth': 9, 'alpha': 0.534, 'lambda': 35.203022565554626, 'max_bin': 488}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:49:29,717] Trial 280 finished with value: 0.7001623637711958 and parameters: {'n_estimators': 642, 'eta': 0.09994021411750839, 'max_depth': 10, 'alpha': 0.6958000000000001, 'lambda': 25.325513342377402, 'max_bin': 445}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:49:36,907] Trial 281 finished with value: 0.7014897173762548 and parameters: {'n_estimators': 609, 'eta': 0.09014511448497628, 'max_depth': 10, 'alpha': 0.4133, 'lambda': 29.498689701318582, 'max_bin': 471}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:49:42,154] Trial 282 finished with value: 0.6997071934600461 and parameters: {'n_estimators': 509, 'eta': 0.09606925885770322, 'max_depth': 9, 'alpha': 0.5089, 'lambda': 8.576788909264794, 'max_bin': 482}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:49:49,417] Trial 283 finished with value: 0.7029383504964538 and parameters: {'n_estimators': 586, 'eta': 0.08395530624489568, 'max_depth': 8, 'alpha': 0.5707, 'lambda': 30.334855551697895, 'max_bin': 474}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:49:57,129] Trial 284 finished with value: 0.7043754990560577 and parameters: {'n_estimators': 681, 'eta': 0.09362824262437798, 'max_depth': 10, 'alpha': 0.7367, 'lambda': 32.29978417344241, 'max_bin': 458}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:50:05,415] Trial 285 finished with value: 0.7051092111079078 and parameters: {'n_estimators': 572, 'eta': 0.055006845330380374, 'max_depth': 9, 'alpha': 0.7064, 'lambda': 28.630174105210667, 'max_bin': 313}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:50:14,433] Trial 286 finished with value: 0.7058605868813463 and parameters: {'n_estimators': 526, 'eta': 0.07099369641406517, 'max_depth': 10, 'alpha': 0.6587000000000001, 'lambda': 37.09786891507971, 'max_bin': 495}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:50:20,955] Trial 287 finished with value: 0.7034868669191037 and parameters: {'n_estimators': 596, 'eta': 0.0890932067518464, 'max_depth': 9, 'alpha': 0.8480000000000001, 'lambda': 33.76242161263577, 'max_bin': 480}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:50:29,272] Trial 288 finished with value: 0.7034194010048436 and parameters: {'n_estimators': 634, 'eta': 0.07861983212648756, 'max_depth': 11, 'alpha': 0.8047000000000001, 'lambda': 29.75485110372216, 'max_bin': 421}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:50:36,189] Trial 289 finished with value: 0.7051045035129346 and parameters: {'n_estimators': 616, 'eta': 0.08706611518731566, 'max_depth': 9, 'alpha': 0.5921000000000001, 'lambda': 27.947597684757394, 'max_bin': 466}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:50:41,586] Trial 290 finished with value: 0.702753129413396 and parameters: {'n_estimators': 561, 'eta': 0.09559827892003804, 'max_depth': 10, 'alpha': 0.7553000000000001, 'lambda': 17.714673684000598, 'max_bin': 326}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:50:48,190] Trial 291 finished with value: 0.7003144142224028 and parameters: {'n_estimators': 652, 'eta': 0.09169332868705106, 'max_depth': 10, 'alpha': 0.6824, 'lambda': 30.632695235955268, 'max_bin': 488}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:50:54,482] Trial 292 finished with value: 0.6986769595136332 and parameters: {'n_estimators': 602, 'eta': 0.09786772798290502, 'max_depth': 9, 'alpha': 0.5278, 'lambda': 28.810145456151627, 'max_bin': 482}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:51:01,234] Trial 293 finished with value: 0.7024187200427862 and parameters: {'n_estimators': 543, 'eta': 0.09351199672548557, 'max_depth': 10, 'alpha': 0.7123, 'lambda': 29.35912605774837, 'max_bin': 491}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:51:07,241] Trial 294 finished with value: 0.6973648237530993 and parameters: {'n_estimators': 583, 'eta': 0.09646593454504922, 'max_depth': 8, 'alpha': 0.9739000000000001, 'lambda': 38.53667118648453, 'max_bin': 476}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:51:13,987] Trial 295 finished with value: 0.701837980674173 and parameters: {'n_estimators': 628, 'eta': 0.07505313716233393, 'max_depth': 9, 'alpha': 0.4969, 'lambda': 24.86870952023631, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:51:20,170] Trial 296 finished with value: 0.7040428927296094 and parameters: {'n_estimators': 562, 'eta': 0.090627331927179, 'max_depth': 10, 'alpha': 0.3803, 'lambda': 31.301458534817222, 'max_bin': 471}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:51:26,284] Trial 297 finished with value: 0.703866492631733 and parameters: {'n_estimators': 611, 'eta': 0.09442390319292154, 'max_depth': 9, 'alpha': 0.6428, 'lambda': 27.486042495643822, 'max_bin': 485}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:51:33,876] Trial 298 finished with value: 0.7009236372249573 and parameters: {'n_estimators': 668, 'eta': 0.08137798409695653, 'max_depth': 10, 'alpha': 0.462, 'lambda': 26.234918353660518, 'max_bin': 495}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:51:42,436] Trial 299 finished with value: 0.70664008602544 and parameters: {'n_estimators': 592, 'eta': 0.06875771427771321, 'max_depth': 10, 'alpha': 0.5608000000000001, 'lambda': 30.090505798645797, 'max_bin': 464}. Best is trial 121 with value: 0.7107864949635834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7108\n",
      "\tBest params:\n",
      "\t\tn_estimators: 570\n",
      "\t\teta: 0.0960461800647675\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.6382\n",
      "\t\tlambda: 28.76844285630968\n",
      "\t\tmax_bin: 485\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.689126    0.712882    0.708425    0.727859   \n",
      "1                    TP   32.000000   41.000000   39.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  195.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    5.000000    2.000000   \n",
      "4                    FN   34.000000   27.000000   29.000000   28.000000   \n",
      "5              Accuracy    0.869403    0.876866    0.873134    0.888060   \n",
      "6             Precision    0.969697    0.872340    0.886364    0.953488   \n",
      "7           Sensitivity    0.484848    0.602941    0.573529    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.975000    0.989900   \n",
      "9              F1 score    0.646465    0.713043    0.696429    0.732143   \n",
      "10  F1 score (weighted)    0.852568    0.868694    0.863132    0.878499   \n",
      "11     F1 score (macro)    0.783187    0.817329    0.808120    0.830694   \n",
      "12    Balanced Accuracy    0.739949    0.786471    0.774265    0.792076   \n",
      "13                  MCC    0.629225    0.655593    0.644346    0.695908   \n",
      "14                  NPV    0.855300    0.877800    0.870500    0.875600   \n",
      "15              ROC_AUC    0.739949    0.786471    0.774265    0.792076   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.704049    0.658896  \n",
      "1    38.000000   36.000000  \n",
      "2   199.000000  197.000000  \n",
      "3     3.000000    3.000000  \n",
      "4    28.000000   32.000000  \n",
      "5     0.884328    0.869403  \n",
      "6     0.926829    0.923077  \n",
      "7     0.575758    0.529412  \n",
      "8     0.985100    0.985000  \n",
      "9     0.710280    0.672897  \n",
      "10    0.874186    0.856119  \n",
      "11    0.819010    0.795656  \n",
      "12    0.780453    0.757206  \n",
      "13    0.671325    0.634790  \n",
      "14    0.876700    0.860300  \n",
      "15    0.780453    0.757206  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_xgb_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "#Y_trainSet5 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_5_cat = np.where(((y_pred_xgb_5 >= 2) | (y_pred_xgb_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_xgb_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_xgb_5_cat)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:51:59,347] Trial 300 finished with value: 0.07368769511808432 and parameters: {'n_estimators': 691, 'eta': 0.00021485873439396064, 'max_depth': 9, 'alpha': 0.6828000000000001, 'lambda': 32.191712105925795, 'max_bin': 478}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:52:06,954] Trial 301 finished with value: 0.6884168821469976 and parameters: {'n_estimators': 639, 'eta': 0.07766950951155663, 'max_depth': 8, 'alpha': 0.6052000000000001, 'lambda': 28.30979145201158, 'max_bin': 489}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:52:15,279] Trial 302 finished with value: 0.6929317059131457 and parameters: {'n_estimators': 571, 'eta': 0.08821375419210539, 'max_depth': 10, 'alpha': 0.6294000000000001, 'lambda': 24.149033561031537, 'max_bin': 454}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:52:24,299] Trial 303 finished with value: 0.6850784057249818 and parameters: {'n_estimators': 618, 'eta': 0.08562239943827879, 'max_depth': 9, 'alpha': 0.5403, 'lambda': 29.40037208188788, 'max_bin': 469}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:52:32,125] Trial 304 finished with value: 0.683776955052437 and parameters: {'n_estimators': 532, 'eta': 0.09237628309755767, 'max_depth': 9, 'alpha': 0.731, 'lambda': 36.35308504321668, 'max_bin': 357}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:52:41,013] Trial 305 finished with value: 0.6880978229173104 and parameters: {'n_estimators': 655, 'eta': 0.09833296356725332, 'max_depth': 10, 'alpha': 0.664, 'lambda': 30.951764030810427, 'max_bin': 343}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:52:50,354] Trial 306 finished with value: 0.6874950811312688 and parameters: {'n_estimators': 590, 'eta': 0.09497671455647821, 'max_depth': 9, 'alpha': 0.48200000000000004, 'lambda': 39.100023387226386, 'max_bin': 483}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:53:06,775] Trial 307 finished with value: 0.4179323180264679 and parameters: {'n_estimators': 546, 'eta': 0.0022031185329697764, 'max_depth': 10, 'alpha': 0.44220000000000004, 'lambda': 34.806445564938215, 'max_bin': 495}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:53:15,621] Trial 308 finished with value: 0.6894850305346908 and parameters: {'n_estimators': 600, 'eta': 0.08997979156658084, 'max_depth': 10, 'alpha': 0.5142, 'lambda': 21.555782871755035, 'max_bin': 460}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:53:23,389] Trial 309 finished with value: 0.6850186714061427 and parameters: {'n_estimators': 507, 'eta': 0.09290799332104371, 'max_depth': 9, 'alpha': 0.7136, 'lambda': 23.303612678225143, 'max_bin': 475}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:53:31,862] Trial 310 finished with value: 0.6877032058529693 and parameters: {'n_estimators': 633, 'eta': 0.09668842612004241, 'max_depth': 10, 'alpha': 0.5853, 'lambda': 28.015217284263652, 'max_bin': 490}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:53:42,655] Trial 311 finished with value: 0.6813179801219027 and parameters: {'n_estimators': 576, 'eta': 0.04663604039557453, 'max_depth': 9, 'alpha': 0.8735, 'lambda': 39.983406401631704, 'max_bin': 472}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:53:53,731] Trial 312 finished with value: 0.6883728859298313 and parameters: {'n_estimators': 614, 'eta': 0.08340121490407672, 'max_depth': 10, 'alpha': 0.6919000000000001, 'lambda': 32.9904307255005, 'max_bin': 479}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:54:03,015] Trial 313 finished with value: 0.6874011973690697 and parameters: {'n_estimators': 550, 'eta': 0.06560351678997098, 'max_depth': 8, 'alpha': 0.164, 'lambda': 37.91384213970905, 'max_bin': 467}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:54:05,907] Trial 314 finished with value: 0.6691047839879479 and parameters: {'n_estimators': 103, 'eta': 0.08788053808740372, 'max_depth': 9, 'alpha': 0.8205, 'lambda': 29.996657222936552, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:54:11,479] Trial 315 finished with value: 0.6797354226680883 and parameters: {'n_estimators': 224, 'eta': 0.05741130215877243, 'max_depth': 10, 'alpha': 0.6558, 'lambda': 26.949568013007465, 'max_bin': 487}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:54:19,919] Trial 316 finished with value: 0.6807936584807215 and parameters: {'n_estimators': 566, 'eta': 0.09160237753362359, 'max_depth': 9, 'alpha': 0.3966, 'lambda': 28.795800577480076, 'max_bin': 338}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:54:29,457] Trial 317 finished with value: 0.6841439331410444 and parameters: {'n_estimators': 595, 'eta': 0.09866001609893385, 'max_depth': 11, 'alpha': 0.5523, 'lambda': 31.73888770693947, 'max_bin': 482}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:54:38,379] Trial 318 finished with value: 0.6871917955629905 and parameters: {'n_estimators': 644, 'eta': 0.09505946762190708, 'max_depth': 10, 'alpha': 0.6336, 'lambda': 30.6666203374529, 'max_bin': 492}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:54:46,955] Trial 319 finished with value: 0.6877443897789023 and parameters: {'n_estimators': 621, 'eta': 0.09339161689610051, 'max_depth': 10, 'alpha': 0.7666000000000001, 'lambda': 29.23070152079846, 'max_bin': 476}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:54:54,144] Trial 320 finished with value: 0.6895731000698236 and parameters: {'n_estimators': 527, 'eta': 0.06343032564625585, 'max_depth': 9, 'alpha': 0.6756, 'lambda': 12.973280331287409, 'max_bin': 485}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:55:07,907] Trial 321 finished with value: 0.6872655217430614 and parameters: {'n_estimators': 675, 'eta': 0.050924082156958934, 'max_depth': 10, 'alpha': 0.5285000000000001, 'lambda': 34.07808750551742, 'max_bin': 496}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:55:16,591] Trial 322 finished with value: 0.6877719066437499 and parameters: {'n_estimators': 577, 'eta': 0.07386928474123299, 'max_depth': 9, 'alpha': 0.020900000000000002, 'lambda': 29.87691413177835, 'max_bin': 464}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:55:26,499] Trial 323 finished with value: 0.6816973927913499 and parameters: {'n_estimators': 607, 'eta': 0.04935035283994285, 'max_depth': 8, 'alpha': 0.7026, 'lambda': 27.932618578979163, 'max_bin': 470}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:55:35,436] Trial 324 finished with value: 0.6853308017663287 and parameters: {'n_estimators': 651, 'eta': 0.09675636234638037, 'max_depth': 9, 'alpha': 0.42700000000000005, 'lambda': 35.81238348230285, 'max_bin': 480}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:55:45,548] Trial 325 finished with value: 0.6866407362876918 and parameters: {'n_estimators': 588, 'eta': 0.08958924656759452, 'max_depth': 10, 'alpha': 0.6117, 'lambda': 28.784013102153178, 'max_bin': 491}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:55:56,672] Trial 326 finished with value: 0.6880635583876373 and parameters: {'n_estimators': 631, 'eta': 0.060092048001662325, 'max_depth': 9, 'alpha': 0.49960000000000004, 'lambda': 30.92999613674345, 'max_bin': 474}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:56:05,822] Trial 327 finished with value: 0.6847598198143251 and parameters: {'n_estimators': 558, 'eta': 0.08565562431319665, 'max_depth': 10, 'alpha': 0.4592, 'lambda': 27.449040901955712, 'max_bin': 486}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:56:13,650] Trial 328 finished with value: 0.6938575787572662 and parameters: {'n_estimators': 605, 'eta': 0.09475417095116391, 'max_depth': 11, 'alpha': 0.7254, 'lambda': 20.307487483305742, 'max_bin': 496}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:56:22,724] Trial 329 finished with value: 0.6867204834904014 and parameters: {'n_estimators': 491, 'eta': 0.07210508008079874, 'max_depth': 10, 'alpha': 0.7395, 'lambda': 29.504903516398176, 'max_bin': 450}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:56:31,075] Trial 330 finished with value: 0.6892524468010728 and parameters: {'n_estimators': 664, 'eta': 0.09974169767766612, 'max_depth': 9, 'alpha': 0.6718000000000001, 'lambda': 26.16748450037119, 'max_bin': 415}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:56:39,680] Trial 331 finished with value: 0.690547210283054 and parameters: {'n_estimators': 697, 'eta': 0.0924246844412151, 'max_depth': 9, 'alpha': 0.5760000000000001, 'lambda': 36.71366218479168, 'max_bin': 460}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:56:49,694] Trial 332 finished with value: 0.6849723563009387 and parameters: {'n_estimators': 626, 'eta': 0.09087896210996763, 'max_depth': 10, 'alpha': 0.4771, 'lambda': 31.55247626978829, 'max_bin': 480}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:56:58,345] Trial 333 finished with value: 0.6844082895301702 and parameters: {'n_estimators': 575, 'eta': 0.08717297864332604, 'max_depth': 8, 'alpha': 0.9249, 'lambda': 30.34409342089566, 'max_bin': 468}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:57:08,315] Trial 334 finished with value: 0.6825706348823827 and parameters: {'n_estimators': 540, 'eta': 0.08040468436910267, 'max_depth': 10, 'alpha': 0.6393, 'lambda': 28.418312550949466, 'max_bin': 488}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:57:20,854] Trial 335 finished with value: 0.6834766841673845 and parameters: {'n_estimators': 514, 'eta': 0.0415308462601468, 'max_depth': 10, 'alpha': 0.7034, 'lambda': 33.592163332516776, 'max_bin': 441}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:57:29,632] Trial 336 finished with value: 0.6866195310267743 and parameters: {'n_estimators': 609, 'eta': 0.0774855747965663, 'max_depth': 9, 'alpha': 0.7846000000000001, 'lambda': 29.266158780381573, 'max_bin': 472}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:57:39,478] Trial 337 finished with value: 0.6826576442018875 and parameters: {'n_estimators': 585, 'eta': 0.0972386177789787, 'max_depth': 9, 'alpha': 0.5393, 'lambda': 32.36752420681781, 'max_bin': 500}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:57:48,810] Trial 338 finished with value: 0.687923570600132 and parameters: {'n_estimators': 555, 'eta': 0.09527949989746888, 'max_depth': 10, 'alpha': 0.5969, 'lambda': 30.238728223128398, 'max_bin': 392}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:57:56,154] Trial 339 finished with value: 0.6833290232013074 and parameters: {'n_estimators': 467, 'eta': 0.0896039354951632, 'max_depth': 9, 'alpha': 0.5179, 'lambda': 27.044417086991654, 'max_bin': 492}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:58:06,447] Trial 340 finished with value: 0.685811595071913 and parameters: {'n_estimators': 644, 'eta': 0.09307005010721324, 'max_depth': 10, 'alpha': 0.6609, 'lambda': 28.402776875350977, 'max_bin': 376}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:58:15,861] Trial 341 finished with value: 0.6852369545780859 and parameters: {'n_estimators': 599, 'eta': 0.08338947615143852, 'max_depth': 9, 'alpha': 0.5693, 'lambda': 35.24424237147246, 'max_bin': 484}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:58:25,577] Trial 342 finished with value: 0.6895029314992409 and parameters: {'n_estimators': 620, 'eta': 0.07050292232604437, 'max_depth': 10, 'alpha': 0.6222, 'lambda': 25.811296143966935, 'max_bin': 477}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:58:32,278] Trial 343 finished with value: 0.6834248088996171 and parameters: {'n_estimators': 567, 'eta': 0.09777326151487595, 'max_depth': 8, 'alpha': 0.7517, 'lambda': 37.41463030375099, 'max_bin': 462}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:58:42,844] Trial 344 finished with value: 0.6843069030793563 and parameters: {'n_estimators': 591, 'eta': 0.05496098033138532, 'max_depth': 9, 'alpha': 0.6901, 'lambda': 29.2188427585285, 'max_bin': 494}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:58:52,863] Trial 345 finished with value: 0.6864738431325638 and parameters: {'n_estimators': 713, 'eta': 0.09395703431745217, 'max_depth': 10, 'alpha': 0.49560000000000004, 'lambda': 31.109926253910366, 'max_bin': 482}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:59:02,354] Trial 346 finished with value: 0.6836034737483163 and parameters: {'n_estimators': 631, 'eta': 0.09178232416582587, 'max_depth': 10, 'alpha': 0.41540000000000005, 'lambda': 29.81683274975672, 'max_bin': 406}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:59:09,981] Trial 347 finished with value: 0.6856468021973874 and parameters: {'n_estimators': 681, 'eta': 0.07550784861285854, 'max_depth': 9, 'alpha': 0.7174, 'lambda': 27.947494577676174, 'max_bin': 366}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:59:21,336] Trial 348 finished with value: 0.6711991089061107 and parameters: {'n_estimators': 530, 'eta': 0.01824540481303539, 'max_depth': 9, 'alpha': 0.644, 'lambda': 21.925279203334306, 'max_bin': 348}. Best is trial 121 with value: 0.7107864949635834.\n",
      "[I 2023-12-20 07:59:28,288] Trial 349 finished with value: 0.6803629180060307 and parameters: {'n_estimators': 748, 'eta': 0.09589407441826737, 'max_depth': 8, 'alpha': 0.3211, 'lambda': 32.76078152905224, 'max_bin': 472}. Best is trial 121 with value: 0.7107864949635834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7108\n",
      "\tBest params:\n",
      "\t\tn_estimators: 570\n",
      "\t\teta: 0.0960461800647675\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.6382\n",
      "\t\tlambda: 28.76844285630968\n",
      "\t\tmax_bin: 485\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.689126    0.712882    0.708425    0.727859   \n",
      "1                    TP   32.000000   41.000000   39.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  195.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    5.000000    2.000000   \n",
      "4                    FN   34.000000   27.000000   29.000000   28.000000   \n",
      "5              Accuracy    0.869403    0.876866    0.873134    0.888060   \n",
      "6             Precision    0.969697    0.872340    0.886364    0.953488   \n",
      "7           Sensitivity    0.484848    0.602941    0.573529    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.975000    0.989900   \n",
      "9              F1 score    0.646465    0.713043    0.696429    0.732143   \n",
      "10  F1 score (weighted)    0.852568    0.868694    0.863132    0.878499   \n",
      "11     F1 score (macro)    0.783187    0.817329    0.808120    0.830694   \n",
      "12    Balanced Accuracy    0.739949    0.786471    0.774265    0.792076   \n",
      "13                  MCC    0.629225    0.655593    0.644346    0.695908   \n",
      "14                  NPV    0.855300    0.877800    0.870500    0.875600   \n",
      "15              ROC_AUC    0.739949    0.786471    0.774265    0.792076   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.704049    0.658896    0.728272  \n",
      "1    38.000000   36.000000   37.000000  \n",
      "2   199.000000  197.000000  198.000000  \n",
      "3     3.000000    3.000000    2.000000  \n",
      "4    28.000000   32.000000   31.000000  \n",
      "5     0.884328    0.869403    0.876866  \n",
      "6     0.926829    0.923077    0.948718  \n",
      "7     0.575758    0.529412    0.544118  \n",
      "8     0.985100    0.985000    0.990000  \n",
      "9     0.710280    0.672897    0.691589  \n",
      "10    0.874186    0.856119    0.864341  \n",
      "11    0.819010    0.795656    0.807333  \n",
      "12    0.780453    0.757206    0.767059  \n",
      "13    0.671325    0.634790    0.659108  \n",
      "14    0.876700    0.860300    0.864600  \n",
      "15    0.780453    0.757206    0.767059  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_xgb_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "#Y_trainSet6 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_6_cat = np.where(((y_pred_xgb_6 >= 2) | (y_pred_xgb_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_xgb_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_xgb_6_cat)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 07:59:39,956] Trial 350 finished with value: 0.7149715502271825 and parameters: {'n_estimators': 610, 'eta': 0.09157451938115865, 'max_depth': 10, 'alpha': 0.5506, 'lambda': 30.46652912531221, 'max_bin': 455}. Best is trial 350 with value: 0.7149715502271825.\n",
      "[I 2023-12-20 07:59:51,134] Trial 351 finished with value: 0.7154177562924535 and parameters: {'n_estimators': 610, 'eta': 0.08905912211598419, 'max_depth': 10, 'alpha': 0.533, 'lambda': 30.627566315327233, 'max_bin': 466}. Best is trial 351 with value: 0.7154177562924535.\n",
      "[I 2023-12-20 08:00:03,873] Trial 352 finished with value: 0.7226681908653416 and parameters: {'n_estimators': 614, 'eta': 0.06117559311841911, 'max_depth': 10, 'alpha': 0.5549000000000001, 'lambda': 31.701499839532268, 'max_bin': 460}. Best is trial 352 with value: 0.7226681908653416.\n",
      "[I 2023-12-20 08:00:17,435] Trial 353 finished with value: 0.7227373805685 and parameters: {'n_estimators': 613, 'eta': 0.060961615399032915, 'max_depth': 10, 'alpha': 0.5484, 'lambda': 31.658712584151232, 'max_bin': 452}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:00:30,404] Trial 354 finished with value: 0.7195299963291186 and parameters: {'n_estimators': 607, 'eta': 0.06322588034988495, 'max_depth': 10, 'alpha': 0.5495, 'lambda': 31.40096034263263, 'max_bin': 455}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:00:43,256] Trial 355 finished with value: 0.7211933225425113 and parameters: {'n_estimators': 615, 'eta': 0.06024876854386588, 'max_depth': 10, 'alpha': 0.5595, 'lambda': 31.112528712918767, 'max_bin': 452}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:00:55,672] Trial 356 finished with value: 0.7193057698691226 and parameters: {'n_estimators': 612, 'eta': 0.061558197562796936, 'max_depth': 10, 'alpha': 0.5572, 'lambda': 32.40646376864985, 'max_bin': 452}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:01:09,105] Trial 357 finished with value: 0.720153485482856 and parameters: {'n_estimators': 616, 'eta': 0.061748097259898535, 'max_depth': 10, 'alpha': 0.5596, 'lambda': 32.10041491759484, 'max_bin': 451}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:01:21,410] Trial 358 finished with value: 0.7189051645163187 and parameters: {'n_estimators': 610, 'eta': 0.0635245668891176, 'max_depth': 10, 'alpha': 0.56, 'lambda': 31.839474405313926, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:01:34,034] Trial 359 finished with value: 0.7190233586754079 and parameters: {'n_estimators': 609, 'eta': 0.061666221404057676, 'max_depth': 10, 'alpha': 0.5611, 'lambda': 33.01216300892477, 'max_bin': 451}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:01:47,216] Trial 360 finished with value: 0.7209328501233991 and parameters: {'n_estimators': 612, 'eta': 0.061461362011178716, 'max_depth': 10, 'alpha': 0.5604, 'lambda': 32.46472811735648, 'max_bin': 445}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:02:00,930] Trial 361 finished with value: 0.7220556521462498 and parameters: {'n_estimators': 614, 'eta': 0.060547609065791166, 'max_depth': 10, 'alpha': 0.558, 'lambda': 32.167247504480144, 'max_bin': 449}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:02:13,836] Trial 362 finished with value: 0.7179899823239386 and parameters: {'n_estimators': 613, 'eta': 0.0633445277357364, 'max_depth': 10, 'alpha': 0.5597, 'lambda': 32.48101464270256, 'max_bin': 451}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:02:26,587] Trial 363 finished with value: 0.7198348568249904 and parameters: {'n_estimators': 611, 'eta': 0.06217219048755339, 'max_depth': 10, 'alpha': 0.5605, 'lambda': 32.59197480995336, 'max_bin': 447}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:02:39,564] Trial 364 finished with value: 0.7171141533010837 and parameters: {'n_estimators': 617, 'eta': 0.06260735545407223, 'max_depth': 10, 'alpha': 0.5617, 'lambda': 32.68391578466575, 'max_bin': 448}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:02:52,800] Trial 365 finished with value: 0.7196024604105395 and parameters: {'n_estimators': 615, 'eta': 0.06057040425719277, 'max_depth': 10, 'alpha': 0.5494, 'lambda': 33.11473755797312, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:03:05,516] Trial 366 finished with value: 0.7187393014991865 and parameters: {'n_estimators': 619, 'eta': 0.061218636746437044, 'max_depth': 10, 'alpha': 0.559, 'lambda': 33.2950436666694, 'max_bin': 448}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:03:19,029] Trial 367 finished with value: 0.7211875246511371 and parameters: {'n_estimators': 617, 'eta': 0.061679123666348826, 'max_depth': 10, 'alpha': 0.5714, 'lambda': 33.207360407761136, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:03:32,386] Trial 368 finished with value: 0.7219870420819274 and parameters: {'n_estimators': 620, 'eta': 0.06171622755619129, 'max_depth': 10, 'alpha': 0.5676, 'lambda': 33.22474202059496, 'max_bin': 448}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:03:44,947] Trial 369 finished with value: 0.7196117063293084 and parameters: {'n_estimators': 609, 'eta': 0.061718919047114275, 'max_depth': 10, 'alpha': 0.5616, 'lambda': 33.238028031217134, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:03:56,836] Trial 370 finished with value: 0.7179397135861991 and parameters: {'n_estimators': 616, 'eta': 0.06241305556648629, 'max_depth': 10, 'alpha': 0.5571, 'lambda': 33.32471002182494, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:04:10,563] Trial 371 finished with value: 0.7201406393299662 and parameters: {'n_estimators': 618, 'eta': 0.062188859359561616, 'max_depth': 10, 'alpha': 0.5557, 'lambda': 33.29547635236387, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:04:23,799] Trial 372 finished with value: 0.7211414211418329 and parameters: {'n_estimators': 620, 'eta': 0.06151534433038742, 'max_depth': 10, 'alpha': 0.5749000000000001, 'lambda': 33.505878756374585, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:04:36,359] Trial 373 finished with value: 0.7185884195638117 and parameters: {'n_estimators': 633, 'eta': 0.06215568190167169, 'max_depth': 10, 'alpha': 0.558, 'lambda': 33.45239362482419, 'max_bin': 445}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:04:49,521] Trial 374 finished with value: 0.7201828608398727 and parameters: {'n_estimators': 634, 'eta': 0.06185388080919661, 'max_depth': 10, 'alpha': 0.56, 'lambda': 33.386809381844046, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:05:02,900] Trial 375 finished with value: 0.7216571296278758 and parameters: {'n_estimators': 638, 'eta': 0.06225842716663991, 'max_depth': 10, 'alpha': 0.5625, 'lambda': 33.409459654199, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:05:16,254] Trial 376 finished with value: 0.7191999390212078 and parameters: {'n_estimators': 642, 'eta': 0.06144678490724056, 'max_depth': 10, 'alpha': 0.5682, 'lambda': 33.2486692416566, 'max_bin': 445}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:05:28,711] Trial 377 finished with value: 0.7209402412528093 and parameters: {'n_estimators': 639, 'eta': 0.06170780860337475, 'max_depth': 10, 'alpha': 0.5709000000000001, 'lambda': 33.21896625660488, 'max_bin': 446}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:05:42,964] Trial 378 finished with value: 0.7207662201231325 and parameters: {'n_estimators': 644, 'eta': 0.06108789197451454, 'max_depth': 10, 'alpha': 0.5756, 'lambda': 33.298952332486685, 'max_bin': 442}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:05:55,888] Trial 379 finished with value: 0.7209367484032716 and parameters: {'n_estimators': 640, 'eta': 0.060962607019976314, 'max_depth': 10, 'alpha': 0.5749000000000001, 'lambda': 33.525350382524046, 'max_bin': 440}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:06:08,851] Trial 380 finished with value: 0.7205409835826168 and parameters: {'n_estimators': 647, 'eta': 0.06098043472557472, 'max_depth': 10, 'alpha': 0.5878, 'lambda': 33.61732293808658, 'max_bin': 439}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:06:22,355] Trial 381 finished with value: 0.7201380634694818 and parameters: {'n_estimators': 650, 'eta': 0.060339822754076726, 'max_depth': 10, 'alpha': 0.5808, 'lambda': 33.849474093120385, 'max_bin': 439}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:06:37,229] Trial 382 finished with value: 0.7214705749450183 and parameters: {'n_estimators': 657, 'eta': 0.06021222963718149, 'max_depth': 10, 'alpha': 0.5840000000000001, 'lambda': 33.982852874620875, 'max_bin': 438}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:06:51,079] Trial 383 finished with value: 0.7195572379866402 and parameters: {'n_estimators': 655, 'eta': 0.05992918652075675, 'max_depth': 10, 'alpha': 0.5842, 'lambda': 34.07776365640349, 'max_bin': 438}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:07:04,504] Trial 384 finished with value: 0.7204445684233053 and parameters: {'n_estimators': 657, 'eta': 0.05948666952819823, 'max_depth': 10, 'alpha': 0.5804, 'lambda': 34.331575864754846, 'max_bin': 441}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:07:17,782] Trial 385 finished with value: 0.718469296174605 and parameters: {'n_estimators': 658, 'eta': 0.05909203919396947, 'max_depth': 10, 'alpha': 0.5846, 'lambda': 34.2744771112829, 'max_bin': 437}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:07:31,182] Trial 386 finished with value: 0.7192129084420797 and parameters: {'n_estimators': 653, 'eta': 0.059254063608772944, 'max_depth': 10, 'alpha': 0.5805, 'lambda': 34.08038904323048, 'max_bin': 439}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:07:45,701] Trial 387 finished with value: 0.7194314849615796 and parameters: {'n_estimators': 660, 'eta': 0.05742369211320773, 'max_depth': 10, 'alpha': 0.5877, 'lambda': 34.39557500351411, 'max_bin': 440}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:07:59,542] Trial 388 finished with value: 0.7188909220889892 and parameters: {'n_estimators': 666, 'eta': 0.05807974450578742, 'max_depth': 10, 'alpha': 0.5856, 'lambda': 33.89409269050878, 'max_bin': 433}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:08:13,787] Trial 389 finished with value: 0.7210058017027896 and parameters: {'n_estimators': 653, 'eta': 0.05991800289031615, 'max_depth': 10, 'alpha': 0.5849, 'lambda': 34.86770782752506, 'max_bin': 441}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:08:26,053] Trial 390 finished with value: 0.7196694449821412 and parameters: {'n_estimators': 647, 'eta': 0.06054096393291293, 'max_depth': 10, 'alpha': 0.5913, 'lambda': 34.894101732756916, 'max_bin': 440}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:08:38,505] Trial 391 finished with value: 0.7170334073102687 and parameters: {'n_estimators': 652, 'eta': 0.059645740952632115, 'max_depth': 10, 'alpha': 0.5852, 'lambda': 34.966994594897436, 'max_bin': 441}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:08:54,142] Trial 392 finished with value: 0.7213229470702405 and parameters: {'n_estimators': 679, 'eta': 0.05992802582434478, 'max_depth': 10, 'alpha': 0.5977, 'lambda': 34.445110349887265, 'max_bin': 436}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:09:07,075] Trial 393 finished with value: 0.7186213493274586 and parameters: {'n_estimators': 697, 'eta': 0.06553634389229421, 'max_depth': 10, 'alpha': 0.6023000000000001, 'lambda': 34.533347729020306, 'max_bin': 433}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:09:19,854] Trial 394 finished with value: 0.7202166330230519 and parameters: {'n_estimators': 668, 'eta': 0.060482614115896206, 'max_depth': 10, 'alpha': 0.5978, 'lambda': 33.489225408194116, 'max_bin': 428}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:09:34,105] Trial 395 finished with value: 0.7205478807650575 and parameters: {'n_estimators': 681, 'eta': 0.05720685104465992, 'max_depth': 10, 'alpha': 0.5982000000000001, 'lambda': 34.71168988469614, 'max_bin': 428}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:09:48,299] Trial 396 finished with value: 0.7190192299402897 and parameters: {'n_estimators': 683, 'eta': 0.0569716642109418, 'max_depth': 10, 'alpha': 0.6011000000000001, 'lambda': 34.749368604005056, 'max_bin': 426}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:10:01,729] Trial 397 finished with value: 0.7169856807157017 and parameters: {'n_estimators': 679, 'eta': 0.0594120908360385, 'max_depth': 11, 'alpha': 0.5811000000000001, 'lambda': 33.86613624810672, 'max_bin': 430}. Best is trial 353 with value: 0.7227373805685.\n",
      "[I 2023-12-20 08:10:15,944] Trial 398 finished with value: 0.7227457002901749 and parameters: {'n_estimators': 685, 'eta': 0.06460101637469859, 'max_depth': 10, 'alpha': 0.5996, 'lambda': 35.59493514629963, 'max_bin': 435}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:10:29,435] Trial 399 finished with value: 0.7206891166384162 and parameters: {'n_estimators': 724, 'eta': 0.06563421901194412, 'max_depth': 10, 'alpha': 0.6029, 'lambda': 35.36384437303431, 'max_bin': 435}. Best is trial 398 with value: 0.7227457002901749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7227\n",
      "\tBest params:\n",
      "\t\tn_estimators: 685\n",
      "\t\teta: 0.06460101637469859\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.5996\n",
      "\t\tlambda: 35.59493514629963\n",
      "\t\tmax_bin: 435\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.689126    0.712882    0.708425    0.727859   \n",
      "1                    TP   32.000000   41.000000   39.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  195.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    5.000000    2.000000   \n",
      "4                    FN   34.000000   27.000000   29.000000   28.000000   \n",
      "5              Accuracy    0.869403    0.876866    0.873134    0.888060   \n",
      "6             Precision    0.969697    0.872340    0.886364    0.953488   \n",
      "7           Sensitivity    0.484848    0.602941    0.573529    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.975000    0.989900   \n",
      "9              F1 score    0.646465    0.713043    0.696429    0.732143   \n",
      "10  F1 score (weighted)    0.852568    0.868694    0.863132    0.878499   \n",
      "11     F1 score (macro)    0.783187    0.817329    0.808120    0.830694   \n",
      "12    Balanced Accuracy    0.739949    0.786471    0.774265    0.792076   \n",
      "13                  MCC    0.629225    0.655593    0.644346    0.695908   \n",
      "14                  NPV    0.855300    0.877800    0.870500    0.875600   \n",
      "15              ROC_AUC    0.739949    0.786471    0.774265    0.792076   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.704049    0.658896    0.728272    0.613857  \n",
      "1    38.000000   36.000000   37.000000   33.000000  \n",
      "2   199.000000  197.000000  198.000000  198.000000  \n",
      "3     3.000000    3.000000    2.000000    3.000000  \n",
      "4    28.000000   32.000000   31.000000   34.000000  \n",
      "5     0.884328    0.869403    0.876866    0.861940  \n",
      "6     0.926829    0.923077    0.948718    0.916667  \n",
      "7     0.575758    0.529412    0.544118    0.492537  \n",
      "8     0.985100    0.985000    0.990000    0.985100  \n",
      "9     0.710280    0.672897    0.691589    0.640777  \n",
      "10    0.874186    0.856119    0.864341    0.846106  \n",
      "11    0.819010    0.795656    0.807333    0.777663  \n",
      "12    0.780453    0.757206    0.767059    0.738806  \n",
      "13    0.671325    0.634790    0.659108    0.606478  \n",
      "14    0.876700    0.860300    0.864600    0.853400  \n",
      "15    0.780453    0.757206    0.767059    0.738806  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_xgb_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "#Y_trainSet7 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_7_cat = np.where(((y_pred_xgb_7 >= 2) | (y_pred_xgb_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_xgb_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_xgb_7_cat)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:10:40,191] Trial 400 finished with value: 0.7091809302404478 and parameters: {'n_estimators': 730, 'eta': 0.06558686304691369, 'max_depth': 10, 'alpha': 0.5999, 'lambda': 35.67862243181243, 'max_bin': 434}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:10:49,570] Trial 401 finished with value: 0.7080276374017179 and parameters: {'n_estimators': 752, 'eta': 0.06509382741284012, 'max_depth': 10, 'alpha': 0.604, 'lambda': 35.02868493299036, 'max_bin': 441}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:11:02,178] Trial 402 finished with value: 0.7079765667514332 and parameters: {'n_estimators': 704, 'eta': 0.05678855343313068, 'max_depth': 10, 'alpha': 0.5820000000000001, 'lambda': 33.946789523688324, 'max_bin': 436}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:11:12,735] Trial 403 finished with value: 0.7080095223169487 and parameters: {'n_estimators': 714, 'eta': 0.06437865403501114, 'max_depth': 10, 'alpha': 0.5746, 'lambda': 35.652171952191544, 'max_bin': 443}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:11:25,873] Trial 404 finished with value: 0.7096908722607737 and parameters: {'n_estimators': 693, 'eta': 0.05845641651368115, 'max_depth': 10, 'alpha': 0.6064, 'lambda': 33.72000611042038, 'max_bin': 431}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:11:37,205] Trial 405 finished with value: 0.7068092892910712 and parameters: {'n_estimators': 734, 'eta': 0.06379903270911368, 'max_depth': 10, 'alpha': 0.5795, 'lambda': 34.76956853317969, 'max_bin': 426}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:11:49,620] Trial 406 finished with value: 0.7090314508693863 and parameters: {'n_estimators': 798, 'eta': 0.060521511124484076, 'max_depth': 10, 'alpha': 0.6023000000000001, 'lambda': 33.58686297393712, 'max_bin': 438}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:12:02,683] Trial 407 finished with value: 0.7070264790637828 and parameters: {'n_estimators': 672, 'eta': 0.05830333871722061, 'max_depth': 10, 'alpha': 0.5754, 'lambda': 35.563093273784666, 'max_bin': 442}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:12:13,207] Trial 408 finished with value: 0.7073233433931054 and parameters: {'n_estimators': 673, 'eta': 0.05617085941712357, 'max_depth': 10, 'alpha': 0.5961000000000001, 'lambda': 34.19443682929505, 'max_bin': 435}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:12:24,696] Trial 409 finished with value: 0.7061710919252142 and parameters: {'n_estimators': 687, 'eta': 0.06405882603121373, 'max_depth': 10, 'alpha': 0.5741, 'lambda': 33.14129691086715, 'max_bin': 451}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:12:37,953] Trial 410 finished with value: 0.7082747989640714 and parameters: {'n_estimators': 666, 'eta': 0.06014182056553829, 'max_depth': 10, 'alpha': 0.5406000000000001, 'lambda': 34.48147040142881, 'max_bin': 429}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:12:49,149] Trial 411 finished with value: 0.7071342380052323 and parameters: {'n_estimators': 716, 'eta': 0.06726007182693067, 'max_depth': 11, 'alpha': 0.6104, 'lambda': 32.66159811354126, 'max_bin': 442}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:13:00,217] Trial 412 finished with value: 0.7070043861950313 and parameters: {'n_estimators': 643, 'eta': 0.06239124750994475, 'max_depth': 10, 'alpha': 0.5799000000000001, 'lambda': 33.61473402874334, 'max_bin': 451}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:13:12,663] Trial 413 finished with value: 0.7058684443801624 and parameters: {'n_estimators': 701, 'eta': 0.05837352945101479, 'max_depth': 10, 'alpha': 0.5957, 'lambda': 35.038947561389435, 'max_bin': 436}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:13:24,078] Trial 414 finished with value: 0.7081714309935654 and parameters: {'n_estimators': 674, 'eta': 0.06452478027291533, 'max_depth': 10, 'alpha': 0.5363, 'lambda': 32.416858272007325, 'max_bin': 442}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:13:37,694] Trial 415 finished with value: 0.7111560500508947 and parameters: {'n_estimators': 763, 'eta': 0.06094754842181105, 'max_depth': 10, 'alpha': 0.5719000000000001, 'lambda': 36.01633257341464, 'max_bin': 447}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:13:49,778] Trial 416 finished with value: 0.7073747305295354 and parameters: {'n_estimators': 661, 'eta': 0.05622382782313269, 'max_depth': 10, 'alpha': 0.6125, 'lambda': 34.26577978733913, 'max_bin': 436}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:13:59,899] Trial 417 finished with value: 0.70713522838865 and parameters: {'n_estimators': 642, 'eta': 0.0666775610138634, 'max_depth': 10, 'alpha': 0.54, 'lambda': 33.45907461101399, 'max_bin': 455}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:14:10,297] Trial 418 finished with value: 0.7094574378850194 and parameters: {'n_estimators': 687, 'eta': 0.06293171071422711, 'max_depth': 10, 'alpha': 0.5728, 'lambda': 32.81481877641754, 'max_bin': 423}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:14:22,607] Trial 419 finished with value: 0.706927285375093 and parameters: {'n_estimators': 649, 'eta': 0.059153346408206194, 'max_depth': 10, 'alpha': 0.6008, 'lambda': 34.71803550988085, 'max_bin': 443}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:14:33,352] Trial 420 finished with value: 0.7051623673474523 and parameters: {'n_estimators': 662, 'eta': 0.06089791800742199, 'max_depth': 10, 'alpha': 0.5461, 'lambda': 35.4682545040442, 'max_bin': 431}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:14:47,096] Trial 421 finished with value: 0.7060150995353883 and parameters: {'n_estimators': 731, 'eta': 0.058071689892314454, 'max_depth': 12, 'alpha': 0.5882000000000001, 'lambda': 33.61976846484414, 'max_bin': 450}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:14:56,796] Trial 422 finished with value: 0.708151114594177 and parameters: {'n_estimators': 636, 'eta': 0.06465469831456087, 'max_depth': 10, 'alpha': 0.5713, 'lambda': 32.2027877922575, 'max_bin': 438}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:15:09,813] Trial 423 finished with value: 0.7080199463261796 and parameters: {'n_estimators': 680, 'eta': 0.05535682966922702, 'max_depth': 10, 'alpha': 0.6129, 'lambda': 36.177153377718255, 'max_bin': 444}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:15:21,024] Trial 424 finished with value: 0.7093476519443963 and parameters: {'n_estimators': 642, 'eta': 0.06205433438481155, 'max_depth': 10, 'alpha': 0.546, 'lambda': 34.28142495694428, 'max_bin': 449}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:15:32,715] Trial 425 finished with value: 0.7080463205291431 and parameters: {'n_estimators': 653, 'eta': 0.060713422850993015, 'max_depth': 10, 'alpha': 0.5701, 'lambda': 32.94332826155551, 'max_bin': 429}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:15:46,768] Trial 426 finished with value: 0.7107863763275091 and parameters: {'n_estimators': 702, 'eta': 0.05363841349309253, 'max_depth': 10, 'alpha': 0.5954, 'lambda': 33.67918894898293, 'max_bin': 454}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:15:57,778] Trial 427 finished with value: 0.7100768396005492 and parameters: {'n_estimators': 671, 'eta': 0.0660762250440791, 'max_depth': 10, 'alpha': 0.5712, 'lambda': 32.16031162774931, 'max_bin': 436}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:16:10,940] Trial 428 finished with value: 0.7091197141825887 and parameters: {'n_estimators': 636, 'eta': 0.06311253953564357, 'max_depth': 10, 'alpha': 0.6178, 'lambda': 35.16304525260981, 'max_bin': 442}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:16:25,373] Trial 429 finished with value: 0.7070652532266897 and parameters: {'n_estimators': 658, 'eta': 0.05751608482316312, 'max_depth': 10, 'alpha': 0.53, 'lambda': 34.36662024062391, 'max_bin': 446}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:16:37,947] Trial 430 finished with value: 0.7088009817114743 and parameters: {'n_estimators': 688, 'eta': 0.05962065366133572, 'max_depth': 10, 'alpha': 0.5897, 'lambda': 32.993671852286575, 'max_bin': 439}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:16:48,194] Trial 431 finished with value: 0.7095717004492152 and parameters: {'n_estimators': 637, 'eta': 0.06424529672645352, 'max_depth': 10, 'alpha': 0.5492, 'lambda': 33.55096819864555, 'max_bin': 420}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:16:59,976] Trial 432 finished with value: 0.7081936212585443 and parameters: {'n_estimators': 662, 'eta': 0.06179361242476422, 'max_depth': 10, 'alpha': 0.5707, 'lambda': 35.26872446059613, 'max_bin': 433}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:17:10,187] Trial 433 finished with value: 0.7063459928307957 and parameters: {'n_estimators': 634, 'eta': 0.0677987407397331, 'max_depth': 10, 'alpha': 0.6174000000000001, 'lambda': 36.35061537810583, 'max_bin': 453}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:17:24,028] Trial 434 finished with value: 0.7075639053393542 and parameters: {'n_estimators': 676, 'eta': 0.0565095978330782, 'max_depth': 11, 'alpha': 0.5895, 'lambda': 32.2268143272799, 'max_bin': 448}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:17:35,000] Trial 435 finished with value: 0.703624426878694 and parameters: {'n_estimators': 649, 'eta': 0.05957970048734795, 'max_depth': 10, 'alpha': 0.5503, 'lambda': 34.02714356356354, 'max_bin': 442}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:17:45,704] Trial 436 finished with value: 0.7081465786704966 and parameters: {'n_estimators': 719, 'eta': 0.06311386640121591, 'max_depth': 10, 'alpha': 0.5691, 'lambda': 32.92508277718636, 'max_bin': 456}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:17:52,426] Trial 437 finished with value: 0.7024906431261673 and parameters: {'n_estimators': 636, 'eta': 0.06135815110861994, 'max_depth': 6, 'alpha': 0.595, 'lambda': 34.779021414471764, 'max_bin': 426}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:18:02,868] Trial 438 finished with value: 0.7080074822724896 and parameters: {'n_estimators': 691, 'eta': 0.0585085263314714, 'max_depth': 10, 'alpha': 0.526, 'lambda': 31.978782194176624, 'max_bin': 438}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:18:15,171] Trial 439 finished with value: 0.7057431161229886 and parameters: {'n_estimators': 659, 'eta': 0.06522633895972223, 'max_depth': 10, 'alpha': 0.5525, 'lambda': 36.87007927293033, 'max_bin': 449}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:18:27,240] Trial 440 finished with value: 0.7076634980395353 and parameters: {'n_estimators': 635, 'eta': 0.060368619815382545, 'max_depth': 10, 'alpha': 0.6105, 'lambda': 33.390760602614506, 'max_bin': 434}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:18:40,085] Trial 441 finished with value: 0.7044087163784775 and parameters: {'n_estimators': 672, 'eta': 0.054025067601780816, 'max_depth': 10, 'alpha': 0.5820000000000001, 'lambda': 35.84019147989021, 'max_bin': 443}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:18:51,198] Trial 442 finished with value: 0.7069558078001292 and parameters: {'n_estimators': 831, 'eta': 0.06299962015411328, 'max_depth': 10, 'alpha': 0.5366000000000001, 'lambda': 34.05277138522256, 'max_bin': 452}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:19:03,327] Trial 443 finished with value: 0.7071161915657707 and parameters: {'n_estimators': 653, 'eta': 0.0568027206546562, 'max_depth': 10, 'alpha': 0.5627, 'lambda': 32.60853659557331, 'max_bin': 445}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:19:13,926] Trial 444 finished with value: 0.7075562315088388 and parameters: {'n_estimators': 710, 'eta': 0.06658919353035804, 'max_depth': 10, 'alpha': 0.6186, 'lambda': 34.398389387924, 'max_bin': 429}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:19:24,611] Trial 445 finished with value: 0.7065789830333199 and parameters: {'n_estimators': 633, 'eta': 0.05896526582191524, 'max_depth': 10, 'alpha': 0.5915, 'lambda': 32.00279816496372, 'max_bin': 439}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:19:37,891] Trial 446 finished with value: 0.7066451313598356 and parameters: {'n_estimators': 681, 'eta': 0.061450137268524656, 'max_depth': 10, 'alpha': 0.5718, 'lambda': 35.00995287851083, 'max_bin': 457}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:19:48,111] Trial 447 finished with value: 0.7160913385641767 and parameters: {'n_estimators': 647, 'eta': 0.06431933281378806, 'max_depth': 10, 'alpha': 0.5206000000000001, 'lambda': 16.408563427854645, 'max_bin': 434}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:20:01,415] Trial 448 finished with value: 0.7098881073641936 and parameters: {'n_estimators': 777, 'eta': 0.05532566250486845, 'max_depth': 10, 'alpha': 0.5454, 'lambda': 33.15969186679437, 'max_bin': 446}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:20:12,824] Trial 449 finished with value: 0.7074091431836582 and parameters: {'n_estimators': 662, 'eta': 0.06040437188017681, 'max_depth': 10, 'alpha': 0.598, 'lambda': 33.60311153119253, 'max_bin': 450}. Best is trial 398 with value: 0.7227457002901749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7227\n",
      "\tBest params:\n",
      "\t\tn_estimators: 685\n",
      "\t\teta: 0.06460101637469859\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.5996\n",
      "\t\tlambda: 35.59493514629963\n",
      "\t\tmax_bin: 435\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.689126    0.712882    0.708425    0.727859   \n",
      "1                    TP   32.000000   41.000000   39.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  195.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    5.000000    2.000000   \n",
      "4                    FN   34.000000   27.000000   29.000000   28.000000   \n",
      "5              Accuracy    0.869403    0.876866    0.873134    0.888060   \n",
      "6             Precision    0.969697    0.872340    0.886364    0.953488   \n",
      "7           Sensitivity    0.484848    0.602941    0.573529    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.975000    0.989900   \n",
      "9              F1 score    0.646465    0.713043    0.696429    0.732143   \n",
      "10  F1 score (weighted)    0.852568    0.868694    0.863132    0.878499   \n",
      "11     F1 score (macro)    0.783187    0.817329    0.808120    0.830694   \n",
      "12    Balanced Accuracy    0.739949    0.786471    0.774265    0.792076   \n",
      "13                  MCC    0.629225    0.655593    0.644346    0.695908   \n",
      "14                  NPV    0.855300    0.877800    0.870500    0.875600   \n",
      "15              ROC_AUC    0.739949    0.786471    0.774265    0.792076   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.704049    0.658896    0.728272    0.613857    0.660581  \n",
      "1    38.000000   36.000000   37.000000   33.000000   40.000000  \n",
      "2   199.000000  197.000000  198.000000  198.000000  195.000000  \n",
      "3     3.000000    3.000000    2.000000    3.000000    5.000000  \n",
      "4    28.000000   32.000000   31.000000   34.000000   28.000000  \n",
      "5     0.884328    0.869403    0.876866    0.861940    0.876866  \n",
      "6     0.926829    0.923077    0.948718    0.916667    0.888889  \n",
      "7     0.575758    0.529412    0.544118    0.492537    0.588235  \n",
      "8     0.985100    0.985000    0.990000    0.985100    0.975000  \n",
      "9     0.710280    0.672897    0.691589    0.640777    0.707965  \n",
      "10    0.874186    0.856119    0.864341    0.846106    0.867682  \n",
      "11    0.819010    0.795656    0.807333    0.777663    0.814975  \n",
      "12    0.780453    0.757206    0.767059    0.738806    0.781618  \n",
      "13    0.671325    0.634790    0.659108    0.606478    0.655693  \n",
      "14    0.876700    0.860300    0.864600    0.853400    0.874400  \n",
      "15    0.780453    0.757206    0.767059    0.738806    0.781618  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_xgb_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "#Y_trainSet8 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_8_cat = np.where(((y_pred_xgb_8 >= 2) | (y_pred_xgb_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_xgb_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_xgb_8_cat)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:20:26,226] Trial 450 finished with value: 0.718778490167765 and parameters: {'n_estimators': 630, 'eta': 0.05799819297878565, 'max_depth': 10, 'alpha': 0.5639000000000001, 'lambda': 34.52464795771282, 'max_bin': 440}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:20:39,042] Trial 451 finished with value: 0.7172408225291482 and parameters: {'n_estimators': 695, 'eta': 0.06283851213576892, 'max_depth': 11, 'alpha': 0.5844, 'lambda': 35.4416405402366, 'max_bin': 445}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:20:50,437] Trial 452 finished with value: 0.7202535604910597 and parameters: {'n_estimators': 740, 'eta': 0.06604140332523759, 'max_depth': 10, 'alpha': 0.6222, 'lambda': 22.451468999679587, 'max_bin': 454}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:21:02,071] Trial 453 finished with value: 0.7211114162852155 and parameters: {'n_estimators': 716, 'eta': 0.06767680521028857, 'max_depth': 10, 'alpha': 0.6294000000000001, 'lambda': 14.211173577288847, 'max_bin': 453}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:21:10,513] Trial 454 finished with value: 0.7215504645596422 and parameters: {'n_estimators': 739, 'eta': 0.06754791423915332, 'max_depth': 10, 'alpha': 0.6201, 'lambda': 16.151030090364742, 'max_bin': 455}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:21:21,246] Trial 455 finished with value: 0.7191330836246687 and parameters: {'n_estimators': 732, 'eta': 0.068420712753616, 'max_depth': 10, 'alpha': 0.63, 'lambda': 20.892445060463032, 'max_bin': 454}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:21:30,800] Trial 456 finished with value: 0.7175534514105194 and parameters: {'n_estimators': 746, 'eta': 0.06912889340726318, 'max_depth': 10, 'alpha': 0.6194000000000001, 'lambda': 12.936625258875203, 'max_bin': 458}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:21:40,657] Trial 457 finished with value: 0.7157527060729758 and parameters: {'n_estimators': 742, 'eta': 0.0672575979254358, 'max_depth': 10, 'alpha': 0.6259, 'lambda': 18.671876052668267, 'max_bin': 457}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:21:49,474] Trial 458 finished with value: 0.7182401714972335 and parameters: {'n_estimators': 778, 'eta': 0.0658315178863146, 'max_depth': 10, 'alpha': 0.6223000000000001, 'lambda': 14.613125513609159, 'max_bin': 452}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:21:59,084] Trial 459 finished with value: 0.7188787718991739 and parameters: {'n_estimators': 760, 'eta': 0.06575561137528066, 'max_depth': 10, 'alpha': 0.6106, 'lambda': 13.46324062581083, 'max_bin': 432}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:22:09,274] Trial 460 finished with value: 0.7160201381773581 and parameters: {'n_estimators': 726, 'eta': 0.06976170850114041, 'max_depth': 10, 'alpha': 0.6008, 'lambda': 36.42953500283318, 'max_bin': 449}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:22:18,813] Trial 461 finished with value: 0.7213092054665422 and parameters: {'n_estimators': 713, 'eta': 0.06773524770599307, 'max_depth': 11, 'alpha': 0.6092000000000001, 'lambda': 16.305458315385227, 'max_bin': 457}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:22:28,300] Trial 462 finished with value: 0.7182197635832068 and parameters: {'n_estimators': 739, 'eta': 0.06832675082037097, 'max_depth': 12, 'alpha': 0.6212, 'lambda': 15.818665894192181, 'max_bin': 459}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:22:39,980] Trial 463 finished with value: 0.7196975895953241 and parameters: {'n_estimators': 709, 'eta': 0.06463803327540084, 'max_depth': 11, 'alpha': 0.6352, 'lambda': 15.27679915409919, 'max_bin': 423}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:22:49,113] Trial 464 finished with value: 0.7174165048868291 and parameters: {'n_estimators': 725, 'eta': 0.0647424300343185, 'max_depth': 11, 'alpha': 0.6057, 'lambda': 13.888809900589358, 'max_bin': 455}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:22:58,699] Trial 465 finished with value: 0.7219727007866552 and parameters: {'n_estimators': 715, 'eta': 0.06671013906673301, 'max_depth': 11, 'alpha': 0.5963, 'lambda': 16.612421792534132, 'max_bin': 457}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:23:07,146] Trial 466 finished with value: 0.71960907786643 and parameters: {'n_estimators': 763, 'eta': 0.06724471699924642, 'max_depth': 11, 'alpha': 0.6082000000000001, 'lambda': 16.55308709123332, 'max_bin': 457}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:23:17,085] Trial 467 finished with value: 0.7221038482829849 and parameters: {'n_estimators': 721, 'eta': 0.06973875704315781, 'max_depth': 11, 'alpha': 0.6294000000000001, 'lambda': 16.598032732543288, 'max_bin': 452}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:23:30,017] Trial 468 finished with value: 0.7166914424157804 and parameters: {'n_estimators': 718, 'eta': 0.05282894994698808, 'max_depth': 11, 'alpha': 0.6306, 'lambda': 17.338134118744243, 'max_bin': 450}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:23:40,754] Trial 469 finished with value: 0.7202573601466048 and parameters: {'n_estimators': 706, 'eta': 0.06913150824479851, 'max_depth': 11, 'alpha': 0.5883, 'lambda': 17.03999335183693, 'max_bin': 460}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:23:50,804] Trial 470 finished with value: 0.7202291774386791 and parameters: {'n_estimators': 722, 'eta': 0.06739817824159591, 'max_depth': 11, 'alpha': 0.5959, 'lambda': 15.987562197761209, 'max_bin': 443}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:24:00,208] Trial 471 finished with value: 0.7194565726344988 and parameters: {'n_estimators': 705, 'eta': 0.07221861204748238, 'max_depth': 11, 'alpha': 0.6359, 'lambda': 18.378367714699102, 'max_bin': 451}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:24:10,197] Trial 472 finished with value: 0.7211324144565587 and parameters: {'n_estimators': 749, 'eta': 0.06362367275720125, 'max_depth': 12, 'alpha': 0.5821000000000001, 'lambda': 15.22566119216699, 'max_bin': 441}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:24:18,980] Trial 473 finished with value: 0.71841387733599 and parameters: {'n_estimators': 751, 'eta': 0.0704049295077598, 'max_depth': 11, 'alpha': 0.6088, 'lambda': 14.722383337353628, 'max_bin': 436}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:24:27,403] Trial 474 finished with value: 0.7192147516638259 and parameters: {'n_estimators': 740, 'eta': 0.06847609541859988, 'max_depth': 12, 'alpha': 0.5774, 'lambda': 15.614949427172936, 'max_bin': 448}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:24:39,009] Trial 475 finished with value: 0.7223990035702607 and parameters: {'n_estimators': 766, 'eta': 0.06467119654760668, 'max_depth': 12, 'alpha': 0.5992000000000001, 'lambda': 16.2891939720598, 'max_bin': 455}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:24:52,555] Trial 476 finished with value: 0.7217452564222409 and parameters: {'n_estimators': 773, 'eta': 0.06434399843954942, 'max_depth': 12, 'alpha': 0.6371, 'lambda': 17.067648300942242, 'max_bin': 459}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:25:03,246] Trial 477 finished with value: 0.7172174453219402 and parameters: {'n_estimators': 762, 'eta': 0.06647645365918459, 'max_depth': 12, 'alpha': 0.6432, 'lambda': 16.397837671156132, 'max_bin': 459}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:25:15,308] Trial 478 finished with value: 0.7211189006190634 and parameters: {'n_estimators': 805, 'eta': 0.0706941216483061, 'max_depth': 12, 'alpha': 0.6331, 'lambda': 17.02877267289519, 'max_bin': 458}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:25:26,217] Trial 479 finished with value: 0.719658358715689 and parameters: {'n_estimators': 801, 'eta': 0.07107933974652013, 'max_depth': 12, 'alpha': 0.6383, 'lambda': 17.404114285071152, 'max_bin': 459}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:25:38,988] Trial 480 finished with value: 0.7207154885821677 and parameters: {'n_estimators': 793, 'eta': 0.06950436090577779, 'max_depth': 12, 'alpha': 0.6475000000000001, 'lambda': 16.70022546076805, 'max_bin': 456}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:25:50,360] Trial 481 finished with value: 0.7215285299756291 and parameters: {'n_estimators': 762, 'eta': 0.06461116181377229, 'max_depth': 12, 'alpha': 0.6294000000000001, 'lambda': 15.459345735884742, 'max_bin': 461}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:26:00,936] Trial 482 finished with value: 0.7224333586502232 and parameters: {'n_estimators': 808, 'eta': 0.06426440150382298, 'max_depth': 12, 'alpha': 0.6483, 'lambda': 15.38394262989604, 'max_bin': 461}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:26:12,728] Trial 483 finished with value: 0.718315011352607 and parameters: {'n_estimators': 829, 'eta': 0.06745807589090289, 'max_depth': 12, 'alpha': 0.6498, 'lambda': 15.564678002854363, 'max_bin': 460}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:26:24,761] Trial 484 finished with value: 0.721019516025055 and parameters: {'n_estimators': 774, 'eta': 0.06434053452770161, 'max_depth': 12, 'alpha': 0.6278, 'lambda': 14.951386038306547, 'max_bin': 461}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:26:35,079] Trial 485 finished with value: 0.7218335089471795 and parameters: {'n_estimators': 854, 'eta': 0.06388737809635245, 'max_depth': 12, 'alpha': 0.6496000000000001, 'lambda': 14.902146416687245, 'max_bin': 462}. Best is trial 398 with value: 0.7227457002901749.\n",
      "[I 2023-12-20 08:26:46,873] Trial 486 finished with value: 0.7242038160951185 and parameters: {'n_estimators': 855, 'eta': 0.06430359769393518, 'max_depth': 12, 'alpha': 0.6419, 'lambda': 15.118611388777623, 'max_bin': 460}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:26:59,511] Trial 487 finished with value: 0.721076731888008 and parameters: {'n_estimators': 872, 'eta': 0.06460515614406935, 'max_depth': 12, 'alpha': 0.6529, 'lambda': 15.227783279269033, 'max_bin': 461}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:27:11,335] Trial 488 finished with value: 0.7171449627259826 and parameters: {'n_estimators': 866, 'eta': 0.07049839706030503, 'max_depth': 12, 'alpha': 0.6553, 'lambda': 14.187516449264765, 'max_bin': 463}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:27:23,203] Trial 489 finished with value: 0.716521012472312 and parameters: {'n_estimators': 895, 'eta': 0.06767919261854438, 'max_depth': 12, 'alpha': 0.6468, 'lambda': 15.78919195111507, 'max_bin': 462}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:27:34,646] Trial 490 finished with value: 0.7219563897361205 and parameters: {'n_estimators': 861, 'eta': 0.06439732007529597, 'max_depth': 12, 'alpha': 0.6544, 'lambda': 15.114283299732678, 'max_bin': 456}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:27:46,330] Trial 491 finished with value: 0.7221956088239209 and parameters: {'n_estimators': 815, 'eta': 0.06654982365621084, 'max_depth': 12, 'alpha': 0.6549, 'lambda': 16.32176570649196, 'max_bin': 456}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:27:56,652] Trial 492 finished with value: 0.7195136405349418 and parameters: {'n_estimators': 849, 'eta': 0.06607824095766426, 'max_depth': 12, 'alpha': 0.6666000000000001, 'lambda': 16.581193155296333, 'max_bin': 458}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:28:08,206] Trial 493 finished with value: 0.718120767552352 and parameters: {'n_estimators': 859, 'eta': 0.06412816659484882, 'max_depth': 12, 'alpha': 0.6571, 'lambda': 17.796756320080505, 'max_bin': 456}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:28:18,893] Trial 494 finished with value: 0.7207769173623235 and parameters: {'n_estimators': 835, 'eta': 0.07011134427385673, 'max_depth': 12, 'alpha': 0.6517000000000001, 'lambda': 16.078225697600043, 'max_bin': 463}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:28:30,975] Trial 495 finished with value: 0.722623271827778 and parameters: {'n_estimators': 795, 'eta': 0.06363700762374584, 'max_depth': 12, 'alpha': 0.6371, 'lambda': 15.497627341174452, 'max_bin': 455}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:28:42,754] Trial 496 finished with value: 0.7210386039985568 and parameters: {'n_estimators': 826, 'eta': 0.06422743548136879, 'max_depth': 12, 'alpha': 0.6387, 'lambda': 15.161628578343201, 'max_bin': 454}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:28:53,552] Trial 497 finished with value: 0.7199880829263351 and parameters: {'n_estimators': 814, 'eta': 0.06638799920573371, 'max_depth': 12, 'alpha': 0.6254000000000001, 'lambda': 15.788151012954625, 'max_bin': 454}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:29:05,162] Trial 498 finished with value: 0.7203810003073152 and parameters: {'n_estimators': 790, 'eta': 0.06360824419205155, 'max_depth': 12, 'alpha': 0.6601, 'lambda': 14.575690806908604, 'max_bin': 463}. Best is trial 486 with value: 0.7242038160951185.\n",
      "[I 2023-12-20 08:29:16,763] Trial 499 finished with value: 0.7198943815835609 and parameters: {'n_estimators': 843, 'eta': 0.06347404637089961, 'max_depth': 12, 'alpha': 0.6406000000000001, 'lambda': 16.23209906439229, 'max_bin': 455}. Best is trial 486 with value: 0.7242038160951185.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7242\n",
      "\tBest params:\n",
      "\t\tn_estimators: 855\n",
      "\t\teta: 0.06430359769393518\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.6419\n",
      "\t\tlambda: 15.118611388777623\n",
      "\t\tmax_bin: 460\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.689126    0.712882    0.708425    0.727859   \n",
      "1                    TP   32.000000   41.000000   39.000000   41.000000   \n",
      "2                    TN  201.000000  194.000000  195.000000  197.000000   \n",
      "3                    FP    1.000000    6.000000    5.000000    2.000000   \n",
      "4                    FN   34.000000   27.000000   29.000000   28.000000   \n",
      "5              Accuracy    0.869403    0.876866    0.873134    0.888060   \n",
      "6             Precision    0.969697    0.872340    0.886364    0.953488   \n",
      "7           Sensitivity    0.484848    0.602941    0.573529    0.594203   \n",
      "8           Specificity    0.995000    0.970000    0.975000    0.989900   \n",
      "9              F1 score    0.646465    0.713043    0.696429    0.732143   \n",
      "10  F1 score (weighted)    0.852568    0.868694    0.863132    0.878499   \n",
      "11     F1 score (macro)    0.783187    0.817329    0.808120    0.830694   \n",
      "12    Balanced Accuracy    0.739949    0.786471    0.774265    0.792076   \n",
      "13                  MCC    0.629225    0.655593    0.644346    0.695908   \n",
      "14                  NPV    0.855300    0.877800    0.870500    0.875600   \n",
      "15              ROC_AUC    0.739949    0.786471    0.774265    0.792076   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.704049    0.658896    0.728272    0.613857    0.660581    0.672381  \n",
      "1    38.000000   36.000000   37.000000   33.000000   40.000000   34.000000  \n",
      "2   199.000000  197.000000  198.000000  198.000000  195.000000  199.000000  \n",
      "3     3.000000    3.000000    2.000000    3.000000    5.000000    2.000000  \n",
      "4    28.000000   32.000000   31.000000   34.000000   28.000000   33.000000  \n",
      "5     0.884328    0.869403    0.876866    0.861940    0.876866    0.869403  \n",
      "6     0.926829    0.923077    0.948718    0.916667    0.888889    0.944444  \n",
      "7     0.575758    0.529412    0.544118    0.492537    0.588235    0.507463  \n",
      "8     0.985100    0.985000    0.990000    0.985100    0.975000    0.990000  \n",
      "9     0.710280    0.672897    0.691589    0.640777    0.707965    0.660194  \n",
      "10    0.874186    0.856119    0.864341    0.846106    0.867682    0.854425  \n",
      "11    0.819010    0.795656    0.807333    0.777663    0.814975    0.789681  \n",
      "12    0.780453    0.757206    0.767059    0.738806    0.781618    0.748756  \n",
      "13    0.671325    0.634790    0.659108    0.606478    0.655693    0.631748  \n",
      "14    0.876700    0.860300    0.864600    0.853400    0.874400    0.857800  \n",
      "15    0.780453    0.757206    0.767059    0.738806    0.781618    0.748756  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"rmse\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_xgb_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "#Y_trainSet9 = np.where(((Y_te>=2) | (Y_te<=-2)), 1, 0) \n",
    "y_pred_xgb_9_cat = np.where(((y_pred_xgb_9 >= 2) | (y_pred_xgb_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_xgb_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_xgb_9_cat)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvYAAAHJCAYAAADuJX3FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABwWUlEQVR4nO3dd3hTZf8G8PukSfem0BY6oEArAgUElFEslCX++gplFhxUZSjKK25wMVQUHPiKKCAKOAqyylKksikgS2wFBISWTaGle2ec3x81oWmSNmkz2nB/rstLes7JOU++TdL7nDzneQRRFEUQEREREVGjJrF1A4iIiIiIqP4Y7ImIiIiI7ACDPRERERGRHWCwJyIiIiKyAwz2RERERER2gMGeiIiIiMgOMNgTEREREdkBBnsiIiIiIjvAYE9EREREZAcY7IlspG/fvhAEwaLHSEhIgCAIuHjxokWPY6wVK1ZAEASsWLHC1k0xC3t7PpZkjdc7EdHdjsGe7jrHjh3Dk08+ibCwMLi4uMDT0xMdO3bEq6++imvXrpntOA0tVFvDnj17IAgCZs2aZeumGE0dzhMSEgxuo35effv2NeuxZ82aBUEQsGfPHrPu1xrUr++q/7m5uaFjx4544403kJeXZ5HjWuL3QERkL6S2bgCRtYiiiOnTp2P+/PmQSqUYOHAgRo0ahYqKChw8eBAff/wxvvzyS6xcuRIjR460eHu+++47lJSUWPQYH3zwAaZPn44WLVpY9DjGiouLQ48ePRAYGGjrppiFvT2fuhg6dCg6d+4MAMjMzMSWLVvwwQcfYN26dThy5Ai8vb1t2j4iorsJgz3dNebMmYP58+ejZcuW2Lp1K9q3b6+1fv369XjssccQHx+P5ORkxMTEWLQ9ISEhFt0/AAQGBjao0Onl5QUvLy9bN8Ns7O351MWwYcO0vu34+OOP8cADD+D06dNYuHAh3n77bds1jojoLsOuOHRXyMjIwHvvvQeZTIbNmzfrhHoAGDFiBBYsWAClUolnn30WKpVKs65qX+qtW7eiV69ecHNzg4+PD0aOHIl//vlHa1+CIGDlypUAgFatWmm6KrRs2VKzjb4+x1W7shw7dgwPPfQQvL294e3tjREjRuDKlSsAgH/++QejR49G06ZN4eLign79+iEtLU3nOenrDtSyZUudLhRV/6sa0s6dO4fp06ejW7duaNq0KZycnBAaGoqJEyfi8uXLOsfq168fAGD27Nla+1R3NampT/qxY8cwfPhwNGvWTHOcZ599FtevX6/xeS1ZsgQdO3aEs7Mz/P39MXHiRIt1A6nO0PM5ceIExowZg9DQUDg5OaFJkyaIjIzECy+8ALlcDqDy9zB79mwAQL9+/bTqVdX169cxZcoUtGzZEo6OjmjatCni4uJw9OjRGtvz888/48EHH4SnpycEQUBubi5cXV3RunVriKKo9/nExsZCEAQcP368zjVxd3fH+PHjAQCHDx+udXuVSoUvv/wS3bt3h7u7O9zc3NCtWzd8+eWXet+DALB3716tejWmrl9ERJbEK/Z0V1i+fDkUCgVGjRqFjh07GtxuwoQJmDNnDs6dO4e9e/dqgqrahg0bsG3bNsTFxaFv3774888/sX79euzevRsHDx5EREQEAGDmzJnYuHEjUlNT8cILL2i6IxjbLeHo0aOYN28eoqOjMWHCBPz111/YsGEDTp48iaSkJERFReHee+/FE088gcuXL2P9+vUYMGAA0tPT4e7uXuO+p02bpjf4btmyBX/88QdcXV21nu/ixYvRr18/9OrVC46Ojjh58iS++eYbbN68GcePH0dQUBCAyiu3ALBy5UpER0dr9YOuekKjz6ZNmzBq1CgIgoCRI0ciJCQEx44dw+LFi7Fp0yakpKQgLCxM53GvvfYatm/fjv/85z8YNGgQdu/ejWXLlml+f7bw559/omfPnpBIJHjkkUfQqlUrFBQU4Pz58/jqq6/w/vvvQyaTYdq0adi4cSP27t2L8ePH661Reno6oqKicOPGDfTv3x9jx47FlStXsHbtWvz8889Yu3Ythg4dqvO4tWvX4tdff8XDDz+MZ555BhkZGfDx8UF8fDyWL1+OHTt2YODAgVqPuXLlCrZt24auXbuia9eu9aqBoRMHfcaNG4effvoJISEhmDBhAgRBQFJSEp577jns27cPq1evBgB07twZM2fOxOzZsxEaGqp1Aso+90RE/xKJ7gL9+vUTAYhLly6tdduxY8eKAMR3331Xs2z58uUiABGAuGXLFq3tP/vsMxGAGBMTo7V8/PjxIgAxIyND73Gio6PF6m/B3bt3a47zww8/aK176qmnRACil5eX+N5772mte//990UA4meffWZSG9SSk5NFqVQqtmnTRszKytIsv3r1qlhWVqaz/S+//CJKJBJx8uTJets/c+ZMvcdR13H58uWaZYWFhaKvr6/o4OAgHjhwQGv7uXPnigDEAQMG6H1eISEh4qVLlzTL5XK52KdPHxGA+Pvvv9f4nKu3qVOnTuLMmTP1/qc+XnR0dK3P58UXXxQBiElJSTrHysnJEZVKpebnmTNnigDE3bt3623bwIEDRQDihx9+qLV8//79okQiEX18fMSCggKd9giCIG7btk1nf8eOHRMBiCNGjNBZ9/bbbxv9HhHFO7+Dqs9dFEWxuLhYbN++vQhAnD17tma5vtf7jz/+KAIQu3XrJhYVFWmWFxUViffdd5/e94G+3wMREVXiFXu6K2RmZgIAgoODa91WvY2+LiAxMTGIjY3VWvb8889j4cKF2LVrFy5duoTQ0NB6t7dPnz549NFHtZaNHz8e3377LXx8fDB9+nStdY899hjefPNN/PnnnyYf6+TJkxg5ciS8vLzwyy+/wM/PT7PO0E23Q4YMwb333ovk5GSTj1fdxo0bkZOTg0cffRS9evXSWvfKK69gyZIl2LFjh97avvPOO1r3KkilUjz55JPYv38/jh49igceeMDodqSmpiI1NbV+TwbQdBep+s2Hmo+Pj9H7uXr1Kn777TeEhobi5Zdf1loXFRWF+Ph4JCYmIikpCU888YTW+kceeQQPPfSQzj67du2K7t27Y/Pmzbh58yb8/f0BAEqlEt988w08PDwwbtw4o9sIVP7+1F29bt68iS1btuDatWto3bo1pk6dWuNjv/32WwCVN3m7ublplru5ueHDDz/EoEGD8M033+i8F4iISD/2sae7gvhv1wBjxtFWb6Nv2+joaJ1lDg4OiIqKAlDZt9oc9HWFaN68OYDKLgkODg561129etWk49y4cQP/93//h/LyciQlJaFt27Za60VRxA8//IABAwagadOmkEqlmn7NJ0+eNMvwoOqaVe/2BAAymUxTc3217datm84y9YlZbm6uSe0YP348RFHU+9/u3buN3k98fDwcHBwwbNgwjB8/Ht999x0uXLhgUluAO8+3T58+kEp1r8EMGDAAAPDHH3/orKvphGbKlCmQy+WaUA1UdsO6fv06HnvsMa2AbYxNmzZh9uzZmD17NlauXAlPT0+8+uqrOHLkSK0nMidOnIBEItH7vurXrx8cHBz0Pj8iItKPwZ7uCuqRYdQ3n9ZEHY71jSajvsJZXUBAAAAgPz+/rk3Uom+kFXW4q2md+sZMYxQXFyM2NhZXrlzB8uXL0adPH51tXnrpJTz++OM4ffo0Bg8ejJdffhkzZ87EzJkzERoaioqKCqOPZ4i6ZuoaVqf+PeirbU21UCqV9W5bXXTv3h379+9HTEwM1q5di/Hjx6NNmzZo164dfvrpJ6P3U5+6GHoMAIwZMwa+vr5YtmyZ5oR3yZIlAIBnnnnG6PapLV++XHMCVFJSgtOnT2P+/Pnw9fWt9bH5+fnw9fWFTCbTWSeVSuHn54eCggKT20REdLdiVxy6K0RFRWH37t3YsWMHJkyYYHA7pVKpuTrbu3dvnfU3b97U+zh1V5/GMvShSqXC2LFj8ccff+D999/H2LFjdba5desWPv/8c3To0AEHDx6Eh4eH1vpVq1aZpS3qmqlrWN2NGze0tmsMevbsia1bt6K8vBzHjx/Hr7/+ioULF2Ls2LFo2rSpUUOp1qcuNX0z5eLigoSEBHz66af47bffEB4ejuTkZPTo0QORkZHGPD2z8fLyQk5ODuRyuU64VygUyM7Ohqenp1XbRETUmPGKPd0VEhIS4ODggA0bNuD06dMGt/v2229x/fp1RERE6O0eoG+kFaVSiZSUFABAly5dNMvV3WVsdeW4JtOmTcOWLVvw1FNP4Y033tC7TXp6OlQqFQYNGqQT6q9evYr09HSdx9TlOatrpm/2VYVCoantfffdZ/Q+GwonJyf06tULc+bMweeffw5RFLFx40bN+prqpa5LSkoKFAqFznr1CWhd6vLss89CEAQsWbIEX3/9NVQqFSZPnmzyfuqrS5cuUKlU2Ldvn866ffv2QalU6jw/iUTSIN9TREQNAYM93RXCwsLwxhtvQC6X4z//+Y/ecL9x40a88MILcHBwwJdffgmJRPftsWvXLmzdulVr2RdffIELFy6gX79+Wjd3NmnSBIBx3X+s6bPPPsPChQvRv39/LF682OB26uEXU1JStIJUUVERJk6cqDds1uU5Dxs2DL6+vli1ahV+//13nbamp6djwIABVpnQyxz279+vt3uM+tseZ2dnzbKa6hUUFISBAwfi4sWL+Oyzz7TWHT58GImJifDx8UFcXJzJbWzTpg0GDhyIzZs3Y+nSpfD29saYMWNM3k99PfXUUwCAGTNmaM3CXFJSorlB/Omnn9Z6TJMmTRrce4qIqKFgVxy6a8yaNQvFxcX49NNP0alTJwwePBjt27eHXC7HwYMHcfjwYbi4uGDVqlUGu0o88sgjiIuLQ1xcHNq0aYPU1FT88ssv8PX1xZdffqm1bf/+/fHRRx9h4sSJGDFiBNzd3eHt7Y3nn3/eGk9Xr8zMTLz88ssQBAEdO3bE+++/r7NN586dMWzYMAQEBCA+Ph6rV69G586dMWjQIOTn5+O3336Ds7MzOnfurDMKT0REBFq0aIHVq1dDJpMhJCQEgiDg8ccfNzhakLu7O7799luMGjUK0dHRGDVqFEJCQnD8+HEkJycjICBA0we8Mfjkk0+QnJyMvn37IiwsDO7u7jh16hS2bdsGb29vTJo0SbNtv379IJFIMGPGDPz111+am03feustAMDixYvRu3dvvPrqq0hOTka3bt0049hLJBIsX75c59sUYz377LNITk5GdnY2/vvf/8LFxaX+T95E48aNw6ZNm7BmzRq0b98ew4YNgyAI2LhxIzIyMjB69GidEXH69++P1atXY+jQoejSpQukUikefPBBPPjgg1ZvPxFRg2ObUTaJbOfw4cPiE088IbZs2VJ0dnYW3dzcxPbt24svv/yyeOXKFb2PqTpe+datW8UePXqIrq6uopeXlzh8+HDx7Nmzeh/3ySefiPfcc4/o6OgoAhBDQ0M162oax17fOPAZGRkiAHH8+PF6jwU943tXH8devY+a/qu6/+LiYvGNN94QW7duLTo5OYlBQUHilClTxOzsbL3tF0VRPHLkiBgTEyN6enqKgiBojdOub9z3qo8bNmyY6OfnJ8pkMjE4OFh85plnxGvXrulsW9P4/LWNpV+duk2G6lp1n8aMY799+3YxISFBbNeunejp6Sm6urqK4eHh4tSpU8WLFy/q7Pv7778XO3XqJDo7O2t+B1VdvXpVfOaZZ8SQkBBRJpOJTZo0EYcOHSoeOXLE4HPRV9/qFAqF6OfnJwIQT506Vev21Rkax94QQ68XpVIpLlq0SOzatavo4uIiuri4iPfdd5/4xRdfaI35r3bz5k1x7NixYrNmzUSJRGLS75qIyN4JomjCFIFEd6kVK1bgySefxPLly7VmvCRqrC5cuIC2bdsiKipKbx93IiJqfNjHnojoLvTRRx9BFEWbdg0jIiLzYh97IqK7xKVLl/D999/jn3/+wffff48uXbpg5MiRtm4WERGZCYM9EdFdIiMjA2+//Tbc3NwwePBgfPXVV3pHfyIiosaJfeyJiIiIiOwAL9UQEREREdkBBnsiIiIiIjvAYE9EREREZAcY7ImIiIiI7MBdPSpObm4uFAqF2ffbtGlTZGVlmX2/pI11th7W2jpYZ+tgna3H3LWWSqXw8fEx2/6I7M1dHewVCgXkcrlZ9ykIgmbfHHDIclhn62GtrYN1tg7W2XpYayLrY1ccIiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR24q2+eJSIiIjJVaWkpbt68CVEUeWMwWZQgCBAEAf7+/nBxcal1ewZ7IiIiIiOVlpbi2rVr8PDwgETCjg9keSqVCteuXUOLFi1qDfd8RRIREREZ6ebNmwz1ZFUSiQQeHh64efNm7dtaoT1EREREdkEURYZ6sjqJRGJUty++MomIiIiMxD71ZCsM9kRERGQWDLREDR+DPRER1VvV0UH0BUBDoVClUtW4T1OWm+uxjUFNz6Pqupp+J1W30bdepVKhuEKJBXuvYPjyUxj67UkMX34KC/ZeQXGFUudx9lJbArp27YolS5bUe5v6Wr16Ndq0aWPRY5hDQ2onR8UhIqI6Ka5QYlHKVWw/m4cyuQoiAAGAk1SAp5MDerb0hCAI+P1SIRQqFaQSCaJaeWBEp6Z4+5eLSM8pgzoLtm7ijI8fCYOrowO+/v0G9qcXaD3m0fua4cc/spCSUbncQRDQJ8wTk3u1gJujA0RRRHGFEksP3dBso37sY1398eMftyr3qVTB2ekMeoa4Y1LPQLg5OtiyhCYprlBiycHrWs+vT5gnJvVsDgBYeug69qcXoEKpRGlF5QmTCoBcKcLRQYCXs/TO9qKILw9ex/azeShXVG7rLJUgqqUn/rldiku55VAZyOlrU7OxNjUbEgEQRUAiAI5SCVwdJZD++3uZ1LM53J0YMRqaa9eu4aOPPsLOnTuRk5MDf39/DBkyBC+//DJ8fX1N2tf27dvh6upqtrZ17doVkyZNwuTJkzXLhg4div79+5vtGNVt2bIFEydOxLFjxxAUFKSzvlevXujbty/mzp1rsTaYG991RERksuIKJSb8dBaXcsu1losAyhQiyhQKbDqVo/O4dWm3sS7tts7y87fLMGz5ab3HMvQY9XIJKgOs0Y8tUWBtbimOXC7AsjERDT7cF1co8dneK/jl71xUz9rrUrNx5HIhAOBybrnOerUyhYiyIrnBWgJAiVyF5H/y9K4TRBVc5eV61wEAyoGy4sp/bsspwLZjVyEKAgQ3NwyM8MZzvVs0+DrbiiiKEATB4se5ePEiHn74YbRu3RpLlixBSEgIzp49i9mzZ2Pnzp3Ytm0bfHx8jN6fn5+fBVtbycXFxaix2+vqoYcegq+vL3766Se8/PLLWusOHz6M8+fPY+nSpRY7viUw2BMRkcmWHrquCfXuFSXwLSuAABGCKMLyEcVM8oFV62/jyfub27olBuUUV+DNXzJQLBfR0tBG+ZX/M7i+niSiiPa3M+BRUWzS40qlTtjQti82/nUbJ64WNYqTKGsprlDiq5Sr2HchFwqVCKlEwIOtffBsVJDFajR9+nQ4OjpizZo1mrAcFBSEDh064IEHHsDcuXPx0UcfabYvKirCM888g19//RUeHh544YUXMGHCBM366lfYCwoKMHv2bGzbtg1lZWXo3Lkz5syZgw4dOmge8+uvv+KTTz7BmTNn4Obmhh49emDFihUYNmwYrly5grfffhtvv/02AODWrVtYvXo13nrrLZw/fx7nz59Hr169cODAAbRt21azz6+++grLli3DsWPHIAgCzp49i1mzZuHQoUNwdXVF37598e6776JJkyY6NZHJZBg5ciRWr16Nl156SesEa9WqVejUqRM6dOiAr776CqtXr8alS5fg7e2NQYMG4Z133oG7u7veWk+dOhX5+fn47rvvNMveeustnDx5Ehs3bgRQeUL3xRdfYOXKlbh16xbCwsLw8ssv4z//+Y/Rv1N9GOyJiMhk+y5UpkmJSokhF3+Ho1Ju4xbVjeqmAHl5gK2boaVCqcIfVwtxMaccxRVKdLJ1g/4lCgJUgvG35ikldwLqpdxyLD10HS9GB1uiaY1KcYUSTyWewsXbZVrfNK398yaOXs7Ht+Pamz3c5+bmYvfu3XjjjTd0roD7+/tjxIgR2LRpE+bPn68Jt4sWLcK0adPw6quvYvfu3Xj77bfRpk0b9O3bV2f/oihi3Lhx8PHxQWJiIjw9PbFy5UqMHDkShw4dgo+PD3777Tc8+eSTmDZtGhYtWoSKigrs2LEDALB8+XL069cPjz/+OB577DG9z6FNmzbo1KkT1q9fj+nTp2uWb9iwAcOHD4cgCLh58yaGDRuGxx57DHPmzEFZWRnmzJmDiRMnYsOGDXr3++ijj2Lx4sU4ePAgevfuDQAoLi7Gpk2b8M477wCoHGry/fffR3BwMC5fvozXX38dc+bMwfz58037RVTxwQcf4Oeff8b8+fMRFhaG33//HVOmTEGTJk3Qq1evOu+XwZ6IiExyq7AcN4sqg7xfWT4clXIoJQ7IdvGCWHndHmIjuWwvASA0bw4r9IQwSrlCheWHM5Fd7AqVoyvgaOsWVSqROuPPpm1QLnWq8z5S0gvwYrQZG9VIfZVyVSfUA4BKBC7mlOGrlKt4JSbUrMdMT0+HKIpaV7qratu2LfLy8pCdnY2mTZsCAO6//37897//BQC0bt0aR44cwZIlS/QG+5SUFPz99984ffo0nJwqXyPqq/dbtmzBE088gQULFmDYsGF4/fXXNY9TX8338fGBg4MD3N3d4e/vb/B5jBgxAt98840m2F+4cAGpqan44osvAFSeIHTs2BFvvvmm5jH/+9//0LlzZ1y4cAGtW7fW2WdERAS6du2KVatWaYL95s2boVKpMHz4cADQ6vcfGhqK6dOn47XXXqtzsC8uLsbixYuxfv16dO/eHQDQsmVLHD58GN999x2DPZEx9PVjVKlUmkkf9PVxtFbfR6LGorhCiccTz2h+blaSCwC46t4UKS0ayrVl40kEYPaATkZNOGTs54F6dBj1tqZ8jizaewXrfZtBZdp9jI2CQqXiZyqAfRdyDd4TohKB/RdyzR7sa1P9NQsA3bp109qmW7duBvubp6amori4GBEREVrLy8rKcPHiRQDAqVOn8Pjjj9ernXFxcZg9ezaOHTuGbt26Yd26dejQoYPmuGlpaThw4ABatmyp89iLFy/qDfYAMG7cOLz99tv48MMP4e7ujsTERDz88MPw8vICUHni8tlnn+HcuXMoLCyEUqlEWVkZiouL4ebmZvLzOHfuHMrKyjBq1Cit5XK5HB07djR5f1Ux2DcS5vwwtPcP1qrPr3KUjOuaETYcBAFdg9xx5lYJMnLu3GjmIABN3GR4MMwTj3cLwA/Hb+qMylF9lAf1car/n+wDf5/6LT10HYXllbHESVGB9rcvAgBuuRp/011DIorAsOWnNO/zqqPsCIKg8xmiHolmYo9AzeeBekSeRSnXkHyucpQZ9WgxTlIJXGSSf/tQe2FSz+Za+6/+//3pBQZDX2PnIJHc9e8pURShMDTc0L/kKtHsnz+tWrWCIAg4d+4cHn74YZ3158+fh7e3t95+6MZQqVTw9/dHUlKSzjp1OHZ2dq7Tvqvy9/dH7969sWHDBnTr1g1JSUl44okntNoxaNAgTT/96o81JC4uDm+//TY2btyIXr164fDhw5pvFq5cuYJx48Zh/PjxmD59Onx8fHD48GFMmzYNCoVC7/70XSiQy+90V1QP85uYmIiAAO2ugOpvPOqKwb4B0/cHpeofnvruSz0kWWO9manqGMpF5QqtIfIcBAE9W3rgxLVinZEifjmTq7MvpQjcqmHECPVyZweguZcTCsuVKCxXolwhar7Cd3SoHOIvuo23wboyLDZs5nzPmfq7bixjgKv71kMUEXPlOKSqyj9sNxtrsAeQXVz5HNTvcxeZBK4yCQARBWUqyKsFsbWp2Vifmg1HB6BCaXhEHqVYOdJMiVyleVxSWja8XaUo0vP54eEoQWGFvcZ6oE+Yp62bYHOCIEAqqflzQSoRzP53wtfXF9HR0Vi+fDkmT56s1c/+5s2bWL9+PUaNGqV13OPHj2vt4/jx4wa78kRGRuLWrVuQSqUICQnRu829996Lffv2YezYsXrXy2QyKJXKWp/LyJEjMWfOHMTFxeHixYuIi4vTasfWrVsREhICqdT4iOvu7o5HHnkEq1atwqVLlxAaGqrplvPnn39CoVBg9uzZmsC+adOmGvfXpEkTnDlzRmvZyZMnIZPJAFR2/3FycsLVq1fr1e1GHwb7BqT6leZJa87hUo52P7x1abeRdPI2/nNvEzwXpX1lqep+ANS+r9RsHLtShKWjwxtNuFePm518Ng+l/46bbcjGk1WG2hNFSET9fzBlKiVcFWVGtyH334EhXP79ryp5EbDjdg4O/pGBL0eGw9dVhpIKJX784yZ+v1QEpUoFB4kEPULd8VjXALg2groLggCFVApVTk6jCZ/GqvreKalQ4tXNF3A1r1zzPlEC2JmVjd3HMjAo3BdP3h8AZ2nlH119f3hLKpT44Xgmfr9UCKVK1PyuH73PH256xvQuLlfgh+M3cfhyEVQqEY4yB3QLcsNjXf0b5GtDFEW4l+SjokwBr/Ii+JYVAAB+D2yPfCcPG7fOfErlKpTKaw7YKgBltWcQHQrxzokEAM04/pVDhNZhh41ESx8nzXj7d7sHW/tg7Z839c4TIBEq11vChx9+iP/7v//DmDFjMGPGDK3hLgMCAvDGG29obX/kyBEsXLgQDz/8MPbs2YPNmzfjxx9/1Lvv6OhodOvWDePHj9fcZJuZmYmdO3diyJAh6Ny5M1555RWMGDECLVu2RFxcHBQKBXbu3ImpU6cCAIKDg/H7778jLi4Ojo6OBr89+L//+z+89tpreO2119C7d28EBgZq1j311FP44YcfMHnyZDz33HPw9fVFRkYGNm7ciE8//RQODoY/V8eNG4dHHnkE586dw5QpUzSf8S1btoRCocCyZcswaNAgHDlyBCtXrqyx1lFRUVi0aBF++ukndO/eHWvXrsWZM2c03Wzc3d0xZcoUvPPOO1CpVHjggQdQVFSEI0eOwM3NDfHx8TXuvyYM9jam70rzg629UCpXISNHf9hUqoCNJ29j5z+5cJU5QPnv8HIeTg64UVCBcmXlp4WzVIJ+bbzx981iZOTojj8sAsjIKcOza8/hq1F1D/eWvApd/WRH37jZVTkpyiFTaf9xlKqUuD/zNJqW5lmkjYasmXtQ6+fuVf6tOgls3ilFbPsmcHRo4BNAC0CuuwfKiwpR45lUAyRChAAB4r8NFyCgQqnC8SuFuJxXDpUoAiKgVIkoV4roAKCDoZ1dAFZuu/OjRADa+LnggVBPODpIkF8mR1LabShFUet3rfwL+OFnwEUmQUtfZ0QGuuPEtUKczy7TfCWv9dpIbdivjQHnb6Go/M577B+fYFzw1p3YhQgA3BwdMCjCG1M4jr3Gs1FBOHo5HxdzyrTCvUQAWvq64Nkoy7yfwsLCkJycjI8++ggTJ05Ebm4umjVrhiFDhuCVV17RGcP+2WefRVpaGj755BO4ublh9uzZiImJ0btvQRCwatUqzJ07F9OmTcPt27fRrFkz9OjRQ3Mzbu/evbFs2TJ8+umnWLhwITw8PNCjRw/NPl5//XW88soruP/++1FeXo5bt27pPZaHhwcGDRqEzZs343//+5/WuoCAAGzduhVz5szBmDFjUFFRgaCgIMTExNR6H02PHj3Qpk0bpKenY8yYMZrlHTt2xJw5c7Bw4UK8//776NGjB9588008//zzBvcVExODl156CXPmzEF5eTnGjh2L0aNH4++//9ZsM336dPj5+eHzzz/HpUuX4OXlhY4dO2LatGk1trM2gmhvl+BMkJWVpdXnyRwEQUBgYCBu3LihNZV31eBbVK7A0kM3sC89H9nFcoOz+wGAq7wMD177E86KCrO2szofVyniuzQzOkhUKFU4eDEf6bfLoBJFSAQBYU2c0aull959VA1YQpVRrvX9XKFQ4dClAs2+BQhwlkqQX6aAXGm4WAJEuMqNv/JeVZnUEdYefVsQgMhANzzY2tuqxzWZIMDTwxMFhQV3Li82YBVKEb9fzMeF22UokyuhqHLhVYBlzk0chMpuF+bSkF8b+y7kIfV65ddW5VJH7G3RGUWOd2afdJcJiG7jg93n81D2b19zdd31lUgA4CyTwEUmoLRCBUEQ4Fzl3+rZTHtX6RJVIldh0f6rSD5X+zd3dztDr02pAHi6OKCgVAmFnvUejhK4OjlApar8bC2Vqwx2E6q8R0mK6NbemNjjzmy+giCgefPmWn8P60smk2mCoq2kp6fDw6N+31Cpx7HffyEXcpUImURAHwuPY29uHTp0wPTp0w0OT0nmV1hYiLCwsBq3aRDBfvv27di8eTPy8vIQFBSEhIQEtGvXTu+2ixYtwt69e3WWBwUF4dNPPzXpuJYM9ucvXcWSg9c0V+IlggB3Rwmu5ZWjXN9noyjCu7xIp7vIvTkXEVqQadY2GnJPM1f0auWpFbQB3VBeoVRh66nbyCtVaP1BFQTA2/nOlcaqYzGXK1RQiiIcBAGODgKcZRJUKESo/t2no4OAonIlFCqxXn+kRUGAUnDQGWqv0NENBwPbo0SqO4OdShC0xlu2pkAPR6x/sr1Njm0sfSerDVVWUQUe//EMCsobf5eGhvraUHftu5hTpvNe9XSS4PtH26Gpe+UYjVW7BRZXKLH04HWkZBRoJuTpredmVfXjjB1RRn2MErlK7/4f6+qPMd+dhh28JLQ0dZOib2tvpGQUoEKpQkmFUutEqE9rL0zsEQhBELD00HWkpN+pS9S/91e5yiSVdTOwvvpNxOpukGX/ni07SyUYFOGDKb21BxZQs8Rnh70E+6oa271XJSUlOHLkCMaMGYOtW7dqhmsky2sUwf7gwYNYuHAhJkyYgIiICOzYsQM7d+7EggUL9E5XXFJSgoqKO1evlUolXn31VTz00EMYPXq0Sce2VLCXuPlg4Cd7jA4XMqUCD177EwHF+qf5BoD9LTqhRFb/O8pr4+MqhQME3BfkBkEQcOxKIYrKlahQiXCUCPBwksLdSYKLNXSHCfF2RHGFiNzSmr+NsIRCmSvKpQ1k4GcjNHWTYeNT7Rv0h3pjCfbFFUoMX35SM2JLY9eQXxvqm4zVYdBBAPpUGfGlNpYOMtX3P3z5KWQWWvZbT2ur+vow9kTImJOkugznaQiDvX1asmQJPv30U4wePRrvvvuurZtzVzEm2Nu8j/3WrVsRExOD/v37AwASEhKQmpqK5ORkjBs3Tmd7V1dXuLre+dr3yJEjKC4uRr9+/azWZkPUw51tOvkHRAASUYWQgkw4652RUYSrvBwylQJ+pXnwLi+CUuKAMgfdUJrhFYjLntaZGTH738/eM1cq2wi4a0+QIgIoA+Cie+Vbsw915rf8eUij52CB0Q/uVlWHYbQHDfm14ebogBejg/FidN1CuqWfV/X99wnzxNrUbIse09qqvj6qPt+aamtMEDdGQ31dknVMnjxZa8ImalhsGuwVCgXS09MxbNgwreWRkZE4e/asUfvYtWsXOnbsWOMZvFwu17oyLwiCZqgnc31Aqb+e1tzwKorodf0vo7vRlEmdsDuoC3JcvMzSHmoc+oR5Nvg/kvrCQ0O0Pz3f1k0wqwfDvCxSc3NfLW/orwsAmNyrBY5cLqzxxvvGRCJY7vVhTo3ls4PIntg02BcUFEClUmkmL1Dz8vJCXl5erY/Pzc3Fn3/+qZny2JCkpCSsW7dO83OrVq0wb948s36dN2vzKVzKrQz1PmUFaJdzCaEFmVAJElzxaKb3xswyqSMqHGRQChJc9Ay0SlcbajikEgEzh3fV2ze1Iao+iUZDIooiROE0APN2rbOllEuFcD2ag1cGR9T7NVJUrsDH289ix983IVeKkDkIGNDOHy8PCoeHs8xMLW7Yfp4WgA9/+Rsb/riqGVe+MZIIQJtm7nhn+H387CAiHQ3iU0Hf2bwxZ/h79uyBm5sb7r///hq3i4uLQ2xsrM6+s7KyDM4aZqrtJ69DJVaOYjPk4u8Q/u1PeMz/HvzjE2yWY1D9ODsIaO7liGK5CgqliOIKBcoVthvB8T/3+qIwJwuFNjq+sQRBQEBAADIzMxt0H3uJnc3XeT2vDN8duoi9ZzLx9ZgIuMq0Z+00ti91cYUSE386qzOPxYqDF/H9oYvwc5fhwTBvTO5luH98Y7u5z5Dnevjh+Z5N4e/vj8zMzH9HKLuOfen5yCuR13iDrXqsL0OvMneZgB8fvxdN3R01N/LuT89HhVKF3FJFne83cvh39lpXRwlkDhL0aeWFSb2a37WfHVKp1OZ97IkaMpsGe09PT0gkEp2r8/n5+TpX8asTRRG7d+9Gnz59ap1dTCaTaWb70ref+hJFEXJl5ce9q7xME+oPBXZAuneLeu+fTCf5d3w9EUCYrxM+GdoazTzuTNOsNdLD/qvYcjrH4HCFod6O+F9cG6w4mqkZEUL9sqnrq6eljxOmRLUw+fVny4BVdabfhiiqlSfWpWbb1dCHKrFyrolHlv0FV0cJJIIATycJCsvvjDL14L83rQLAEs2oMJVzYvQJ84RCBZ1Qr6YUgZuFcqxPy8KxK4VYOjr83xlXoRktpfps1RN7BDaaK8WGqN9Dd+4VCNa8twy9xtWPKSpX4OtDN7TrXO3GYVeZBNOigzAtOgiiKKJErjI4kpCaesjIB8O092XoptiG/F6srqF/dhDZE5t+OkulUoSFhSEtLU3rqntaWlqtwyedPn0amZmZBidLsKbKKaIr/xhKxcpLPnlO7gz1NiARgBGRfngxOhgqlcrghBRV/7C/1j8Uz/UJ0oyLXX0oN/UMv6/FhOK1mFDt4fWqDRP3QKg7AAGHLxWiQqlCcbkCctWdE4Cqw8MZe3VUPQJJ9UnM1MPVGTuKRV1HyrCFurRpUs/mOHalyODEbqYSADg6oEEMk1giV2m6j9wq0l63NjUbG1KzoYLuiea6NMMjbVWlPoF4aEla5XjzBk5c16ZmY0NaNvzcZOgT5qkZqtIeGNsf3N1Jihf7BuPFvsa9TgVBgJujA5aODq/8duBCPvLLFKhQinB0kMDLxQEPhnnVesLU0N6jRNQw2fyyS2xsLBYuXIiwsDCEh4djx44dyM7OxsCBAwEAiYmJyMnJ0Znha9euXWjbti1CQkJs0WwdfcI8sT4tGw7/znpqq3HR72YSAWjp46y5elnbLHNVqQP+a/3vBHdDf0h1r/bp/wNffQi64gqlZpbhlIy/IZVIENXKA5N6NteMNV396uhjXf0xbeMFnSt9a1OzsS41Gz6uUjg6VO5ncq8WmqCvvkpo6IQAgN7jmXKyYG7VT2CqtsmY8KgOT0sOXsOGv27X2vXBWSqgTN/MPLhzgjjtwSCtq7jq32FKeuXY4UVlCr3zUqhfi4tHtUVJhRJPJJ616Nj65tqzMZNsKUXgZpEc69JuI+nkbfzn3iaak9+7jSnvE0PfDjCwE5E52Xwce+DOBFW5ubkIDg7G+PHjce+99wKonJAqKysLs2bN0mxfUlKCSZMmISEhAQMGDKjzcc05jr16VBxcykCvq2m46eqLHaF1n7TBVSaBm6OD5irwiWvFuJxbbvBrXA9HAdGtvXH0ShHySnX7irrK7lwpFgQBn+25gl/O5NpNtwVXmQT/d6+v0SHQ2tSvD0NdIsw1I6rk34wgkwAKlf6g5iYDmrg54WpeuU5bHCSAt4sUsiqh2t1JWu+xqGsLMIYmlpIIQKiPM5aODtf8XvV1Sai+79rGLQ/wcMR34yIwee0/uJSrZ1p3H2csqXLMmp6TvomXoqqdkKgn99l6OkdrJlx70cpX+3fUkDWWeRnsAcexJ0uaOnUq8vPz8d1339m6KVbTKCaosiVzT1BVXKHE+g2/QzjyO665NsHR1t1xf4g7FCpoplcHKrtj9GvjjZOZxbiSV24wVFS9cqq+mln9a1xPZwmiW3trhQh9fUX1haqqgaRCqUJOiaJRBv1Qb0csi7+nQYeK+bsuY+NJ47pFNBRVQ3Wb0CDcuHEDgHGT01T/hqKmK/C1TSwlEYBH2vtC5iDRmsnZ08kBheVKKEVRZ98L9l7B+rRsvVftq3bXqj7Rkr5QboraTmCqnwQ4CEBemcLgNweNRdWaNnQM9tbDYN+wTJ06FT/99JPmZx8fH3Tu3BnvvPMO2rc3zyzX8+fPx7Zt27B7926D28yYMQO7du3C4cOHddbduHEDXbp0wbJly7QGPdGHwV4/BnszzzyrPHMGzqdOo7iJLxz79tVaVz0Q1TVUWOJrXFEUsWDvVWz4S38YMsRFKsDRQUC+mSYGEgD8372+mPBAAF7clK73ZjMBgK+rFC5OMvQKccfEnoENOtRnFVVg2LenGuVJkyESQfseBPXU9ItSrmL72TyUyVV6n2/1K/CiKOKzfVdrnTzIQajs913Tq0wAEOrjhK/HREAURZOvxlu7W4T6ePYyK6q/uwxJT3WwdTNqxWBvPQz2DcvUqVORlZWF//3vfwCAW7du4cMPP8Tp06dx4sQJsxzDmGD/119/oX///ti8eTN69OihtW7BggX4+uuvkZqaanDQEzUGe/1s3sfe7igquxIIekbqqR4a6jp7ozkn/ajar7lCqYREgNHBvpWPE5aOiQCAGq+4GkMAMCKyidbNeOqbzQxNW9+8efMG/8e5uEKJR384bVehHqh8jZTIVdh48ja2nL4NL2cH5JUoax1wUn2T5uQ1Z1EiF6FQqXC7uPYhZ43p+y0CuJhbjoGL0+AsFeDh5IAwX2cUViihUqHWE2dr93VWH69PmH2M5pNdLEdRuaLRj5hDZM8cHR3h7+8PAPD398fUqVPxyCOPIDs7G35+fgAqr5q/88472LNnDyQSCR544AG89957mnsaDxw4gDlz5uDs2bOQSqWIiIjA4sWLceDAAXz88ccAgGbNmgEAPv/8c8THx2u1oWPHjoiMjERiYqJOsF+9ejVGjRoFiUSCadOmISUlBbdu3UKLFi3w5JNPYtKkSQafW9euXTFp0iStWXH79euHIUOG4LXXXgNQOX/S7NmzsW3bNpSVlaFz586YM2cOOnRo+BcljMVPYDMTlZUhRV+wr4mtblasqd93TZylgtZVzx8ebae3jzRQOf6zYw03KgKAn5sML0YHa9WhphOfxnLD2dJD11FU0dgjW82UKiCnxLTbN9NzLDsDaJlCRJlCgdvFCoT6OmPJqLYNNnCaezQfW1GKwNe/32gU3XGIzEkURcBMc+KYRCqt19/CoqIirFu3Dq1atYKvry+AynsY4+Li0KNHD2zatAlSqRSffvop4uPjNUF//PjxeOyxx7B48WLI5XL88ccfEAQBQ4cOxd9//43du3dj7dq1ACqHNddn3LhxmDNnDubOnQt3d3cAwMGDB5GRkYFx48ZBpVIhMDAQX3/9NXx9fXH06FG88sor8Pf3x9ChQ+v0fEVRxLhx4+Dj44PExER4enpi5cqVGDlyJA4dOgQfH5867behaZh/6Roz9ZvboeGXdumh63UK9QDg7SLTCkpN3R2x/sn2Ol2Lev87WsvjP56psbuBg0So8QOqsQT56vZdyLd1E+5qKgCXcssadOBUj+bz7NpzOH+7cYf7lPQCvBht61YQWZlCgZLvv7f6YV0ffxyopbtKdb/99htatmwJoDLE+/v748cff9SMIrdx40ZIJBIsWLBA83f3888/R9u2bXHgwAF07twZBQUFGDRoEFq1agUACA8P1+zfzc0NDg4Omm8FDBkxYgRmzZqFLVu2YOzYsQAqR0Hs1q0bIiIqewK8/vrrmu1DQ0Nx9OhRbNq0qc7BPiUlBX///TdOnz4NJ6fKeW3UV++3bNmCJ554ok77bWgafvpsbNTBXtpw+3yr7U8vqFOolwiV3Qeqq+kKu3o4UEM3M+rbX2MniiKUDbib0N1CJTb8wOnm6IDFoyMwcsUp5JXa4MqfmShUIodwJGrAevfujfnz5wMA8vLysHz5csTHx2P79u0IDg5GamoqMjIyNKFdraysDBcvXkS/fv0QHx+PMWPGIDo6Gg8++CCGDh1aa5CvzsvLCw8//DASExMxduxYFBUVYevWrXjvvfc026xYsQI//vgjrl69itLSUsjl8np1mUlNTUVxcbHmxKH6c7MXDPbmVkMf+4ZEFCv7N9fEQajst6zv5kP1WOj69isIulff1d0NDN3MaGh/jVnVicvIthpD4HRzdMD2F6Mx8JM9Fh3z3lUmwcBwb5TJRWw/l2vWfdf2zRuRXZJKK6+e2+C4pnJ1ddW6+bJTp05o3bo1fvjhB8yYMQMqlQqdOnXCl19+qfNYdR/8zz//HBMnTsSuXbuwceNGfPDBB1i7di26detmUlseffRRjBgxAunp6Th48CAAYNiwYQCATZs24Z133sGsWbPQvXt3uLm5YdGiRfjjjz8M7k/fzNGKKl2kVCoV/P39kZSUpPNYLy8vk9rekDXs9NkIqfvY1+UNZ03GhM4mbjJEt/aqddQeYyYWqjrzormGFmwM7OXGyMauauBsyAHf39MZG57qgCUHr+nMaHzimu7wuIa4O1aOUlT9JDrUu/KGd/X77ZWYYCzafxXbz+aitMo9MAIAJ6kAdycJPJ0ccKNQjlJ57Qe2x2/eiGojCILJXWIaCkEQIJFIUFpaCgCIjIzEpk2b0LRp0xpH/unYsSM6duyIF154AUOGDMGGDRvQrVs3ODo6QlXLRUO1qKgohIaGYvXq1UhJScHQoUM1/e1///13dO/eHU899ZRm+9quqvv5+eHmzZuanwsLC3H58mXNz5GRkbh16xakUmmDmdzUEhp2+myMGlEf+9q6x0S39qp11B5DN+CuT8vGsStFWpPW1HUUoMZM/U2FvmE7yXp6hLpjwd4rdZ7V1poMvU/UJ9A/n85BidzwH04PRwl+eKwdfjh+s9aTaH0zLgN3rnxVfY/WNiynVAK7/OaNyJ5UVFRowm9+fj6++eYbFBcXY/DgwQAq+74vWrQITzzxBF5//XUEBgbi2rVr+Pnnn/Hcc89BLpfj+++/x+DBgxEQEIDz588jPT0do0ePBgAEBwfj0qVL+Ouvv9C8eXO4u7tr+rNXJwgCxo4di8WLFyMvLw8zZ87UrGvVqhXWrFmDXbt2ITQ0FGvXrsWff/5ZYyCPiorC6tWrMXjwYHh5eeHDDz/UmoE+Ojoa3bp1w/jx4/H222+jTZs2yMzMxM6dOzFkyBB07ty5vuVtEBp++mxsNF1xGlZY0MeU7jGGQrihG3BVYuVNi0sPXdd70+LdEOoB/d9UqIftfPS+ZgbH6geAJq4O8HGRorBCBZUKKCyTo6yWHhotfZzQPsBNa0I0RwnwULsmePL+ALy86UKjv0HTVB6Oksqr3bnltZ58NjT6Ronan16AErnhgO3q6ICm7o4mn0RX36b67L61dd3zcpbCVcauZ0QN2a5du9CxY0cAgLu7O9q2bYtly5ahd+/eACq76mzatAnvvvsunnzySRQVFSEgIAAPPvggPDw8UFpain/++Qc//fQTcnNz4e/vj6eeegrjx48HAMTGxuLnn3/G8OHDkZ+fr3e4y6ri4+Mxf/58tGnTBg888IBm+fjx43Hy5ElMmjQJgiAgLi4OTz75JHbu3GlwXy+88AIuXbqERx99FJ6ennj99de1rtgLgoBVq1Zh7ty5mDZtGm7fvo1mzZqhR48eNp8bwZw4QZWZJ6iq+GUb3EpLUd69GySN4Kue+s68WdtVvECPytFyzK2xTjJTPWRVr7869E/sEag16pAoiiiRqzDhp7O4lKs7VKRUAsTe2wTPRd2ZB0DfDLGab1iqncyp9+Hp7IDSChUqlCJUIhr9twyeThI82Nobv5zO0XujeEOaMdWY17Qoihj67Ulk1zD2f1M3GTY+1d7sJ8+1vdcDPByxwQLvdXNrrJ8djREnqCIyL05QZQuNpI+9Wn26xxhzFa8x3LRoTXWdpEwQBLg5OmDZmIgqJwKVXUrUQ4pWPxHTt6/a7nVwlUm0+qJnFVXgicSzFr2Z09ycpRJ4OztoJjN7/MczBkd/agwj5lRlzL0xlrqB9W4c2YqIqLFpHOmzMfm3j73gUPev9m0VhE09pi1Dhj0ypk7muE/BlJOJZh5O2PBUB/yYmo9f/7qOCqUKpXIVRFGECNQ46Zi1qbuQLa4yGZU9nnzaKmDfjSNbERE1Ngz2ZiYqlIBEYvIVe2NGlmmIeBXPdswRRI09mZj5SHtM6u4LlUqldUW/pu5BQOXoKjVFf08nBxRVKPUGxU+HhuHHP27pjA4DCDh0sQD5ZQpUKEU4Okjg5eKAB8O8dN4v9njyaauAfbeObEVE1Jgw2JubUgFRIjMp2JsyskxDw6t4d5eqAbhq96BFKVeRfPbODbvOUgkGRfjgyfsDMG3jBYOvjwXDWtc4ektN3yyol9V2td3eTj5tGbDvxpGtiIgaE948a6abZ9VX3L03rYVEpcJv9/ZF1wh/o/7QLth7BetTsxv8zX2G1PcG3LrgDXDWY0qtDd2wa8zrw1JB0dANw+qTiyUN5MS5rq9pBmzT8LPDeuz15tmMjAzNeOtE1lRUVKQzK3B1vGJvBprgcLsUY0srR424VqTERSOvuO9PL2jUN/fxKh6pGbph19g+/ZZg711I+H4jsi5BEKBSqbTGSCeytKpdYWvCYG8G6rHcJeKdkUMUEodax3IH7O/mvsbQRrIdW70+ePJJRObi7++Pa9euwcPDg+GerEKlUqGwsBAtWrSodVsGezNQX3GXqe4Ee6VQ+Wav7Yq7Pd7cR9SQ8b1ERPXh4uKCFi1a4ObNm5UjhLFLF1mQIFRmwBYtWsDFxaXW7Rns66nqFXeJKCLPyR0CAFQJD7Vdcbe3m/uIiIjsmYuLC1q2bGnrZhDpYLCvp6pX3Etlzvg5rLfONhKh5quEHFmGiIiIiOqLncPMoE+YJyQ1fLufX6bA8OWnsGDvFRRX6M7gqb65b0SkHwI9HNHUTYZAD0eMiPRrMCN2GMKvIImIiIgaBl6xNwNDV9zVyhQiMgsrahyXvjHd3NdYJ9MiIiIisme8Ym8GWlfcPR3hItNf1qqj5NSkoYf6SWvOYX1qNjILK5BdrNCctExac07vNxJEREREZHkM9maivuK+4ckOaOLmZHA79Sg5jZV6aM/qA3Qae9JCRERERJbBYG9moihCrq8/ThXqUXIaI2Mm0yIiIiIi62OwNzNBECBzqLkrTWMdl96UybSIiIiIyLoY7C1gQDt/g6PkNOZx6TmZFhEREVHDxWBvAa8MjkCoj7NOuLeHcelrGtqzMZ+0EBERETV2DPYW4O4kxddjIhrluPS1mdSzud2etBARERE1ZhzH3kIa07j0plAP7bn00HWkpBdAoRIhlQiI4jj2RERERDbFYG8F9hLq1ez1pIWIiIioMWNXHKoXhnoiIiKihoHBnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDkht3QAA2L59OzZv3oy8vDwEBQUhISEB7dq1M7i9XC7HunXrsH//fuTl5aFJkyaIi4tDTEyMFVtNRERERNRw2DzYHzx4ECtWrMCECRMQERGBHTt2YO7cuViwYAH8/Pz0PmbBggXIz8/HM888g4CAABQUFECpVFq55UREREREDYfNg/3WrVsRExOD/v37AwASEhKQmpqK5ORkjBs3Tmf7P//8E6dPn8YXX3wBd3d3AECzZs2s2mYiIiIioobGpsFeoVAgPT0dw4YN01oeGRmJs2fP6n3MsWPH0Lp1a2zatAn79u2Ds7Mzunbtivj4eDg6Oup9jFwuh1wu1/wsCAJcXFw0/zYn9f7MvV/SxjpbD2ttHayzdbDO1sNaE1mfTYN9QUEBVCoVvLy8tJZ7eXkhLy9P72Nu3ryJM2fOQCaT4dVXX0VBQQG++eYbFBUVYcqUKXofk5SUhHXr1ml+btWqFebNm4emTZua7blUFxAQYLF90x2ss/Ww1tbBOlsH62w9rDWR9di8Kw6g/2ze0Bm+KIoAgP/+979wdXUFUHlF/tNPP8WECRP0XrWPi4tDbGyszr6zsrKgUCjq3f7q7Q4ICEBmZqamrWR+rLP1sNbWwTpbB+tsPZaotVQqtehFOaLGzqbB3tPTExKJROfqfH5+vs5VfDVvb2/4+vpqQj0AtGjRAqIo4vbt2wgMDNR5jEwmg0wm07s/S32wi6LIPxpWwDpbD2ttHayzdbDO1sNaE1mPTcexl0qlCAsLQ1pamtbytLQ0RERE6H3MPffcg9zcXJSVlWmW3bhxA4IgoEmTJhZtLxERERFRQ2XzCapiY2Oxc+dO7Nq1C1evXsWKFSuQnZ2NgQMHAgASExPxxRdfaLaPioqCh4cHvvzyS1y9ehWnT5/GDz/8gH79+hm8eZaIiIiIyN7ZvI99r169UFhYiPXr1yM3NxfBwcGYMWOGpg9dbm4usrOzNds7Ozvjrbfewrfffovp06fDw8MDPXv2RHx8vK2eAhERERGRzQniXdzxLSsrS2sYTHMQBAGBgYG4ceMG+xRaEOtsPay1dbDO1sE6W48lai2TyXjzLFENbN4Vh4iIiIiI6o/BnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDjDYExERERHZAQZ7KxFF0dZNICIiIiI7JrV1A+xZcYUSSw9dx/70AihUKkglEvQJ88Skns3h5uhg6+YRERERkR1hsLeQ4golJq05h0s5ZVBVWb4+LRvHrhRh6ehwhnsiIiIiMht2xbGQJQev64R6AFCJwKXcMiw9dN0m7SIiIiIi+8RgbyEpGfk6oV5NJQIp6QVWbQ8RERER2TcGewsQRREKZc03yypUIm+oJSIiIiKzYbC3AEEQIHUQatzGQSJAEGrehoiIiIjIWAz2FhLVygsSA7ldIgB9wjyt2yAiIiIismsM9hYyuVdzhPo464R7iQC09HHGpJ7NbdMwIiIiIrJLHO7SQtwcHbB0dDiWHrqOlPQCKFQipBIBURzHnoiIiIgsgMHegtwcHfBidDBejK68oZZ96omIiIjIUtgVx0oY6omIiIjIkhjsiYiIiIjsAIM9EREREZEdYLAnIiIiIrIDDPZERERERHaAwZ6IiIiIyA4w2BMRERER2QEGeyIiIiIiO8BgT0RERERkBxjsiYiIiIjsAIM9EREREZEdkNb1gdeuXcPp06dRWFiImJgYeHt7IycnB+7u7nB0dDRnG4mIiIiIqBYmB3uVSoUlS5Zgz549mmWdO3eGt7c3li5dilatWmHMmDHmbCMREREREdXC5K44GzZsQEpKCh5//HF88sknWuu6dOmCP//801xtIyIiIiIiI5l8xX7Pnj0YMWIEYmNjoVKptNY1a9YMt27dMlvjiIiIiIjIOCZfsc/JyUF4eLjedTKZDGVlZfVuFBERERERmcbkYO/l5WXwqvz169fh6+tb70YREREREZFpTA72Xbp0wYYNG5CTk6NZJggCSkpKsG3bNnTt2tWsDSQiIiIiotqZ3Md+9OjROHHiBF588UW0b98eALBq1SpcuXIFDg4OGDlypMmN2L59OzZv3oy8vDwEBQUhISEB7dq107vtqVOnMHv2bJ3lCxYsQIsWLUw+NhERERGRPTA52Ht7e+ODDz7AmjVrcOLECUgkEly6dAn33XcfxowZA3d3d5P2d/DgQaxYsQITJkxAREQEduzYgblz52LBggXw8/Mz+LjPPvsMrq6ump89PT1NfSpERERERHajThNUeXt7Y9KkSWZpwNatWxETE4P+/fsDABISEpCamork5GSMGzfO4OO8vLzg5uZmljYQERERETV2dZ551hwUCgXS09MxbNgwreWRkZE4e/ZsjY997bXXIJfLERQUhOHDh6NDhw4WbCkRERERUcNmcrD/8ssva1wvCAKeffZZo/ZVUFAAlUoFLy8vreVeXl7Iy8vT+xgfHx9MmjQJYWFhUCgU2LdvH959913MnDkT9957r97HyOVyyOVyrTa6uLho/m1O6v2Ze7+kjXW2HtbaOlhn62CdrYe1JrI+k4P9qVOndJYVFRWhrKwMrq6udeoeo+9Nb+iDoHnz5mjevLnm5/DwcGRnZ2PLli0Gg31SUhLWrVun+blVq1aYN28emjZtanJbjRUQEGCxfdMdrLP1sNbWwTpbB+tsPaw1kfWYHOwXLVqkd/nJkyexbNkyvPTSS0bvy9PTExKJROfqfH5+vs5V/JqEh4dj//79BtfHxcUhNjZW87P6pCErKwsKhcLo4xhDEAQEBAQgMzMToiiadd90B+tsPay1dbDO1sE6W48lai2VSi16UY6osTNbH/sOHTrgoYcewvLlyzFz5kzjDi6VIiwsDGlpabj//vs1y9PS0tC9e3ejj52RkQFvb2+D62UyGWQymd51lvpgF0WRfzSsgHW2HtbaOlhn62CdrYe1JrIekyeoqklQUBDOnz9v0mNiY2Oxc+dO7Nq1C1evXsWKFSuQnZ2NgQMHAgASExPxxRdfaLb/+eefceTIEdy4cQNXrlxBYmIiDh8+jIceesicT4WIiIiIqFEx66g4p0+fNnk8+V69eqGwsBDr169Hbm4ugoODMWPGDM1Xbbm5ucjOztZsr1Ao8P333yMnJweOjo4IDg7G9OnTcd9995nzqRARERERNSomB/uqN6GqyeVyXLp0CX/++SceeeQRkxsxePBgDB48WO+65557TuvnoUOHYujQoSYfg4iIiIjInpkc7NeuXau7E6kUzZo1w+jRo+sU7ImIiIiIqH5MDvY//fSTJdpBRERERET1YNabZ4mIiIiIyDYY7ImIiIiI7IBRXXHGjBlj9A4FQcDq1avr3CAiIiIiIjKdUcF+xIgRmtlaiYiIiIio4TEq2I8ePdrS7SAiIiIionpgH3siIiIiIjtQ55lnL1++jGvXrqGiokJnXXR0dL0aRUREREREpjE52JeXl2P+/Pk4efKkwW0Y7ImIiIiIrMvkrjjr16/HrVu3MGvWLADAyy+/jLfeegsPPPAAAgMDMW/ePHO3kYiIiIiIamFysD969CiGDh2KiIgIAICfnx86duyIl156Ca1atUJycrLZG0lERERERDUzOdhnZWWhRYsWkEgqH1q1j32fPn1w9OhR87WOiIiIiIiMYnKwd3NzQ3l5OQDAy8sLN27c0KxTKBSadUREREREZD0mB/uQkBBcv34dANC+fXskJSXhzJkzOH/+PNavX4/Q0FCzN5KIiIiIiGpmcrDv168fysrKAABjx45FeXk5Zs6ciTfffBNZWVl44oknzN5IIiIiIiKqmVHDXa5YsQIxMTEICQlBr169NMubNWuG//3vfzh58iQEQUBERATc3d0t1lgiIiIiItLPqGC/bds2bNu2DWFhYYiJiUHv3r3h6uoKAHB2dka3bt0s2kgiIiIiIqqZUV1x/ve//2Ho0KHIy8vDsmXLMHnyZHzxxRc4ffq0pdtHRERERERGMOqKfUBAAMaNG4f4+HikpqZi9+7dOHToEPbv349mzZohJiYG0dHR8PX1tXR7iYiIiIhID6OCvZpEIkGXLl3QpUsXFBUVYf/+/dizZw9Wr16NNWvWIDIyEjExMXjggQcs1V4iIiIiItLDpGBflbu7O4YMGYIhQ4bg0qVL2L59O3bu3InU1FSsXr3anG0kIiIiIqJa1DnYq6Wnp2P37t34/fffAQCenp71bhQREREREZmmTsG+sLAQ+/fvx+7du3H58mVIJBJ06tQJMTEx6Nq1q7nbSEREREREtTA62IuiiBMnTmDPnj04fvw4FAoF/P39ER8fj759+8LHx8eS7SQiIiIiohoYFewTExOxb98+5ObmwtHRET179kRMTAzuvfdeS7ePiIiIiIiMYFSw37RpE8LCwjB8+HBERUVpJqciIiIiIqKGwahgP3/+fISGhlq6LUREREREVEdGzTzLUE9ERERE1LAZFeyJiIiIiKhhY7AnIiIiIrIDDPZERA2cKIq2bgIRETUC9Z55loiIzK+4Qomlh65jf3oBFCoVpBIJ+oR5YlLP5nBzdLB184iIqAGqc7AvKSnBuXPnUFhYiC5dusDd3d2c7SIiumsVVygxac05XMopg6rK8vVp2Th2pQhLR4cz3BMRkY46Bft169Zh06ZNqKioAAB88MEHcHd3x5w5cxAZGYlhw4aZs41ERHeVpYeu64R6AFCJwKXcMiw9dB0vRgfbpG1ERNRwmdzHfvv27Vi3bh369euH6dOna62777778Mcff5itcUREd6P96QU6oV5NJQIp6QVWbQ8RETUOJl+x//XXXxEbG4vHHnsMKpX2n57AwEDcuHHDbI0jIrrbiKIIhcpQrK+kUIkQRRGCIFipVURE1BiYfMX+1q1b6NSpk951Li4uKCkpqXejiIjuVoIgQCqp+aPZQSIw1BMRkQ6Tg72rqyvy8/P1rrt16xY8PT3r3SgiortZnzBPSAzkdolQuZ6IiKg6k4N9hw4dsGnTJpSVlWmWCYIApVKJ3377zeDVfCIiMs6kns0R6uOsE+4lAtDSxxmTeja3TcOIiKhBM7mP/ZgxYzBjxgy89NJLuP/++wFU9ru/ePEisrOz8eKLL5q9kUREdxM3RwcsHR2OpYeuIyW9AAqVCKlEQBTHsSciohqYHOwDAgLw7rvvYuXKldi+fTsAYN++fWjfvj2mTp0KPz8/kxuxfft2bN68GXl5eQgKCkJCQgLatWtX6+POnDmDWbNmITg4GB999JHJxyUiaqjcHB3wYnQwXowGb5QlIiKj1Gkc+6CgILz55puQy+UoLCyEu7s7HB0d69SAgwcPYsWKFZgwYQIiIiKwY8cOzJ07FwsWLKjxJKGkpASLFi1Cx44dkZeXV6djExE1Bgz1RERkDJP72B8/flwzzKVMJoOvr2+dQz0AbN26FTExMejfv7/mar2fnx+Sk5NrfNzSpUvRu3dvtG3bts7HJiIiIiKyFyZfsZ8/fz68vLzw4IMPom/fvggKCqrzwRUKBdLT03Vmqo2MjMTZs2cNPm737t24efMmpk6divXr19d6HLlcDrlcrvlZEAS4uLho/m1O6v3xCptlsc7Ww1pbB+tsHayz9bDWRNZncrCfPn069uzZg23btmHLli1o06YN+vXrh969e2vCsrEKCgqgUqng5eWltdzLy8tg95obN24gMTERs2fPhoODcTeQJSUlYd26dZqfW7VqhXnz5qFp06YmtdcUAQEBFts33cE6Ww9rbR2ss3WwztbDWhNZj8nBvkuXLujSpQuKi4uRkpKCvXv34uuvv8bKlStx//33o1+/fujQoYNJ+9R3Nq9vmUqlwueff45Ro0aheXPjh3uLi4tDbGyszr6zsrKgUChMamttBEFAQEAAMjMzIYqiWfdNd7DO1sNaWwfrbB2ss/VYotZSqdSiF+WIGrs63TwLAG5ubhg8eDAGDx6Mq1evYs+ePdi7dy8OHDiA1atXG7UPT09PSCQSnavz+fn5OlfxAaC0tBQXLlxARkYGvv32WwCVo0WIooj4+Hi89dZbek8qZDIZZDKZ3jZY6oNd3S6yLNbZelhr62CdrYN1th7Wmsh66hzs1URRxO3bt5GdnY2SkhKT3rxSqRRhYWFIS0vTjIkPAGlpaejevbvO9i4uLvj444+1liUnJ+PkyZN46aWX0KxZs7o/ESIiIiKiRqzOwT4zM1NzlT4nJwe+vr6IjY1Fv379TNpPbGwsFi5ciLCwMISHh2PHjh3Izs7GwIEDAQCJiYnIycnB888/D4lEgpCQEK3He3p6QiaT6SwnIiIiIrqbmBzsd+/ejT179uDMmTOQSqXo1q0b+vXrh8jISEgkJo+eiV69eqGwsBDr169Hbm4ugoODMWPGDE0futzcXGRnZ5u8XyIiIiKiu4kgmtjxbcyYMWjZsiX69euHqKgouLu7W6ptFpeVlaU1DKY5CIKAwMBA3Lhxg30KLYh1th7W2jpYZ+tgna3HErWWyWS8eZaoBnUaxz40NNQSbSEiIiIiojoyue8MQz0RERERUcNj1BX7devWISYmBr6+vloTPRkycuTIejeMiIiIiIiMZ1SwX7t2LTp37gxfX1+sXbu21u0Z7ImIiIiIrMuoYP/TTz/p/TcRERERETUMpo9PSUREREREDY7JwX7MmDE4f/683nXp6ekYM2ZMvRtFRERERESmMesVe5VKBUEQzLlLIiIiIiIyglmDfXp6OlxdXc25SyIiIiIiMoJRN8/+8ssv+OWXXzQ/f/TRR5DJZFrbVFRUID8/Hz169DBvC4mIiIiIqFZGBXtPT08EBQUBALKysuDv769zZV4mkyEkJAQPP/yw+VtJREREREQ1MirYR0VFISoqCgAwe/ZsTJgwAS1atLBow4iIiIiIyHhGBfuqZs6caYl2EBERERFRPZh88+zu3buxZs0avevWrFmDvXv31rtRRERERERkGpOD/bZt2+Du7q53naenJ7Zt21bvRhERERERkWlMDvaZmZkIDg7Wuy4oKAg3btyod6OIiIiIiMg0dRrHvqSkxOBylUpVrwYREREREZHpTA72ISEhOHDggN51KSkpCAkJqXejiIiIiIjINCYH+4ceegiHDx/GF198gX/++Qc5OTn4559/sGjRIhw+fBgPPfSQJdpJREREREQ1MHm4y6ioKFy7dg0bN27E/v37NcslEglGjBiBPn36mLWBRERERERUO5ODPQCMGTMG/fr1Q1paGgoKCuDp6YlOnTqhadOm5m4fEREREREZoU7BHgCaNWuGAQMGmLMtRERERERUR3UK9nK5HHv27MGpU6dQVFSEp59+GoGBgTh69ChCQkLg7+9v7nYSEREREVENTA72BQUFmD17Nq5evQpvb2/k5eWhtLQUAHD06FGkpqZiwoQJZm8oEREREREZZvKoOD/88ANKSkrwwQcf4Msvv9Ra1759e5w+fdpsjSMiIiIiIuOYHOz/+OMPjB49GmFhYRAEQWtdkyZNcPv2bbM1joiIiIiIjGNysC8tLTU4+o1CoeDMs0RERERENmBysG/WrBnOnTund9358+fRvHnzejeKiIiIiIhMY3Kwj4qKwqZNm3D06FGIoggAEAQB58+fx7Zt2zhBFRERERGRDZg8Ks7QoUNx9uxZfPzxx3BzcwMAvP/++ygsLETnzp3x8MMPm72RRERERERUM5ODvVQqxYwZM3Dw4EH88ccfyM/Ph4eHB7p27YpevXpBIjH5SwAiIiIiIqqnOk1QJQgCevfujd69e5u7PUREREREVAe8vE5EREREZAeMumI/e/ZsTJgwAS1atMDs2bNr3FYQBLi7uyMiIgKDBg2CTCYzS0OJiIiIiMgwk7viiKKoMzFV9fU3b97E0aNHceXKFTzzzDP1aiAREREREdXOqGA/c+ZMzb9nzZpl1I537dqFxMTEOjWKiIiIiIhMY7E+9u3atcN9991nqd0TEREREVEVdRoVR6VS4eDBgzh16hQKCwvh4eGB9u3bo2fPnnBwcAAABAYGYsqUKWZtLBERERER6WdysC8oKMDcuXORkZEBiUQCDw8PFBYWYteuXdiyZQvefPNNeHp6WqKtRERERERkgMnBfuXKlbh+/TqmTp2qmZBKfQX/66+/xsqVKzF16lRLtJWIiIiIiAwwOdgfP34c8fHxiIqK0iyTSCSIiopCfn4+1q5da9YGEhERERFR7eo03GVQUJDedcHBwRBF0eRGbN++HZs3b0ZeXh6CgoKQkJCAdu3a6d32zJkz+PHHH3Ht2jWUl5ejadOmGDBgAGJjY00+LhERERGRvTA52Hfs2BF//fUXIiMjddalpaWhffv2Ju3v4MGDWLFiBSZMmICIiAjs2LEDc+fOxYIFC+Dn56ezvZOTEwYPHozQ0FA4OTnhzJkz+Prrr+Hs7IwBAwaY+nSIiIiIiOyCUcG+qKhI8++RI0fi448/hkqlQlRUFLy9vZGXl4f9+/fjyJEjeOWVV0xqwNatWxETE4P+/fsDABISEpCamork5GSMGzdOZ/tWrVqhVatWmp+bNWuGI0eO4O+//2awJyIiIqK7llHB/umnn9ZZtnXrVmzdulVn+euvv46ffvrJqIMrFAqkp6dj2LBhWssjIyNx9uxZo/aRkZGBs2fPIj4+3uA2crkccrlc87MgCHBxcdH825zU+zP3fkkb62w9rLV1sM7WwTpbD2tNZH1GBfsRI0ZY5I1ZUFAAlUoFLy8vreVeXl7Iy8ur8bHPPPMMCgoKoFQqMWrUKM0Vf32SkpKwbt06zc+tWrXCvHnz0LRp03q1vyYBAQEW2zfdwTpbD2ttHayzdbDO1sNaE1mPUcF+9OjRFm2EvpOG2k4k5syZg7KyMpw7dw6JiYkICAjQGqmnqri4OK2ba9X7zsrKgkKhqEfLdQmCgICAAGRmZtbpRmIyDutsPay1dbDO1sE6W48lai2VSi16UY6osavTzLOiKKKwsBCCIMDd3b3OV/M9PT0hkUh0rs7n5+frXMWvrlmzZgCAkJAQzTCbhoK9TCaDTCbTu85SH+yiKPKPhhWwztbDWlsH62wdrLP1sNZE1mNSsD937hw2btyIkydPory8HEDlKDUdOnRAXFwc2rZta9rBpVKEhYUhLS0N999/v2Z5WloaunfvbvR+RFE0+5V3IiIiIqLGxOhgv337dqxYsQIAEBYWpvkqLCsrCydOnMCJEyeQkJCAwYMHm9SA2NhYLFy4EGFhYQgPD8eOHTuQnZ2NgQMHAgASExORk5OD559/HgDw66+/ws/PDy1atABQOa79li1bMGTIEJOOS0RERERkT4wK9ufOncPy5cvRpUsXTJgwAU2aNNFaf/v2bXz99ddYsWIFWrdujTZt2hjdgF69eqGwsBDr169Hbm4ugoODMWPGDM2JQ25uLrKzszXbi6KIVatW4datW5BIJAgICMCjjz7KoS6JiIiI6K4miEZ0fPv000+Rm5uL2bNnQyKR6N1GpVJh5syZ8PHxwUsvvWT2hlpCVlaW1jCY5iAIAgIDA3Hjxg32KbQg1tl6WGvrYJ2tg3W2HkvUWiaT8eZZohroT+nVnDlzBoMHDzYY6gFAIpFg0KBBOHPmjNkaR0RERERExjEq2BcVFcHPz6/W7Zo2bao1Sy0REREREVmHUcHew8MDWVlZtW6XnZ0NDw+PejeKiIiIiIhMY1Swj4iIQHJyMlQqlcFtVCoVfv31V9xzzz1maxwRERERERnHqGAfGxuLf/75Bx9//DFyc3N11ufk5ODjjz/GhQsX8J///MfsjSQiIiIiopoZNdxleHg4xo8fj5UrV2LKlClo3bq1ZubXW7du4cKFCxBFEQkJCSYNdUlEREREROZh9ARVQ4YMQatWrbBx40acOnUK//zzDwDA0dERnTp1QlxcHCIiIizWUCIiIiIiMszoYA8A99xzD6ZPnw6VSoXCwkIAlTfW1jQMJhERERERWZ5JwV5NIpHAy8vL3G0hIiIiIqI64qV2IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDB3oJEUbR1E4iIiIjoLiG1dQPsTXGFErM2n8L2k9chV6oglUjQJ8wTk3o2h5ujg62bR0RERER2isHejIorlJi05hwu5ZZBVeVi/fq0bBy7UoSlo8MZ7omIiIjIItgVx4yWHrqOSznaoR4AVCJwKbcMSw9dt03DiIiIiMjuMdib0f70AqgMrFOJQEp6gVXbQ0RERER3DwZ7MxFFEQqVoVhfSaESeUMtEREREVkEg72ZCIIAqaTmcjpIBAiCYKUWEREREdHdhMHejPqEeUJiILdLhMr1RERERESWwGBvRpN6Nkeoj7NOuJcIQEsfZ0zq2dw2DSMiIiIiu8fhLs3IzdEBX4+JwI+p+fj15HUolCKkEgFRHMeeiIiIiCyMwd7M3BwdMPOR9pjU3RcqlYp96omIiIjIKtgVx4IY6omIiIjIWhjsiYiIiIjsAIM9EREREZEdYLAnIiIiIrIDDPZERERERHaAwZ6IiIiIyA40iOEut2/fjs2bNyMvLw9BQUFISEhAu3bt9G57+PBhJCcn4+LFi1AoFAgKCsKoUaPQuXNn6zaaiIiIiKgBsfkV+4MHD2LFihUYPnw45s2bh3bt2mHu3LnIzs7Wu/3ff/+NyMhIzJgxAx9++CHat2+PefPmISMjw8otJyIiIiJqOGwe7Ldu3YqYmBj0799fc7Xez88PycnJerdPSEjA0KFD0aZNGwQGBmLcuHEIDAzE8ePHrdxyIiIiIqKGw6bBXqFQID09HZ06ddJaHhkZibNnzxq1D5VKhdLSUri7u1uiiUREREREjYJN+9gXFBRApVLBy8tLa7mXlxfy8vKM2sfWrVtRXl6Onj17GtxGLpdDLpdrfhYEAS4uLpp/m5N6f5x11rJYZ+thra2DdbYO1tl6WGsi62sQN8/qe9Mb80GQkpKCtWvX4tVXX9U5OagqKSkJ69at0/zcqlUrzJs3D02bNq1bg40QEBBgsX3THayz9bDW1sE6WwfrbD2sNZH12DTYe3p6QiKR6Fydz8/PrzGoA5U33S5evBgvvfQSIiMja9w2Li4OsbGxmp/VJw1ZWVlQKBR1a7wBgiAgICAAmZmZEEXRrPumO1hn62GtrYN1tg7W2XosUWupVGrRi3JEjZ1Ng71UKkVYWBjS0tJw//33a5anpaWhe/fuBh+XkpKCr776Ci+88ALuu+++Wo8jk8kgk8n0rrPUB7soivyjYQWss/Ww1tbBOlsH62w9rDWR9dh8VJzY2Fjs3LkTu3btwtWrV7FixQpkZ2dj4MCBAIDExER88cUXmu1TUlKwaNEiPPHEEwgPD0deXh7y8vJQUlJiq6dARERERGRzNu9j36tXLxQWFmL9+vXIzc1FcHAwZsyYofmqLTc3V2tM+x07dkCpVOKbb77BN998o1keHR2N5557zurtJyIiIiJqCATxLv5+LCsrS2u0HHMQBAGBgYG4ceMGv3q0INbZelhr62CdrYN1th5L1Fomk7GPPVENbN4Vh4iIiIiI6o/BnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdkBq6wYAwPbt27F582bk5eUhKCgICQkJaNeund5tc3Nz8d133yE9PR2ZmZkYMmQIEhISrNtgIiIiIqIGxuZX7A8ePIgVK1Zg+PDhmDdvHtq1a4e5c+ciOztb7/ZyuRyenp4YPnw4QkNDrdxaIiIiIqKGyebBfuvWrYiJiUH//v01V+v9/PyQnJysd/tmzZrhySefRHR0NFxdXa3cWiIiIiKihsmmXXEUCgXS09MxbNgwreWRkZE4e/as2Y4jl8shl8s1PwuCABcXF82/zUm9P3Pvl7SxztbDWlsH62wdrLP1sNZE1mfTYF9QUACVSgUvLy+t5V5eXsjLyzPbcZKSkrBu3TrNz61atcK8efPQtGlTsx2juoCAAIvtm+5gna2HtbYO1tk6WGfrYa2JrKdB3Dyr72zenGf4cXFxiI2N1dl3VlYWFAqF2Y6j3ndAQAAyMzMhiqJZ9013sM7Ww1pbB+tsHayz9Vii1lKp1KIX5YgaO5sGe09PT0gkEp2r8/n5+TpX8etDJpNBJpPpXWepD3ZRFPlHwwpYZ+thra2DdbYO1tl6WGsi67HpzbNSqRRhYWFIS0vTWp6WloaIiAgbtYqIiIiIqPGxeVec2NhYLFy4EGFhYQgPD8eOHTuQnZ2NgQMHAgASExORk5OD559/XvOYixcvAgDKyspQUFCAixcvQiqVIigoyBZPgYiIiIjI5mwe7Hv16oXCwkKsX78eubm5CA4OxowZMzR96HJzc3XGtH/ttdc0/05PT0dKSgqaNm2KRYsWWbXtREREREQNhc2DPQAMHjwYgwcP1rvuueee01m2Zs0aSzeJiIiIiKhRsfkEVUREREREVH8M9kREREREdoDBnoiogeCQgEREVB8Noo89EdHdqrhCiaWHriMlowAqnIYEKkS18sSkns3h5uhg6+YREVEjwmBPRGQjxRVKTFpzDpdyyqCqsnx9WjaOXSnC0tHhDPdERGQ0dsUhIrKRpYeu64R6AFCJwKXcMiw9dN0m7SIiosaJwZ6IyEb2pxfohHo1lQikpBdYtT1ERNS4MdgTEdmAKIpQqAzF+koKlcgbaomIyGgM9kRENiAIAqSSmj+CHSQCBEGwUouIiKixY7AnIrKRPmGekBjI7RKhcj0REZGxGOyJiGxkUs/mCPVx1gn3EgFo6eOMST2b26ZhRETUKHG4SyIiG3FzdMDS0eGacexFSCBwHHsiIqojBnsiIhtyc3TAi9HBeKmvgICAAGRmZvKGWSIiqhN2xSEiaiB4oywREdUHgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR2Q2roBtiSVWu7pW3LfdAfrbD2stXWwztbBOluPOWvN3xtRzQRRFEVbN4KIiIiIiOqHXXHMrLS0FK+//jpKS0tt3RS7xjpbD2ttHayzdbDO1sNaE1kfg72ZiaKIjIwM8IsQy2KdrYe1tg7W2TpYZ+thrYmsj8GeiIiIiMgOMNgTEREREdkBBnszk8lkGDlyJGQyma2bYtdYZ+thra2DdbYO1tl6WGsi6+OoOEREREREdoBX7ImIiIiI7ACDPRERERGRHWCwJyIiIiKyAwz2RERERER2QGrrBtiT7du3Y/PmzcjLy0NQUBASEhLQrl07WzerUTl9+jQ2b96MjIwM5Obm4pVXXsH999+vWS+KItauXYudO3eiqKgIbdu2xdNPP43g4GDNNnK5HN9//z0OHDiAiooKdOjQARMmTECTJk1s8ZQanKSkJBw5cgTXrl2Do6MjwsPD8dhjj6F58+aabVhn80hOTkZycjKysrIAAEFBQRg5ciS6dOkCgHW2lKSkJKxatQoPP/wwEhISALDW5rJmzRqsW7dOa5mXlxe+/vprAKwzka3xir2ZHDx4ECtWrMDw4cMxb948tGvXDnPnzkV2dratm9aolJeXo2XLlnjqqaf0rt+0aRN+/vlnPPXUU/jggw/g7e2N9957T2vK8hUrVuDIkSN44YUXMGfOHJSVleHDDz+ESqWy1tNo0E6fPo3Bgwfj/fffx1tvvQWVSoX33nsPZWVlmm1YZ/Pw9fXFuHHj8MEHH+CDDz5Ahw4dMH/+fFy5cgUA62wJ58+fx44dOxAaGqq1nLU2n+DgYCxdulTz3yeffKJZxzoT2ZhIZjFjxgxx6dKlWsumTZsm/vjjjzZqUeM3atQo8fDhw5qfVSqVOHHiRDEpKUmzrKKiQhw/fryYnJwsiqIoFhcXi/Hx8eKBAwc029y+fVscPXq0eOLECWs1vVHJz88XR40aJZ46dUoURdbZ0hISEsSdO3eyzhZQWloq/ve//xVTU1PFmTNnisuXLxdFka9pc/rpp5/EV155Re861pnI9njF3gwUCgXS09PRqVMnreWRkZE4e/asjVplf27duoW8vDytOstkMtx7772aOqenp0OpVCIyMlKzja+vL0JCQnDu3Dmrt7kxKCkpAQC4u7sDYJ0tRaVS4cCBAygvL0d4eDjrbAHLli1Dly5dtOoF8DVtbpmZmZg8eTKee+45fPbZZ7h58yYA1pmoIWAfezMoKCiASqWCl5eX1nIvLy/k5eXZplF2SF1LfXVWd3nKy8uDVCrVhNSq2/B3oUsURaxcuRL33HMPQkJCALDO5nb58mW8+eabkMvlcHZ2xiuvvIKgoCBN0GGdzePAgQPIyMjABx98oLOOr2nzadu2LZ577jk0b94ceXl52LBhA9566y18+umnrDNRA8Bgb0aCIBi1jOqnek1FIyZPNmabu9E333yDy5cvY86cOTrrWGfzaN68OT766CMUFxfj8OHDWLRoEWbPnq1ZzzrXX3Z2NlasWIE333wTjo6OBrdjretPfeM3AISEhCA8PBxTp07F3r170bZtWwCsM5EtsSuOGXh6ekIikehcbcjPz9e5ckF15+3tDQA6dS4oKNDU2dvbGwqFAkVFRTrbqB9Plb799lscP34cM2fO1BqNgnU2L6lUioCAALRu3Rrjxo1Dy5Yt8csvv7DOZpSeno78/HxMnz4d8fHxiI+Px+nTp7Ft2zbEx8dr6slam5+zszNCQkJw48YNvqaJGgAGezOQSqUICwtDWlqa1vK0tDRERETYqFX2p1mzZvD29taqs0KhwOnTpzV1DgsLg4ODg9Y2ubm5uHz5MsLDw63e5oZIFEV88803OHz4MN555x00a9ZMaz3rbFmiKEIul7POZtSxY0d8/PHHmD9/vua/1q1bIyoqCvPnz4e/vz9rbSFyuRzXrl2Dj48PX9NEDQC74phJbGwsFi5ciLCwMISHh2PHjh3Izs7GwIEDbd20RqWsrAyZmZman2/duoWLFy/C3d0dfn5+ePjhh5GUlITAwEAEBAQgKSkJTk5OiIqKAgC4uroiJiYG33//PTw8PODu7o7vv/8eISEhOjfU3a2++eYbpKSk4LXXXoOLi4vm6pqrqyscHR0hCALrbCaJiYno0qULmjRpgrKyMhw4cACnTp3Cm2++yTqbkYuLi+YeETUnJyd4eHholrPW5vHdd9+hW7du8PPzQ35+PtavX4/S0lJER0fzNU3UAAgiO7aZjXqCqtzcXAQHB2P8+PG49957bd2sRuXUqVNa/Y/VoqOj8dxzz2kmP9mxYweKi4vRpk0bPP3001p/1CsqKvDDDz8gJSVFa/ITPz8/az6VBmv06NF6l0+ZMgV9+/YFANbZTL766iucPHkSubm5cHV1RWhoKIYOHaoJMKyz5cyaNQstW7bUmaCKta6fzz77DH///TcKCgrg6emJtm3bIj4+HkFBQQBYZyJbY7AnIiIiIrID7GNPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDjDYExERERHZAc48S0QNiqEJtKqbOXMm2rdvr7N81qxZWv83RX0eS0REZGsM9kTUoLz33ntaP69fvx6nTp3CO++8o7VcPdNldRMmTLBY24iIiBoyBnsialDCw8O1fvb09IQgCDrLqysvL4eTk5PBwE9ERGTvGOyJqNGZNWsWCgsL8fTTTyMxMREXL15Et27dMG3aNL3dadauXYsTJ07gxo0bUKlUCAgIwODBg9GvXz8IgmCbJ0FERGRmDPZE1Cjl5uZi4cKFGDp0KMaOHVtjQM/KysKAAQPg5+cHAPjnn3/w7bffIicnByNHjrRWk4mIiCyKwZ6IGqWioiK89NJL6NChQ63bTpkyRfNvlUqF9u3bQxRFbNu2DSNGjOBVeyIisgsM9kTUKLm5uRkV6gHg5MmTSEpKwvnz51FaWqq1Lj8/H97e3hZoIRERkXUx2BNRo+Tj42PUdufPn8d7772H9u3bY/LkyWjSpAmkUimOHj2KDRs2oKKiwsItJSIisg4GeyJqlIztPnPgwAE4ODjg9ddfh6Ojo2b50aNHLdU0IiIim+DMs0Rk1wRBgIODAySSOx93FRUV2Ldvnw1bRUREZH68Yk9Edu2+++7D1q1b8fnnn2PAgAEoLCzEli1bIJPJbN00IiIis+IVeyKyax06dMCzzz6Ly5cvY968eVi9ejV69OiBoUOH2rppREREZiWIoijauhFERERERFQ/vGJPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgMM9kREREREdoDBnoiIiIjIDjDYExERERHZAQZ7IiIiIiI7wGBPRERERGQHGOyJiIiIiOwAgz0RERERkR1gsCciIiIisgP/D53EHr9yDyuvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.705004</td>\n",
       "      <td>0.073735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>2.406011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>98.400000</td>\n",
       "      <td>1.429841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1.100505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>2.884826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.879032</td>\n",
       "      <td>0.023728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.053389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.578908</td>\n",
       "      <td>0.080263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.979090</td>\n",
       "      <td>0.010965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.702764</td>\n",
       "      <td>0.065740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.868691</td>\n",
       "      <td>0.027890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.813393</td>\n",
       "      <td>0.039966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.041031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.657901</td>\n",
       "      <td>0.070666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.875100</td>\n",
       "      <td>0.023239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.778998</td>\n",
       "      <td>0.041031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.705004     0.073735\n",
       "1                    TP        19.300000     2.406011\n",
       "2                    TN        98.400000     1.429841\n",
       "3                    FP         2.100000     1.100505\n",
       "4                    FN        14.100000     2.884826\n",
       "5              Accuracy         0.879032     0.023728\n",
       "6             Precision         0.902500     0.053389\n",
       "7           Sensitivity         0.578908     0.080263\n",
       "8           Specificity         0.979090     0.010965\n",
       "9              F1 score         0.702764     0.065740\n",
       "10  F1 score (weighted)         0.868691     0.027890\n",
       "11     F1 score (macro)         0.813393     0.039966\n",
       "12    Balanced Accuracy         0.778998     0.041031\n",
       "13                  MCC         0.657901     0.070666\n",
       "14                  NPV         0.875100     0.023239\n",
       "15              ROC_AUC         0.778998     0.041031"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.689126</td>\n",
       "      <td>0.712882</td>\n",
       "      <td>0.708425</td>\n",
       "      <td>0.727859</td>\n",
       "      <td>0.704049</td>\n",
       "      <td>0.658896</td>\n",
       "      <td>0.728272</td>\n",
       "      <td>0.613857</td>\n",
       "      <td>0.660581</td>\n",
       "      <td>0.672381</td>\n",
       "      <td>0.687633</td>\n",
       "      <td>0.036277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>37.100000</td>\n",
       "      <td>3.281260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>197.300000</td>\n",
       "      <td>2.162817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>1.619328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>30.400000</td>\n",
       "      <td>2.716207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.874627</td>\n",
       "      <td>0.007707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.923051</td>\n",
       "      <td>0.032246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.549304</td>\n",
       "      <td>0.043774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.984010</td>\n",
       "      <td>0.008091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.646465</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.710280</td>\n",
       "      <td>0.672897</td>\n",
       "      <td>0.691589</td>\n",
       "      <td>0.640777</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>0.660194</td>\n",
       "      <td>0.687178</td>\n",
       "      <td>0.030740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.852568</td>\n",
       "      <td>0.868694</td>\n",
       "      <td>0.863132</td>\n",
       "      <td>0.878499</td>\n",
       "      <td>0.874186</td>\n",
       "      <td>0.856119</td>\n",
       "      <td>0.864341</td>\n",
       "      <td>0.846106</td>\n",
       "      <td>0.867682</td>\n",
       "      <td>0.854425</td>\n",
       "      <td>0.862575</td>\n",
       "      <td>0.010191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.783187</td>\n",
       "      <td>0.817329</td>\n",
       "      <td>0.808120</td>\n",
       "      <td>0.830694</td>\n",
       "      <td>0.819010</td>\n",
       "      <td>0.795656</td>\n",
       "      <td>0.807333</td>\n",
       "      <td>0.777663</td>\n",
       "      <td>0.814975</td>\n",
       "      <td>0.789681</td>\n",
       "      <td>0.804365</td>\n",
       "      <td>0.017205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.739949</td>\n",
       "      <td>0.786471</td>\n",
       "      <td>0.774265</td>\n",
       "      <td>0.792076</td>\n",
       "      <td>0.780453</td>\n",
       "      <td>0.757206</td>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.781618</td>\n",
       "      <td>0.748756</td>\n",
       "      <td>0.766666</td>\n",
       "      <td>0.019464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.629225</td>\n",
       "      <td>0.655593</td>\n",
       "      <td>0.644346</td>\n",
       "      <td>0.695908</td>\n",
       "      <td>0.671325</td>\n",
       "      <td>0.634790</td>\n",
       "      <td>0.659108</td>\n",
       "      <td>0.606478</td>\n",
       "      <td>0.655693</td>\n",
       "      <td>0.631748</td>\n",
       "      <td>0.648421</td>\n",
       "      <td>0.024958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.855300</td>\n",
       "      <td>0.877800</td>\n",
       "      <td>0.870500</td>\n",
       "      <td>0.875600</td>\n",
       "      <td>0.876700</td>\n",
       "      <td>0.860300</td>\n",
       "      <td>0.864600</td>\n",
       "      <td>0.853400</td>\n",
       "      <td>0.874400</td>\n",
       "      <td>0.857800</td>\n",
       "      <td>0.866640</td>\n",
       "      <td>0.009472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.739949</td>\n",
       "      <td>0.786471</td>\n",
       "      <td>0.774265</td>\n",
       "      <td>0.792076</td>\n",
       "      <td>0.780453</td>\n",
       "      <td>0.757206</td>\n",
       "      <td>0.767059</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.781618</td>\n",
       "      <td>0.748756</td>\n",
       "      <td>0.766666</td>\n",
       "      <td>0.019464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.689126    0.712882    0.708425    0.727859   \n",
       "1                    TP   32.000000   41.000000   39.000000   41.000000   \n",
       "2                    TN  201.000000  194.000000  195.000000  197.000000   \n",
       "3                    FP    1.000000    6.000000    5.000000    2.000000   \n",
       "4                    FN   34.000000   27.000000   29.000000   28.000000   \n",
       "5              Accuracy    0.869403    0.876866    0.873134    0.888060   \n",
       "6             Precision    0.969697    0.872340    0.886364    0.953488   \n",
       "7           Sensitivity    0.484848    0.602941    0.573529    0.594203   \n",
       "8           Specificity    0.995000    0.970000    0.975000    0.989900   \n",
       "9              F1 score    0.646465    0.713043    0.696429    0.732143   \n",
       "10  F1 score (weighted)    0.852568    0.868694    0.863132    0.878499   \n",
       "11     F1 score (macro)    0.783187    0.817329    0.808120    0.830694   \n",
       "12    Balanced Accuracy    0.739949    0.786471    0.774265    0.792076   \n",
       "13                  MCC    0.629225    0.655593    0.644346    0.695908   \n",
       "14                  NPV    0.855300    0.877800    0.870500    0.875600   \n",
       "15              ROC_AUC    0.739949    0.786471    0.774265    0.792076   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.704049    0.658896    0.728272    0.613857    0.660581    0.672381   \n",
       "1    38.000000   36.000000   37.000000   33.000000   40.000000   34.000000   \n",
       "2   199.000000  197.000000  198.000000  198.000000  195.000000  199.000000   \n",
       "3     3.000000    3.000000    2.000000    3.000000    5.000000    2.000000   \n",
       "4    28.000000   32.000000   31.000000   34.000000   28.000000   33.000000   \n",
       "5     0.884328    0.869403    0.876866    0.861940    0.876866    0.869403   \n",
       "6     0.926829    0.923077    0.948718    0.916667    0.888889    0.944444   \n",
       "7     0.575758    0.529412    0.544118    0.492537    0.588235    0.507463   \n",
       "8     0.985100    0.985000    0.990000    0.985100    0.975000    0.990000   \n",
       "9     0.710280    0.672897    0.691589    0.640777    0.707965    0.660194   \n",
       "10    0.874186    0.856119    0.864341    0.846106    0.867682    0.854425   \n",
       "11    0.819010    0.795656    0.807333    0.777663    0.814975    0.789681   \n",
       "12    0.780453    0.757206    0.767059    0.738806    0.781618    0.748756   \n",
       "13    0.671325    0.634790    0.659108    0.606478    0.655693    0.631748   \n",
       "14    0.876700    0.860300    0.864600    0.853400    0.874400    0.857800   \n",
       "15    0.780453    0.757206    0.767059    0.738806    0.781618    0.748756   \n",
       "\n",
       "           ave       std  \n",
       "0     0.687633  0.036277  \n",
       "1    37.100000  3.281260  \n",
       "2   197.300000  2.162817  \n",
       "3     3.200000  1.619328  \n",
       "4    30.400000  2.716207  \n",
       "5     0.874627  0.007707  \n",
       "6     0.923051  0.032246  \n",
       "7     0.549304  0.043774  \n",
       "8     0.984010  0.008091  \n",
       "9     0.687178  0.030740  \n",
       "10    0.862575  0.010191  \n",
       "11    0.804365  0.017205  \n",
       "12    0.766666  0.019464  \n",
       "13    0.648421  0.024958  \n",
       "14    0.866640  0.009472  \n",
       "15    0.766666  0.019464  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.721406</td>\n",
       "      <td>0.053239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.887685</td>\n",
       "      <td>0.025934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.908828</td>\n",
       "      <td>0.056694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.612529</td>\n",
       "      <td>0.095113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.979108</td>\n",
       "      <td>0.013800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.727542</td>\n",
       "      <td>0.074490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.878899</td>\n",
       "      <td>0.030117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.828348</td>\n",
       "      <td>0.044823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.795817</td>\n",
       "      <td>0.048059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.683695</td>\n",
       "      <td>0.077660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.884454</td>\n",
       "      <td>0.025486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.795817</td>\n",
       "      <td>0.048059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.721406     0.053239\n",
       "1              Accuracy         0.887685     0.025934\n",
       "2             Precision         0.908828     0.056694\n",
       "3           Sensitivity         0.612529     0.095113\n",
       "4           Specificity         0.979108     0.013800\n",
       "5              F1 score         0.727542     0.074490\n",
       "6   F1 score (weighted)         0.878899     0.030117\n",
       "7      F1 score (macro)         0.828348     0.044823\n",
       "8     Balanced Accuracy         0.795817     0.048059\n",
       "9                   MCC         0.683695     0.077660\n",
       "10                  NPV         0.884454     0.025486\n",
       "11              ROC_AUC         0.795817     0.048059"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBRegressor(objective=\"reg:squarederror\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=4,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"rmse\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_xgb_cat = np.where(((y_pred_optimized_xgb >= 2) | (y_pred_optimized_xgb <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_xgb_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_xgb))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_xgb_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_xgb_cat))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "xgb_run0 = data_xgb[['y_test_idx0', 'y_test0', 'y_pred_xgb0']]\n",
    "xgb_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "xgb_run0.reset_index(inplace=True, drop=True)\n",
    "xgb_run1 = data_xgb[['y_test_idx1', 'y_test1', 'y_pred_xgb1']]\n",
    "xgb_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "xgb_run1.reset_index(inplace=True, drop=True)\n",
    "xgb_run2 = data_xgb[['y_test_idx2', 'y_test2', 'y_pred_xgb2']]\n",
    "xgb_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "xgb_run2.reset_index(inplace=True, drop=True)\n",
    "xgb_run3 = data_xgb[['y_test_idx3', 'y_test3', 'y_pred_xgb3']]\n",
    "xgb_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "xgb_run3.reset_index(inplace=True, drop=True)\n",
    "xgb_run4 = data_xgb[['y_test_idx4', 'y_test4', 'y_pred_xgb4']]\n",
    "xgb_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "xgb_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "xgb_5preds = pd.concat([chembl_id, xgb_run0, xgb_run1, xgb_run2, xgb_run3, xgb_run4], axis=1)\n",
    "xgb_5preds = xgb_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_xgb0', 'y_pred_xgb1', 'y_pred_xgb2', 'y_pred_xgb3', 'y_pred_xgb4']]\n",
    "xgb_5preds['y_pred_xgb_ave'] = xgb_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "xgb_5preds['y_pred_xgb_std'] = xgb_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "\n",
    "mat_met_optimized_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "02aaad2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(xgb_5preds['y_test0'], xgb_5preds['y_pred_xgb_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b19aca7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcsAAAHECAYAAABWVAGSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABK3klEQVR4nO3dd3xUVd4/8M+ZzKT3RgIhkECAAIJBQMpiACmCrAg/UBfZR8Coq0HxUdSHVdfuGisqWBZZERQFwUjEQqQ3QUUUlSKEhB5SSK9Tzu+PmxkZkpA2yZ07+bxfr7zI3Ln3zvdkNJ+cc889I6SUEkRERFQvndoFEBEROTuGJRERUQMYlkRERA1gWBIRETWAYUlERNQAhiUREVEDGJZEREQNYFgSERE1gGFJRETUAIYlERFRAxiW5LLee+89CCEwYcKEeve5/vrrIYTAf/7znzqf//bbbzFz5kx0794dvr6+8PDwQMeOHTF+/Hi8+uqryM3NrXVM165dIYSw+zIYDOjUqROmTZuGPXv2OKyNrWHZsmUQQmDZsmVNPvbSdru5uSEkJASjRo3CihUrUNfqmllZWbb9/f39UVZWVue5KysrERwcbNv32LFjtfb59NNPcd111yE8PBwGgwEhISHo3bs3Zs6ciQ8++KDe173cV2FhYZN/DuR69GoXQNRakpKS8MUXXyAtLQ2LFy9GcnKy3fNvv/02vvrqK1x//fW488477Z4rKirCzJkzsX79enh4eCAxMRE33HADPD09kZOTg927d+PBBx/E448/jkOHDiE6OrrW68+bNw+BgYEAgNLSUhw4cACfffYZ1q1bh7S0tMuGuNY98cQTAACj0Yhjx44hNTUVW7duxY8//ojXX3+9zmP0ej1KSkrw6aefYtasWbWeX7t2LQoKCqDX62EymWo9f+edd2LJkiXw8vLC9ddfj5iYGJSVlSEjI8P2+rfddlut4wICAnD//ffX2xZPT8/GNZpcmyRyYefPn5dhYWHS29tbHj582Lb9yJEj0tvbW4aGhsrs7Gy7Y0wmkxw9erQEIMeOHSvPnDlT57l/+OEHOWbMGHno0CG77V26dJEAZGZmZq1jXn75ZQlAJiYmtrhtreX999+XAOT777/f5GMByLp+rezcuVPqdDophKj1c8nMzJQA5NVXXy07dOgghw8fXue5R44cKcPCwuSwYcMkAHn06FHbczt27JAAZFRUlDx16lStY0tLS+X69evrfN0uXbo0uZ3U/nAYllxaeHg4lixZgvLycsycORMmkwkmkwkzZ85EeXk5lixZgg4dOtgds2LFCmzevBm9evXCunXr0LFjxzrPPXDgQHz77bfo3r17o+sZN24cANQ5fGuxWPDWW29h0KBB8PX1hY+PDwYOHIi33noLFoulzvN9++23GD9+PIKDg+Hp6Ym4uDg88sgjdQ4dHjt2DElJSejWrRs8PT0RFBSE+Ph43HXXXcjPzwcAjBw5ErNnzwYAzJ492244Misrq9HtvNTw4cMRHx8PKSV+/PHHOvfR6/W47bbbsGvXLhw+fLhW7du2bcP//M//wGAw1Dp2165dAID/9//+H6Kiomo97+Pjg+uvv77Z9RNxGJZc3uTJkzFnzhz897//xdNPPw0A+OGHHzB79mzceOONtfZfunQpAGD+/Pnw8vJq8Px6feP/N9q4cSMAYPDgwbWemzFjBlatWoXo6GgkJSVBCIHU1FQkJydj+/bt+OSTT+z2f+uttzB37lz4+PjgpptuQlhYGLZs2YIXX3wRaWlp2L17N4KCggAAZ8+exeDBg1FSUoKJEydi2rRpqKysRGZmJj788EPce++9CAkJwaxZsxAYGIh169Zh8uTJuPLKK22vZx1Sbi5r4F/u55WUlIQXX3wRS5cuxUsvvWTb/t5770FKiaSkpDrDNiwsDADwxx9/tKhGonqp3bUlagvFxcUyJiZGurm5STc3N9m1a1dZXFxcaz+j0SgNBoMEIDMyMpr1WtZh2Hnz5sknnnhCPvHEE3L+/Ply/PjxUqfTyREjRshz587ZHfPRRx9JAHLgwIGytLTUtr20tFQOGDBAApAffvihbXtmZqY0GAzS399fHjlyxO5cd911lwQgk5KSbNtef/11CUC+9tprteotLS2V5eXltsetMQy7Y8cOqdPppLu7e61hbetwqHX49ZprrpHh4eGyurpaSqm8JxEREbbnExMTaw3DnjlzRgYGBkoActKkSXL58uXy0KFD0mw211ur9XUDAgJs79OlX2+//XaTfwbkmhiW1G5YQwCA/Prrr+vc5/z587Z9Kioqaj3/9ddf1/qFumnTJrt9rGFZ11d0dLR84403av0Sv/baayUA+e2339Z6zfT0dAlAjho1yrbtmWeekQDko48+Wmv//Px86evrKz09PWVlZaWUUso33nhDApDvvvtuo39OLQlL68/mn//8p7z55pulu7u7FELIhQsX1jrm0rBcvny5BCDXrl0rpZQyNTXVrp66wlJKKbdu3Sq7d+9u9/P28/OTEyZMkB9//HGtn7n1dS/31b9//yb/DMg1MSypXSgvL5e9evWy/RK8/fbb69wvOzv7smE5b968Wr9QLw2suib4VFRUyF9//VVOnz5dApAzZsywOyY4OFjqdDpbb+piRqNRurm5yYCAANu2qVOnSgBy48aNdbbjmmuukQDk/v37pZRSZmVlSV9fX6nX6+W0adPku+++K3/77TdpsVhqHeuIsLz0SwhR7/kuDcvy8nIZGBgoJ06cKKWUcuLEidLf31+WlZVJKesPSymlNJvNcvv27fKZZ56RU6dOlR06dLDVMH78eFlVVVXrdTnBhxqDE3yoXXj44Ydx+PBhzJs3D1deeSWWLl2K9evX19ovJCTENoHk7NmztZ5fuHAhpPJHJt5///1Gv76npyf69u2Ljz76CF27dsXKlSvx3Xff2Z4vKipCcHBwnZNX9Ho9QkNDUVxcbLc/AERERNT5epGRkXb7denSBd9//z2mTp2K9PR03HXXXejbty+6dOmCRYsWNbodjWX9GZWWliI9PR2dOnXCP/7xD2zbtq3BY728vDBjxgxs2LABe/bswYYNG/C3v/0N3t7eDR6r0+kwYsQIPPbYY1i7di3OnTuHDRs2ICIiAhs2bMDbb7/tiOZRO8SwJJeXnp6OxYsX44orrkBKSgpWrFgBDw8P3HHHHbZZoFZ6vd42+Wbz5s0Or8VgMGDAgAEAgO+//962PSAgABcuXIDRaKx1jMlkQl5eHvz9/e32B4Ds7Ow6X+fcuXN2+wFAfHw8Vq1ahfz8fPz444944YUXYLFYcO+99zYp+JvCx8cHY8eOxfr16+1mITckKSkJZrMZ06dPh9lsxu23396s1xdCYNy4cXj22WcBAJs2bWrWeYgYluTSLly4gNmzZ8NgMODDDz+Eh4cH+vbti2eeeQbZ2dm4++67ax2TlJQEAHjllVdQUVHh8JoKCgoAwO52kISEBFgsFmzfvr3W/tu3b4fZbLaFrHV/ANi6dWut/QsLC/Hzzz/D09MT8fHxtZ7X6/W46qqr8Mgjj+Djjz8GAKSmptqed3NzAwCYzeZmtK5u/fv3xx133IHTp0/jtddea3D/hIQEJCQk4PTp0+jXrx8GDRrUotf38/MDgDpXECJqDIYlubS7774bZ8+exbPPPot+/frZtj/44IMYMWIEPv30U1tgWP3973/HqFGjcPjwYUyePNnWS7tUc5ZB++GHH7Bjxw4AQGJiom37nDlzAAALFiyw63mVl5fj//7v/wDArnc1c+ZMGAwGvPnmm7WWfXv88cdRXFyMmTNnwsPDA4DSiz1//nyteqzbLl6lJiQkBABw6tSpJrfvch577DF4enri5Zdftv3BcDkrVqxAamoqPvroowb3/eabb/DZZ5/V2TMvLS3FwoULAQDXXHNNk+smAnifJbmwFStWYPXq1bjmmmvw4IMP2j2n0+nwwQcfoF+/fkhOTkZiYqJt8QE3Nzd89tlnmDlzJr788kvExMRg5MiR6N27t225u/379+Onn36Cr6+vrZd3qYULF9ruTaysrMSxY8eQlpYGk8mEuXPn2vUUZ8yYgXXr1mH16tXo06cPbrzxRggh8PnnnyMzMxM33XQTbr31Vtv+Xbt2xcKFC5GcnIwBAwbY7rPctm0bvvvuO/Tq1QspKSm2/VeuXInFixcjMTER3bt3R1BQEDIyMvDFF1/Aw8MD8+bNs+07dOhQeHt7Y+HChcjPz7ct2nDvvffaDes2VadOnXDXXXfh9ddfx4svvoh///vfl92/T58+6NOnT6POffjwYfzv//4vgoKCMGLECMTFxUGv1+P06dP48ssvUVhYiKuvvhpz586tdWxhYSGefPLJes89a9YsdO3atVF1kAtTdXoRUSs5ceKEDAgIkP7+/jIrK6ve/ZYsWSIByOuuu67O5zds2CBnzJghY2JipJeXl3R3d5cRERFy7Nix8pVXXpE5OTm1jqnr1hGdTidDQ0Pl2LFj5apVq+p8LbPZLBcvXiyvuuoq6eXlJb28vOSAAQPkokWL6r1fcMOGDXLs2LEyMDBQuru7y27dusmHHnpIFhQU2O23Z88e+Y9//EP269dPBgUFSU9PT9mtWzc5a9Ys+euvv9Y679dffy2HDBkifXx8bG2oa/m+S1n3rU92drb09vaW3t7etmUGL50N25C6ZsPm5ubKpUuXyltuuUXGx8fLwMBAqdfrZWhoqBw5cqRcvHix3UzYi1+3oa8tW7Y0qi5ybUJKDuITERFdDq9ZEhERNYBhSURE1ACGJRERUQMYlkRERA1gWBIRETWAYUlERNQAhiUREVEDGJZEREQNaNfL3RUUFMBkMrXoHGFhYcjNzXVQRc6FbdMmtk2b2DZ16PV6BAUFNbxfG9TitEwmU50LLzeWEMJ2HldbCIlt0ya2TZvYNufHYVgiIqIGMCyJiIgawLAkIiJqAMOSiIioAe16gg8RkVaVlZXBZDLZJtA4s4qKClRXV6v2+t7e3tDrWxZ3DEsiIo2pqqqCEAIBAQFql9IoBoOhRXcetITFYkFJSQl8fHxaFJgchiUi0piqqip4eXmpXYYm6HQ6+Pn5oby8vGXncVA9RETUhrQw/OosdLqWRx3DkoiIqAEMSyIiogYwLImIiBrA2bBERNSqwsPDL/v89OnTsXDhwmad++qrr0ZSUhLuuOOOZh3fWAxLIiJqVb/++qvtE57S0tLw8ssvY/v27bbnPT091Sqt0TgM20zSZIL84zdUfL9T0yvpExG1tg4dOiA8PBzh4eHw8/ODEML2ODw8HHv27MF1112H2NhYDB06FK+++qrdxye+8sorGDRoEGJiYjBgwAA8/vjjAIBp06bh9OnTePLJJ9GpUyd06tSp1drAnmVzmYwwv7gAeQDcFn8KuHuoXRERtUNSSqC6Sp0Xd/do8S0sW7duxX333Yenn34aV199NU6cOIGHH34YAPDAAw9g/fr1WLJkCd566y307NkTOTk5OHjwIABgyZIlGDt2LG699VbceuutLW7O5TAsm8vDExACkBKorGBYEpE6qqtgmXuTKi+tW7Ra+V3YAm+88QaSk5Nx001KG7p06YKHHnoIzz33HB544AGcOXMGYWFhGDFiBAwGAzp16oSEhAQAQFBQENzc3ODr69vgddGWYlg2kxBC+Y+kskL58g9UuyQiIs05cOAAfvnlF7zxxhu2bRaLBZWVlaioqMCkSZPw3nvvYejQoRg1ahRGjx6NsWPHtnit16ZiWLaEp1dNWLZsGSUiomZz91B6eCq9dktJKfHggw9iwoQJtZ7z8PBAp06dsH37duzYsQM7duzAP//5T7z99ttYu3YtDAZDi1+/sRiWLeFZszZjZYW6dRBRu2Ub5dKovn37IiMjAzExMfXu4+XlhXHjxmHcuHG47bbbkJiYiMOHD+OKK66AwWCA2Wxu9ToZli1RE5aysgJcpZGIqOn+93//F7fddhs6duyISZMmQafT4eDBgzh8+DAeeeQRrFq1ChaLBQkJCfDy8sLatWvh6elpm/nauXNn7N27F5MnT4aHhweCg4NbpU7eOtICwtNb+YY9SyKiZhk5ciQ++OADbN++HRMnTsRf//pXLFmyBFFRUQCAgIAAfPTRR7jxxhsxZswY7Ny5E8uWLbOF4vz583Hq1CkMHz4cV1xxRavVyZ5lS1iHPqoq1a2DiEgjbr75Ztx8881220aOHImRI0fWuf91112H6667rt7zXXXVVdi4caMjS6wTe5YtwWuWRETtAsOyJRiWRETtAsOyJWwTfHjrCBGRK2NYtoBgz5KIqF1gWLaEB8OSiKg9YFi2BHuWRESa0NJPh2JYtoT1PkveOkJEbUgIgerqarXL0AQpJcrKylq8lizvs2yJi1bwISJqK76+vigtLUVlpTb+UHd3d1c13D08PODh0bJ1bBmWLSCsn+7NsCSiNiSEgJ+fn9plNIoQApGRkTh37lyLh0LVxGHYluA1SyKidoFh2RKcDUtE1C4wLFvC2rOsqoC0WNSthYiIWg3DsiWsYQkA1dq40E5ERE3HsGwJdw9AV/Mj1MisNCIiajqGZQsIISC8rJ9pyfVhiYhcFcOyhXQ+NdO3y8vULYSIiFoNw7KFdL7WsCxVtxAiImo1DMsW0vn6AwBkGcOSiMhVMSxb6M+eJYdhiYhcFcOyhaw9Sw7DEhG5LoZlCzEsiYhcH8OyhWzDsLxmSUTkslT/1JGDBw8iLS0NmZmZKCgowPz58zF48OB699+7dy/S09ORlZUFk8mEqKgoTJ8+HVdeeWXbFX0R2wQf9iyJiFyW6j3LqqoqdO3aFXPmzGnU/ocOHUK/fv2wYMECvPDCC+jTpw9SUlKQmZnZypXWjRN8iIhcn+o9y4SEBCQkJDR6/1mzZtk9njFjBn788Ufs27cPMTExDq6uYTq/AOUbDsMSEbks1cOypSwWCyoqKuDr61vvPkajEUaj0fZYCAEvLy/b980lhICw9SxLWnQuZ2Ntiyu1yYpt0ya2TZtcpW2aD8v169ejqqoKQ4cOrXef1NRUrFmzxvY4JiYGKSkpCAsLa/HrGy1KCIuKckRGRrb4fM4mIiJC7RJaDdumTWybNmm9bZoOy507d+LTTz/FQw89hICAgHr3mzJlCiZNmmR7bP0LJzc3FyaTqdmvL4RAWE3PUlaU4+ypUxB6Tf9IbYQQiIiIQHZ2NqSUapfjUGybNrFt2uTsbdPr9Y3qOGn2N/vu3bvxzjvv4IEHHkC/fv0uu6/BYIDBYKjzuZa+ebaF1FEzI9av/tDWIimlU/4H7ghsmzaxbdqk9bapPhu2OXbu3InFixfjvvvuw4ABA1StRbi5AdaP6eIkHyIil6R6z7KyshLZ2dm2xzk5OcjKyoKvry9CQ0OxcuVKXLhwAXPnzgXwZ1DOmjULPXr0QGFhIQDA3d0d3t7eajQB8PYFKsq5ig8RkYtSPSwzMjLw1FNP2R4vX74cAJCYmIjk5GQUFBQgLy/P9vzGjRthNpuxdOlSLF261Lbdur8qvH2B/ByGJRGRi1I9LPv06YPVq1fX+/ylAfjkk0+2ckVNJ3x8IaF8TJe2J0cTEVFdNHnN0ul4+yj/chUfIiKXxLB0BO+aBRE4DEtE5JIYlo7gw7AkInJlDEsHED78mC4iIlfGsHSEmmFYfkwXEZFrYlg6gu2aJSf4EBG5IoalI/jUzIYtK1G3DiIiahUMSwcQnA1LROTSGJaOYA3LMg7DEhG5IoalI1hvHamqgGzBR34REZFzYlg6gnUFHwCoYO+SiMjVMCwdQOj4MV1ERK6MYekonORDROSyGJaOYltMnWFJRORqGJaOYl3Fh8OwREQuh2HpKD5cxYeIyFUxLB3Etpg6h2GJiFwOw9JReM2SiMhlMSwdxbaKD8OSiMjVMCwdhRN8iIhcFsPSUXx4nyURkatiWDoIP3mEiMh1MSwdhWFJROSyGJaOYvsAaN5nSUTkahiWjuLNj+kiInJVDEtH4cd0ERG5LIalg/BjuoiIXBfD0pE4yYeIyCUxLB2J91oSEbkkhqUjcRUfIiKXxLB0JG9+TBcRkStiWDqQsA7DlhWrWwgRETkUw9KR/AKUf0sYlkREroRh6Uh+/sq/JUXq1kFERA7FsHQkv0AAgGRYEhG5FIalAwn2LImIXBLD0pF8rdcsGZZERK6EYelI/jVhWVoMabGoWwsRETkMw9KRfGuGYS0WruJDRORCGJYOJPQGwKvm00d4+wgRkctgWDqaH69bEhG5Goalo3FGLBGRy2FYOlpNz5L3WhIRuQ6GpYMJ6zBsKcOSiMhVMCwdzRqWxQxLIiJXwbB0NOs1y1LOhiUichUMS0ezrg9bXKhqGURE5DgMSwcT7FkSEbkchqWjcX1YIiKXw7B0NK4PS0TkchiWjsb1YYmIXA7D0sHs14flUCwRkStgWLYGrg9LRORS9GoXcPDgQaSlpSEzMxMFBQWYP38+Bg8eXO/+BQUFWL58OY4fP47s7GxMmDABs2bNaruCG8PPH8g5y08eISJyEar3LKuqqtC1a1fMmTOnUfsbjUb4+/tj6tSp6NKlSytX10y29WEL1a2DiIgcQvWeZUJCAhISEhq9f3h4OGbPng0A2LJlS2uV1SLCLwASYM+SiMhFqN6zdEm8ZklE5FJU71m2BaPRCKPRaHsshICXl5ft++ayHnvpOYR/YE3PsqhF51dTfW1zBWybNrFt2uQqbWsXYZmamoo1a9bYHsfExCAlJQVhYWEOOX9ERITd47LOXXABgHtVOcIjIx3yGmq5tG2uhG3TJrZNm7TetnYRllOmTMGkSZNsj61/4eTm5sJkMjX7vEIIREREIDs7G1JK23brwj1Vebk4d+5cs8+vpvra5grYNm1i27TJ2dum1+sb1XFqF2FpMBhgMBjqfM4Rb56U0v48ts+0LHTK/ziaolbbXAjbpk1smzZpvW2qh2VlZSWys7Ntj3NycpCVlQVfX1+EhoZi5cqVuHDhAubOnWvbJysry3ZscXExsrKyoNfrERUV1dbl180/UPm3rATSZILQq/5jJiKiFlD9t3hGRgaeeuop2+Ply5cDABITE5GcnIyCggLk5eXZHfPwww/bvj9+/Dh27tyJsLAwLF68uG2KboiPHyB0gLQApUVAYIjaFRERUQuoHpZ9+vTB6tWr630+OTm51rbL7e8MhE6nrOJTXKh8MSyJiDSN91m2FutQbHGhmlUQEZEDMCxbS01YymIuTEBEpHUMy1YirD1Lrg9LRKR5DMvWctHtI0REpG0My9Ziu2bJYVgiIq1jWLYW2zXLQlXLICKilmNYthLhF6h8w7AkItI8hmVrsU3w4TAsEZHWMSxby0WzYaV1ZXUiItIkhmVr8fNX/rVYgLJSdWshIqIWYVi2EqE3AN6+ygPea0lEpGkMy9bEJe+IiFwCw7I18fYRIiKXwLBsRYKr+BARuQSGZWvi7SNERC6BYdmaeM2SiMglMCxbE69ZEhG5BIZlKxL+NdcsOQxLRKRpDMvWxPVhiYhcAsOyNV10zVJKqWopRETUfAzL1uQfpPxrrAYqytWthYiImo1h2YqEhwfg6aU84FAsEZFmMSxbG28fISLSPIZla7MOxRYXqFsHERE1G8OytfFeSyIizWNYtjLBYVgiIs1jWLY2hiURkeYxLFtbQCAADsMSEWkZw7KVcRiWiEj7GJatzbrkXRFnwxIRaRXDsrUFWG8d4ZJ3RERaxbBsbQHByr8mI1BWom4tRETULAzLViYMBsDXT3lQeEHdYoiIqFkYlm0hMET5tzBf3TqIiKhZGJZtoSYsJXuWRESaxLBsAyKw5role5ZERJrEsGwL1mHYAvYsiYi0iGHZFmp6lrKIYUlEpEUMyzYgbD1LDsMSEWkRw7ItWK9ZsmdJRKRJDMu2EFQTlsWFkCaTurUQEVGTMSzbgm8AoNcDUnJGLBGRBjEs24DQ6YDgcOVB3nl1iyEioiZjWLaV0A4AAMmwJCLSHIZlGxE1YcmeJRGR9jAs2wrDkohIsxiWbYXDsEREmsWwbCMizNqzzFG3ECIiajKGZVuxDsMWXYCsrlK3FiIiahKGZVvx8QM8vZTv89m7JCLSklYLS4vF0lqn1iQhBCf5EBFpVJPCcu7cucjKyrI9llLi3XffRV5ent1+R48exd/+9jeHFOhSOMmHiEiTmhSWubm5MF20tqmUEps3b0ZxcbHDC3NFvNeSiEibeM2yLbFnSUSkSXq1Czh48CDS0tKQmZmJgoICzJ8/H4MHD27wmA8++ACnT59GUFAQbrjhBowbN66NKm4+EdoBEmDPkohIY1TvWVZVVaFr166YM2dOo/bPycnBv//9b8THxyMlJQVTpkzB+++/jz179rRypQ7AYVgiIk1ySM9SCNHsYxMSEpCQkNDo/dPT0xEaGopZs2YBAKKiopCRkYEvvvgCQ4YMaXYdbSKk5pNHyssgy0shvH3VrYeIiBqlyWH5xhtvwN3d3W7bwoULYTAYbI+rq6tbXlk9jh49in79+tltu/LKK7FlyxaYTCbo9bWbZDQaYTQabY+FEPDy8rJ931zWYxt7DuHlDYtfAFBSBJGfA+Hj1+zXbm1NbZuWsG3axLZpk6u0rUlhGR8fX6vBvXv3rnPfkJCQ5ld1GYWFhQgICLDbFhAQALPZjJKSEgQFBdU6JjU1FWvWrLE9jomJQUpKCsLCwhxSU0RERKP3Pd+xM6qPFCHQVA3vyEiHvH5rakrbtIZt0ya2TZu03rYmheWTTz7ZSmU0zaWBLaWsc7vVlClTMGnSpFrHX3orTHPqiIiIQHZ2tq2GhpgDggEABUcPoyg2vtmv3dqa0zatYNu0iW3TJmdvm16vb1THSfXZsE0VGBiIwsJCu23FxcVwc3ODr2/d1wANBoPdMPHFHPHmSSkbf56a65Yyzzn/w7lUk9qmMWybNrFt2qT1tjkkLEtLS7Fu3TqcOnUKwcHBmDBhAjp37uyIU9cSFxeHffv22W375ZdfEBsbW+f1SqdjvdcylzNiiYi0okm3jixfvhx333233bbKykosWLAAaWlp2L9/PzZt2oTHHnsMZ8+ebdQ5KysrkZWVZVtGLycnB1lZWbYl9FauXIlFixbZ9h83bhzy8vJs91lu3rwZmzdvxl//+temNEU1XMWHiEh7mtQV++OPPzB8+HC7bd988w1ycnJw/fXXY9q0aTh9+jReeeUVfP7557jnnnsaPGdGRgaeeuop2+Ply5cDABITE5GcnIyCggK7tWfDw8OxYMECfPDBB9iwYQOCgoIwe/Zs579txMoalvk5kFJqfoYYEVF70KSwPH/+PCZOnGi3bd++ffD398fMmTOh0+nQo0cPTJo0Cd98802jztmnTx+sXr263ueTk5NrbevduzdSUlKaUrrzCA4DhA4wVgNFBUBgsNoVERFRA5o0DFteXm53a4bZbEZGRgZ69+4Nne7PU8XExNSahEMKodcDQTW31XAolohIE5oUlgEBASgoKLA9zszMhNlsRrdu3ez2E0JoY7KNWrigOhGRpjQpLGNjY7Fp0ybb9N8dO3YAAPr27Wu335kzZ+pcHIAUnORDRKQtTer+TZ48GY8//jjuv/9++Pn54ejRo+jVqxdiY2Pt9tu3b1+t3iZdhGFJRKQpTepZxsXF4eGHH0ZQUBAqKiowevRoPPTQQ3b7FBYW4sKFCxg0aJBDC3UpYRyGJSLSkiZfWBwwYAAGDBhQ7/OBgYF46aWXWlSUq+PnWhIRaYvqn2fZLlmHYQvyIM1mdWshIqIGNalnuW3btiadPDExsUn7txv+QYDeAJiMwIVcIEzbq/ETEbm6JoXlW2+91aSTMyzrJnQ6IDQcyD6jDMUyLImInFqTr1l6e3tj6NChGD58uO0DlKkZQjsA2Wcg886DC94RETm3Jn+e5ZYtW7Bjxw7s3LkTQ4YMwejRo9GrV6/Wqs9libAIZZJPzjm1SyEiogY0KSzj4+MRHx+POXPmYOfOndiyZQueeOIJREREYNSoUUhMTORiBI3VIQoAILPPqFwIERE1pFlr0nl6emLMmDEYM2aM7WOyvvzyS6xatQqTJ0/GLbfc4ug6XY6I7KT0LLNPq10KERE1oMW3jkRFRWHUqFEYOnQopJQ4fZq//BulpmeJ3GxIk0ndWoiI6LKavdp5eXk5du3ahS1btiAjIwORkZG45ZZbOAO2sYJCAHcPoLpKmREb0UntioiIqB5NDsvffvsNW7Zswd69e6HT6TBkyBD8/e9/R3x8fGvU57KETqcE5MnjylAsw5KIyGk1KSzvvfde5OTkoEePHpgzZw6GDRsGT0/P1qrN5YmIKMiTxyHPn+HtI0RETqxJYZmTkwMvLy9UVFTgq6++wldffVXvvkIIrhHbkA41vclzvM5LROTMmnzriBDsAzlMpPX2EYYlEZEza/KiBI1l/YBoqp+IiFJuHznPey2JiJxZq3zqyM6dO/HAAw+0xqldS3hH5d/SEsiSYnVrISKiejV5Nmx5eTm+//57FBUVITIyEgMHDoROp2Tu3r17sXr1apw+fRqhoaEOL9bVCA8PICQcyM9RZsT69Va7JCIiqkOTwjI7Oxv/+te/UFRUZNvWu3dvPPTQQ3j99dfx888/w8fHB7feeismTJjg8GJdUkQnID8HMvs0RBzDkojIGTUpLD/55BNUVFRg+vTp6NatG86fP4/U1FQ8/vjjOH36NEaPHo2ZM2fCx8entep1OSIiCvL3/crHdRERkVNqUlgeOnQIU6dOxZQpU2zbIiIi8O9//xtjx45FUlKSwwt0eTWLEXBGLBGR82rSBJ/i4mL07NnTbpv147mGDRvmuKraEWG91/L8WXULISKiejUpLC0WC9zd3e22WR9zJZ9msoZlHhdUJyJyVk2eDXv27Fnb7FdACVDr9kvFxsa2oLR2IigE8PAEqiqBvGwgIkrtioiI6BJNDsvFixfXuf3NN9+stW3VqlVNr6idEUIAHTrWLKh+hmFJROSEmhSWd999d2vV0a6JDp1qFlQ/ywXViYicUJPCcuTIka1URjtnm+TD20eIiJxRqyx3R03E20eIiJwaw9IJiA41a8RyYQIiIqfEsHQG1mHYkiLI8lJ1ayEioloYlk5AeHkDAcHKAy5OQETkdBiWzsJ23ZJDsUREzoZh6SR43ZKIyHkxLJ1FzXVLeZ4zYomInA3D0kmImmFY9iyJiJwPw9JZdIxW/s0+A2k0qlsLERHZYVg6i5BwwNsXMJuAsyfVroaIiC7CsHQSQgigSzcAgDxxTOVqiIjoYgxLJyKilbDEyQx1CyEiIjsMS2di7VmePK5yIUREdDGGpROx9SxPZUKaTOoWQ0RENgxLZxIWAXh5AyYjkH1K7WqIiKgGw9KJCJ0O6BwLAJAnOBRLROQsGJZOhpN8iIicD8PS2fD2ESIip8OwdDKiy0WTfCxmdYshIiIADEvn06Ej4OEJVFfxsy2JiJwEw9LJCJ0b0DkGACBP8LolEZEzYFg6IdskH4YlEZFT0KtdAABs2LABaWlpKCwsRFRUFGbNmoX4+Ph69//mm2+wYcMG5OTkIDQ0FFOnTkViYmIbVtzKoq0r+TAsiYicgephuXv3bixbtgxJSUno2bMnNm7ciOeffx6vvfYaQkNDa+2fnp6Ojz/+GHfddRe6deuGY8eO4d1334WPjw8GDhyoQgscT3TpBgkAp45DWizK/ZdERKQa1X8Lr1+/HqNHj8a1115r61WGhoYiPT29zv23b9+OMWPGYNiwYejQoQOGDx+O0aNHY926dW1ceSuK7AwY3IGKciA3W+1qiIjaPVXD0mQy4fjx4+jfv7/d9n79+uHIkSN1HmM0GmEwGOy2ubu749ixYzC5yHqqws0NiOoKgIuqExE5A1WHYYuLi2GxWBAQEGC3PSAgAIWFhXUe079/f2zevBmDBw9GTEwMjh8/ji1btsBsNqOkpARBQUG1jjEajTAajbbHQgh4eXnZvm8u67EtOUe9547uBpn5B3AyA2LwCIefv8HXb8W2qY1t0ya2TZtcpW2qX7ME6v4h1veDnTZtGgoLC/Hoo49CSomAgAAkJiYiLS0Nunqu7aWmpmLNmjW2xzExMUhJSUFYWJhD6o+IiHDIeS5W2m8ACrZ9DffsUwiPjHT4+RurNdrmLNg2bWLbtEnrbVM1LP39/aHT6Wr1IouKimr1Nq3c3d1xzz334M4770RRURGCgoKwceNGeHl5wc/Pr85jpkyZgkmTJtkeW4M4Nze3RUO3QghEREQgOzsbUspmn6cuMkCZ3FR19BDOnj3b5n+VtWbb1Ma2aRPbpk3O3ja9Xt+ojpOqYanX6xEbG4sDBw5g8ODBtu0HDhzAoEGDGjw2JCQEALBr1y4MGDCg3p6lwWCodZ3TyhFvnpTS8WHZMRpwcwPKSiDzc4CQcIeev9F1tELbnAXbpk1smzZpvW2qD8NOmjQJb775JmJjY9GjRw9s3LgReXl5GDt2LABg5cqVuHDhAubOnQsAOHv2LI4dO4a4uDiUlZVh/fr1OHXqFJKTk9VshsMJgwHoGA2cygSyjqoWlkRE5ARhOWzYMJSUlGDt2rUoKChA586dsWDBAlu3uKCgAHl5ebb9LRYL1q9fj7Nnz8LNzQ19+vTBs88+i/Bw1wsT0a0X5KlMyGOHIK4arnY5RETtluphCQDjx4/H+PHj63zu0h5jVFQUXnzxxbYoS33d4oGtX0MeO6R2JURE7ZrqixJQ/UT3miX/Th2HrKpStxgionaMYenMQsKBwBDAbAay/lC7GiKidoth6cSEEBA9+wIA5M/fq1wNEVH7xbB0cmKQsnqP/GEHpMWscjVERO0Tw9LZ9UkAfPyAogvAkd/UroaIqF1iWDo5oTdADBgKAJAHflS5GiKi9olhqQU9rwAAyAzeQkJEpAaGpQaI7r2Vb05mQFZVqlsMEVE7xLDUABESBgSHKreQZPIWEiKitsaw1Ahr71IeO6hyJURE7Q/DUiusYXmU1y2JiNoaw1IjRFzN0ncZhyHNvN+SiKgtMSy1omM04OUNVFUAZ7LUroaIqF1hWGqE0LkB3XoB4FAsEVFbY1hqiG2Szx+/qlwJEVH7wrDUENEnQfnm958hjdXqFkNE1I4wLLWkS3flI7uqKoDDB9Suhoio3WBYaogQAiLhagCA3L9H5WqIiNoPhqXGiAHDAADy+x2QZaUqV0NE1D4wLLWm5xVApy5AVQXktq/VroaIqF1gWGqMEAJi/FQAgNz0BSf6EBG1AYalBolBI5SF1YsLIfdsVbscIiKXx7DUIKHXQ4yZDACQ6amQUqpcERGRa2NYapQYMRbQG4DsM8C5U2qXQ0Tk0hiWGiU8vYGefQEA8rd9KldDROTaGJYaJvoOAADI335SuRIiItfGsNQw0fcq5Zs/focsK1G3GCIiF8aw1LIOnYCoroDZBLmV91wSEbUWhqWGCSEgxk0BAMjN63nPJRFRK2FYapwYNAII4j2XREStiWGpcco9lzcAqLnn0mJRuSIiItfDsHQB4ppxgJePcs/lwf1ql0NE5HIYli5AeHpDDEkEAMjvt6tcDRGR62FYuggx6BoAgPx5Lyf6EBE5GMPSVXTrpUz0qSgHDvygdjVERC6FYekihE4HMXQ0AMCS/jkXVyciciCGpQsRo69XFlc/fgQ4dkjtcoiIXAbD0oWIgCCIoaMAAHLLlypXQ0TkOhiWLkaMnAAAkPu/gywpVrcYIiIXwbB0MSK6G9ClO2AyQe7epHY5REQugWHpgkTidQAAueEzyMpylashItI+hqULEkNHA+EdgZIiyG/T1C6HiEjzGJYuSOj1EDf8DQAgt2+AtJhVroiISNsYli5KDBgGePsChfnA4QNql0NEpGkMSxclDAaIwSMAAHLXZpWrISLSNoalCxPDxwAA5L6dkLnZKldDRKRdDEsXJrrGAb2vBMxmyK/XqF0OEZFmMSxdnO6vtwAA5K6NkKez1C2GiEijGJYuTnTvDQwYBlgssHz0DqSZM2OJiJqKYdkO6G66HXD3AI4dhPzsA7XLISLSHIZlOyBCwqCbPQ8AINM/hzx6UOWKiIi0hWHZToiBf4G4ZjwAwPLJEi5UQETUBAzLdkRMvhXw8gZOZkBu+FztcoiININh2Y4I/0CIm5MAAHLdh5Anj6tcERGRNujVLgAANmzYgLS0NBQWFiIqKgqzZs1CfHx8vfvv2LEDaWlpOHfuHLy9vXHllVfi73//O/z8/Nqwam0Sw66FPPAD8NN3sKx8B7pHUiCEULssIiKnpnrPcvfu3Vi2bBmmTp2KlJQUxMfH4/nnn0deXl6d+x8+fBiLFi3CqFGj8Oqrr+KBBx5ARkYG3nnnnTauXJuEENDdfIcyOzbjMOSPO9UuiYjI6akeluvXr8fo0aNx7bXX2nqVoaGhSE9Pr3P/P/74A+Hh4Zg4cSLCw8PRq1cvjBkzBsePc0ixsURwKMT4qQAA+eVqSItF5YqIiJybqsOwJpMJx48fx4033mi3vV+/fjhy5Eidx/Ts2ROffPIJfvrpJyQkJKCoqAh79uxBQkJCva9jNBphNBptj4UQ8PLysn3fXNZjtTiMqRtzA8zffg6cOQH8+iPElVfbPa/ltjWEbdMmtk2bXKVtqoZlcXExLBYLAgIC7LYHBASgsLCwzmN69uyJ++67DwsXLoTRaITZbMbAgQMxZ86cel8nNTUVa9b8uTZqTEwMUlJSEBYW5pB2REREOOQ8ba3w+ukoWbsc7t9tRtiEG+vcR6ttawy2TZvYNm3SetucYoJPXX9x1PdXyOnTp/H+++9j2rRp6N+/PwoKCvDhhx9iyZIluPvuu+s8ZsqUKZg0aVKtc+fm5sJkMrWo7oiICGRnZ0NK2ezzqEUOGA6sXY7Kn77D2d8PQAT/+ceD1tt2OWybNrFt2uTsbdPr9Y3qOKkalv7+/tDpdLV6kUVFRbV6m1apqano2bMnbrjhBgBAly5d4OnpiX/961+45ZZbEBQUVOsYg8EAg8FQ5/kc8eZJKZ3yP4IGhUcCPa8AjvwKy86NtkXXL6bZtjUC26ZNbJs2ab1tqk7w0ev1iI2NxYEDB+y2HzhwAD179qzzmKqqqlq9Tp1OaYaW3wi1iL+MBVDzqSSc6ENEVCfVZ8NOmjQJmzZtwubNm3H69GksW7YMeXl5GDtW+SW+cuVKLFq0yLb/wIED8f333yM9PR3nz5/H4cOH8f7776N79+4IDg5WqxmaJQYMBbx8gPwc4PAvapdDROSUVL9mOWzYMJSUlGDt2rUoKChA586dsWDBAtsYckFBgd09lyNHjkRFRQW++eYbLF++HD4+PujTpw9mzpypVhM0Tbh7QAxJhNzyFSyffwRdbC8ITy+1yyIicipCtuOxy9zcXLtbSppKCIHIyEicO3dO00PAMuccLM89CJSXAr36QXffv6Bz93CJttXFVd63urBt2sS2qcdgMDRqgo/qw7CkPhEeCd39TwIeXsDhA7C8/QKkqfl/RBARuRqGJQEAREwP6O57HHB3B379EZb3XoE082O8iIgAhiVdRPToC93d/wTc9JA/7sKFN551ymETIqK2xrAkO6LvAOjufAjQ6VC+8QtY1n2kdklERKpjWFItYsBQ6P5nLgBArl8F+csPKldERKQuhiXVSfeXsfCd/DcAgGXFIsgLdX9kGhFRe8CwpHoFzpoLdOwMFBXAkvIw5LGDapdERKQKhiXVS7h7wG3ek0BEJ+BCHiwp/wfLrk1ql0VE1OYYlnRZIiQcuv97CWLIKACAXPkO5KFfOEuWiNoVhiU1SPj4QsyeB/TqB1RXwfLq45BLXoa08D5MImofVF8blrRB6HTQ3fkw5NplkHu2Qv6wAzLjEODrDxHXB2LK/0B4eKhdJhFRq2BYUqMJP3+IWffB0mcA5NJXgQt5wIU8yJPHIX/fDzFxOsSgv0Do6/7sUCIirWJYUpPpBv0FMq43kJcNmZ8Lueo9IPs05H9fg/z8Q+jmPQHRMVrtMomIHIbXLKlZRGAwRPfe0F2dCN0zb0NM+TsQEARcyIXl9acgT2aoXSIRkcMwLKnFhI8vdBOnQ/fkmzW3meTC8uwDsHy5mrNmicglMCzJYYSvP3QPPgsx+BpASsjPP4Rc9R6kxaJ2aURELcKwJIcSgSHQ3TEf4uYkAIDc9AUsz8+H/OV7lSsjImo+hiW1Ct2YGyBufwAwuAMnjsGy6FnlMzLPnlS7NCKiJmNYUqvRDRkJXcpSiHE3AgDk3m2wPPcA5AlO/iEibWFYUqsSfgHQTZ8D3T9fBrr3BqqrYXlpAcxvPgN55oTa5RERNQrDktqEiOkB3b2PA9HdgKpK4MAPsDxzPyxrlkFWVapdHhHRZXFRAmozwttH6WGezoJl/Srg5z2QGz6D/HEnxF/GQiROgPDzV7tMIqJa2LOkNiXc3CC6dINb8j+hS34UCA4D8nMg130Ey3MPwLJ9A2RBvtplEhHZYc+SVCOuvBq6+P6Q32+H/HoNkJsNuWIxpJc3dLPvh0gYonaJREQA2LMklQkPT+hGjIPuny9DjJsCRMUAFeWwvPMC5E+7uQIQETkFhiU5BeHrD9302dA9+oryQdMWCyxvvwDLcw9C5ueqXR4RtXMMS3IqQq+HuO1eiMTr/lzQ4IWHIX/7Se3SiKgdY1iS0xF6PXQz74Hu2beBiCigMB+W15+EZd1HkJXlapdHRO0Qw5KclggOg+7RlyFGXQ8AkOtXwTJvBsxvPQ95cD+kxQxpNEKazSpXSkSujrNhyakJT2+IGXfB0qU75BcfA/k5wP49sOzfA+gNgNkEQADdekJ3cxJE1zi1SyYiF8SwJE3QDb8WGH4t5JkTkNu+htyzDagoq3lWAscOwfL8fIiREyAmTAeM1UBYBIQQapZNRC6CYUmaIjp1gZjxD8hb7lR6mR4eQFUV5LqPIPdug9zyFeSWr5Sde/WDbvwUoFd/CD3/Uyei5uNvENIkodMBYRF/Pk56EHL4GFhWvgNknwGEAA4fgOXwAcDHD+KKq4CgECA4TFkIISLiMmcnIrLHsCSXIeL7Q/fUIqC0BKiqhEz/HHLfLqCkCHLPVtt+ZgDnwjrA0rUH4OUNMfqvEJ2iVaubiJwfw5JcitC5Af6Byve3/gPyb3cARw9BHv4FqCiHPJ0FZByGOfc8kHseACB3fgtxzXUQN/wNwi9AveKJyGkxLMmlCZ0b0LMvRM++f240ViMo+yQuHPoNlmMHgf17ILd+Bbl3K8TVIyEGDAU8vYHgUIiAIPWKJyKnwbCkdke4e8Dr6mugi46DGHcj5OEDsHz6X+DkcSU0t9ZMEHLTA72vhAiLgOh5BRAZBUREcYYtUTvEsKR2T/TqB92jrwIH90Pu2w154AfAYgFKi4Fff4QEIDevV3buGgcxehLEoBGcYUvUjvD/diLUzK7texVE36ts2+TxI5CnM4ETxyFPHAPOZAFZRyH/+xrk5yuAsEiIgGDAxweIioEYdi0DlMhF8f9sonqI2J4QsT1tj2VJEeT2DZDpqcCFPOBCHi7+ADH5zVqIoaMhhoyECOOtKUSuhGFJ1EjCLwDi+psgh46G/G4z4OOnDNVWlEHu3qR8eHXaSsi0lUCnLoCbHqLXFRBXDgFCwgF3d8DgARgMSk+WiDSDYUnURCI4FOL6m+y2yb/+DXL/Hsg9W4BDB4AzJ5TtJzMg0z+vfZLwSIjeCcratkIHmI0QV14N9LkKwmBog1YQUVMwLIkcQHh6QQwdBQwdBVmYr1znrK4CftkL+ft+ZR3biz8dJeccZM45u3PIXZuUxeHd3YEOnSD6D4boNwgI76hsKy4ELuQC/kEQIWFt20Cido5hSeRgIjAECAyBAIBBf7Ftl2YzYKxS1rL9bZ+yLJ+HJ2AxAxUVkD9sB4oKAJMRyPwDMvMPyM8/VA42uCuLw1tFdoZIGKKsQBQVA8T3h3Bza9N2ErUnDEuiNiLc3AA3b8DTG2L4mFrPy5vmALnZgLFamYn7ww7gxDGgvEwJSiGAgGCgpBA4dwry3CnlOEAJXTc9UFmOU256QKdTHuv1StAGBgMeXhBBIYDBAJSVAgHBEAOHA2aT0suVEqJXP05OIqoDw5LISQghgPBI5ftOXYAR4yClVNa6rSxXhl89PCDLy5R7QX/frwTdoZ+Vfaws1bVPnp8DAHazdwFAblxn/xgAPL2A4DBl4fly5WPQRJduEP0GK6FrMADFRUBEJ4iapQWJXB3DksiJCSEAP3/ly7rN2wdiyEhgyEgAgLSYgfNnAbMZws8f4aGhyDl3DtJkVIZ0q6qAwnzIqkogPxeQFsDTG/KP34CsY0r4deio9F6PHgIqK4CzJ5WvGjLzD8itX9cuMDgM6Npd+dBtD09lCLrPAKVne+Io5IkMZVZwXG9l6JizgEmjGJZEGid0bkBkZ+V7IaAPi4AwSUDa9yNrLdI37sZa55JVlco9pAW5kBfyILy8ASkhf9sHmXkUKCkCqqsAX39lslHNl/zpO+V424sJu9dXhoq9gI6dlXMY3CGiY4GO0YCXDyAtEOEdgdBwoKgQqCiFrKqC6BoH0aGjfY2lxZAF+cqkp5AOEG5uyh8M504rH8cWGAxpNCq9cYsFKMhThp3DI4GCfMhTmUB5qdIGAeDMCeV8FotSs04H0TEaoks3SCmVPwA6xyhD3TU/Y2p/GJZEZCM8PJU1cCOj7MJVDPxLrX1lRTlwMgMy6xhw4hik2aT0VC/kKqHj7Qv06KOE69GDQFUFkPnHn8fXXHO1Pa6jHikE4OMLmEyAhyfOmM2wlBb/uYO7OxAZDeScU2YcC6GEYElRi34O8sAPtnrs6vL1A7rGAUajsmBFx2glZIsuQHTsAsTFA14+doEqS4qUn4nODQjtoPTsK2qG1XmbkGYwLImoWYSXN9DzCmWR+RpSSuU6p9kE+AXYQkOazUDOWWVo18dPmcR08jhw/ozSm7VI4PwZJVQCgpTAA4DjR/68HltZAYv1hXz9gepKoLpamQQFAO4eSjBfHJRCAP5BgJcXcP4cEBQMdI6F8A+ELLyg7NKpCxDWQQkzoVNmLJ/KVHqgbnqlLus5S0uA335S2nTkV7ufhy1U3d2VXrSnl/L6l9wiZGP9+XXoBBEeiYrYOMiKClgKC5Th6oAgQAhlqUUJwMND+RScLt0AswVwc1PaISVQkK/8PHz8AF//RvV+ZX7NyEBIGERw3bciSZNJmUx2KhPC0xMICgUglB65EMrPJyAY8PX7870uLlRmeldVQuadhygvRVnXbrCcyoIsKVZ66OERwPEjkOfPKn9YVVcBZSXKH0XW27A8vZR2eXgCeeeVj9fz9FLeL50boNdD9OgLdO/dJn90CCllXX/QtQu5ubkwGo3NPl4IgcjISJw7dw6u9mNk27TJ1dom884r11Dd3SGqqxEaHo48uAHuHpAWi3Kt9uwJIDgciI4Fii4ogRYcqvRsIZVhaijXdq3fN7mOygrlF/qZE8rMYZ0OyDis1OfmBuHjB3n0IFCYX/tga2CbTcqKTwDg5mZ/321zBQQpdVWU/7lNb1C2h0cqYRIcpqxpfPakcg3bYrG1RalPpwyPl5cpfyh4eCnPV1Uq/zaG3qCEm14P1PwR0lbETbdDN3Zys483GAwIC2v4vmX2LInIaYnQDn9+LwTcIyMhav4QEDqdbcjYJjhM+arrXM0MSkBZdAKeXoB/IER8f2XjiHG19pPVVUpYVFcqoV1dBcT1UXrhqBm61uuVHtnxw0rvOvsMkJ8DfVkxjMXFQGCQEmiFF5R/Izsrr22sVnrm+TlK2Fosyn25gPLY3VMZijYZlX3ycyAP/XKZRumAkDAg7/yfwQnYz6wGlGvKnWOUP1rKSmquRUult2usVv4AMBmB0pqOhxDK8o7evkqP0z8Q7lUVqPb0VkYEyssgz52C6BgNxPUGhE7pGfoHAnoDZOZRyF9/AHRuEEGhyvC99bpxWYnSNrMZKCuBPPQLRO8rm/OWNhnDkojIQYS7h+32nzqfrwlNAMrwYffeynYhENHIEQFpsUDodJCV5cqkJk8vICwCQm9QZkAXXgCKCiBPHoc8cgAoKoTo0k2ZtezhpfSKdTplODowGDL7jBKYvn7KPblVlcqQtoen8nXRcHqd9RiNQHEBUFmp9ExDO0D4XjR7WwiEN2G0Q8T2BK6d1OB+ANp09MQpwnLDhg1IS0tDYWEhoqKiMGvWLMTHx9e57+LFi7Ft27Za26OiovDqq6+2dqlERKqy3n4jPL2BmB72z+kNyiSi0A4Q3XoBoyY2fL6ITkBEp+bXYzAoPUkVtOXMZNXDcvfu3Vi2bBmSkpLQs2dPbNy4Ec8//zxee+01hIaG1tp/9uzZuPXWW22PzWYzHnroIQwZMqQtyyYionZE9TuE169fj9GjR+Paa6+19SpDQ0ORnp5e5/7e3t4IDAy0fWVkZKCsrAyjRo1q48qJiKi9ULVnaTKZcPz4cdx444122/v164cjR4406hybN2/GFVdccdnZTEaj0W7WqxACXl5etu+by3qsK96kzLZpE9umTWyb81M1LIuLi2GxWBAQEGC3PSAgAIWFhQ0eX1BQgJ9//hn33XffZfdLTU3FmjVrbI9jYmKQkpLSqOnCjRER4boLT7Nt2sS2aRPb5rxUv2YJ1P0XR2P+Ctm6dSt8fHwwePDgy+43ZcoUTJr05+wq67lzc3NhMpmaWK19jREREcjOznaJe9ouxrZpE9umTWybevR6vfPfZ+nv7w+dTlerF1lUVFSrt3kpKSW2bNmCESNGQK+/fDMMBgMM9azw4Ig3T0rplP8ROALbpk1smzaxbc5L1Qk+er0esbGxOHDggN32AwcOoGfPnpc99uDBg8jOzsbo0aNbs0QiIiL1Z8NOmjQJmzZtwubNm3H69GksW7YMeXl5GDt2LABg5cqVWLRoUa3jNm/ejLi4OERHR7d1yURE1M6ofs1y2LBhKCkpwdq1a1FQUIDOnTtjwYIFtjHkgoIC5OXl2R1TXl6OvXv3YtasWSpUTERE7Y3qYQkA48ePx/jx4+t8Ljk5udY2b29vfPjhh61dFhEREQAnGIYlIiJydgxLIiKiBjAsiYiIGsCwJCIiaoBTTPBRS0OLGbT1eZwR26ZNbJs2sW1tr7F1CanlJRWIiIjaAIdhW6CiogKPPPIIKioq1C7F4dg2bWLbtIltc34MyxaQUiIzM1PT6x3Wh23TJrZNm9g258ewJCIiagDDkoiIqAEMyxYwGAyYNm1avR//pWVsmzaxbdrEtjk/zoYlIiJqAHuWREREDWBYEhERNYBhSURE1ACGJRERUQOcc7E+DdiwYQPS0tJQWFiIqKgozJo1C/Hx8WqX1SSrV6/GmjVr7LYFBARgyZIlAJSbiT/99FNs2rQJpaWliIuLw+23347OnTurUe5lHTx4EGlpacjMzERBQQHmz5+PwYMH255vTFuMRiNWrFiBXbt2obq6Gn379kVSUhJCQkLUaJJNQ21bvHgxtm3bZndMXFwcnnvuOdtjZ21bamoqvv/+e5w5cwbu7u7o0aMHZs6ciY4dO9r20ep715i2afW9S09PR3p6OnJzcwEAUVFRmDZtGhISEgBo9z27HM6GbYbdu3fjzTffRFJSEnr27ImNGzdi06ZNeO211xAaGqp2eY22evVq7N27F48//rhtm06ng7+/PwDg888/R2pqKu655x5ERkbis88+w6FDh7Bw4UJ4eXmpVXad9u/fjyNHjiAmJgavvPJKrUBpTFuWLFmCffv24Z577oGfnx+WL1+O0tJSpKSkQKdTbxCmobYtXrwYRUVFuOeee2zb9Ho9fH19bY+dtW3PPfcchg8fjm7dusFsNuOTTz7ByZMn8eqrr8LT0xOAdt+7xrRNq+/djz/+CJ1Oh4iICADAtm3bkJaWhhdffBGdO3fW7Ht2WZKabMGCBfI///mP3bb7779ffvTRRypV1DyrVq2S8+fPr/M5i8Ui77jjDpmammrbVl1dLW+77TaZnp7eRhU2z/Tp0+XevXttjxvTlrKyMnnLLbfIXbt22fbJz8+XN910k9y/f39bld6gS9smpZSLFi2SKSkp9R6jlbZJKWVRUZGcPn26/P3336WUrvXeXdo2KV3rvZs1a5bctGmTS71nF3PC+HZuJpMJx48fR//+/e229+vXD0eOHFGpqubLzs7GXXfdheTkZCxcuBDnz58HAOTk5KCwsNCunQaDAb1799ZcOxvTluPHj8NsNqNfv362fYKDgxEdHY0//vijzWtuqoMHDyIpKQnz5s3DO++8g6KiIttzWmpbeXk5ANh6Vq703l3aNiutv3cWiwW7du1CVVUVevTo4VLv2cV4zbKJiouLYbFYEBAQYLc9ICAAhYWF6hTVTHFxcUhOTkbHjh1RWFiIzz77DI899hheffVVW1vqamdeXp4K1TZfY9pSWFhYa/jLuo+zv68JCQkYOnQoQkNDkZOTg1WrVuHpp5/GCy+8AIPBoJm2SSnxwQcfoFevXoiOjgbgOu9dXW0DtP3enTx5Eo8++iiMRiM8PT0xf/58REVF2QJR6+/ZpRiWzSSEaNQ2Z2a9GA8A0dHR6NGjB+69915s27YNcXFxAGq3SWr4Endz2qKF9g4bNsz2fXR0NLp164Z77rkHP/30E66++up6j3O2ti1duhQnT57E008/Xes5rb939bVNy+9dx44d8dJLL6GsrAx79+7F4sWL8dRTT9me1/p7dikOwzaRv78/dDpdrb9+ioqKav0lpTWenp6Ijo7GuXPnEBgYCAC12llcXKy5djamLYGBgTCZTCgtLa21j/V4rQgKCkJYWBjOnTsHQBtt++9//4t9+/bhiSeesJsN6QrvXX1tq4uW3ju9Xo+IiAh069YNM2bMQNeuXfHVV1+5xHtWF4ZlE+n1esTGxuLAgQN22w8cOICePXuqVJVjGI1GnDlzBkFBQQgPD0dgYKBdO00mEw4ePKi5djamLbGxsXBzc7Pbp6CgACdPnkSPHj3avOaWKCkpQX5+PoKCggA4d9uklFi6dCn27t2Lf/3rXwgPD7d7XsvvXUNtq4uW3rtLSSlhNBo1/Z5dDodhm2HSpEl48803ERsbix49emDjxo3Iy8vD2LFj1S6tSZYvX46BAwciNDQURUVFWLt2LSoqKpCYmAghBCZOnIjU1FRERkYiIiICqamp8PDwwF/+8he1S6+lsrIS2dnZtsc5OTnIysqCr68vQkNDG2yLt7c3Ro8ejRUrVsDPzw++vr5YsWIFoqOj7SYhqOFybfP19cXq1asxZMgQBAYGIjc3Fx9//DH8/Pxst5c4c9uWLl2KnTt34uGHH4aXl5etN+Lt7Q13d/dG/XforO1rqG2VlZWafe9WrlyJhIQEhISEoLKyErt27cLvv/+ORx99VNPv2eXwPstmsi5KUFBQgM6dO+O2225D79691S6rSRYuXIhDhw6huLgY/v7+iIuLwy233IKoqCgAf95YvHHjRpSVlaF79+64/fbb7SYoOIvff//d7nqJVWJiIpKTkxvVlurqanz44YfYuXOn3U3Sat87e7m23XHHHXjppZeQmZmJsrIyBAUFoU+fPrj55pvt6nbWtt100011br/nnnswcuRIAI3779AZ29dQ26qrqzX73r399tv47bffUFBQAG9vb3Tp0gWTJ0+2BZ1W37PLYVgSERE1gNcsiYiIGsCwJCIiagDDkoiIqAEMSyIiogYwLImIiBrAsCQiImoAw5KIiKgBDEsiIqIGMCyJiIgawLAkIiJqAMOSiIioAQxLIiKiBvx/ddkZunHAyL0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt  \n",
    "# retrieve performance metrics\n",
    "results = optimized_xgb_0.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "    \n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Test')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    " # plot classification error\n",
    "#fig, ax = pyplot.subplots(figsize=(5,5))\n",
    "#ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "#ax.plot(x_axis, results['validation_1']['error'], label='Test')\n",
    "#ax.legend()\n",
    "    \n",
    "#pyplot.ylabel('Classification Error')\n",
    "#pyplot.title('XGBoost Classification Error')\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost baseline model r2_score 0.6806 with a standard deviation of 0.0719\n",
      "XGBoost optimized model r2_score 0.7180 with a standard deviation of 0.0529\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_reg, X, Y, cv=10, scoring=\"r2\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"r2\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb.joblib']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_reg, \"OUTPUT/xgb_reg.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"OUTPUT/optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.670702     0.100533\n",
      "1                    TP        20.700000     2.710064\n",
      "2                    TN        98.200000     1.988858\n",
      "3                    FP         2.300000     1.946507\n",
      "4                    FN        12.700000     3.128720\n",
      "5              Accuracy         0.887970     0.032074\n",
      "6             Precision         0.900579     0.086731\n",
      "7           Sensitivity         0.620791     0.087166\n",
      "8           Specificity         0.977130     0.019380\n",
      "9              F1 score         0.732763     0.081611\n",
      "10  F1 score (weighted)         0.880000     0.035900\n",
      "11     F1 score (macro)         0.830934     0.050730\n",
      "12    Balanced Accuracy         0.798960     0.048762\n",
      "13                  MCC         0.685103     0.097150\n",
      "14                  NPV         0.885870     0.026080\n",
      "15              ROC_AUC         0.798960     0.048762\n",
      "CPU times: user 1.31 s, sys: 5.22 s, total: 6.52 s\n",
      "Wall time: 332 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_reg = KNeighborsRegressor()\n",
    "    \n",
    "    knn_reg.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=4)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsRegressor(**param_grid, n_jobs=4)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:37:00,140] A new study created in memory with name: KNNregressor\n",
      "[I 2023-12-20 08:37:00,317] Trial 0 finished with value: 0.5824926560250308 and parameters: {'n_neighbors': 15, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 52}. Best is trial 0 with value: 0.5824926560250308.\n",
      "[I 2023-12-20 08:37:00,477] Trial 1 finished with value: 0.5835275969448117 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 1 with value: 0.5835275969448117.\n",
      "[I 2023-12-20 08:37:00,638] Trial 2 finished with value: 0.5620217086819621 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 1 with value: 0.5835275969448117.\n",
      "[I 2023-12-20 08:37:00,795] Trial 3 finished with value: 0.5245874010146381 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 1 with value: 0.5835275969448117.\n",
      "[I 2023-12-20 08:37:00,946] Trial 4 finished with value: 0.6565379696966964 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 4 with value: 0.6565379696966964.\n",
      "[I 2023-12-20 08:37:01,193] Trial 5 finished with value: 0.6741221812805664 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 5 with value: 0.6741221812805664.\n",
      "[I 2023-12-20 08:37:01,437] Trial 6 finished with value: 0.6922131213173545 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:01,594] Trial 7 finished with value: 0.5245874010146381 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:01,754] Trial 8 finished with value: 0.6233425726962329 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 74}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:02,013] Trial 9 finished with value: 0.6578067336938933 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 92}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:02,272] Trial 10 finished with value: 0.5717213532950877 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:02,534] Trial 11 finished with value: 0.6654631364469269 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:02,800] Trial 12 finished with value: 0.6741221812805664 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:03,068] Trial 13 finished with value: 0.6879753994415637 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:03,333] Trial 14 finished with value: 0.6779348750448232 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:03,598] Trial 15 finished with value: 0.6434824584804015 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 90}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:03,866] Trial 16 finished with value: 0.6828675725870611 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:04,132] Trial 17 finished with value: 0.6578067336938933 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:04,391] Trial 18 finished with value: 0.5717213532950877 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:04,555] Trial 19 finished with value: 0.6704879839583573 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 90}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:04,825] Trial 20 finished with value: 0.6578067336938933 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:05,092] Trial 21 finished with value: 0.6879753994415637 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:05,358] Trial 22 finished with value: 0.6879753994415637 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:05,627] Trial 23 finished with value: 0.671140465506931 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:05,890] Trial 24 finished with value: 0.6922131213173545 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:06,153] Trial 25 finished with value: 0.6892330546915477 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:06,415] Trial 26 finished with value: 0.6922131213173545 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:06,683] Trial 27 finished with value: 0.6922131213173545 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:06,856] Trial 28 finished with value: 0.5387844550591205 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:07,028] Trial 29 finished with value: 0.5971270495344527 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:07,201] Trial 30 finished with value: 0.6565379696966964 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:07,472] Trial 31 finished with value: 0.6922131213173545 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:07,736] Trial 32 finished with value: 0.6922131213173545 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:07,993] Trial 33 finished with value: 0.6828675725870611 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:08,249] Trial 34 finished with value: 0.6779348750448232 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:08,416] Trial 35 finished with value: 0.6519087808366949 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:08,601] Trial 36 finished with value: 0.6892330546915477 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:08,769] Trial 37 finished with value: 0.5481918032347904 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:08,930] Trial 38 finished with value: 0.6199013758908372 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 58}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:09,134] Trial 39 finished with value: 0.6391084138903821 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:09,318] Trial 40 finished with value: 0.5924405008595937 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:09,492] Trial 41 finished with value: 0.6922131213173545 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:09,684] Trial 42 finished with value: 0.6922131213173545 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:09,875] Trial 43 finished with value: 0.6879753994415637 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:10,136] Trial 44 finished with value: 0.6892330546915477 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:10,398] Trial 45 finished with value: 0.6741221812805664 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:10,561] Trial 46 finished with value: 0.6704879839583573 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 64}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:10,824] Trial 47 finished with value: 0.6735398234499284 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:11,086] Trial 48 finished with value: 0.5328319329084813 and parameters: {'n_neighbors': 21, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:11,348] Trial 49 finished with value: 0.6892330546915477 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 6 with value: 0.6922131213173545.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6922\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 86\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNregressor\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.698571\n",
      "1                    TP   44.000000\n",
      "2                    TN  198.000000\n",
      "3                    FP    4.000000\n",
      "4                    FN   22.000000\n",
      "5              Accuracy    0.902985\n",
      "6             Precision    0.916667\n",
      "7           Sensitivity    0.666667\n",
      "8           Specificity    0.980200\n",
      "9              F1 score    0.771930\n",
      "10  F1 score (weighted)    0.897395\n",
      "11     F1 score (macro)    0.855159\n",
      "12    Balanced Accuracy    0.823432\n",
      "13                  MCC    0.726824\n",
      "14                  NPV    0.900000\n",
      "15              ROC_AUC    0.823432\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_knn_0)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_knn_0_cat = np.where(((y_pred_knn_0 >= 2) | (y_pred_knn_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_knn_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_knn_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_knn_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:37:11,576] Trial 50 finished with value: 0.6186713341630184 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 48}. Best is trial 6 with value: 0.6922131213173545.\n",
      "[I 2023-12-20 08:37:11,847] Trial 51 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:12,104] Trial 52 finished with value: 0.6854035803768378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:12,366] Trial 53 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:12,628] Trial 54 finished with value: 0.6912413205695186 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:12,892] Trial 55 finished with value: 0.6784882346862461 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:13,155] Trial 56 finished with value: 0.6693131638214521 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:13,396] Trial 57 finished with value: 0.6854035803768378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:13,652] Trial 58 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:13,817] Trial 59 finished with value: 0.6582113888810881 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:13,977] Trial 60 finished with value: 0.6736440231202581 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:14,189] Trial 61 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:14,394] Trial 62 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:14,654] Trial 63 finished with value: 0.6784882346862461 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:14,922] Trial 64 finished with value: 0.6912413205695186 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:15,189] Trial 65 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:15,459] Trial 66 finished with value: 0.6854035803768378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:15,730] Trial 67 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:16,000] Trial 68 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:16,172] Trial 69 finished with value: 0.6813473509019634 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:16,442] Trial 70 finished with value: 0.6784882346862461 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:16,714] Trial 71 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:16,985] Trial 72 finished with value: 0.6912413205695186 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:17,255] Trial 73 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:17,523] Trial 74 finished with value: 0.6854035803768378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:17,788] Trial 75 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:18,052] Trial 76 finished with value: 0.6854035803768378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:18,311] Trial 77 finished with value: 0.6912413205695186 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:18,575] Trial 78 finished with value: 0.6693131638214521 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:18,837] Trial 79 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:19,011] Trial 80 finished with value: 0.6052197712952914 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:19,282] Trial 81 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:19,555] Trial 82 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:19,827] Trial 83 finished with value: 0.620466467377071 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:20,100] Trial 84 finished with value: 0.6912413205695186 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:20,370] Trial 85 finished with value: 0.6854035803768378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:20,538] Trial 86 finished with value: 0.6911508443019394 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 81}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:20,780] Trial 87 finished with value: 0.6912413205695186 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:20,986] Trial 88 finished with value: 0.6784882346862461 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:21,237] Trial 89 finished with value: 0.6169284874914344 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:21,470] Trial 90 finished with value: 0.543808343785492 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:21,720] Trial 91 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:21,948] Trial 92 finished with value: 0.6912413205695186 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:22,192] Trial 93 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:22,457] Trial 94 finished with value: 0.5691782922965267 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:22,702] Trial 95 finished with value: 0.6912413205695186 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:22,966] Trial 96 finished with value: 0.699183999496056 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:23,231] Trial 97 finished with value: 0.649296436899654 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:23,394] Trial 98 finished with value: 0.6911508443019394 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:23,658] Trial 99 finished with value: 0.6784882346862461 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 51 with value: 0.699183999496056.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6992\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 72\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.698571    0.677845\n",
      "1                    TP   44.000000   44.000000\n",
      "2                    TN  198.000000  197.000000\n",
      "3                    FP    4.000000    3.000000\n",
      "4                    FN   22.000000   24.000000\n",
      "5              Accuracy    0.902985    0.899254\n",
      "6             Precision    0.916667    0.936170\n",
      "7           Sensitivity    0.666667    0.647059\n",
      "8           Specificity    0.980200    0.985000\n",
      "9              F1 score    0.771930    0.765217\n",
      "10  F1 score (weighted)    0.897395    0.892568\n",
      "11     F1 score (macro)    0.855159    0.850542\n",
      "12    Balanced Accuracy    0.823432    0.816029\n",
      "13                  MCC    0.726824    0.723239\n",
      "14                  NPV    0.900000    0.891400\n",
      "15              ROC_AUC    0.823432    0.816029\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_knn_1)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_knn_1_cat = np.where(((y_pred_knn_1 >= 2) | (y_pred_knn_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_knn_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_knn_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_knn_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:37:23,891] Trial 100 finished with value: 0.6666300225168657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 83}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:24,157] Trial 101 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:24,422] Trial 102 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:24,678] Trial 103 finished with value: 0.6757839220832663 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:24,933] Trial 104 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:25,188] Trial 105 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:25,442] Trial 106 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:25,706] Trial 107 finished with value: 0.6757839220832663 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:25,962] Trial 108 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:26,227] Trial 109 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:26,494] Trial 110 finished with value: 0.6281089990053104 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:26,759] Trial 111 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:27,025] Trial 112 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:27,291] Trial 113 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:27,557] Trial 114 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:27,822] Trial 115 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:28,088] Trial 116 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:28,355] Trial 117 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:28,521] Trial 118 finished with value: 0.64423913237366 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:28,787] Trial 119 finished with value: 0.6757839220832663 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:29,053] Trial 120 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:29,319] Trial 121 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:29,586] Trial 122 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:29,853] Trial 123 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:30,119] Trial 124 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:30,384] Trial 125 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:30,553] Trial 126 finished with value: 0.5911137687617289 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 74}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:30,774] Trial 127 finished with value: 0.6757839220832663 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:31,001] Trial 128 finished with value: 0.5920776586163539 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:31,229] Trial 129 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:31,444] Trial 130 finished with value: 0.6432728462535773 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:31,696] Trial 131 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:31,956] Trial 132 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:32,209] Trial 133 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:32,470] Trial 134 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:32,724] Trial 135 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:32,913] Trial 136 finished with value: 0.6757839220832663 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:33,068] Trial 137 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:33,212] Trial 138 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:33,361] Trial 139 finished with value: 0.6789804070439522 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:33,525] Trial 140 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:33,669] Trial 141 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:33,824] Trial 142 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:33,968] Trial 143 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:34,122] Trial 144 finished with value: 0.6757839220832663 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:34,266] Trial 145 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:34,421] Trial 146 finished with value: 0.6920098907843011 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:34,580] Trial 147 finished with value: 0.6789804070439522 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 79}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:34,846] Trial 148 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:35,112] Trial 149 finished with value: 0.6833308265510308 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 51 with value: 0.699183999496056.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.6992\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 72\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.698571    0.677845    0.679705\n",
      "1                    TP   44.000000   44.000000   45.000000\n",
      "2                    TN  198.000000  197.000000  194.000000\n",
      "3                    FP    4.000000    3.000000    6.000000\n",
      "4                    FN   22.000000   24.000000   23.000000\n",
      "5              Accuracy    0.902985    0.899254    0.891791\n",
      "6             Precision    0.916667    0.936170    0.882353\n",
      "7           Sensitivity    0.666667    0.647059    0.661765\n",
      "8           Specificity    0.980200    0.985000    0.970000\n",
      "9              F1 score    0.771930    0.765217    0.756303\n",
      "10  F1 score (weighted)    0.897395    0.892568    0.886268\n",
      "11     F1 score (macro)    0.855159    0.850542    0.843379\n",
      "12    Balanced Accuracy    0.823432    0.816029    0.815882\n",
      "13                  MCC    0.726824    0.723239    0.700341\n",
      "14                  NPV    0.900000    0.891400    0.894000\n",
      "15              ROC_AUC    0.823432    0.816029    0.815882\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_knn_2)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_knn_2_cat = np.where(((y_pred_knn_2 >= 2) | (y_pred_knn_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_knn_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_knn_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_knn_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:37:35,461] Trial 150 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:35,725] Trial 151 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:35,991] Trial 152 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:36,257] Trial 153 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:36,523] Trial 154 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:36,790] Trial 155 finished with value: 0.6597400482363825 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:37,055] Trial 156 finished with value: 0.5381501591536215 and parameters: {'n_neighbors': 17, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:37,321] Trial 157 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:37,586] Trial 158 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:37,848] Trial 159 finished with value: 0.6597400482363825 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:38,114] Trial 160 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:38,381] Trial 161 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:38,647] Trial 162 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:38,912] Trial 163 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:39,179] Trial 164 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:39,446] Trial 165 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:39,713] Trial 166 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:39,980] Trial 167 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:40,143] Trial 168 finished with value: 0.6644711673083143 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:40,402] Trial 169 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:40,657] Trial 170 finished with value: 0.6597400482363825 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:40,924] Trial 171 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:41,190] Trial 172 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:41,458] Trial 173 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:41,725] Trial 174 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:41,988] Trial 175 finished with value: 0.5532208703041464 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:42,145] Trial 176 finished with value: 0.6550191080144312 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 83}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:42,408] Trial 177 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:42,674] Trial 178 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:42,940] Trial 179 finished with value: 0.6379511809897868 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:43,206] Trial 180 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:43,474] Trial 181 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 95}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:43,740] Trial 182 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:44,007] Trial 183 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:44,275] Trial 184 finished with value: 0.6355263199474769 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:44,542] Trial 185 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:44,810] Trial 186 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:44,986] Trial 187 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:45,139] Trial 188 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:45,293] Trial 189 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:45,438] Trial 190 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:45,584] Trial 191 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:45,729] Trial 192 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:45,886] Trial 193 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:46,042] Trial 194 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:46,187] Trial 195 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:46,335] Trial 196 finished with value: 0.646237056135689 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 60}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:46,537] Trial 197 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:46,789] Trial 198 finished with value: 0.6718463605355142 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:47,047] Trial 199 finished with value: 0.6659320853834826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 51 with value: 0.699183999496056.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.6992\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 72\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.698571    0.677845    0.679705    0.719101\n",
      "1                    TP   44.000000   44.000000   45.000000   44.000000\n",
      "2                    TN  198.000000  197.000000  194.000000  193.000000\n",
      "3                    FP    4.000000    3.000000    6.000000    6.000000\n",
      "4                    FN   22.000000   24.000000   23.000000   25.000000\n",
      "5              Accuracy    0.902985    0.899254    0.891791    0.884328\n",
      "6             Precision    0.916667    0.936170    0.882353    0.880000\n",
      "7           Sensitivity    0.666667    0.647059    0.661765    0.637681\n",
      "8           Specificity    0.980200    0.985000    0.970000    0.969800\n",
      "9              F1 score    0.771930    0.765217    0.756303    0.739496\n",
      "10  F1 score (weighted)    0.897395    0.892568    0.886268    0.877729\n",
      "11     F1 score (macro)    0.855159    0.850542    0.843379    0.832578\n",
      "12    Balanced Accuracy    0.823432    0.816029    0.815882    0.803765\n",
      "13                  MCC    0.726824    0.723239    0.700341    0.681877\n",
      "14                  NPV    0.900000    0.891400    0.894000    0.885300\n",
      "15              ROC_AUC    0.823432    0.816029    0.815882    0.803765\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_knn_3)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_knn_3_cat = np.where(((y_pred_knn_3 >= 2) | (y_pred_knn_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_knn_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_knn_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_knn_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:37:47,399] Trial 200 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:47,637] Trial 201 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:47,846] Trial 202 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:48,024] Trial 203 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:48,216] Trial 204 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:48,491] Trial 205 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:48,671] Trial 206 finished with value: 0.6528746594997029 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:48,951] Trial 207 finished with value: 0.6360345441871369 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:49,232] Trial 208 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:49,513] Trial 209 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:49,794] Trial 210 finished with value: 0.6646623438907399 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:50,075] Trial 211 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:50,359] Trial 212 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:50,641] Trial 213 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:50,923] Trial 214 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:51,206] Trial 215 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:51,488] Trial 216 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:51,769] Trial 217 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:52,051] Trial 218 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:52,332] Trial 219 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:52,615] Trial 220 finished with value: 0.5515115790482509 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:52,895] Trial 221 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:53,159] Trial 222 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:53,421] Trial 223 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:53,692] Trial 224 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:53,973] Trial 225 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:54,253] Trial 226 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:54,433] Trial 227 finished with value: 0.6528746594997029 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:54,718] Trial 228 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:55,000] Trial 229 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:55,283] Trial 230 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:55,568] Trial 231 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:55,839] Trial 232 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:56,112] Trial 233 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:56,386] Trial 234 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:56,666] Trial 235 finished with value: 0.6360345441871369 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:56,936] Trial 236 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:57,118] Trial 237 finished with value: 0.6569104897659297 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:57,359] Trial 238 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:57,548] Trial 239 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:57,736] Trial 240 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:57,928] Trial 241 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:58,148] Trial 242 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:58,367] Trial 243 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:58,616] Trial 244 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:58,814] Trial 245 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:59,043] Trial 246 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:59,271] Trial 247 finished with value: 0.6640506679392918 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:59,510] Trial 248 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:37:59,742] Trial 249 finished with value: 0.6581905080749869 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 51 with value: 0.699183999496056.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.6992\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 72\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698571    0.677845    0.679705    0.719101   \n",
      "1                    TP   44.000000   44.000000   45.000000   44.000000   \n",
      "2                    TN  198.000000  197.000000  194.000000  193.000000   \n",
      "3                    FP    4.000000    3.000000    6.000000    6.000000   \n",
      "4                    FN   22.000000   24.000000   23.000000   25.000000   \n",
      "5              Accuracy    0.902985    0.899254    0.891791    0.884328   \n",
      "6             Precision    0.916667    0.936170    0.882353    0.880000   \n",
      "7           Sensitivity    0.666667    0.647059    0.661765    0.637681   \n",
      "8           Specificity    0.980200    0.985000    0.970000    0.969800   \n",
      "9              F1 score    0.771930    0.765217    0.756303    0.739496   \n",
      "10  F1 score (weighted)    0.897395    0.892568    0.886268    0.877729   \n",
      "11     F1 score (macro)    0.855159    0.850542    0.843379    0.832578   \n",
      "12    Balanced Accuracy    0.823432    0.816029    0.815882    0.803765   \n",
      "13                  MCC    0.726824    0.723239    0.700341    0.681877   \n",
      "14                  NPV    0.900000    0.891400    0.894000    0.885300   \n",
      "15              ROC_AUC    0.823432    0.816029    0.815882    0.803765   \n",
      "\n",
      "          Set4  \n",
      "0     0.713544  \n",
      "1    47.000000  \n",
      "2   198.000000  \n",
      "3     4.000000  \n",
      "4    19.000000  \n",
      "5     0.914179  \n",
      "6     0.921569  \n",
      "7     0.712121  \n",
      "8     0.980200  \n",
      "9     0.803419  \n",
      "10    0.910214  \n",
      "11    0.874263  \n",
      "12    0.846160  \n",
      "13    0.759870  \n",
      "14    0.912400  \n",
      "15    0.846160  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_knn_4)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_knn_4_cat = np.where(((y_pred_knn_4 >= 2) | (y_pred_knn_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_knn_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_knn_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_knn_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:38:00,036] Trial 250 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:00,296] Trial 251 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:00,556] Trial 252 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:00,816] Trial 253 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:01,077] Trial 254 finished with value: 0.5897824208418088 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:01,339] Trial 255 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:01,597] Trial 256 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:01,755] Trial 257 finished with value: 0.6683208411333379 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 90}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:02,019] Trial 258 finished with value: 0.6535337489428774 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:02,286] Trial 259 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:02,559] Trial 260 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:02,823] Trial 261 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:03,102] Trial 262 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:03,388] Trial 263 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:03,570] Trial 264 finished with value: 0.6683208411333379 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 58}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:03,846] Trial 265 finished with value: 0.6710811031512393 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:04,071] Trial 266 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:04,341] Trial 267 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:04,603] Trial 268 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:04,871] Trial 269 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:05,155] Trial 270 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:05,439] Trial 271 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:05,722] Trial 272 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:06,006] Trial 273 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:06,290] Trial 274 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:06,572] Trial 275 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:06,850] Trial 276 finished with value: 0.6043310185363563 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:07,129] Trial 277 finished with value: 0.6535337489428774 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 98}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:07,411] Trial 278 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:07,594] Trial 279 finished with value: 0.6008923698047532 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:07,878] Trial 280 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:08,163] Trial 281 finished with value: 0.5437703821006123 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:08,447] Trial 282 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:08,709] Trial 283 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:08,895] Trial 284 finished with value: 0.6710811031512393 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:09,048] Trial 285 finished with value: 0.6683208411333379 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 77}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:09,226] Trial 286 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:09,425] Trial 287 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:09,656] Trial 288 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:09,901] Trial 289 finished with value: 0.5637404739453017 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:10,101] Trial 290 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:10,332] Trial 291 finished with value: 0.6478175093170881 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:10,543] Trial 292 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:10,753] Trial 293 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:10,976] Trial 294 finished with value: 0.6164173450541759 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:11,177] Trial 295 finished with value: 0.6710811031512393 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:11,398] Trial 296 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:11,608] Trial 297 finished with value: 0.6535337489428774 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:11,860] Trial 298 finished with value: 0.6823427977103319 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:12,113] Trial 299 finished with value: 0.6779888161277619 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 51 with value: 0.699183999496056.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.6992\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 72\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698571    0.677845    0.679705    0.719101   \n",
      "1                    TP   44.000000   44.000000   45.000000   44.000000   \n",
      "2                    TN  198.000000  197.000000  194.000000  193.000000   \n",
      "3                    FP    4.000000    3.000000    6.000000    6.000000   \n",
      "4                    FN   22.000000   24.000000   23.000000   25.000000   \n",
      "5              Accuracy    0.902985    0.899254    0.891791    0.884328   \n",
      "6             Precision    0.916667    0.936170    0.882353    0.880000   \n",
      "7           Sensitivity    0.666667    0.647059    0.661765    0.637681   \n",
      "8           Specificity    0.980200    0.985000    0.970000    0.969800   \n",
      "9              F1 score    0.771930    0.765217    0.756303    0.739496   \n",
      "10  F1 score (weighted)    0.897395    0.892568    0.886268    0.877729   \n",
      "11     F1 score (macro)    0.855159    0.850542    0.843379    0.832578   \n",
      "12    Balanced Accuracy    0.823432    0.816029    0.815882    0.803765   \n",
      "13                  MCC    0.726824    0.723239    0.700341    0.681877   \n",
      "14                  NPV    0.900000    0.891400    0.894000    0.885300   \n",
      "15              ROC_AUC    0.823432    0.816029    0.815882    0.803765   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.713544    0.685845  \n",
      "1    47.000000   48.000000  \n",
      "2   198.000000  197.000000  \n",
      "3     4.000000    3.000000  \n",
      "4    19.000000   20.000000  \n",
      "5     0.914179    0.914179  \n",
      "6     0.921569    0.941176  \n",
      "7     0.712121    0.705882  \n",
      "8     0.980200    0.985000  \n",
      "9     0.803419    0.806723  \n",
      "10    0.910214    0.909798  \n",
      "11    0.874263    0.875783  \n",
      "12    0.846160    0.845441  \n",
      "13    0.759870    0.765876  \n",
      "14    0.912400    0.907800  \n",
      "15    0.846160    0.845441  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_knn_5)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_knn_5_cat = np.where(((y_pred_knn_5 >= 2) | (y_pred_knn_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_knn_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_knn_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_knn_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:38:12,438] Trial 300 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:12,610] Trial 301 finished with value: 0.649168435142831 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:12,877] Trial 302 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:13,134] Trial 303 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:13,402] Trial 304 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:13,672] Trial 305 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:13,942] Trial 306 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:14,211] Trial 307 finished with value: 0.5557047540257789 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:14,383] Trial 308 finished with value: 0.608481301362392 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:14,645] Trial 309 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:14,913] Trial 310 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:15,182] Trial 311 finished with value: 0.6634483597684496 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:15,450] Trial 312 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:15,708] Trial 313 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:15,978] Trial 314 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:16,236] Trial 315 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 89}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:16,492] Trial 316 finished with value: 0.627420091641734 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:16,760] Trial 317 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:17,028] Trial 318 finished with value: 0.646262053536206 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:17,287] Trial 319 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:17,557] Trial 320 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:17,826] Trial 321 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:18,096] Trial 322 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:18,364] Trial 323 finished with value: 0.6634483597684496 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:18,634] Trial 324 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:18,807] Trial 325 finished with value: 0.6497826823893412 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:19,075] Trial 326 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:19,341] Trial 327 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:19,601] Trial 328 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:19,869] Trial 329 finished with value: 0.5953791961084158 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:20,137] Trial 330 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:20,302] Trial 331 finished with value: 0.6497826823893412 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 57}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:20,491] Trial 332 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:20,642] Trial 333 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:20,803] Trial 334 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:20,951] Trial 335 finished with value: 0.623755614313432 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:21,100] Trial 336 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:21,250] Trial 337 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 70}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:21,399] Trial 338 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:21,548] Trial 339 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:21,697] Trial 340 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:21,857] Trial 341 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:22,098] Trial 342 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:22,364] Trial 343 finished with value: 0.5716336536443567 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:22,638] Trial 344 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:22,910] Trial 345 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 74}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:23,084] Trial 346 finished with value: 0.649168435142831 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:23,356] Trial 347 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 72}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:23,629] Trial 348 finished with value: 0.660127993524993 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 51 with value: 0.699183999496056.\n",
      "[I 2023-12-20 08:38:23,901] Trial 349 finished with value: 0.6622002837093166 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 51 with value: 0.699183999496056.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.6992\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 72\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698571    0.677845    0.679705    0.719101   \n",
      "1                    TP   44.000000   44.000000   45.000000   44.000000   \n",
      "2                    TN  198.000000  197.000000  194.000000  193.000000   \n",
      "3                    FP    4.000000    3.000000    6.000000    6.000000   \n",
      "4                    FN   22.000000   24.000000   23.000000   25.000000   \n",
      "5              Accuracy    0.902985    0.899254    0.891791    0.884328   \n",
      "6             Precision    0.916667    0.936170    0.882353    0.880000   \n",
      "7           Sensitivity    0.666667    0.647059    0.661765    0.637681   \n",
      "8           Specificity    0.980200    0.985000    0.970000    0.969800   \n",
      "9              F1 score    0.771930    0.765217    0.756303    0.739496   \n",
      "10  F1 score (weighted)    0.897395    0.892568    0.886268    0.877729   \n",
      "11     F1 score (macro)    0.855159    0.850542    0.843379    0.832578   \n",
      "12    Balanced Accuracy    0.823432    0.816029    0.815882    0.803765   \n",
      "13                  MCC    0.726824    0.723239    0.700341    0.681877   \n",
      "14                  NPV    0.900000    0.891400    0.894000    0.885300   \n",
      "15              ROC_AUC    0.823432    0.816029    0.815882    0.803765   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.713544    0.685845    0.724100  \n",
      "1    47.000000   48.000000   43.000000  \n",
      "2   198.000000  197.000000  194.000000  \n",
      "3     4.000000    3.000000    6.000000  \n",
      "4    19.000000   20.000000   25.000000  \n",
      "5     0.914179    0.914179    0.884328  \n",
      "6     0.921569    0.941176    0.877551  \n",
      "7     0.712121    0.705882    0.632353  \n",
      "8     0.980200    0.985000    0.970000  \n",
      "9     0.803419    0.806723    0.735043  \n",
      "10    0.910214    0.909798    0.877559  \n",
      "11    0.874263    0.875783    0.830529  \n",
      "12    0.846160    0.845441    0.801176  \n",
      "13    0.759870    0.765876    0.678110  \n",
      "14    0.912400    0.907800    0.885800  \n",
      "15    0.846160    0.845441    0.801176  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_knn_6)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_knn_6_cat = np.where(((y_pred_knn_6 >= 2) | (y_pred_knn_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_knn_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_knn_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_knn_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:38:24,276] Trial 350 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:24,547] Trial 351 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:24,818] Trial 352 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:25,089] Trial 353 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:25,351] Trial 354 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:25,621] Trial 355 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:25,882] Trial 356 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:26,148] Trial 357 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:26,421] Trial 358 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:26,692] Trial 359 finished with value: 0.7160247874379679 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:26,961] Trial 360 finished with value: 0.7091762765132972 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:27,224] Trial 361 finished with value: 0.7091762765132972 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:27,487] Trial 362 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:27,751] Trial 363 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:28,014] Trial 364 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:28,180] Trial 365 finished with value: 0.6817097167417727 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:28,449] Trial 366 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:28,712] Trial 367 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:28,968] Trial 368 finished with value: 0.6635945371445322 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:29,225] Trial 369 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:29,483] Trial 370 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:29,748] Trial 371 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:30,022] Trial 372 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:30,295] Trial 373 finished with value: 0.6913300525537635 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:30,568] Trial 374 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:30,843] Trial 375 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:31,115] Trial 376 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:31,390] Trial 377 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:31,664] Trial 378 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:31,938] Trial 379 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:32,211] Trial 380 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:32,483] Trial 381 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:32,653] Trial 382 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:32,801] Trial 383 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:32,950] Trial 384 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:33,099] Trial 385 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:33,247] Trial 386 finished with value: 0.6913300525537635 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:33,397] Trial 387 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:33,546] Trial 388 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:33,695] Trial 389 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:33,845] Trial 390 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:33,994] Trial 391 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:34,150] Trial 392 finished with value: 0.6870983116287483 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:34,420] Trial 393 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:34,694] Trial 394 finished with value: 0.6635945371445322 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:34,969] Trial 395 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:35,144] Trial 396 finished with value: 0.6870983116287483 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 77}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:35,413] Trial 397 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:35,681] Trial 398 finished with value: 0.7019824182060568 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:35,940] Trial 399 finished with value: 0.6981184829938616 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7160\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 79\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698571    0.677845    0.679705    0.719101   \n",
      "1                    TP   44.000000   44.000000   45.000000   44.000000   \n",
      "2                    TN  198.000000  197.000000  194.000000  193.000000   \n",
      "3                    FP    4.000000    3.000000    6.000000    6.000000   \n",
      "4                    FN   22.000000   24.000000   23.000000   25.000000   \n",
      "5              Accuracy    0.902985    0.899254    0.891791    0.884328   \n",
      "6             Precision    0.916667    0.936170    0.882353    0.880000   \n",
      "7           Sensitivity    0.666667    0.647059    0.661765    0.637681   \n",
      "8           Specificity    0.980200    0.985000    0.970000    0.969800   \n",
      "9              F1 score    0.771930    0.765217    0.756303    0.739496   \n",
      "10  F1 score (weighted)    0.897395    0.892568    0.886268    0.877729   \n",
      "11     F1 score (macro)    0.855159    0.850542    0.843379    0.832578   \n",
      "12    Balanced Accuracy    0.823432    0.816029    0.815882    0.803765   \n",
      "13                  MCC    0.726824    0.723239    0.700341    0.681877   \n",
      "14                  NPV    0.900000    0.891400    0.894000    0.885300   \n",
      "15              ROC_AUC    0.823432    0.816029    0.815882    0.803765   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.713544    0.685845    0.724100    0.630683  \n",
      "1    47.000000   48.000000   43.000000   43.000000  \n",
      "2   198.000000  197.000000  194.000000  195.000000  \n",
      "3     4.000000    3.000000    6.000000    6.000000  \n",
      "4    19.000000   20.000000   25.000000   24.000000  \n",
      "5     0.914179    0.914179    0.884328    0.888060  \n",
      "6     0.921569    0.941176    0.877551    0.877551  \n",
      "7     0.712121    0.705882    0.632353    0.641791  \n",
      "8     0.980200    0.985000    0.970000    0.970100  \n",
      "9     0.803419    0.806723    0.735043    0.741379  \n",
      "10    0.910214    0.909798    0.877559    0.881773  \n",
      "11    0.874263    0.875783    0.830529    0.834975  \n",
      "12    0.846160    0.845441    0.801176    0.805970  \n",
      "13    0.759870    0.765876    0.678110    0.685527  \n",
      "14    0.912400    0.907800    0.885800    0.890400  \n",
      "15    0.846160    0.845441    0.801176    0.805970  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_knn_7)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_knn_7_cat = np.where(((y_pred_knn_7 >= 2) | (y_pred_knn_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_knn_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_knn_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_knn_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:38:36,293] Trial 400 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:36,539] Trial 401 finished with value: 0.6647820565881358 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:36,807] Trial 402 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:37,062] Trial 403 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:37,305] Trial 404 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:37,579] Trial 405 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:37,848] Trial 406 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:38,105] Trial 407 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:38,369] Trial 408 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:38,624] Trial 409 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:38,879] Trial 410 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:39,054] Trial 411 finished with value: 0.6613246783342457 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:39,323] Trial 412 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:39,582] Trial 413 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:39,840] Trial 414 finished with value: 0.636152408876722 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:40,106] Trial 415 finished with value: 0.6647820565881358 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:40,371] Trial 416 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:40,638] Trial 417 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:40,907] Trial 418 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:41,079] Trial 419 finished with value: 0.6613246783342457 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:41,341] Trial 420 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:41,609] Trial 421 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:41,866] Trial 422 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:42,134] Trial 423 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:42,402] Trial 424 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:42,669] Trial 425 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 86}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:42,937] Trial 426 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:43,205] Trial 427 finished with value: 0.6647820565881358 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:43,463] Trial 428 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:43,732] Trial 429 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:44,000] Trial 430 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:44,269] Trial 431 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:44,540] Trial 432 finished with value: 0.6388808607538031 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:44,808] Trial 433 finished with value: 0.636152408876722 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:44,973] Trial 434 finished with value: 0.6613246783342457 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 76}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:45,161] Trial 435 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:45,309] Trial 436 finished with value: 0.6849532962008549 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:45,457] Trial 437 finished with value: 0.6647820565881358 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:45,604] Trial 438 finished with value: 0.6849532962008549 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:45,762] Trial 439 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:45,912] Trial 440 finished with value: 0.6212279154228983 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:46,066] Trial 441 finished with value: 0.6613246783342457 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:46,249] Trial 442 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:46,400] Trial 443 finished with value: 0.6849532962008549 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:46,561] Trial 444 finished with value: 0.6849532962008549 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:46,836] Trial 445 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:47,110] Trial 446 finished with value: 0.6849532962008549 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:47,375] Trial 447 finished with value: 0.6700820760930126 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:47,650] Trial 448 finished with value: 0.6759208585658186 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:47,908] Trial 449 finished with value: 0.6849532962008549 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7160\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 79\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698571    0.677845    0.679705    0.719101   \n",
      "1                    TP   44.000000   44.000000   45.000000   44.000000   \n",
      "2                    TN  198.000000  197.000000  194.000000  193.000000   \n",
      "3                    FP    4.000000    3.000000    6.000000    6.000000   \n",
      "4                    FN   22.000000   24.000000   23.000000   25.000000   \n",
      "5              Accuracy    0.902985    0.899254    0.891791    0.884328   \n",
      "6             Precision    0.916667    0.936170    0.882353    0.880000   \n",
      "7           Sensitivity    0.666667    0.647059    0.661765    0.637681   \n",
      "8           Specificity    0.980200    0.985000    0.970000    0.969800   \n",
      "9              F1 score    0.771930    0.765217    0.756303    0.739496   \n",
      "10  F1 score (weighted)    0.897395    0.892568    0.886268    0.877729   \n",
      "11     F1 score (macro)    0.855159    0.850542    0.843379    0.832578   \n",
      "12    Balanced Accuracy    0.823432    0.816029    0.815882    0.803765   \n",
      "13                  MCC    0.726824    0.723239    0.700341    0.681877   \n",
      "14                  NPV    0.900000    0.891400    0.894000    0.885300   \n",
      "15              ROC_AUC    0.823432    0.816029    0.815882    0.803765   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.713544    0.685845    0.724100    0.630683    0.713009  \n",
      "1    47.000000   48.000000   43.000000   43.000000   47.000000  \n",
      "2   198.000000  197.000000  194.000000  195.000000  196.000000  \n",
      "3     4.000000    3.000000    6.000000    6.000000    4.000000  \n",
      "4    19.000000   20.000000   25.000000   24.000000   21.000000  \n",
      "5     0.914179    0.914179    0.884328    0.888060    0.906716  \n",
      "6     0.921569    0.941176    0.877551    0.877551    0.921569  \n",
      "7     0.712121    0.705882    0.632353    0.641791    0.691176  \n",
      "8     0.980200    0.985000    0.970000    0.970100    0.980000  \n",
      "9     0.803419    0.806723    0.735043    0.741379    0.789916  \n",
      "10    0.910214    0.909798    0.877559    0.881773    0.901955  \n",
      "11    0.874263    0.875783    0.830529    0.834975    0.864982  \n",
      "12    0.846160    0.845441    0.801176    0.805970    0.835588  \n",
      "13    0.759870    0.765876    0.678110    0.685527    0.744031  \n",
      "14    0.912400    0.907800    0.885800    0.890400    0.903200  \n",
      "15    0.846160    0.845441    0.801176    0.805970    0.835588  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_knn_8)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_knn_8_cat = np.where(((y_pred_knn_8 >= 2) | (y_pred_knn_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_knn_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_knn_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_knn_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:38:48,253] Trial 450 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:48,530] Trial 451 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:48,806] Trial 452 finished with value: 0.6171225580420795 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:49,082] Trial 453 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:49,358] Trial 454 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:49,634] Trial 455 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:49,811] Trial 456 finished with value: 0.6587912901921743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:50,088] Trial 457 finished with value: 0.6638515091333065 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:50,364] Trial 458 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:50,640] Trial 459 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:50,918] Trial 460 finished with value: 0.5650907951166217 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:51,195] Trial 461 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:51,371] Trial 462 finished with value: 0.6587912901921743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:51,647] Trial 463 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:51,923] Trial 464 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:52,200] Trial 465 finished with value: 0.5840128813494081 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:52,477] Trial 466 finished with value: 0.6638515091333065 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:52,755] Trial 467 finished with value: 0.5464669481976703 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:53,030] Trial 468 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:53,286] Trial 469 finished with value: 0.6385090759191637 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:53,545] Trial 470 finished with value: 0.6616998321545486 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:53,820] Trial 471 finished with value: 0.6334758494464979 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:54,062] Trial 472 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:54,338] Trial 473 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:54,615] Trial 474 finished with value: 0.6638515091333065 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:54,883] Trial 475 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:55,161] Trial 476 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:55,439] Trial 477 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:55,616] Trial 478 finished with value: 0.6587912901921743 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 86}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:55,880] Trial 479 finished with value: 0.6638515091333065 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:56,150] Trial 480 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:56,442] Trial 481 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:56,712] Trial 482 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:56,984] Trial 483 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:57,151] Trial 484 finished with value: 0.646356461617663 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:57,362] Trial 485 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:57,524] Trial 486 finished with value: 0.6248048150733536 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 77}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:57,684] Trial 487 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:57,854] Trial 488 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:58,022] Trial 489 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 85}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:58,190] Trial 490 finished with value: 0.6117778052029899 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 88}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:58,356] Trial 491 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:58,615] Trial 492 finished with value: 0.6522135914168343 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:58,864] Trial 493 finished with value: 0.6578754273558914 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 81}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:59,124] Trial 494 finished with value: 0.6638515091333065 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:59,386] Trial 495 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 82}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:59,658] Trial 496 finished with value: 0.6685032780757941 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 80}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:38:59,930] Trial 497 finished with value: 0.6717118080231742 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:39:00,205] Trial 498 finished with value: 0.5914031872134078 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 350 with value: 0.7160247874379679.\n",
      "[I 2023-12-20 08:39:00,477] Trial 499 finished with value: 0.6638515091333065 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 350 with value: 0.7160247874379679.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7160\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 79\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698571    0.677845    0.679705    0.719101   \n",
      "1                    TP   44.000000   44.000000   45.000000   44.000000   \n",
      "2                    TN  198.000000  197.000000  194.000000  193.000000   \n",
      "3                    FP    4.000000    3.000000    6.000000    6.000000   \n",
      "4                    FN   22.000000   24.000000   23.000000   25.000000   \n",
      "5              Accuracy    0.902985    0.899254    0.891791    0.884328   \n",
      "6             Precision    0.916667    0.936170    0.882353    0.880000   \n",
      "7           Sensitivity    0.666667    0.647059    0.661765    0.637681   \n",
      "8           Specificity    0.980200    0.985000    0.970000    0.969800   \n",
      "9              F1 score    0.771930    0.765217    0.756303    0.739496   \n",
      "10  F1 score (weighted)    0.897395    0.892568    0.886268    0.877729   \n",
      "11     F1 score (macro)    0.855159    0.850542    0.843379    0.832578   \n",
      "12    Balanced Accuracy    0.823432    0.816029    0.815882    0.803765   \n",
      "13                  MCC    0.726824    0.723239    0.700341    0.681877   \n",
      "14                  NPV    0.900000    0.891400    0.894000    0.885300   \n",
      "15              ROC_AUC    0.823432    0.816029    0.815882    0.803765   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.713544    0.685845    0.724100    0.630683    0.713009    0.745201  \n",
      "1    47.000000   48.000000   43.000000   43.000000   47.000000   41.000000  \n",
      "2   198.000000  197.000000  194.000000  195.000000  196.000000  198.000000  \n",
      "3     4.000000    3.000000    6.000000    6.000000    4.000000    3.000000  \n",
      "4    19.000000   20.000000   25.000000   24.000000   21.000000   26.000000  \n",
      "5     0.914179    0.914179    0.884328    0.888060    0.906716    0.891791  \n",
      "6     0.921569    0.941176    0.877551    0.877551    0.921569    0.931818  \n",
      "7     0.712121    0.705882    0.632353    0.641791    0.691176    0.611940  \n",
      "8     0.980200    0.985000    0.970000    0.970100    0.980000    0.985100  \n",
      "9     0.803419    0.806723    0.735043    0.741379    0.789916    0.738739  \n",
      "10    0.910214    0.909798    0.877559    0.881773    0.901955    0.883508  \n",
      "11    0.874263    0.875783    0.830529    0.834975    0.864982    0.835252  \n",
      "12    0.846160    0.845441    0.801176    0.805970    0.835588    0.798507  \n",
      "13    0.759870    0.765876    0.678110    0.685527    0.744031    0.697863  \n",
      "14    0.912400    0.907800    0.885800    0.890400    0.903200    0.883900  \n",
      "15    0.846160    0.845441    0.801176    0.805970    0.835588    0.798507  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_knn_9)\n",
    "# now convert the resuls to binary with cutoff \n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_knn_9_cat = np.where(((y_pred_knn_9 >= 2) | (y_pred_knn_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_knn_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_knn_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_knn_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.691196</td>\n",
       "      <td>0.093625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>2.485514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>97.900000</td>\n",
       "      <td>1.791957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.776388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>2.820559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.896931</td>\n",
       "      <td>0.028783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.895721</td>\n",
       "      <td>0.073513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.665459</td>\n",
       "      <td>0.079265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.974150</td>\n",
       "      <td>0.017702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.761846</td>\n",
       "      <td>0.070625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.891134</td>\n",
       "      <td>0.031601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.848029</td>\n",
       "      <td>0.044302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.819804</td>\n",
       "      <td>0.044139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.711881</td>\n",
       "      <td>0.085228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.897660</td>\n",
       "      <td>0.024047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.819804</td>\n",
       "      <td>0.044139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.691196     0.093625\n",
       "1                    TP        22.200000     2.485514\n",
       "2                    TN        97.900000     1.791957\n",
       "3                    FP         2.600000     1.776388\n",
       "4                    FN        11.200000     2.820559\n",
       "5              Accuracy         0.896931     0.028783\n",
       "6             Precision         0.895721     0.073513\n",
       "7           Sensitivity         0.665459     0.079265\n",
       "8           Specificity         0.974150     0.017702\n",
       "9              F1 score         0.761846     0.070625\n",
       "10  F1 score (weighted)         0.891134     0.031601\n",
       "11     F1 score (macro)         0.848029     0.044302\n",
       "12    Balanced Accuracy         0.819804     0.044139\n",
       "13                  MCC         0.711881     0.085228\n",
       "14                  NPV         0.897660     0.024047\n",
       "15              ROC_AUC         0.819804     0.044139"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.698571</td>\n",
       "      <td>0.677845</td>\n",
       "      <td>0.679705</td>\n",
       "      <td>0.719101</td>\n",
       "      <td>0.713544</td>\n",
       "      <td>0.685845</td>\n",
       "      <td>0.724100</td>\n",
       "      <td>0.630683</td>\n",
       "      <td>0.713009</td>\n",
       "      <td>0.745201</td>\n",
       "      <td>0.698760</td>\n",
       "      <td>0.032035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>44.600000</td>\n",
       "      <td>2.170509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>1.885618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.354006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>2.330951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.914179</td>\n",
       "      <td>0.914179</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.906716</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.897761</td>\n",
       "      <td>0.011427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.931818</td>\n",
       "      <td>0.908642</td>\n",
       "      <td>0.026240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.660844</td>\n",
       "      <td>0.033175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.969800</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.977540</td>\n",
       "      <td>0.006812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.756303</td>\n",
       "      <td>0.739496</td>\n",
       "      <td>0.803419</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.735043</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.789916</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.764816</td>\n",
       "      <td>0.027399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.897395</td>\n",
       "      <td>0.892568</td>\n",
       "      <td>0.886268</td>\n",
       "      <td>0.877729</td>\n",
       "      <td>0.910214</td>\n",
       "      <td>0.909798</td>\n",
       "      <td>0.877559</td>\n",
       "      <td>0.881773</td>\n",
       "      <td>0.901955</td>\n",
       "      <td>0.883508</td>\n",
       "      <td>0.891877</td>\n",
       "      <td>0.012466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.855159</td>\n",
       "      <td>0.850542</td>\n",
       "      <td>0.843379</td>\n",
       "      <td>0.832578</td>\n",
       "      <td>0.874263</td>\n",
       "      <td>0.875783</td>\n",
       "      <td>0.830529</td>\n",
       "      <td>0.834975</td>\n",
       "      <td>0.864982</td>\n",
       "      <td>0.835252</td>\n",
       "      <td>0.849744</td>\n",
       "      <td>0.017237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.823432</td>\n",
       "      <td>0.816029</td>\n",
       "      <td>0.815882</td>\n",
       "      <td>0.803765</td>\n",
       "      <td>0.846160</td>\n",
       "      <td>0.845441</td>\n",
       "      <td>0.801176</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.835588</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.819195</td>\n",
       "      <td>0.017917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.726824</td>\n",
       "      <td>0.723239</td>\n",
       "      <td>0.700341</td>\n",
       "      <td>0.681877</td>\n",
       "      <td>0.759870</td>\n",
       "      <td>0.765876</td>\n",
       "      <td>0.678110</td>\n",
       "      <td>0.685527</td>\n",
       "      <td>0.744031</td>\n",
       "      <td>0.697863</td>\n",
       "      <td>0.716356</td>\n",
       "      <td>0.032436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.891400</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.885300</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>0.907800</td>\n",
       "      <td>0.885800</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.883900</td>\n",
       "      <td>0.895420</td>\n",
       "      <td>0.009967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.823432</td>\n",
       "      <td>0.816029</td>\n",
       "      <td>0.815882</td>\n",
       "      <td>0.803765</td>\n",
       "      <td>0.846160</td>\n",
       "      <td>0.845441</td>\n",
       "      <td>0.801176</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.835588</td>\n",
       "      <td>0.798507</td>\n",
       "      <td>0.819195</td>\n",
       "      <td>0.017917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.698571    0.677845    0.679705    0.719101   \n",
       "1                    TP   44.000000   44.000000   45.000000   44.000000   \n",
       "2                    TN  198.000000  197.000000  194.000000  193.000000   \n",
       "3                    FP    4.000000    3.000000    6.000000    6.000000   \n",
       "4                    FN   22.000000   24.000000   23.000000   25.000000   \n",
       "5              Accuracy    0.902985    0.899254    0.891791    0.884328   \n",
       "6             Precision    0.916667    0.936170    0.882353    0.880000   \n",
       "7           Sensitivity    0.666667    0.647059    0.661765    0.637681   \n",
       "8           Specificity    0.980200    0.985000    0.970000    0.969800   \n",
       "9              F1 score    0.771930    0.765217    0.756303    0.739496   \n",
       "10  F1 score (weighted)    0.897395    0.892568    0.886268    0.877729   \n",
       "11     F1 score (macro)    0.855159    0.850542    0.843379    0.832578   \n",
       "12    Balanced Accuracy    0.823432    0.816029    0.815882    0.803765   \n",
       "13                  MCC    0.726824    0.723239    0.700341    0.681877   \n",
       "14                  NPV    0.900000    0.891400    0.894000    0.885300   \n",
       "15              ROC_AUC    0.823432    0.816029    0.815882    0.803765   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.713544    0.685845    0.724100    0.630683    0.713009    0.745201   \n",
       "1    47.000000   48.000000   43.000000   43.000000   47.000000   41.000000   \n",
       "2   198.000000  197.000000  194.000000  195.000000  196.000000  198.000000   \n",
       "3     4.000000    3.000000    6.000000    6.000000    4.000000    3.000000   \n",
       "4    19.000000   20.000000   25.000000   24.000000   21.000000   26.000000   \n",
       "5     0.914179    0.914179    0.884328    0.888060    0.906716    0.891791   \n",
       "6     0.921569    0.941176    0.877551    0.877551    0.921569    0.931818   \n",
       "7     0.712121    0.705882    0.632353    0.641791    0.691176    0.611940   \n",
       "8     0.980200    0.985000    0.970000    0.970100    0.980000    0.985100   \n",
       "9     0.803419    0.806723    0.735043    0.741379    0.789916    0.738739   \n",
       "10    0.910214    0.909798    0.877559    0.881773    0.901955    0.883508   \n",
       "11    0.874263    0.875783    0.830529    0.834975    0.864982    0.835252   \n",
       "12    0.846160    0.845441    0.801176    0.805970    0.835588    0.798507   \n",
       "13    0.759870    0.765876    0.678110    0.685527    0.744031    0.697863   \n",
       "14    0.912400    0.907800    0.885800    0.890400    0.903200    0.883900   \n",
       "15    0.846160    0.845441    0.801176    0.805970    0.835588    0.798507   \n",
       "\n",
       "           ave       std  \n",
       "0     0.698760  0.032035  \n",
       "1    44.600000  2.170509  \n",
       "2   196.000000  1.885618  \n",
       "3     4.500000  1.354006  \n",
       "4    22.900000  2.330951  \n",
       "5     0.897761  0.011427  \n",
       "6     0.908642  0.026240  \n",
       "7     0.660844  0.033175  \n",
       "8     0.977540  0.006812  \n",
       "9     0.764816  0.027399  \n",
       "10    0.891877  0.012466  \n",
       "11    0.849744  0.017237  \n",
       "12    0.819195  0.017917  \n",
       "13    0.716356  0.032436  \n",
       "14    0.895420  0.009967  \n",
       "15    0.819195  0.017917  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.704124</td>\n",
       "      <td>0.066491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.904703</td>\n",
       "      <td>0.020451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.910285</td>\n",
       "      <td>0.054644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.687247</td>\n",
       "      <td>0.069880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.976924</td>\n",
       "      <td>0.014379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.780945</td>\n",
       "      <td>0.052457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.899658</td>\n",
       "      <td>0.022438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.859994</td>\n",
       "      <td>0.032403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.035680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.734881</td>\n",
       "      <td>0.060820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.904268</td>\n",
       "      <td>0.019580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.035680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.704124     0.066491\n",
       "1              Accuracy         0.904703     0.020451\n",
       "2             Precision         0.910285     0.054644\n",
       "3           Sensitivity         0.687247     0.069880\n",
       "4           Specificity         0.976924     0.014379\n",
       "5              F1 score         0.780945     0.052457\n",
       "6   F1 score (weighted)         0.899658     0.022438\n",
       "7      F1 score (macro)         0.859994     0.032403\n",
       "8     Balanced Accuracy         0.832084     0.035680\n",
       "9                   MCC         0.734881     0.060820\n",
       "10                  NPV         0.904268     0.019580\n",
       "11              ROC_AUC         0.832084     0.035680"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsRegressor(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=4,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_knn_cat = np.where(((y_pred_optimized_knn >= 2) | (y_pred_optimized_knn <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_knn_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_knn))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_knn_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_knn_cat))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "knn_run0 = data_knn[['y_test_idx0', 'y_test0', 'y_pred_knn0']]\n",
    "knn_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "knn_run0.reset_index(inplace=True, drop=True)\n",
    "knn_run1 = data_knn[['y_test_idx1', 'y_test1', 'y_pred_knn1']]\n",
    "knn_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "knn_run1.reset_index(inplace=True, drop=True)\n",
    "knn_run2 = data_knn[['y_test_idx2', 'y_test2', 'y_pred_knn2']]\n",
    "knn_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "knn_run2.reset_index(inplace=True, drop=True)\n",
    "knn_run3 = data_knn[['y_test_idx3', 'y_test3', 'y_pred_knn3']]\n",
    "knn_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "knn_run3.reset_index(inplace=True, drop=True)\n",
    "knn_run4 = data_knn[['y_test_idx4', 'y_test4', 'y_pred_knn4']]\n",
    "knn_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "knn_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "knn_5preds = pd.concat([chembl_id, knn_run0, knn_run1, knn_run2, knn_run3, knn_run4], axis=1)\n",
    "knn_5preds = knn_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_knn0', 'y_pred_knn1', 'y_pred_knn2', 'y_pred_knn3', 'y_pred_knn4']]\n",
    "knn_5preds['y_pred_knn_ave'] = knn_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "knn_5preds['y_pred_knn_std'] = knn_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "\n",
    "mat_met_optimized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0bc43db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(knn_5preds['y_test0'], knn_5preds['y_pred_knn_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model r2_score 0.6839 with a standard deviation of 0.0760\n",
      "KNN optimized model r2_score 0.7086 with a standard deviation of 0.0715\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"KNN baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn.joblib']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_reg, \"OUTPUT/knn_reg.joblib\")\n",
    "#joblib.dump(optimized_knn, \"OUTPUT/optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn.joblib\")\n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    R2         0.678877     0.073310\n",
      "1                    TP        15.100000     2.378141\n",
      "2                    TN        99.600000     1.429841\n",
      "3                    FP         0.900000     0.875595\n",
      "4                    FN        18.300000     2.983287\n",
      "5              Accuracy         0.856632     0.026404\n",
      "6             Precision         0.941979     0.059686\n",
      "7           Sensitivity         0.453375     0.079148\n",
      "8           Specificity         0.991010     0.008780\n",
      "9              F1 score         0.609639     0.079321\n",
      "10  F1 score (weighted)         0.836466     0.032970\n",
      "11     F1 score (macro)         0.760901     0.047464\n",
      "12    Balanced Accuracy         0.722192     0.042110\n",
      "13                  MCC         0.590441     0.080931\n",
      "14                  NPV         0.845130     0.022919\n",
      "15              ROC_AUC         0.722192     0.042110\n",
      "CPU times: user 2.04 s, sys: 0 ns, total: 2.04 s\n",
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r2_scores = np.empty(10)\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_reg = SVR()\n",
    "    \n",
    "    svm_reg.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test) \n",
    "    # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "    r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "    # now convert the resuls to binary with cutoff 6.3\n",
    "    y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "    y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "    Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "    Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "    f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "    MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = r2_score(y_test, y_pred)\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggest_categorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    r2_scores = np.empty(10)\n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVR(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        # r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "        r2_scores[idx] = r2_score(y_test, y_pred)\n",
    "        # now convert the resuls to binary with cutoff 6.3\n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_cat = np.where(((y_pred >= 2) | (y_pred <= -2)), 1, 0)\n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test_cat, y_pred_cat)\n",
    "        Precision[idx] = precision_score(y_test_cat, y_pred_cat)\n",
    "        Sensitivity[idx] = recall_score(y_test_cat, y_pred_cat)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test_cat, y_pred_cat)\n",
    "        f1_scores_W[idx] = f1_score(y_test_cat, y_pred_cat, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test_cat, y_pred_cat, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test_cat, y_pred_cat)\n",
    "        MCC[idx] = matthews_corrcoef(y_test_cat, y_pred_cat)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test_cat, y_pred_cat)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(r2_scores, ddof=1), np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:39:07,889] A new study created in memory with name: SVM_regressor_CV\n",
      "[I 2023-12-20 08:39:09,288] Trial 0 finished with value: 0.2752388885477531 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 0 with value: 0.2752388885477531.\n",
      "[I 2023-12-20 08:39:10,683] Trial 1 finished with value: 0.028130700965275944 and parameters: {'C': 0.5, 'gamma': 6.103515625e-05}. Best is trial 0 with value: 0.2752388885477531.\n",
      "[I 2023-12-20 08:39:12,089] Trial 2 finished with value: 0.2561534251966175 and parameters: {'C': 0.25, 'gamma': 0.0625}. Best is trial 0 with value: 0.2752388885477531.\n",
      "[I 2023-12-20 08:39:13,616] Trial 3 finished with value: 0.5320994647997808 and parameters: {'C': 0.5, 'gamma': 0.00390625}. Best is trial 3 with value: 0.5320994647997808.\n",
      "[I 2023-12-20 08:39:15,061] Trial 4 finished with value: 0.1254898755780079 and parameters: {'C': 0.125, 'gamma': 0.0009765625}. Best is trial 3 with value: 0.5320994647997808.\n",
      "[I 2023-12-20 08:39:16,554] Trial 5 finished with value: 0.4636864853997921 and parameters: {'C': 1.0, 'gamma': 0.0009765625}. Best is trial 3 with value: 0.5320994647997808.\n",
      "[I 2023-12-20 08:39:18,036] Trial 6 finished with value: 0.679462487715439 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:19,526] Trial 7 finished with value: 0.6762940767461587 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:21,009] Trial 8 finished with value: 0.4709353309542113 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:22,510] Trial 9 finished with value: 0.5448762756210532 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:24,181] Trial 10 finished with value: 0.6703732113833506 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:25,703] Trial 11 finished with value: 0.6762940767461587 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:27,144] Trial 12 finished with value: 0.6762940767461587 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:28,634] Trial 13 finished with value: 0.004579333526326423 and parameters: {'C': 0.03125, 'gamma': 0.00048828125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:30,160] Trial 14 finished with value: 0.6794613583569898 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:32,069] Trial 15 finished with value: 0.019498918779671793 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:33,742] Trial 16 finished with value: 0.020461384786785775 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:35,126] Trial 17 finished with value: 0.6454268733652878 and parameters: {'C': 4.0, 'gamma': 0.001953125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:36,871] Trial 18 finished with value: 0.023158445556084695 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:38,411] Trial 19 finished with value: 0.4704663711020675 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:39,945] Trial 20 finished with value: -0.016503008981128907 and parameters: {'C': 0.0078125, 'gamma': 0.000244140625}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:41,460] Trial 21 finished with value: 0.055430230885548604 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:42,980] Trial 22 finished with value: 0.6762940767461587 and parameters: {'C': 2.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:44,712] Trial 23 finished with value: 0.01961243130981114 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:46,319] Trial 24 finished with value: 0.030869045824379937 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:47,783] Trial 25 finished with value: -0.017293200144360088 and parameters: {'C': 0.0078125, 'gamma': 0.125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:49,265] Trial 26 finished with value: 0.0035517657511559553 and parameters: {'C': 0.25, 'gamma': 0.25}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:50,743] Trial 27 finished with value: 0.6475603412358172 and parameters: {'C': 1.0, 'gamma': 0.03125}. Best is trial 6 with value: 0.679462487715439.\n",
      "[I 2023-12-20 08:39:52,232] Trial 28 finished with value: 0.6795410337154534 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 28 with value: 0.6795410337154534.\n",
      "[I 2023-12-20 08:39:53,797] Trial 29 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:39:55,316] Trial 30 finished with value: 0.2752388885477531 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:39:56,860] Trial 31 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:39:58,349] Trial 32 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:39:59,831] Trial 33 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:01,355] Trial 34 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:02,886] Trial 35 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:04,426] Trial 36 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:05,963] Trial 37 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:07,513] Trial 38 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:08,956] Trial 39 finished with value: 0.3961798811117496 and parameters: {'C': 0.5, 'gamma': 0.0625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:10,437] Trial 40 finished with value: 0.2995615879881338 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:11,896] Trial 41 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:13,448] Trial 42 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:14,965] Trial 43 finished with value: 0.6943037771339731 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:16,530] Trial 44 finished with value: 0.6933048963548268 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:18,053] Trial 45 finished with value: 0.34996592447645825 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:19,668] Trial 46 finished with value: 0.6406418922376804 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:21,203] Trial 47 finished with value: -0.016458003136665657 and parameters: {'C': 0.03125, 'gamma': 6.103515625e-05}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:22,788] Trial 48 finished with value: 0.6855348528336073 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:24,283] Trial 49 finished with value: -0.007280355811696349 and parameters: {'C': 0.015625, 'gamma': 0.00048828125}. Best is trial 29 with value: 0.6943037771339731.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (r2_score): 0.6943\n",
      "\tBest params:\n",
      "\t\tC: 16.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_regressor_CV\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    R2    0.698333\n",
      "1                    TP   29.000000\n",
      "2                    TN  202.000000\n",
      "3                    FP    0.000000\n",
      "4                    FN   37.000000\n",
      "5              Accuracy    0.861940\n",
      "6             Precision    1.000000\n",
      "7           Sensitivity    0.439394\n",
      "8           Specificity    1.000000\n",
      "9              F1 score    0.610526\n",
      "10  F1 score (weighted)    0.840847\n",
      "11     F1 score (macro)    0.763313\n",
      "12    Balanced Accuracy    0.719697\n",
      "13                  MCC    0.609402\n",
      "14                  NPV    0.845200\n",
      "15              ROC_AUC    0.719697\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "r2_scores = r2_score(Y_testSet0, y_pred_svm_0)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet0_cat = np.where(((Y_testSet0>=2) | (Y_testSet0<=-2)), 1, 0) \n",
    "y_pred_svm_0_cat = np.where(((y_pred_svm_0 >= 2) | (y_pred_svm_0 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Precision = precision_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Sensitivity = recall_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0_cat, y_pred_svm_0_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0_cat, y_pred_svm_0_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0_cat, y_pred_svm_0_cat)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['R2','TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:40:25,932] Trial 50 finished with value: -0.019870156640204372 and parameters: {'C': 0.0625, 'gamma': 2.0}. Best is trial 29 with value: 0.6943037771339731.\n",
      "[I 2023-12-20 08:40:27,331] Trial 51 finished with value: 0.6954918672284649 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6954918672284649.\n",
      "[I 2023-12-20 08:40:28,798] Trial 52 finished with value: 0.6954918672284649 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.6954918672284649.\n",
      "[I 2023-12-20 08:40:30,254] Trial 53 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:32,056] Trial 54 finished with value: 0.014939667012473845 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:33,567] Trial 55 finished with value: 0.6366311828154735 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:34,908] Trial 56 finished with value: 0.4497449944602402 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:36,283] Trial 57 finished with value: -0.01042111939941609 and parameters: {'C': 0.25, 'gamma': 1.0}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:37,701] Trial 58 finished with value: -0.014813182477453524 and parameters: {'C': 0.125, 'gamma': 0.5}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:39,085] Trial 59 finished with value: 0.22278370949878026 and parameters: {'C': 1.0, 'gamma': 0.000244140625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:40,655] Trial 60 finished with value: 0.05715266314345389 and parameters: {'C': 64.0, 'gamma': 0.25}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:42,087] Trial 61 finished with value: 0.6954918672284649 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:43,578] Trial 62 finished with value: 0.6954918672284649 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:45,064] Trial 63 finished with value: -0.0033997148671741084 and parameters: {'C': 0.5, 'gamma': 4.0}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:46,484] Trial 64 finished with value: 0.6954918672284649 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:47,808] Trial 65 finished with value: -0.021425122658515795 and parameters: {'C': 0.0078125, 'gamma': 0.125}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:49,221] Trial 66 finished with value: 0.16413845989994427 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:50,699] Trial 67 finished with value: 0.5471783804669352 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:52,152] Trial 68 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:53,683] Trial 69 finished with value: 0.6510088485355248 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:55,123] Trial 70 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:56,569] Trial 71 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:58,072] Trial 72 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:40:59,430] Trial 73 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:00,740] Trial 74 finished with value: 0.2249313396800713 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:02,076] Trial 75 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:03,389] Trial 76 finished with value: 0.6140343871347953 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:04,691] Trial 77 finished with value: 0.34344876235746824 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:06,080] Trial 78 finished with value: 0.6814788018918607 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:07,425] Trial 79 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:08,687] Trial 80 finished with value: 0.5751611251192521 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:10,039] Trial 81 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:11,438] Trial 82 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:12,890] Trial 83 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:14,340] Trial 84 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:16,164] Trial 85 finished with value: 0.014939667012473845 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:17,744] Trial 86 finished with value: 0.016793836590025157 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:19,250] Trial 87 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:20,766] Trial 88 finished with value: 0.6366311828154735 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:22,327] Trial 89 finished with value: 0.021066448314966523 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:23,673] Trial 90 finished with value: 0.22399505293306715 and parameters: {'C': 2.0, 'gamma': 0.0001220703125}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:25,151] Trial 91 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:26,595] Trial 92 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:28,076] Trial 93 finished with value: 0.6958669752384442 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 53 with value: 0.6958669752384442.\n",
      "[I 2023-12-20 08:41:29,484] Trial 94 finished with value: 0.6959804304570497 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 94 with value: 0.6959804304570497.\n",
      "[I 2023-12-20 08:41:30,947] Trial 95 finished with value: 0.6086086959331034 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 94 with value: 0.6959804304570497.\n",
      "[I 2023-12-20 08:41:32,524] Trial 96 finished with value: 0.02961989112356287 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 94 with value: 0.6959804304570497.\n",
      "[I 2023-12-20 08:41:33,928] Trial 97 finished with value: 0.0863154395108919 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 94 with value: 0.6959804304570497.\n",
      "[I 2023-12-20 08:41:35,572] Trial 98 finished with value: 0.01517843865151075 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 94 with value: 0.6959804304570497.\n",
      "[I 2023-12-20 08:41:37,140] Trial 99 finished with value: 0.23577357063261228 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 94 with value: 0.6959804304570497.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (r2_score): 0.6960\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    R2    0.698333    0.720682\n",
      "1                    TP   29.000000   37.000000\n",
      "2                    TN  202.000000  195.000000\n",
      "3                    FP    0.000000    5.000000\n",
      "4                    FN   37.000000   31.000000\n",
      "5              Accuracy    0.861940    0.865672\n",
      "6             Precision    1.000000    0.880952\n",
      "7           Sensitivity    0.439394    0.544118\n",
      "8           Specificity    1.000000    0.975000\n",
      "9              F1 score    0.610526    0.672727\n",
      "10  F1 score (weighted)    0.840847    0.853896\n",
      "11     F1 score (macro)    0.763313    0.794110\n",
      "12    Balanced Accuracy    0.719697    0.759559\n",
      "13                  MCC    0.609402    0.621379\n",
      "14                  NPV    0.845200    0.862800\n",
      "15              ROC_AUC    0.719697    0.759559\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "r2_scores = r2_score(Y_testSet1, y_pred_svm_1)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet1_cat = np.where(((Y_testSet1>=2) | (Y_testSet1<=-2)), 1, 0) \n",
    "y_pred_svm_1_cat = np.where(((y_pred_svm_1 >= 2) | (y_pred_svm_1 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Precision = precision_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Sensitivity = recall_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1_cat, y_pred_svm_1_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1_cat, y_pred_svm_1_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1_cat, y_pred_svm_1_cat)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:41:39,014] Trial 100 finished with value: 0.05080336048877593 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 94 with value: 0.6959804304570497.\n",
      "[I 2023-12-20 08:41:40,414] Trial 101 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:41,815] Trial 102 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:43,216] Trial 103 finished with value: 0.26423240642698137 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:44,572] Trial 104 finished with value: 0.5161068236257458 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:45,873] Trial 105 finished with value: 0.6598793580538699 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:47,406] Trial 106 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:48,897] Trial 107 finished with value: 0.5501246717034791 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:50,403] Trial 108 finished with value: 0.28731932950216316 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:51,914] Trial 109 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:53,397] Trial 110 finished with value: 0.02754413436462825 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:54,889] Trial 111 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:56,454] Trial 112 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:57,814] Trial 113 finished with value: 0.5973644464054539 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:41:59,362] Trial 114 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:00,812] Trial 115 finished with value: 0.5225136925437354 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:02,354] Trial 116 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:03,814] Trial 117 finished with value: 0.5225137652384351 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:05,350] Trial 118 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:06,839] Trial 119 finished with value: 0.6166838101545828 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:08,267] Trial 120 finished with value: 0.44916344687690046 and parameters: {'C': 2.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:09,698] Trial 121 finished with value: 0.7106404476592154 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:11,245] Trial 122 finished with value: 0.1535531333922401 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:12,821] Trial 123 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:14,416] Trial 124 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:15,972] Trial 125 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:17,529] Trial 126 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:19,101] Trial 127 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:20,645] Trial 128 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:22,113] Trial 129 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:23,565] Trial 130 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:24,991] Trial 131 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:26,579] Trial 132 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:28,125] Trial 133 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:29,717] Trial 134 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:31,220] Trial 135 finished with value: 0.06561756332736071 and parameters: {'C': 0.015625, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:32,789] Trial 136 finished with value: 0.008348722689724397 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:34,237] Trial 137 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:35,973] Trial 138 finished with value: 0.0070670749913643305 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:37,385] Trial 139 finished with value: 0.6486845053116196 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:38,902] Trial 140 finished with value: 0.012081683650539032 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:40,327] Trial 141 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:41,772] Trial 142 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:43,113] Trial 143 finished with value: 0.6841243644191388 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:44,613] Trial 144 finished with value: 0.700282474795352 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:45,990] Trial 145 finished with value: 0.4516810264415082 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:47,496] Trial 146 finished with value: -0.02782429358696732 and parameters: {'C': 0.0625, 'gamma': 4.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:48,970] Trial 147 finished with value: -0.015785894846684113 and parameters: {'C': 0.25, 'gamma': 0.5}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:50,324] Trial 148 finished with value: 0.5224274676327074 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:51,797] Trial 149 finished with value: 0.2399628126490856 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7106404476592154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (r2_score): 0.7106\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    R2    0.698333    0.720682    0.701664\n",
      "1                    TP   29.000000   37.000000   35.000000\n",
      "2                    TN  202.000000  195.000000  198.000000\n",
      "3                    FP    0.000000    5.000000    2.000000\n",
      "4                    FN   37.000000   31.000000   33.000000\n",
      "5              Accuracy    0.861940    0.865672    0.869403\n",
      "6             Precision    1.000000    0.880952    0.945946\n",
      "7           Sensitivity    0.439394    0.544118    0.514706\n",
      "8           Specificity    1.000000    0.975000    0.990000\n",
      "9              F1 score    0.610526    0.672727    0.666667\n",
      "10  F1 score (weighted)    0.840847    0.853896    0.854821\n",
      "11     F1 score (macro)    0.763313    0.794110    0.792730\n",
      "12    Balanced Accuracy    0.719697    0.759559    0.752353\n",
      "13                  MCC    0.609402    0.621379    0.636650\n",
      "14                  NPV    0.845200    0.862800    0.857100\n",
      "15              ROC_AUC    0.719697    0.759559    0.752353\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "r2_scores = r2_score(Y_testSet2, y_pred_svm_2)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet2_cat = np.where(((Y_testSet2>=2) | (Y_testSet2<=-2)), 1, 0) \n",
    "y_pred_svm_2_cat = np.where(((y_pred_svm_2 >= 2) | (y_pred_svm_2 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Precision = precision_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Sensitivity = recall_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2_cat, y_pred_svm_2_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2_cat, y_pred_svm_2_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2_cat, y_pred_svm_2_cat)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:42:53,443] Trial 150 finished with value: 0.03852440717138429 and parameters: {'C': 1.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:54,785] Trial 151 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:56,114] Trial 152 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:57,474] Trial 153 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:42:58,824] Trial 154 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:00,176] Trial 155 finished with value: 0.027264619147422686 and parameters: {'C': 0.0078125, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:01,465] Trial 156 finished with value: 0.3967797913571022 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:02,752] Trial 157 finished with value: 0.5057838380127061 and parameters: {'C': 0.5, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:04,074] Trial 158 finished with value: 0.5432182516009271 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:05,381] Trial 159 finished with value: 0.6811611038642387 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:06,690] Trial 160 finished with value: 0.6827638986424539 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:08,032] Trial 161 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:09,416] Trial 162 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:10,785] Trial 163 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:12,102] Trial 164 finished with value: 0.1424713068024863 and parameters: {'C': 0.03125, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:13,406] Trial 165 finished with value: 0.21941911744459489 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:14,770] Trial 166 finished with value: 0.6827638986424539 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:16,101] Trial 167 finished with value: 0.5149225601997751 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:17,473] Trial 168 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:18,765] Trial 169 finished with value: 0.3336211272199766 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:20,146] Trial 170 finished with value: 0.5682404280148845 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:21,590] Trial 171 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:23,063] Trial 172 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:24,503] Trial 173 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:25,937] Trial 174 finished with value: 0.6827638986424539 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:27,394] Trial 175 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:28,779] Trial 176 finished with value: 0.6795126741064764 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:30,233] Trial 177 finished with value: 0.012948625089964416 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:31,911] Trial 178 finished with value: -0.02569385882172175 and parameters: {'C': 0.015625, 'gamma': 8.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:33,302] Trial 179 finished with value: 0.6615203901418658 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:34,749] Trial 180 finished with value: 0.6827638986424539 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:36,185] Trial 181 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:37,679] Trial 182 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:39,109] Trial 183 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:40,573] Trial 184 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:41,904] Trial 185 finished with value: -0.0225108802605103 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:43,381] Trial 186 finished with value: 0.6827638986424539 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:44,737] Trial 187 finished with value: 0.022468066302982224 and parameters: {'C': 0.25, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:46,045] Trial 188 finished with value: 0.6226982041715095 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:47,391] Trial 189 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:48,774] Trial 190 finished with value: 0.024435324935020854 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:50,114] Trial 191 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:51,454] Trial 192 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:52,795] Trial 193 finished with value: 0.6637050489140244 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:54,078] Trial 194 finished with value: 0.5154363198731543 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:55,418] Trial 195 finished with value: 0.6827638986424539 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:56,724] Trial 196 finished with value: 0.6166174052915798 and parameters: {'C': 1.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:58,150] Trial 197 finished with value: 0.032922961650538804 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:43:59,632] Trial 198 finished with value: -0.005289281048820138 and parameters: {'C': 0.5, 'gamma': 4.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:01,194] Trial 199 finished with value: 0.0580625098374458 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7106404476592154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (r2_score): 0.7106\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    R2    0.698333    0.720682    0.701664    0.747780\n",
      "1                    TP   29.000000   37.000000   35.000000   41.000000\n",
      "2                    TN  202.000000  195.000000  198.000000  198.000000\n",
      "3                    FP    0.000000    5.000000    2.000000    1.000000\n",
      "4                    FN   37.000000   31.000000   33.000000   28.000000\n",
      "5              Accuracy    0.861940    0.865672    0.869403    0.891791\n",
      "6             Precision    1.000000    0.880952    0.945946    0.976190\n",
      "7           Sensitivity    0.439394    0.544118    0.514706    0.594203\n",
      "8           Specificity    1.000000    0.975000    0.990000    0.995000\n",
      "9              F1 score    0.610526    0.672727    0.666667    0.738739\n",
      "10  F1 score (weighted)    0.840847    0.853896    0.854821    0.882068\n",
      "11     F1 score (macro)    0.763313    0.794110    0.792730    0.835252\n",
      "12    Balanced Accuracy    0.719697    0.759559    0.752353    0.794589\n",
      "13                  MCC    0.609402    0.621379    0.636650    0.708628\n",
      "14                  NPV    0.845200    0.862800    0.857100    0.876100\n",
      "15              ROC_AUC    0.719697    0.759559    0.752353    0.794589\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "r2_scores = r2_score(Y_testSet3, y_pred_svm_3)\n",
    "# now convert the resuls to binary with cutoff 6.3\n",
    "Y_testSet3_cat = np.where(((Y_testSet3>=2) | (Y_testSet3<=-2)), 1, 0) \n",
    "y_pred_svm_3_cat = np.where(((y_pred_svm_3 >= 2) | (y_pred_svm_3 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Precision = precision_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Sensitivity = recall_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3_cat, y_pred_svm_3_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3_cat, y_pred_svm_3_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3_cat, y_pred_svm_3_cat)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:44:03,131] Trial 200 finished with value: 0.23483902135834062 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:04,688] Trial 201 finished with value: 0.38305873576297916 and parameters: {'C': 0.125, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:06,222] Trial 202 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:07,753] Trial 203 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:09,293] Trial 204 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:10,858] Trial 205 finished with value: 0.677158029376824 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:12,393] Trial 206 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:13,918] Trial 207 finished with value: 0.6785157831178596 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:15,478] Trial 208 finished with value: 0.00838878047978906 and parameters: {'C': 0.03125, 'gamma': 0.0625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:17,054] Trial 209 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:18,605] Trial 210 finished with value: 0.6785157831178596 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:20,138] Trial 211 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:21,637] Trial 212 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:23,163] Trial 213 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:24,745] Trial 214 finished with value: 0.6502295946430372 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:26,177] Trial 215 finished with value: 0.6575512920199621 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:27,719] Trial 216 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:29,242] Trial 217 finished with value: 0.6785157831178596 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:30,810] Trial 218 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:32,271] Trial 219 finished with value: 0.6065457884749937 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:33,690] Trial 220 finished with value: -0.039767714425311004 and parameters: {'C': 0.015625, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:35,196] Trial 221 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:36,744] Trial 222 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:38,290] Trial 223 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:39,791] Trial 224 finished with value: 0.6819607852225331 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:41,344] Trial 225 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:42,735] Trial 226 finished with value: 0.5994963384992582 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:44,266] Trial 227 finished with value: 0.6785157831178596 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:45,687] Trial 228 finished with value: 0.5801323498615762 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:47,206] Trial 229 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:48,814] Trial 230 finished with value: 0.0031949964835788713 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:50,382] Trial 231 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:51,941] Trial 232 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:53,489] Trial 233 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:55,003] Trial 234 finished with value: 0.24875903083753173 and parameters: {'C': 0.0625, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:56,552] Trial 235 finished with value: 0.6785157831178596 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:58,286] Trial 236 finished with value: 0.0012077611659093157 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:44:59,754] Trial 237 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:01,217] Trial 238 finished with value: 0.330150901855078 and parameters: {'C': 0.25, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:02,605] Trial 239 finished with value: 0.6515109192669557 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:04,178] Trial 240 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:05,704] Trial 241 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:07,127] Trial 242 finished with value: 0.6634760475253804 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:08,664] Trial 243 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:10,229] Trial 244 finished with value: 0.0074455371144603434 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:11,811] Trial 245 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:13,353] Trial 246 finished with value: 0.663815362946822 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:14,839] Trial 247 finished with value: -0.013642092305524257 and parameters: {'C': 0.125, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:16,309] Trial 248 finished with value: 0.022001240232187958 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:17,820] Trial 249 finished with value: 0.532775195251757 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7106404476592154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (r2_score): 0.7106\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698333    0.720682    0.701664    0.747780   \n",
      "1                    TP   29.000000   37.000000   35.000000   41.000000   \n",
      "2                    TN  202.000000  195.000000  198.000000  198.000000   \n",
      "3                    FP    0.000000    5.000000    2.000000    1.000000   \n",
      "4                    FN   37.000000   31.000000   33.000000   28.000000   \n",
      "5              Accuracy    0.861940    0.865672    0.869403    0.891791   \n",
      "6             Precision    1.000000    0.880952    0.945946    0.976190   \n",
      "7           Sensitivity    0.439394    0.544118    0.514706    0.594203   \n",
      "8           Specificity    1.000000    0.975000    0.990000    0.995000   \n",
      "9              F1 score    0.610526    0.672727    0.666667    0.738739   \n",
      "10  F1 score (weighted)    0.840847    0.853896    0.854821    0.882068   \n",
      "11     F1 score (macro)    0.763313    0.794110    0.792730    0.835252   \n",
      "12    Balanced Accuracy    0.719697    0.759559    0.752353    0.794589   \n",
      "13                  MCC    0.609402    0.621379    0.636650    0.708628   \n",
      "14                  NPV    0.845200    0.862800    0.857100    0.876100   \n",
      "15              ROC_AUC    0.719697    0.759559    0.752353    0.794589   \n",
      "\n",
      "          Set4  \n",
      "0     0.736532  \n",
      "1    37.000000  \n",
      "2   201.000000  \n",
      "3     1.000000  \n",
      "4    29.000000  \n",
      "5     0.888060  \n",
      "6     0.973684  \n",
      "7     0.560606  \n",
      "8     0.995000  \n",
      "9     0.711538  \n",
      "10    0.876619  \n",
      "11    0.821047  \n",
      "12    0.777828  \n",
      "13    0.686274  \n",
      "14    0.873900  \n",
      "15    0.777828  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "r2_scores = r2_score(Y_testSet4, y_pred_svm_4)\n",
    "# now convert the resuls to binary with cutoff 6.4\n",
    "Y_testSet4_cat = np.where(((Y_testSet4>=2) | (Y_testSet4<=-2)), 1, 0) \n",
    "y_pred_svm_4_cat = np.where(((y_pred_svm_4 >= 2) | (y_pred_svm_4 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Precision = precision_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Sensitivity = recall_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4_cat, y_pred_svm_4_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4_cat, y_pred_svm_4_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4_cat, y_pred_svm_4_cat)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:45:19,434] Trial 250 finished with value: 0.5679867804574567 and parameters: {'C': 0.5, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:20,934] Trial 251 finished with value: 0.0362228864429421 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:22,380] Trial 252 finished with value: 0.687734509483451 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:23,860] Trial 253 finished with value: 0.02338232570105254 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:25,240] Trial 254 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:26,623] Trial 255 finished with value: 0.06674822974628226 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:27,927] Trial 256 finished with value: 0.6879000382319018 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:29,265] Trial 257 finished with value: 0.2554945060747098 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:30,631] Trial 258 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:31,903] Trial 259 finished with value: 0.1384882150410203 and parameters: {'C': 0.03125, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:33,202] Trial 260 finished with value: 0.6879000382319018 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:34,528] Trial 261 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:35,844] Trial 262 finished with value: 0.5507360309164196 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:37,095] Trial 263 finished with value: 0.6877463051922391 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:38,462] Trial 264 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:39,854] Trial 265 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:41,159] Trial 266 finished with value: 0.6879000382319018 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:42,467] Trial 267 finished with value: 0.652103308235741 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:43,840] Trial 268 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:45,149] Trial 269 finished with value: 0.07986841333255101 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:46,385] Trial 270 finished with value: 0.5806532142874811 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:47,719] Trial 271 finished with value: 0.6184000453482683 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:49,279] Trial 272 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:50,663] Trial 273 finished with value: 0.3322550517616044 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:52,105] Trial 274 finished with value: 0.6879000382319018 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:53,522] Trial 275 finished with value: 0.23617912849920558 and parameters: {'C': 0.0625, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:55,097] Trial 276 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:56,554] Trial 277 finished with value: 0.6879000382319018 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:57,998] Trial 278 finished with value: 0.48262833088752266 and parameters: {'C': 0.25, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:45:59,431] Trial 279 finished with value: 0.6879000382319018 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:00,897] Trial 280 finished with value: 0.3271737741666442 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:02,482] Trial 281 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:03,876] Trial 282 finished with value: 0.6392808934572827 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:05,187] Trial 283 finished with value: 0.6734573847727405 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:06,625] Trial 284 finished with value: 0.024395744477233382 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:08,158] Trial 285 finished with value: -0.02012721326350273 and parameters: {'C': 0.0078125, 'gamma': 8.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:09,444] Trial 286 finished with value: 0.5679867804574567 and parameters: {'C': 0.5, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:10,783] Trial 287 finished with value: 0.6879000382319018 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:12,110] Trial 288 finished with value: 0.45188635707383584 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:13,396] Trial 289 finished with value: 0.36313973526370164 and parameters: {'C': 0.125, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:14,779] Trial 290 finished with value: 0.02818809717587408 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:16,078] Trial 291 finished with value: 0.6879000382319018 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:17,434] Trial 292 finished with value: 0.6668536395182562 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:18,827] Trial 293 finished with value: 0.03622288644294205 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:20,284] Trial 294 finished with value: 0.02338232570105254 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:21,638] Trial 295 finished with value: 0.5277209854137705 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:23,078] Trial 296 finished with value: 0.6625717475303624 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:24,362] Trial 297 finished with value: 0.6879000382319018 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:25,839] Trial 298 finished with value: 0.23420535483572671 and parameters: {'C': 2.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:27,187] Trial 299 finished with value: -0.016357101376195636 and parameters: {'C': 0.03125, 'gamma': 0.25}. Best is trial 101 with value: 0.7106404476592154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (r2_score): 0.7106\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698333    0.720682    0.701664    0.747780   \n",
      "1                    TP   29.000000   37.000000   35.000000   41.000000   \n",
      "2                    TN  202.000000  195.000000  198.000000  198.000000   \n",
      "3                    FP    0.000000    5.000000    2.000000    1.000000   \n",
      "4                    FN   37.000000   31.000000   33.000000   28.000000   \n",
      "5              Accuracy    0.861940    0.865672    0.869403    0.891791   \n",
      "6             Precision    1.000000    0.880952    0.945946    0.976190   \n",
      "7           Sensitivity    0.439394    0.544118    0.514706    0.594203   \n",
      "8           Specificity    1.000000    0.975000    0.990000    0.995000   \n",
      "9              F1 score    0.610526    0.672727    0.666667    0.738739   \n",
      "10  F1 score (weighted)    0.840847    0.853896    0.854821    0.882068   \n",
      "11     F1 score (macro)    0.763313    0.794110    0.792730    0.835252   \n",
      "12    Balanced Accuracy    0.719697    0.759559    0.752353    0.794589   \n",
      "13                  MCC    0.609402    0.621379    0.636650    0.708628   \n",
      "14                  NPV    0.845200    0.862800    0.857100    0.876100   \n",
      "15              ROC_AUC    0.719697    0.759559    0.752353    0.794589   \n",
      "\n",
      "          Set4        Set5  \n",
      "0     0.736532    0.714568  \n",
      "1    37.000000   38.000000  \n",
      "2   201.000000  197.000000  \n",
      "3     1.000000    3.000000  \n",
      "4    29.000000   30.000000  \n",
      "5     0.888060    0.876866  \n",
      "6     0.973684    0.926829  \n",
      "7     0.560606    0.558824  \n",
      "8     0.995000    0.985000  \n",
      "9     0.711538    0.697248  \n",
      "10    0.876619    0.865508  \n",
      "11    0.821047    0.809982  \n",
      "12    0.777828    0.771912  \n",
      "13    0.686274    0.657389  \n",
      "14    0.873900    0.867800  \n",
      "15    0.777828    0.771912  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "r2_scores = r2_score(Y_testSet5, y_pred_svm_5)\n",
    "# now convert the resuls to binary with cutoff 6.5\n",
    "Y_testSet5_cat = np.where(((Y_testSet5>=2) | (Y_testSet5<=-2)), 1, 0) \n",
    "y_pred_svm_5_cat = np.where(((y_pred_svm_5 >= 2) | (y_pred_svm_5 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Precision = precision_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Sensitivity = recall_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5_cat, y_pred_svm_5_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5_cat, y_pred_svm_5_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5_cat, y_pred_svm_5_cat)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:46:28,940] Trial 300 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:30,488] Trial 301 finished with value: 0.6929699467898144 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:31,965] Trial 302 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:33,470] Trial 303 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:34,953] Trial 304 finished with value: 0.692185533492272 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:36,274] Trial 305 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:37,656] Trial 306 finished with value: 0.5494949349011755 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:39,063] Trial 307 finished with value: 0.6918000947511651 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:40,455] Trial 308 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:41,880] Trial 309 finished with value: 0.6549256121007361 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:43,264] Trial 310 finished with value: 0.07090310861111036 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:44,656] Trial 311 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:46,032] Trial 312 finished with value: 0.04761127565607807 and parameters: {'C': 0.0625, 'gamma': 0.0009765625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:47,347] Trial 313 finished with value: 0.20743573316028976 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:48,759] Trial 314 finished with value: 0.6769491845846709 and parameters: {'C': 16.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:50,354] Trial 315 finished with value: 0.6929699467898144 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:51,809] Trial 316 finished with value: -0.009297880398274316 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:53,185] Trial 317 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:54,607] Trial 318 finished with value: 0.6929699467898144 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:55,995] Trial 319 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:57,336] Trial 320 finished with value: 0.32314826837216465 and parameters: {'C': 1.0, 'gamma': 0.00048828125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:46:58,726] Trial 321 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:00,180] Trial 322 finished with value: 0.6929699467898144 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:01,934] Trial 323 finished with value: 0.008008096490228312 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:03,326] Trial 324 finished with value: 0.35853214136193 and parameters: {'C': 0.125, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:04,796] Trial 325 finished with value: -0.032523677283274044 and parameters: {'C': 0.0078125, 'gamma': 2.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:06,283] Trial 326 finished with value: 0.6929699467898144 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:07,736] Trial 327 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:09,171] Trial 328 finished with value: 0.6729267979893596 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:10,586] Trial 329 finished with value: 0.562086433211393 and parameters: {'C': 0.5, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:11,979] Trial 330 finished with value: 0.6327311582232211 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:13,470] Trial 331 finished with value: 0.012483684832561537 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:14,887] Trial 332 finished with value: 0.6929699467898144 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:16,214] Trial 333 finished with value: 0.4454352576507442 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:17,599] Trial 334 finished with value: 0.13010876721552958 and parameters: {'C': 0.03125, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:19,017] Trial 335 finished with value: 0.6929699467898144 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:20,393] Trial 336 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:21,885] Trial 337 finished with value: 0.008177755947569965 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:23,362] Trial 338 finished with value: 0.018659192884991162 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:24,713] Trial 339 finished with value: 0.6554868249632347 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:26,149] Trial 340 finished with value: 0.6929699467898144 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:27,634] Trial 341 finished with value: 0.24727523239745292 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:29,050] Trial 342 finished with value: 0.5216879338638416 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:30,453] Trial 343 finished with value: 0.6741597870828688 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:31,987] Trial 344 finished with value: 0.049873566631160625 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:33,455] Trial 345 finished with value: 0.6929699467898144 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:34,915] Trial 346 finished with value: 0.060597527566927475 and parameters: {'C': 0.015625, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:36,312] Trial 347 finished with value: 0.5494949349011755 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:37,708] Trial 348 finished with value: 0.6800404085938963 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:39,121] Trial 349 finished with value: 0.6918000947511651 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 101 with value: 0.7106404476592154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (r2_score): 0.7106\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698333    0.720682    0.701664    0.747780   \n",
      "1                    TP   29.000000   37.000000   35.000000   41.000000   \n",
      "2                    TN  202.000000  195.000000  198.000000  198.000000   \n",
      "3                    FP    0.000000    5.000000    2.000000    1.000000   \n",
      "4                    FN   37.000000   31.000000   33.000000   28.000000   \n",
      "5              Accuracy    0.861940    0.865672    0.869403    0.891791   \n",
      "6             Precision    1.000000    0.880952    0.945946    0.976190   \n",
      "7           Sensitivity    0.439394    0.544118    0.514706    0.594203   \n",
      "8           Specificity    1.000000    0.975000    0.990000    0.995000   \n",
      "9              F1 score    0.610526    0.672727    0.666667    0.738739   \n",
      "10  F1 score (weighted)    0.840847    0.853896    0.854821    0.882068   \n",
      "11     F1 score (macro)    0.763313    0.794110    0.792730    0.835252   \n",
      "12    Balanced Accuracy    0.719697    0.759559    0.752353    0.794589   \n",
      "13                  MCC    0.609402    0.621379    0.636650    0.708628   \n",
      "14                  NPV    0.845200    0.862800    0.857100    0.876100   \n",
      "15              ROC_AUC    0.719697    0.759559    0.752353    0.794589   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0     0.736532    0.714568    0.729086  \n",
      "1    37.000000   38.000000   33.000000  \n",
      "2   201.000000  197.000000  199.000000  \n",
      "3     1.000000    3.000000    1.000000  \n",
      "4    29.000000   30.000000   35.000000  \n",
      "5     0.888060    0.876866    0.865672  \n",
      "6     0.973684    0.926829    0.970588  \n",
      "7     0.560606    0.558824    0.485294  \n",
      "8     0.995000    0.985000    0.995000  \n",
      "9     0.711538    0.697248    0.647059  \n",
      "10    0.876619    0.865508    0.848545  \n",
      "11    0.821047    0.809982    0.782055  \n",
      "12    0.777828    0.771912    0.740147  \n",
      "13    0.686274    0.657389    0.627956  \n",
      "14    0.873900    0.867800    0.850400  \n",
      "15    0.777828    0.771912    0.740147  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "r2_scores = r2_score(Y_testSet6, y_pred_svm_6)\n",
    "# now convert the resuls to binary with cutoff 6.6\n",
    "Y_testSet6_cat = np.where(((Y_testSet6>=2) | (Y_testSet6<=-2)), 1, 0) \n",
    "y_pred_svm_6_cat = np.where(((y_pred_svm_6 >= 2) | (y_pred_svm_6 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Precision = precision_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Sensitivity = recall_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6_cat, y_pred_svm_6_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6_cat, y_pred_svm_6_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6_cat, y_pred_svm_6_cat)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:47:40,770] Trial 350 finished with value: 0.6847600637085776 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:42,076] Trial 351 finished with value: 0.25113766395687376 and parameters: {'C': 0.0625, 'gamma': 0.0078125}. Best is trial 101 with value: 0.7106404476592154.\n",
      "[I 2023-12-20 08:47:43,400] Trial 352 finished with value: 0.7180691487213032 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:44,689] Trial 353 finished with value: 0.7180691487213032 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:46,015] Trial 354 finished with value: 0.7180691487213032 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:47,388] Trial 355 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:48,668] Trial 356 finished with value: 0.677461263678498 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:49,994] Trial 357 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:51,326] Trial 358 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:52,661] Trial 359 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:54,000] Trial 360 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:55,374] Trial 361 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:56,671] Trial 362 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:57,993] Trial 363 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:47:59,352] Trial 364 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:00,726] Trial 365 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:02,079] Trial 366 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:03,463] Trial 367 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:04,825] Trial 368 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:06,189] Trial 369 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:07,533] Trial 370 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:08,834] Trial 371 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:10,175] Trial 372 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:11,503] Trial 373 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:12,828] Trial 374 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:14,151] Trial 375 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:15,495] Trial 376 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:16,827] Trial 377 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:18,157] Trial 378 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:19,483] Trial 379 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:20,779] Trial 380 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:22,104] Trial 381 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:23,431] Trial 382 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:24,760] Trial 383 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:26,088] Trial 384 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:27,423] Trial 385 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:28,750] Trial 386 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:30,158] Trial 387 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:31,580] Trial 388 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:33,029] Trial 389 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:34,407] Trial 390 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:35,776] Trial 391 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:37,146] Trial 392 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:38,455] Trial 393 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:39,775] Trial 394 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:41,146] Trial 395 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:42,516] Trial 396 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:43,798] Trial 397 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:45,071] Trial 398 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:46,380] Trial 399 finished with value: 0.7176985564753953 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (r2_score): 0.7181\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698333    0.720682    0.701664    0.747780   \n",
      "1                    TP   29.000000   37.000000   35.000000   41.000000   \n",
      "2                    TN  202.000000  195.000000  198.000000  198.000000   \n",
      "3                    FP    0.000000    5.000000    2.000000    1.000000   \n",
      "4                    FN   37.000000   31.000000   33.000000   28.000000   \n",
      "5              Accuracy    0.861940    0.865672    0.869403    0.891791   \n",
      "6             Precision    1.000000    0.880952    0.945946    0.976190   \n",
      "7           Sensitivity    0.439394    0.544118    0.514706    0.594203   \n",
      "8           Specificity    1.000000    0.975000    0.990000    0.995000   \n",
      "9              F1 score    0.610526    0.672727    0.666667    0.738739   \n",
      "10  F1 score (weighted)    0.840847    0.853896    0.854821    0.882068   \n",
      "11     F1 score (macro)    0.763313    0.794110    0.792730    0.835252   \n",
      "12    Balanced Accuracy    0.719697    0.759559    0.752353    0.794589   \n",
      "13                  MCC    0.609402    0.621379    0.636650    0.708628   \n",
      "14                  NPV    0.845200    0.862800    0.857100    0.876100   \n",
      "15              ROC_AUC    0.719697    0.759559    0.752353    0.794589   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0     0.736532    0.714568    0.729086    0.641458  \n",
      "1    37.000000   38.000000   33.000000   35.000000  \n",
      "2   201.000000  197.000000  199.000000  199.000000  \n",
      "3     1.000000    3.000000    1.000000    2.000000  \n",
      "4    29.000000   30.000000   35.000000   32.000000  \n",
      "5     0.888060    0.876866    0.865672    0.873134  \n",
      "6     0.973684    0.926829    0.970588    0.945946  \n",
      "7     0.560606    0.558824    0.485294    0.522388  \n",
      "8     0.995000    0.985000    0.995000    0.990000  \n",
      "9     0.711538    0.697248    0.647059    0.673077  \n",
      "10    0.876619    0.865508    0.848545    0.859241  \n",
      "11    0.821047    0.809982    0.782055    0.797187  \n",
      "12    0.777828    0.771912    0.740147    0.756219  \n",
      "13    0.686274    0.657389    0.627956    0.643235  \n",
      "14    0.873900    0.867800    0.850400    0.861500  \n",
      "15    0.777828    0.771912    0.740147    0.756219  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "r2_scores = r2_score(Y_testSet7, y_pred_svm_7)\n",
    "# now convert the resuls to binary with cutoff 6.7\n",
    "Y_testSet7_cat = np.where(((Y_testSet7>=2) | (Y_testSet7<=-2)), 1, 0) \n",
    "y_pred_svm_7_cat = np.where(((y_pred_svm_7 >= 2) | (y_pred_svm_7 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Precision = precision_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Sensitivity = recall_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7_cat, y_pred_svm_7_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7_cat, y_pred_svm_7_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7_cat, y_pred_svm_7_cat)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:48:48,060] Trial 400 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:49,477] Trial 401 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:50,894] Trial 402 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:52,325] Trial 403 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:53,753] Trial 404 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:55,164] Trial 405 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:56,551] Trial 406 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:57,996] Trial 407 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:48:59,409] Trial 408 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:00,841] Trial 409 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:02,278] Trial 410 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:03,704] Trial 411 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:05,123] Trial 412 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:06,548] Trial 413 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:08,021] Trial 414 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:09,506] Trial 415 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:11,047] Trial 416 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:12,551] Trial 417 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:14,096] Trial 418 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:15,618] Trial 419 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:17,155] Trial 420 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:18,649] Trial 421 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:20,115] Trial 422 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:21,633] Trial 423 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:23,180] Trial 424 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:24,683] Trial 425 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:26,225] Trial 426 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:27,717] Trial 427 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:29,238] Trial 428 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:30,728] Trial 429 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:32,272] Trial 430 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:33,737] Trial 431 finished with value: 0.35846878011548866 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:35,274] Trial 432 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:36,783] Trial 433 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:38,296] Trial 434 finished with value: 0.6458569520798809 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:39,819] Trial 435 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:41,294] Trial 436 finished with value: 0.6285488045979158 and parameters: {'C': 16.0, 'gamma': 0.00048828125}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:42,672] Trial 437 finished with value: 0.46998492251978885 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:44,095] Trial 438 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:45,519] Trial 439 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:46,939] Trial 440 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:48,354] Trial 441 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:49,854] Trial 442 finished with value: 0.010537627994738574 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:51,208] Trial 443 finished with value: 0.685328027252875 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:52,784] Trial 444 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:54,603] Trial 445 finished with value: 0.009776120164038404 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:56,111] Trial 446 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:57,738] Trial 447 finished with value: 0.6583712476952779 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:49:59,236] Trial 448 finished with value: 0.7091487652774732 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:00,687] Trial 449 finished with value: 0.5395829903771447 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 352 with value: 0.7180691487213032.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (r2_score): 0.7181\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698333    0.720682    0.701664    0.747780   \n",
      "1                    TP   29.000000   37.000000   35.000000   41.000000   \n",
      "2                    TN  202.000000  195.000000  198.000000  198.000000   \n",
      "3                    FP    0.000000    5.000000    2.000000    1.000000   \n",
      "4                    FN   37.000000   31.000000   33.000000   28.000000   \n",
      "5              Accuracy    0.861940    0.865672    0.869403    0.891791   \n",
      "6             Precision    1.000000    0.880952    0.945946    0.976190   \n",
      "7           Sensitivity    0.439394    0.544118    0.514706    0.594203   \n",
      "8           Specificity    1.000000    0.975000    0.990000    0.995000   \n",
      "9              F1 score    0.610526    0.672727    0.666667    0.738739   \n",
      "10  F1 score (weighted)    0.840847    0.853896    0.854821    0.882068   \n",
      "11     F1 score (macro)    0.763313    0.794110    0.792730    0.835252   \n",
      "12    Balanced Accuracy    0.719697    0.759559    0.752353    0.794589   \n",
      "13                  MCC    0.609402    0.621379    0.636650    0.708628   \n",
      "14                  NPV    0.845200    0.862800    0.857100    0.876100   \n",
      "15              ROC_AUC    0.719697    0.759559    0.752353    0.794589   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0     0.736532    0.714568    0.729086    0.641458    0.682945  \n",
      "1    37.000000   38.000000   33.000000   35.000000   38.000000  \n",
      "2   201.000000  197.000000  199.000000  199.000000  198.000000  \n",
      "3     1.000000    3.000000    1.000000    2.000000    2.000000  \n",
      "4    29.000000   30.000000   35.000000   32.000000   30.000000  \n",
      "5     0.888060    0.876866    0.865672    0.873134    0.880597  \n",
      "6     0.973684    0.926829    0.970588    0.945946    0.950000  \n",
      "7     0.560606    0.558824    0.485294    0.522388    0.558824  \n",
      "8     0.995000    0.985000    0.995000    0.990000    0.990000  \n",
      "9     0.711538    0.697248    0.647059    0.673077    0.703704  \n",
      "10    0.876619    0.865508    0.848545    0.859241    0.869025  \n",
      "11    0.821047    0.809982    0.782055    0.797187    0.814469  \n",
      "12    0.777828    0.771912    0.740147    0.756219    0.774412  \n",
      "13    0.686274    0.657389    0.627956    0.643235    0.670201  \n",
      "14    0.873900    0.867800    0.850400    0.861500    0.868400  \n",
      "15    0.777828    0.771912    0.740147    0.756219    0.774412  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "r2_scores = r2_score(Y_testSet8, y_pred_svm_8)\n",
    "# now convert the resuls to binary with cutoff 6.8\n",
    "Y_testSet8_cat = np.where(((Y_testSet8>=2) | (Y_testSet8<=-2)), 1, 0) \n",
    "y_pred_svm_8_cat = np.where(((y_pred_svm_8 >= 2) | (y_pred_svm_8 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Precision = precision_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Sensitivity = recall_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8_cat, y_pred_svm_8_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8_cat, y_pred_svm_8_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8_cat, y_pred_svm_8_cat)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-20 08:50:02,530] Trial 450 finished with value: 0.01957819986408198 and parameters: {'C': 16.0, 'gamma': 1.0}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:03,984] Trial 451 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:05,361] Trial 452 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:06,883] Trial 453 finished with value: 0.02680990508242098 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:08,318] Trial 454 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:09,721] Trial 455 finished with value: 0.5927049233953656 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:11,153] Trial 456 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:12,686] Trial 457 finished with value: 0.05677057124554145 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:14,198] Trial 458 finished with value: 0.25045489904358736 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:15,773] Trial 459 finished with value: 0.014883410446033774 and parameters: {'C': 16.0, 'gamma': 4.0}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:17,167] Trial 460 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:18,605] Trial 461 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:20,021] Trial 462 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:21,430] Trial 463 finished with value: 0.527556866192876 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:22,868] Trial 464 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:24,347] Trial 465 finished with value: 0.5578823382509454 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:25,790] Trial 466 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:27,273] Trial 467 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:28,662] Trial 468 finished with value: 0.3057175123170027 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:30,072] Trial 469 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:31,503] Trial 470 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:32,986] Trial 471 finished with value: 0.7150764528596343 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:34,498] Trial 472 finished with value: 0.7150903300743834 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:35,962] Trial 473 finished with value: -0.02230253931956596 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:37,402] Trial 474 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:38,816] Trial 475 finished with value: 0.6122371796960016 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:40,286] Trial 476 finished with value: 0.6482588446903377 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:41,708] Trial 477 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:43,098] Trial 478 finished with value: 0.4669985823320243 and parameters: {'C': 16.0, 'gamma': 6.103515625e-05}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:44,521] Trial 479 finished with value: 0.0011951869031886942 and parameters: {'C': 0.03125, 'gamma': 0.00048828125}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:45,947] Trial 480 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:47,365] Trial 481 finished with value: 0.7028166996704647 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:48,789] Trial 482 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:50,354] Trial 483 finished with value: 0.01606727591653523 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:51,749] Trial 484 finished with value: 0.7131991925916132 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:53,143] Trial 485 finished with value: 0.6867676909855422 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:54,542] Trial 486 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:55,937] Trial 487 finished with value: 0.08872746226681026 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:57,514] Trial 488 finished with value: 0.6585537907034074 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:50:59,331] Trial 489 finished with value: 0.014705950447263716 and parameters: {'C': 16.0, 'gamma': 8.0}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:00,844] Trial 490 finished with value: 0.7151079546048574 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:02,291] Trial 491 finished with value: -0.01911974588038746 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:03,749] Trial 492 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:05,136] Trial 493 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:06,575] Trial 494 finished with value: 0.020446518308450522 and parameters: {'C': 1.0, 'gamma': 0.5}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:07,994] Trial 495 finished with value: 0.5372652213387645 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:09,398] Trial 496 finished with value: 0.5927049233953656 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:10,841] Trial 497 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:12,265] Trial 498 finished with value: 0.7155271814900284 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n",
      "[I 2023-12-20 08:51:13,639] Trial 499 finished with value: 0.527556866192876 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 352 with value: 0.7180691487213032.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7181\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    R2    0.698333    0.720682    0.701664    0.747780   \n",
      "1                    TP   29.000000   37.000000   35.000000   41.000000   \n",
      "2                    TN  202.000000  195.000000  198.000000  198.000000   \n",
      "3                    FP    0.000000    5.000000    2.000000    1.000000   \n",
      "4                    FN   37.000000   31.000000   33.000000   28.000000   \n",
      "5              Accuracy    0.861940    0.865672    0.869403    0.891791   \n",
      "6             Precision    1.000000    0.880952    0.945946    0.976190   \n",
      "7           Sensitivity    0.439394    0.544118    0.514706    0.594203   \n",
      "8           Specificity    1.000000    0.975000    0.990000    0.995000   \n",
      "9              F1 score    0.610526    0.672727    0.666667    0.738739   \n",
      "10  F1 score (weighted)    0.840847    0.853896    0.854821    0.882068   \n",
      "11     F1 score (macro)    0.763313    0.794110    0.792730    0.835252   \n",
      "12    Balanced Accuracy    0.719697    0.759559    0.752353    0.794589   \n",
      "13                  MCC    0.609402    0.621379    0.636650    0.708628   \n",
      "14                  NPV    0.845200    0.862800    0.857100    0.876100   \n",
      "15              ROC_AUC    0.719697    0.759559    0.752353    0.794589   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0     0.736532    0.714568    0.729086    0.641458    0.682945    0.668332  \n",
      "1    37.000000   38.000000   33.000000   35.000000   38.000000   32.000000  \n",
      "2   201.000000  197.000000  199.000000  199.000000  198.000000  198.000000  \n",
      "3     1.000000    3.000000    1.000000    2.000000    2.000000    3.000000  \n",
      "4    29.000000   30.000000   35.000000   32.000000   30.000000   35.000000  \n",
      "5     0.888060    0.876866    0.865672    0.873134    0.880597    0.858209  \n",
      "6     0.973684    0.926829    0.970588    0.945946    0.950000    0.914286  \n",
      "7     0.560606    0.558824    0.485294    0.522388    0.558824    0.477612  \n",
      "8     0.995000    0.985000    0.995000    0.990000    0.990000    0.985100  \n",
      "9     0.711538    0.697248    0.647059    0.673077    0.703704    0.627451  \n",
      "10    0.876619    0.865508    0.848545    0.859241    0.869025    0.841195  \n",
      "11    0.821047    0.809982    0.782055    0.797187    0.814469    0.769947  \n",
      "12    0.777828    0.771912    0.740147    0.756219    0.774412    0.731343  \n",
      "13    0.686274    0.657389    0.627956    0.643235    0.670201    0.594580  \n",
      "14    0.873900    0.867800    0.850400    0.861500    0.868400    0.849800  \n",
      "15    0.777828    0.771912    0.740147    0.756219    0.774412    0.731343  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# r2 score of the regression model before evaluating categorical evaluation parameters\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "r2_scores = r2_score(Y_testSet9, y_pred_svm_9)\n",
    "# now convert the resuls to binary with cutoff 6.9\n",
    "Y_testSet9_cat = np.where(((Y_testSet9>=2) | (Y_testSet9<=-2)), 1, 0) \n",
    "y_pred_svm_9_cat = np.where(((y_pred_svm_9 >= 2) | (y_pred_svm_9 <= -2)), 1, 0)\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Precision = precision_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Sensitivity = recall_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9_cat, y_pred_svm_9_cat)      \n",
    "f1_scores_W = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9_cat, y_pred_svm_9_cat, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "MCC = matthews_corrcoef(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9_cat, y_pred_svm_9_cat)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(r2_scores), np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "7b0e56b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (r2_score): 0.7181\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (r2_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.706634</td>\n",
       "      <td>0.052244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>1.751190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>99.500000</td>\n",
       "      <td>1.080123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>15.600000</td>\n",
       "      <td>2.366432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.876041</td>\n",
       "      <td>0.016846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.949129</td>\n",
       "      <td>0.039241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.533972</td>\n",
       "      <td>0.061923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990050</td>\n",
       "      <td>0.008111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.681016</td>\n",
       "      <td>0.048869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.862527</td>\n",
       "      <td>0.020810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.802031</td>\n",
       "      <td>0.029384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.762011</td>\n",
       "      <td>0.030001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.652220</td>\n",
       "      <td>0.045521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.864750</td>\n",
       "      <td>0.018284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.762011</td>\n",
       "      <td>0.030001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.706634     0.052244\n",
       "1                    TP        17.800000     1.751190\n",
       "2                    TN        99.500000     1.080123\n",
       "3                    FP         1.000000     0.816497\n",
       "4                    FN        15.600000     2.366432\n",
       "5              Accuracy         0.876041     0.016846\n",
       "6             Precision         0.949129     0.039241\n",
       "7           Sensitivity         0.533972     0.061923\n",
       "8           Specificity         0.990050     0.008111\n",
       "9              F1 score         0.681016     0.048869\n",
       "10  F1 score (weighted)         0.862527     0.020810\n",
       "11     F1 score (macro)         0.802031     0.029384\n",
       "12    Balanced Accuracy         0.762011     0.030001\n",
       "13                  MCC         0.652220     0.045521\n",
       "14                  NPV         0.864750     0.018284\n",
       "15              ROC_AUC         0.762011     0.030001"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.698333</td>\n",
       "      <td>0.720682</td>\n",
       "      <td>0.701664</td>\n",
       "      <td>0.747780</td>\n",
       "      <td>0.736532</td>\n",
       "      <td>0.714568</td>\n",
       "      <td>0.729086</td>\n",
       "      <td>0.641458</td>\n",
       "      <td>0.682945</td>\n",
       "      <td>0.668332</td>\n",
       "      <td>0.704138</td>\n",
       "      <td>0.032759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>3.472111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TN</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.500000</td>\n",
       "      <td>1.957890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FP</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FN</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.943920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.858209</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.011125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Precision</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.945946</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.948442</td>\n",
       "      <td>0.034576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.439394</td>\n",
       "      <td>0.544118</td>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.560606</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.485294</td>\n",
       "      <td>0.522388</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.477612</td>\n",
       "      <td>0.525597</td>\n",
       "      <td>0.047105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.990010</td>\n",
       "      <td>0.007063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.672727</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.697248</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.673077</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.674874</td>\n",
       "      <td>0.039431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.840847</td>\n",
       "      <td>0.853896</td>\n",
       "      <td>0.854821</td>\n",
       "      <td>0.882068</td>\n",
       "      <td>0.876619</td>\n",
       "      <td>0.865508</td>\n",
       "      <td>0.848545</td>\n",
       "      <td>0.859241</td>\n",
       "      <td>0.869025</td>\n",
       "      <td>0.841195</td>\n",
       "      <td>0.859176</td>\n",
       "      <td>0.014081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.763313</td>\n",
       "      <td>0.794110</td>\n",
       "      <td>0.792730</td>\n",
       "      <td>0.835252</td>\n",
       "      <td>0.821047</td>\n",
       "      <td>0.809982</td>\n",
       "      <td>0.782055</td>\n",
       "      <td>0.797187</td>\n",
       "      <td>0.814469</td>\n",
       "      <td>0.769947</td>\n",
       "      <td>0.798009</td>\n",
       "      <td>0.022661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.719697</td>\n",
       "      <td>0.759559</td>\n",
       "      <td>0.752353</td>\n",
       "      <td>0.794589</td>\n",
       "      <td>0.777828</td>\n",
       "      <td>0.771912</td>\n",
       "      <td>0.740147</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.774412</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.757806</td>\n",
       "      <td>0.022921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.609402</td>\n",
       "      <td>0.621379</td>\n",
       "      <td>0.636650</td>\n",
       "      <td>0.708628</td>\n",
       "      <td>0.686274</td>\n",
       "      <td>0.657389</td>\n",
       "      <td>0.627956</td>\n",
       "      <td>0.643235</td>\n",
       "      <td>0.670201</td>\n",
       "      <td>0.594580</td>\n",
       "      <td>0.645569</td>\n",
       "      <td>0.035384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.845200</td>\n",
       "      <td>0.862800</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.876100</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.867800</td>\n",
       "      <td>0.850400</td>\n",
       "      <td>0.861500</td>\n",
       "      <td>0.868400</td>\n",
       "      <td>0.849800</td>\n",
       "      <td>0.861300</td>\n",
       "      <td>0.010544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.719697</td>\n",
       "      <td>0.759559</td>\n",
       "      <td>0.752353</td>\n",
       "      <td>0.794589</td>\n",
       "      <td>0.777828</td>\n",
       "      <td>0.771912</td>\n",
       "      <td>0.740147</td>\n",
       "      <td>0.756219</td>\n",
       "      <td>0.774412</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.757806</td>\n",
       "      <td>0.022921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    R2    0.698333    0.720682    0.701664    0.747780   \n",
       "1                    TP   29.000000   37.000000   35.000000   41.000000   \n",
       "2                    TN  202.000000  195.000000  198.000000  198.000000   \n",
       "3                    FP    0.000000    5.000000    2.000000    1.000000   \n",
       "4                    FN   37.000000   31.000000   33.000000   28.000000   \n",
       "5              Accuracy    0.861940    0.865672    0.869403    0.891791   \n",
       "6             Precision    1.000000    0.880952    0.945946    0.976190   \n",
       "7           Sensitivity    0.439394    0.544118    0.514706    0.594203   \n",
       "8           Specificity    1.000000    0.975000    0.990000    0.995000   \n",
       "9              F1 score    0.610526    0.672727    0.666667    0.738739   \n",
       "10  F1 score (weighted)    0.840847    0.853896    0.854821    0.882068   \n",
       "11     F1 score (macro)    0.763313    0.794110    0.792730    0.835252   \n",
       "12    Balanced Accuracy    0.719697    0.759559    0.752353    0.794589   \n",
       "13                  MCC    0.609402    0.621379    0.636650    0.708628   \n",
       "14                  NPV    0.845200    0.862800    0.857100    0.876100   \n",
       "15              ROC_AUC    0.719697    0.759559    0.752353    0.794589   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0     0.736532    0.714568    0.729086    0.641458    0.682945    0.668332   \n",
       "1    37.000000   38.000000   33.000000   35.000000   38.000000   32.000000   \n",
       "2   201.000000  197.000000  199.000000  199.000000  198.000000  198.000000   \n",
       "3     1.000000    3.000000    1.000000    2.000000    2.000000    3.000000   \n",
       "4    29.000000   30.000000   35.000000   32.000000   30.000000   35.000000   \n",
       "5     0.888060    0.876866    0.865672    0.873134    0.880597    0.858209   \n",
       "6     0.973684    0.926829    0.970588    0.945946    0.950000    0.914286   \n",
       "7     0.560606    0.558824    0.485294    0.522388    0.558824    0.477612   \n",
       "8     0.995000    0.985000    0.995000    0.990000    0.990000    0.985100   \n",
       "9     0.711538    0.697248    0.647059    0.673077    0.703704    0.627451   \n",
       "10    0.876619    0.865508    0.848545    0.859241    0.869025    0.841195   \n",
       "11    0.821047    0.809982    0.782055    0.797187    0.814469    0.769947   \n",
       "12    0.777828    0.771912    0.740147    0.756219    0.774412    0.731343   \n",
       "13    0.686274    0.657389    0.627956    0.643235    0.670201    0.594580   \n",
       "14    0.873900    0.867800    0.850400    0.861500    0.868400    0.849800   \n",
       "15    0.777828    0.771912    0.740147    0.756219    0.774412    0.731343   \n",
       "\n",
       "           ave       std  \n",
       "0     0.704138  0.032759  \n",
       "1    35.500000  3.472111  \n",
       "2   198.500000  1.957890  \n",
       "3     2.000000  1.414214  \n",
       "4    32.000000  2.943920  \n",
       "5     0.873134  0.011125  \n",
       "6     0.948442  0.034576  \n",
       "7     0.525597  0.047105  \n",
       "8     0.990010  0.007063  \n",
       "9     0.674874  0.039431  \n",
       "10    0.859176  0.014081  \n",
       "11    0.798009  0.022661  \n",
       "12    0.757806  0.022921  \n",
       "13    0.645569  0.035384  \n",
       "14    0.861300  0.010544  \n",
       "15    0.757806  0.022921  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.719408</td>\n",
       "      <td>0.046880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.877519</td>\n",
       "      <td>0.020528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.949802</td>\n",
       "      <td>0.041396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.537888</td>\n",
       "      <td>0.081152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.990246</td>\n",
       "      <td>0.008868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.683061</td>\n",
       "      <td>0.068897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.863979</td>\n",
       "      <td>0.025853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.803526</td>\n",
       "      <td>0.040174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.764066</td>\n",
       "      <td>0.040380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.655111</td>\n",
       "      <td>0.063491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.866310</td>\n",
       "      <td>0.020120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.764066</td>\n",
       "      <td>0.040380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    R2         0.719408     0.046880\n",
       "1              Accuracy         0.877519     0.020528\n",
       "2             Precision         0.949802     0.041396\n",
       "3           Sensitivity         0.537888     0.081152\n",
       "4           Specificity         0.990246     0.008868\n",
       "5              F1 score         0.683061     0.068897\n",
       "6   F1 score (weighted)         0.863979     0.025853\n",
       "7      F1 score (macro)         0.803526     0.040174\n",
       "8     Balanced Accuracy         0.764066     0.040380\n",
       "9                   MCC         0.655111     0.063491\n",
       "10                  NPV         0.866310     0.020120\n",
       "11              ROC_AUC         0.764066     0.040380"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "r2_scores_outer = []\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVR(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        y_test_cat = np.where(((y_test>=2) | (y_test<=-2)), 1, 0) \n",
    "        y_pred_optimized_svm_cat = np.where(((y_pred_optimized_svm >= 2) | (y_pred_optimized_svm <= -2)), 1, 0)\n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test_cat, y_pred_optimized_svm_cat)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        \n",
    "        r2_scores_outer.append(r2_score(y_test, y_pred_optimized_svm))\n",
    "        Accuracy_outer.append(accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Precision_outer.append(precision_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Sensitivity_outer.append(recall_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        f1_scores_W_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test_cat, y_pred_optimized_svm_cat, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test_cat, y_pred_optimized_svm_cat))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['R2','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(r2_scores_outer), np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(r2_scores_outer, ddof=1), np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "svm_run0 = data_svm[['y_test_idx0', 'y_test0', 'y_pred_svm0']]\n",
    "svm_run0.sort_values(by='y_test_idx0', inplace=True)\n",
    "svm_run0.reset_index(inplace=True, drop=True)\n",
    "svm_run1 = data_svm[['y_test_idx1', 'y_test1', 'y_pred_svm1']]\n",
    "svm_run1.sort_values(by='y_test_idx1', inplace=True)\n",
    "svm_run1.reset_index(inplace=True, drop=True)\n",
    "svm_run2 = data_svm[['y_test_idx2', 'y_test2', 'y_pred_svm2']]\n",
    "svm_run2.sort_values(by='y_test_idx2', inplace=True)\n",
    "svm_run2.reset_index(inplace=True, drop=True)\n",
    "svm_run3 = data_svm[['y_test_idx3', 'y_test3', 'y_pred_svm3']]\n",
    "svm_run3.sort_values(by='y_test_idx3', inplace=True)\n",
    "svm_run3.reset_index(inplace=True, drop=True)\n",
    "svm_run4 = data_svm[['y_test_idx4', 'y_test4', 'y_pred_svm4']]\n",
    "svm_run4.sort_values(by='y_test_idx4', inplace=True)\n",
    "svm_run4.reset_index(inplace=True, drop=True)\n",
    "chembl_id = df['molecule_chembl_id']\n",
    "svm_5preds = pd.concat([chembl_id,svm_run0, svm_run1, svm_run2, svm_run3, svm_run4], axis=1)\n",
    "svm_5preds = svm_5preds[['molecule_chembl_id', 'y_test_idx0', 'y_test0', 'y_pred_svm0', 'y_pred_svm1', 'y_pred_svm2', 'y_pred_svm3', 'y_pred_svm4']]\n",
    "svm_5preds['y_pred_svm_ave'] = svm_5preds.iloc[:,2:11].mean(axis='columns', numeric_only=True)\n",
    "svm_5preds['y_pred_svm_std'] = svm_5preds.iloc[:,2:].std(axis='columns', numeric_only=True)\n",
    "# maybe also calculate the std for each value\n",
    "mat_met_optimized_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2869d8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scatter plot\n",
    "\n",
    "limits = -5,5\n",
    "plt.figsize=(10,10)\n",
    "\n",
    "plt.scatter(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'], marker=\".\")\n",
    "lin = np.linspace(*limits, 100)\n",
    "\n",
    "plt.ylabel(\"Predicted values (LightGBM)\")\n",
    "plt.xlabel(\"True values\")\n",
    "\n",
    "plt.xlim(limits)\n",
    "plt.ylim(limits)\n",
    "\n",
    "plt.annotate(\"R^2 = {:.3f}\".format(r2_score(svm_5preds['y_test0'], svm_5preds['y_pred_svm_ave'])), (-4, 4))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model r2_score 0.6822 with a standard deviation of 0.0650\n",
      "SVM optimized model r2_score 0.7164 with a standard deviation of 0.0690\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVR \n",
    "svm_baseline_CVscore = cross_val_score(svm_reg, X, Y, cv=10, scoring=\"r2\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"r2\")\n",
    "print(\"SVM baseline model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) r2_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model r2_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm.joblib']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_reg, \"OUTPUT/svm_reg.joblib\")\n",
    "#joblib.dump(optimized_svm, \"OUTPUT/optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "16bdada3-5a01-40a7-acca-1e1263dbaab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "80a6dc72-fa55-4a7e-b3de-2730b7621f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b9aa5f0b-2654-4980-be38-d6c0b7c3167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/PredResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    rf_5preds.to_excel(writer, sheet_name=\"RF\", )\n",
    "    lgbm_5preds.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    xgb_5preds.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    knn_5preds.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    svm_5preds.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
