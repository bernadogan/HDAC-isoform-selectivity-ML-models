{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingclfressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1and6 = Path(HERE).resolve().parents[1]/'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL1095764</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[9850747, 10427556, 6095547, 5213123, 12018511...</td>\n",
       "      <td>-0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4244887</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[11780358, 16948577, 2043088, 10483303, 266155...</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4280303</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[799147, 2011138, 345515, 476717, 193508, 1860...</td>\n",
       "      <td>-0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3910638</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[9076848, 2478511, 10872982, 3500173, 4556712,...</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL2047698</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[13007771, 18890305, 35941221, 11863091, 11250...</td>\n",
       "      <td>-2.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL1095764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4244887  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL4280303  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3      CHEMBL3910638  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL2047698  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "2  [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \n",
       "0  [9850747, 10427556, 6095547, 5213123, 12018511...              -0.60  \n",
       "1  [11780358, 16948577, 2043088, 10483303, 266155...               1.03  \n",
       "2  [799147, 2011138, 345515, 476717, 193508, 1860...              -0.77  \n",
       "3  [9076848, 2478511, 10872982, 3500173, 4556712,...               1.99  \n",
       "4  [13007771, 18890305, 35941221, 11863091, 11250...              -2.95  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1and6/\"HDAC1and6_SemiSel_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type_HDAC1</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>type_HDAC6</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>SelectivityRatio</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>109.647820</td>\n",
       "      <td>6.96</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.141254</td>\n",
       "      <td>9.85</td>\n",
       "      <td>776.247117</td>\n",
       "      <td>2.89</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>616.595002</td>\n",
       "      <td>6.21</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.169824</td>\n",
       "      <td>9.77</td>\n",
       "      <td>3630.780548</td>\n",
       "      <td>3.56</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4243347</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.995262</td>\n",
       "      <td>8.70</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.199526</td>\n",
       "      <td>9.70</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4247128</td>\n",
       "      <td>C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>83.176377</td>\n",
       "      <td>7.08</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.251189</td>\n",
       "      <td>9.60</td>\n",
       "      <td>331.131122</td>\n",
       "      <td>2.52</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4126811</td>\n",
       "      <td>CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>436.515832</td>\n",
       "      <td>6.36</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.331131</td>\n",
       "      <td>9.48</td>\n",
       "      <td>1318.256739</td>\n",
       "      <td>3.12</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>CHEMBL4167599</td>\n",
       "      <td>NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>4073.802778</td>\n",
       "      <td>5.39</td>\n",
       "      <td>IC50</td>\n",
       "      <td>50.118723</td>\n",
       "      <td>7.30</td>\n",
       "      <td>81.283052</td>\n",
       "      <td>1.91</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>CHEMBL4282471</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3388.441561</td>\n",
       "      <td>5.47</td>\n",
       "      <td>IC50</td>\n",
       "      <td>117.489756</td>\n",
       "      <td>6.93</td>\n",
       "      <td>28.840315</td>\n",
       "      <td>1.46</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>CHEMBL470843</td>\n",
       "      <td>O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6309.573445</td>\n",
       "      <td>5.20</td>\n",
       "      <td>IC50</td>\n",
       "      <td>173.780083</td>\n",
       "      <td>6.76</td>\n",
       "      <td>36.307805</td>\n",
       "      <td>1.56</td>\n",
       "      <td>Semi-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>28.183829</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Ki</td>\n",
       "      <td>245.470892</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>6.45</td>\n",
       "      <td>IC50</td>\n",
       "      <td>295.120923</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.202264</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1910 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "1         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "2         CHEMBL4243347  O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...   \n",
       "3         CHEMBL4247128  C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...   \n",
       "4         CHEMBL4126811  CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...   \n",
       "...                 ...                                                ...   \n",
       "1905      CHEMBL4167599  NCCCCNCCCCNCCCN1C(=O)c2ccc3c4c(ccc(c24)C1=O)C(...   \n",
       "1906      CHEMBL4282471  CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(NC(=O)CCCCC(=O)NO)...   \n",
       "1907       CHEMBL470843          O=C(/C=C/c1ccc(-c2cc(CN3CCOCC3)on2)cc1)NO   \n",
       "1908      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "1909      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "     type_HDAC1  Standard_Value_HDAC1  pChEMBL_HDAC1 type_HDAC6  \\\n",
       "0          IC50            109.647820           6.96       IC50   \n",
       "1          IC50            616.595002           6.21       IC50   \n",
       "2          IC50              1.995262           8.70       IC50   \n",
       "3          IC50             83.176377           7.08       IC50   \n",
       "4          IC50            436.515832           6.36       IC50   \n",
       "...         ...                   ...            ...        ...   \n",
       "1905       IC50           4073.802778           5.39       IC50   \n",
       "1906       IC50           3388.441561           5.47       IC50   \n",
       "1907       IC50           6309.573445           5.20       IC50   \n",
       "1908         Ki             28.183829           7.55         Ki   \n",
       "1909       IC50            354.813389           6.45       IC50   \n",
       "\n",
       "      Standard_Value_HDAC6  pChEMBL_HDAC6  SelectivityRatio  \\\n",
       "0                 0.141254           9.85        776.247117   \n",
       "1                 0.169824           9.77       3630.780548   \n",
       "2                 0.199526           9.70         10.000000   \n",
       "3                 0.251189           9.60        331.131122   \n",
       "4                 0.331131           9.48       1318.256739   \n",
       "...                    ...            ...               ...   \n",
       "1905             50.118723           7.30         81.283052   \n",
       "1906            117.489756           6.93         28.840315   \n",
       "1907            173.780083           6.76         36.307805   \n",
       "1908            245.470892           6.61          0.114815   \n",
       "1909            295.120923           6.53          1.202264   \n",
       "\n",
       "      SelectivityWindow            label  \n",
       "0                  2.89  HDAC6-selective  \n",
       "1                  3.56  HDAC6-selective  \n",
       "2                  1.00      Dual-binder  \n",
       "3                  2.52  HDAC6-selective  \n",
       "4                  3.12  HDAC6-selective  \n",
       "...                 ...              ...  \n",
       "1905               1.91   Semi-selective  \n",
       "1906               1.46   Semi-selective  \n",
       "1907               1.56   Semi-selective  \n",
       "1908              -0.94      Dual-binder  \n",
       "1909               0.08       Non-binder  \n",
       "\n",
       "[1910 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1and6/\"HDAC1and6_SemiSel_dataset.csv\", )\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca3dbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 'Dual-binder']['SelectivityWindow'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL1095764</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[9850747, 10427556, 6095547, 5213123, 12018511...</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4244887</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[11780358, 16948577, 2043088, 10483303, 266155...</td>\n",
       "      <td>1.03</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4280303</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[799147, 2011138, 345515, 476717, 193508, 1860...</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL3910638</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[9076848, 2478511, 10872982, 3500173, 4556712,...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>Semi-selective</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL1095764  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4244887  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2      CHEMBL4280303  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "3      CHEMBL3910638  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "2  [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \\\n",
       "0  [9850747, 10427556, 6095547, 5213123, 12018511...              -0.60   \n",
       "1  [11780358, 16948577, 2043088, 10483303, 266155...               1.03   \n",
       "2  [799147, 2011138, 345515, 476717, 193508, 1860...              -0.77   \n",
       "3  [9076848, 2478511, 10872982, 3500173, 4556712,...               1.99   \n",
       "\n",
       "            label  Class  \n",
       "0     Dual-binder    3.0  \n",
       "1  Semi-selective    5.0  \n",
       "2  Semi-selective    5.0  \n",
       "3  Semi-selective    5.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"selectivity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.SelectivityWindow >= 2.0].index, \"selectivity\"] = 1.0\n",
    "df.loc[df[df.SelectivityWindow <= -2.0].index, \"selectivity\"] = 1.0\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y =  df[\"selectivity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['selectivity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['selectivity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        19.800000     3.190263\n",
      "1                    TN       151.500000     1.840894\n",
      "2                    FP         6.100000     2.131770\n",
      "3                    FN        13.600000     3.098387\n",
      "4              Accuracy         0.896859     0.017634\n",
      "5             Precision         0.765574     0.061924\n",
      "6           Sensitivity         0.592665     0.092401\n",
      "7           Specificity         0.961340     0.013378\n",
      "8              F1 score         0.664529     0.072736\n",
      "9   F1 score (weighted)         0.891075     0.020448\n",
      "10     F1 score (macro)         0.801766     0.041092\n",
      "11    Balanced Accuracy         0.776998     0.045641\n",
      "12                  MCC         0.614162     0.075537\n",
      "13                  NPV         0.917940     0.016804\n",
      "14              ROC_AUC         0.776998     0.045641\n",
      "CPU times: user 1min 12s, sys: 9.8 ms, total: 1min 12s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_clf =  RandomForestClassifier(random_state=1121218, max_features = None, n_jobs=8,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_clf.fit(x_train, y_train)\n",
    "        y_pred = rf_clf.predict(x_test)  \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=8, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred,  average=\"macro\")\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=8, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:19:46,736] A new study created in memory with name: RFclassifier\n",
      "[I 2023-12-05 13:21:01,000] Trial 0 finished with value: 0.7813083336348207 and parameters: {'n_estimators': 970}. Best is trial 0 with value: 0.7813083336348207.\n",
      "[I 2023-12-05 13:22:14,842] Trial 1 finished with value: 0.7813083336348207 and parameters: {'n_estimators': 950}. Best is trial 0 with value: 0.7813083336348207.\n",
      "[I 2023-12-05 13:22:52,755] Trial 2 finished with value: 0.7846013782774066 and parameters: {'n_estimators': 483}. Best is trial 2 with value: 0.7846013782774066.\n",
      "[I 2023-12-05 13:23:14,861] Trial 3 finished with value: 0.7924690542465778 and parameters: {'n_estimators': 280}. Best is trial 3 with value: 0.7924690542465778.\n",
      "[I 2023-12-05 13:24:03,033] Trial 4 finished with value: 0.7861865366159795 and parameters: {'n_estimators': 618}. Best is trial 3 with value: 0.7924690542465778.\n",
      "[I 2023-12-05 13:24:48,177] Trial 5 finished with value: 0.7856548480661467 and parameters: {'n_estimators': 581}. Best is trial 3 with value: 0.7924690542465778.\n",
      "[I 2023-12-05 13:25:09,667] Trial 6 finished with value: 0.7887893228098732 and parameters: {'n_estimators': 272}. Best is trial 3 with value: 0.7924690542465778.\n",
      "[I 2023-12-05 13:25:44,156] Trial 7 finished with value: 0.7855944629521872 and parameters: {'n_estimators': 440}. Best is trial 3 with value: 0.7924690542465778.\n",
      "[I 2023-12-05 13:26:43,571] Trial 8 finished with value: 0.7828084437817433 and parameters: {'n_estimators': 763}. Best is trial 3 with value: 0.7924690542465778.\n",
      "[I 2023-12-05 13:26:55,117] Trial 9 finished with value: 0.7837541788018157 and parameters: {'n_estimators': 142}. Best is trial 3 with value: 0.7924690542465778.\n",
      "[I 2023-12-05 13:27:16,849] Trial 10 finished with value: 0.7891320358839421 and parameters: {'n_estimators': 263}. Best is trial 3 with value: 0.7924690542465778.\n",
      "[I 2023-12-05 13:27:40,825] Trial 11 finished with value: 0.7941830793286863 and parameters: {'n_estimators': 291}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:28:07,766] Trial 12 finished with value: 0.7912080472800831 and parameters: {'n_estimators': 328}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:28:38,953] Trial 13 finished with value: 0.7879301677646672 and parameters: {'n_estimators': 381}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:28:47,917] Trial 14 finished with value: 0.7895083017400075 and parameters: {'n_estimators': 104}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:29:05,922] Trial 15 finished with value: 0.7869104965757245 and parameters: {'n_estimators': 217}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:30:01,155] Trial 16 finished with value: 0.7836896057648544 and parameters: {'n_estimators': 682}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:30:39,859] Trial 17 finished with value: 0.7846013782774066 and parameters: {'n_estimators': 476}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:30:55,185] Trial 18 finished with value: 0.787835165083852 and parameters: {'n_estimators': 184}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:31:24,615] Trial 19 finished with value: 0.7898842734004685 and parameters: {'n_estimators': 362}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:31:48,264] Trial 20 finished with value: 0.7922699950923847 and parameters: {'n_estimators': 287}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:32:14,043] Trial 21 finished with value: 0.7928184794602234 and parameters: {'n_estimators': 315}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:32:46,870] Trial 22 finished with value: 0.787887751489677 and parameters: {'n_estimators': 402}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:33:03,612] Trial 23 finished with value: 0.7908872347761132 and parameters: {'n_estimators': 201}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:33:46,061] Trial 24 finished with value: 0.7870964855782678 and parameters: {'n_estimators': 522}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:34:10,501] Trial 25 finished with value: 0.7932310160534056 and parameters: {'n_estimators': 297}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:34:38,507] Trial 26 finished with value: 0.7922011319548634 and parameters: {'n_estimators': 342}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:35:12,932] Trial 27 finished with value: 0.787378813538377 and parameters: {'n_estimators': 422}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:35:25,738] Trial 28 finished with value: 0.7842838154019358 and parameters: {'n_estimators': 151}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:36:33,021] Trial 29 finished with value: 0.7846994547568711 and parameters: {'n_estimators': 832}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:36:51,790] Trial 30 finished with value: 0.7858697441212816 and parameters: {'n_estimators': 226}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:37:17,812] Trial 31 finished with value: 0.7928184794602234 and parameters: {'n_estimators': 317}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:37:44,061] Trial 32 finished with value: 0.7928184794602234 and parameters: {'n_estimators': 320}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:38:21,774] Trial 33 finished with value: 0.7846013782774066 and parameters: {'n_estimators': 464}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:38:42,108] Trial 34 finished with value: 0.7882546992086368 and parameters: {'n_estimators': 246}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:39:07,739] Trial 35 finished with value: 0.7913888646100433 and parameters: {'n_estimators': 312}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:39:51,266] Trial 36 finished with value: 0.7856548480661467 and parameters: {'n_estimators': 535}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:40:22,445] Trial 37 finished with value: 0.7879301677646672 and parameters: {'n_estimators': 383}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:41:11,704] Trial 38 finished with value: 0.783269839906691 and parameters: {'n_estimators': 608}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:41:25,152] Trial 39 finished with value: 0.7884893117260233 and parameters: {'n_estimators': 159}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:42:36,018] Trial 40 finished with value: 0.7838015284565237 and parameters: {'n_estimators': 875}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:43:00,833] Trial 41 finished with value: 0.7913888646100433 and parameters: {'n_estimators': 302}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:43:28,529] Trial 42 finished with value: 0.7903932113517687 and parameters: {'n_estimators': 339}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:43:50,167] Trial 43 finished with value: 0.7895389920642246 and parameters: {'n_estimators': 262}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:44:26,581] Trial 44 finished with value: 0.7855944629521872 and parameters: {'n_estimators': 448}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:45:07,884] Trial 45 finished with value: 0.7855944629521872 and parameters: {'n_estimators': 509}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:45:17,133] Trial 46 finished with value: 0.7901399417202576 and parameters: {'n_estimators': 108}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:45:50,796] Trial 47 finished with value: 0.7863857288635965 and parameters: {'n_estimators': 413}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:46:15,925] Trial 48 finished with value: 0.7913888646100433 and parameters: {'n_estimators': 306}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:46:35,501] Trial 49 finished with value: 0.7900390497948266 and parameters: {'n_estimators': 236}. Best is trial 11 with value: 0.7941830793286863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.7942\n",
      "\tBest params:\n",
      "\t\tn_estimators: 291\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFclassifier\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   47.000000\n",
      "1                    TN  306.000000\n",
      "2                    FP    8.000000\n",
      "3                    FN   21.000000\n",
      "4              Accuracy    0.924084\n",
      "5             Precision    0.854545\n",
      "6           Sensitivity    0.691176\n",
      "7           Specificity    0.974500\n",
      "8              F1 score    0.764228\n",
      "9   F1 score (weighted)    0.920842\n",
      "10     F1 score (macro)    0.859493\n",
      "11    Balanced Accuracy    0.832849\n",
      "12                  MCC    0.725340\n",
      "13                  NPV    0.935800\n",
      "14              ROC_AUC    0.832849\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_rf_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_rf_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_rf_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_rf_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_rf_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_rf_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_rf_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_rf_0)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP), np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                           np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:46:54,765] Trial 50 finished with value: 0.7803817591513028 and parameters: {'n_estimators': 184}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:47:18,652] Trial 51 finished with value: 0.7831001533381323 and parameters: {'n_estimators': 273}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:47:50,330] Trial 52 finished with value: 0.78945158982732 and parameters: {'n_estimators': 365}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:48:19,387] Trial 53 finished with value: 0.7879736486508495 and parameters: {'n_estimators': 334}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:48:43,836] Trial 54 finished with value: 0.7855300747230288 and parameters: {'n_estimators': 279}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:49:41,779] Trial 55 finished with value: 0.7786514620691511 and parameters: {'n_estimators': 672}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:50:15,235] Trial 56 finished with value: 0.7869448906099894 and parameters: {'n_estimators': 385}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:50:33,387] Trial 57 finished with value: 0.7791478233574292 and parameters: {'n_estimators': 204}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:50:54,810] Trial 58 finished with value: 0.7852585338154486 and parameters: {'n_estimators': 243}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:51:25,359] Trial 59 finished with value: 0.7879736486508495 and parameters: {'n_estimators': 351}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:52:48,494] Trial 60 finished with value: 0.7773198260321961 and parameters: {'n_estimators': 966}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:53:13,916] Trial 61 finished with value: 0.7870594064946479 and parameters: {'n_estimators': 289}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:53:41,629] Trial 62 finished with value: 0.7870594064946479 and parameters: {'n_estimators': 317}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:54:18,911] Trial 63 finished with value: 0.7879513831286975 and parameters: {'n_estimators': 429}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:54:35,263] Trial 64 finished with value: 0.7803817591513028 and parameters: {'n_estimators': 184}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:54:59,464] Trial 65 finished with value: 0.7840143954943339 and parameters: {'n_estimators': 276}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:55:33,476] Trial 66 finished with value: 0.78945158982732 and parameters: {'n_estimators': 391}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:55:53,252] Trial 67 finished with value: 0.7798706778779362 and parameters: {'n_estimators': 223}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:56:42,199] Trial 68 finished with value: 0.7806896257563111 and parameters: {'n_estimators': 566}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:57:24,817] Trial 69 finished with value: 0.7888656252848991 and parameters: {'n_estimators': 492}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:57:56,176] Trial 70 finished with value: 0.78945158982732 and parameters: {'n_estimators': 360}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:58:18,632] Trial 71 finished with value: 0.7836615957589159 and parameters: {'n_estimators': 255}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:58:46,893] Trial 72 finished with value: 0.78626658178646 and parameters: {'n_estimators': 323}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:59:12,728] Trial 73 finished with value: 0.7861844064946479 and parameters: {'n_estimators': 295}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 13:59:43,440] Trial 74 finished with value: 0.7879736486508495 and parameters: {'n_estimators': 352}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:00:12,153] Trial 75 finished with value: 0.7879736486508495 and parameters: {'n_estimators': 329}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:00:51,483] Trial 76 finished with value: 0.7861773353101353 and parameters: {'n_estimators': 454}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:01:13,654] Trial 77 finished with value: 0.7844965852860368 and parameters: {'n_estimators': 253}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:01:28,394] Trial 78 finished with value: 0.78042822150013 and parameters: {'n_estimators': 165}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:02:03,393] Trial 79 finished with value: 0.7876501848108199 and parameters: {'n_estimators': 403}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:02:21,800] Trial 80 finished with value: 0.7798706778779362 and parameters: {'n_estimators': 208}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:02:48,825] Trial 81 finished with value: 0.7879736486508495 and parameters: {'n_estimators': 310}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:03:13,779] Trial 82 finished with value: 0.7870594064946479 and parameters: {'n_estimators': 287}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:03:45,949] Trial 83 finished with value: 0.78945158982732 and parameters: {'n_estimators': 370}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:04:13,306] Trial 84 finished with value: 0.7879736486508495 and parameters: {'n_estimators': 313}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:04:33,734] Trial 85 finished with value: 0.7830933697423256 and parameters: {'n_estimators': 232}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:05:03,533] Trial 86 finished with value: 0.7879736486508495 and parameters: {'n_estimators': 343}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:05:15,969] Trial 87 finished with value: 0.7827621299707446 and parameters: {'n_estimators': 138}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:05:39,484] Trial 88 finished with value: 0.78387595678646 and parameters: {'n_estimators': 268}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:06:05,779] Trial 89 finished with value: 0.7870986486508494 and parameters: {'n_estimators': 300}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:06:28,603] Trial 90 finished with value: 0.78387595678646 and parameters: {'n_estimators': 260}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:06:58,074] Trial 91 finished with value: 0.78626658178646 and parameters: {'n_estimators': 337}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:07:30,763] Trial 92 finished with value: 0.78945158982732 and parameters: {'n_estimators': 376}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:07:56,065] Trial 93 finished with value: 0.7870594064946479 and parameters: {'n_estimators': 289}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:08:22,925] Trial 94 finished with value: 0.7855830236508494 and parameters: {'n_estimators': 307}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:08:43,481] Trial 95 finished with value: 0.7847405906686339 and parameters: {'n_estimators': 233}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:09:20,358] Trial 96 finished with value: 0.7879513831286975 and parameters: {'n_estimators': 425}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:10:25,066] Trial 97 finished with value: 0.7813674908982234 and parameters: {'n_estimators': 752}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:10:41,927] Trial 98 finished with value: 0.7803817591513028 and parameters: {'n_estimators': 191}. Best is trial 11 with value: 0.7941830793286863.\n",
      "[I 2023-12-05 14:11:16,482] Trial 99 finished with value: 0.786857360102632 and parameters: {'n_estimators': 399}. Best is trial 11 with value: 0.7941830793286863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.7942\n",
      "\tBest params:\n",
      "\t\tn_estimators: 291\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   47.000000   48.000000\n",
      "1                    TN  306.000000  305.000000\n",
      "2                    FP    8.000000    9.000000\n",
      "3                    FN   21.000000   20.000000\n",
      "4              Accuracy    0.924084    0.924084\n",
      "5             Precision    0.854545    0.842105\n",
      "6           Sensitivity    0.691176    0.705882\n",
      "7           Specificity    0.974500    0.971300\n",
      "8              F1 score    0.764228    0.768000\n",
      "9   F1 score (weighted)    0.920842    0.921397\n",
      "10     F1 score (macro)    0.859493    0.861308\n",
      "11    Balanced Accuracy    0.832849    0.838610\n",
      "12                  MCC    0.725340    0.727059\n",
      "13                  NPV    0.935800    0.938500\n",
      "14              ROC_AUC    0.832849    0.838610\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_rf_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_rf_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_rf_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_rf_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_rf_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_rf_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_rf_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_rf_1)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 14:11:46,489] Trial 100 finished with value: 0.821222311325546 and parameters: {'n_estimators': 326}. Best is trial 100 with value: 0.821222311325546.\n",
      "[I 2023-12-05 14:12:13,020] Trial 101 finished with value: 0.8202629280747704 and parameters: {'n_estimators': 319}. Best is trial 100 with value: 0.821222311325546.\n",
      "[I 2023-12-05 14:12:40,755] Trial 102 finished with value: 0.8179629973331684 and parameters: {'n_estimators': 334}. Best is trial 100 with value: 0.821222311325546.\n",
      "[I 2023-12-05 14:13:07,768] Trial 103 finished with value: 0.821222311325546 and parameters: {'n_estimators': 326}. Best is trial 100 with value: 0.821222311325546.\n",
      "[I 2023-12-05 14:13:34,723] Trial 104 finished with value: 0.8228073486234246 and parameters: {'n_estimators': 327}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:14:01,811] Trial 105 finished with value: 0.8228073486234246 and parameters: {'n_estimators': 327}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:14:31,965] Trial 106 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 364}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:15:02,034] Trial 107 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 366}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:15:31,993] Trial 108 finished with value: 0.8176408764628226 and parameters: {'n_estimators': 363}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:16:01,817] Trial 109 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 361}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:16:31,917] Trial 110 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 364}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:17:01,881] Trial 111 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 362}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:17:32,141] Trial 112 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 367}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:18:08,158] Trial 113 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 438}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:18:44,419] Trial 114 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 441}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:19:18,690] Trial 115 finished with value: 0.8193006726508333 and parameters: {'n_estimators': 416}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:19:52,572] Trial 116 finished with value: 0.8168936279690067 and parameters: {'n_estimators': 410}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:20:24,321] Trial 117 finished with value: 0.815994895728924 and parameters: {'n_estimators': 385}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:21:03,521] Trial 118 finished with value: 0.8168936279690067 and parameters: {'n_estimators': 478}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:21:33,085] Trial 119 finished with value: 0.819548034631047 and parameters: {'n_estimators': 358}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:22:01,915] Trial 120 finished with value: 0.8179629973331684 and parameters: {'n_estimators': 348}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:22:32,886] Trial 121 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 376}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:23:05,499] Trial 122 finished with value: 0.815994895728924 and parameters: {'n_estimators': 395}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:23:34,672] Trial 123 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 353}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:24:08,503] Trial 124 finished with value: 0.8176408764628226 and parameters: {'n_estimators': 411}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:24:38,825] Trial 125 finished with value: 0.8185396087029053 and parameters: {'n_estimators': 367}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:25:05,897] Trial 126 finished with value: 0.821222311325546 and parameters: {'n_estimators': 326}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:25:33,119] Trial 127 finished with value: 0.8196227935211791 and parameters: {'n_estimators': 328}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:26:00,003] Trial 128 finished with value: 0.8196227935211791 and parameters: {'n_estimators': 324}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:26:27,300] Trial 129 finished with value: 0.8196227935211791 and parameters: {'n_estimators': 328}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:26:54,592] Trial 130 finished with value: 0.820984767412954 and parameters: {'n_estimators': 329}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:27:22,007] Trial 131 finished with value: 0.821222311325546 and parameters: {'n_estimators': 330}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:27:49,476] Trial 132 finished with value: 0.8186143675930376 and parameters: {'n_estimators': 331}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:28:16,250] Trial 133 finished with value: 0.8186634102704036 and parameters: {'n_estimators': 322}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:28:43,227] Trial 134 finished with value: 0.8202629280747704 and parameters: {'n_estimators': 325}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:29:07,008] Trial 135 finished with value: 0.820048834824434 and parameters: {'n_estimators': 286}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:29:30,097] Trial 136 finished with value: 0.8191445160332428 and parameters: {'n_estimators': 277}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:29:54,921] Trial 137 finished with value: 0.8201994048909163 and parameters: {'n_estimators': 298}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:30:19,777] Trial 138 finished with value: 0.8199763414848122 and parameters: {'n_estimators': 299}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:30:44,645] Trial 139 finished with value: 0.8199763414848122 and parameters: {'n_estimators': 299}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:31:09,124] Trial 140 finished with value: 0.8209357247355881 and parameters: {'n_estimators': 294}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:31:33,556] Trial 141 finished with value: 0.8190894515736582 and parameters: {'n_estimators': 293}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:31:56,119] Trial 142 finished with value: 0.8191445160332428 and parameters: {'n_estimators': 270}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:32:21,336] Trial 143 finished with value: 0.8208395394445074 and parameters: {'n_estimators': 303}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:32:41,921] Trial 144 finished with value: 0.8191445160332428 and parameters: {'n_estimators': 246}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:33:07,431] Trial 145 finished with value: 0.8208395394445074 and parameters: {'n_estimators': 307}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:33:31,057] Trial 146 finished with value: 0.820048834824434 and parameters: {'n_estimators': 284}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:33:54,621] Trial 147 finished with value: 0.8182813180735478 and parameters: {'n_estimators': 282}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:34:20,563] Trial 148 finished with value: 0.8192400216401404 and parameters: {'n_estimators': 312}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:34:41,962] Trial 149 finished with value: 0.8191445160332428 and parameters: {'n_estimators': 257}. Best is trial 104 with value: 0.8228073486234246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8228\n",
      "\tBest params:\n",
      "\t\tn_estimators: 327\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   47.000000   48.000000   40.000000\n",
      "1                    TN  306.000000  305.000000  304.000000\n",
      "2                    FP    8.000000    9.000000   11.000000\n",
      "3                    FN   21.000000   20.000000   27.000000\n",
      "4              Accuracy    0.924084    0.924084    0.900524\n",
      "5             Precision    0.854545    0.842105    0.784314\n",
      "6           Sensitivity    0.691176    0.705882    0.597015\n",
      "7           Specificity    0.974500    0.971300    0.965100\n",
      "8              F1 score    0.764228    0.768000    0.677966\n",
      "9   F1 score (weighted)    0.920842    0.921397    0.895011\n",
      "10     F1 score (macro)    0.859493    0.861308    0.809571\n",
      "11    Balanced Accuracy    0.832849    0.838610    0.781047\n",
      "12                  MCC    0.725340    0.727059    0.628496\n",
      "13                  NPV    0.935800    0.938500    0.918400\n",
      "14              ROC_AUC    0.832849    0.838610    0.781047\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_rf_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_rf_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_rf_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_rf_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_rf_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_rf_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_rf_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_rf_2)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 14:35:08,745] Trial 150 finished with value: 0.8119584242099196 and parameters: {'n_estimators': 270}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:35:34,985] Trial 151 finished with value: 0.812311499329596 and parameters: {'n_estimators': 301}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:35:59,958] Trial 152 finished with value: 0.8104505191243726 and parameters: {'n_estimators': 286}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:36:26,869] Trial 153 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 309}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:36:55,817] Trial 154 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 334}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:37:17,222] Trial 155 finished with value: 0.8084940510690318 and parameters: {'n_estimators': 244}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:37:44,176] Trial 156 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 309}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:38:13,938] Trial 157 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 343}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:38:39,307] Trial 158 finished with value: 0.8121979405063364 and parameters: {'n_estimators': 291}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:38:58,445] Trial 159 finished with value: 0.8108498122611166 and parameters: {'n_estimators': 217}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:39:21,116] Trial 160 finished with value: 0.8106665047346974 and parameters: {'n_estimators': 260}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:39:47,729] Trial 161 finished with value: 0.8100164224386683 and parameters: {'n_estimators': 305}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:40:12,904] Trial 162 finished with value: 0.8114305626812411 and parameters: {'n_estimators': 290}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:40:36,676] Trial 163 finished with value: 0.8105516806838068 and parameters: {'n_estimators': 274}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:41:06,382] Trial 164 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 343}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:41:33,954] Trial 165 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 317}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:42:00,002] Trial 166 finished with value: 0.8121382837873222 and parameters: {'n_estimators': 299}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:42:27,898] Trial 167 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 321}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:43:45,859] Trial 168 finished with value: 0.8141362714814464 and parameters: {'n_estimators': 910}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:44:15,278] Trial 169 finished with value: 0.8100760791576824 and parameters: {'n_estimators': 340}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:44:39,647] Trial 170 finished with value: 0.8106157267529268 and parameters: {'n_estimators': 278}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:45:07,605] Trial 171 finished with value: 0.8126208187046273 and parameters: {'n_estimators': 322}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:45:33,620] Trial 172 finished with value: 0.8123174525923952 and parameters: {'n_estimators': 298}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:46:02,971] Trial 173 finished with value: 0.8100760791576824 and parameters: {'n_estimators': 340}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:46:30,350] Trial 174 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 315}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:46:53,236] Trial 175 finished with value: 0.8106665047346974 and parameters: {'n_estimators': 262}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:47:21,798] Trial 176 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 328}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:47:47,174] Trial 177 finished with value: 0.8114305626812411 and parameters: {'n_estimators': 290}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:48:08,130] Trial 178 finished with value: 0.8100436153588919 and parameters: {'n_estimators': 239}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:48:38,555] Trial 179 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 351}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:49:05,291] Trial 180 finished with value: 0.8110044739092566 and parameters: {'n_estimators': 306}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:49:33,789] Trial 181 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 328}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:50:02,092] Trial 182 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 327}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:50:26,325] Trial 183 finished with value: 0.8106157267529268 and parameters: {'n_estimators': 278}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:50:52,783] Trial 184 finished with value: 0.8110044739092566 and parameters: {'n_estimators': 304}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:51:22,866] Trial 185 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 347}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:51:56,334] Trial 186 finished with value: 0.8126208187046273 and parameters: {'n_estimators': 387}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:52:23,841] Trial 187 finished with value: 0.8126208187046273 and parameters: {'n_estimators': 316}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:53:19,839] Trial 188 finished with value: 0.8168745453947752 and parameters: {'n_estimators': 651}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:53:44,933] Trial 189 finished with value: 0.8111799740085894 and parameters: {'n_estimators': 287}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:54:14,000] Trial 190 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 335}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:54:42,413] Trial 191 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 327}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:55:08,583] Trial 192 finished with value: 0.8118193098375709 and parameters: {'n_estimators': 300}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:55:31,620] Trial 193 finished with value: 0.8104445658615734 and parameters: {'n_estimators': 263}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:55:59,481] Trial 194 finished with value: 0.8126208187046273 and parameters: {'n_estimators': 320}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:56:29,604] Trial 195 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 348}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:56:53,986] Trial 196 finished with value: 0.8106157267529268 and parameters: {'n_estimators': 279}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:57:20,294] Trial 197 finished with value: 0.8110044739092566 and parameters: {'n_estimators': 302}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:57:49,354] Trial 198 finished with value: 0.8116327672340391 and parameters: {'n_estimators': 334}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:58:17,396] Trial 199 finished with value: 0.8126208187046273 and parameters: {'n_estimators': 322}. Best is trial 104 with value: 0.8228073486234246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8228\n",
      "\tBest params:\n",
      "\t\tn_estimators: 327\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   47.000000   48.000000   40.000000   35.000000\n",
      "1                    TN  306.000000  305.000000  304.000000  306.000000\n",
      "2                    FP    8.000000    9.000000   11.000000   10.000000\n",
      "3                    FN   21.000000   20.000000   27.000000   31.000000\n",
      "4              Accuracy    0.924084    0.924084    0.900524    0.892670\n",
      "5             Precision    0.854545    0.842105    0.784314    0.777778\n",
      "6           Sensitivity    0.691176    0.705882    0.597015    0.530303\n",
      "7           Specificity    0.974500    0.971300    0.965100    0.968400\n",
      "8              F1 score    0.764228    0.768000    0.677966    0.630631\n",
      "9   F1 score (weighted)    0.920842    0.921397    0.895011    0.884243\n",
      "10     F1 score (macro)    0.859493    0.861308    0.809571    0.783922\n",
      "11    Balanced Accuracy    0.832849    0.838610    0.781047    0.749329\n",
      "12                  MCC    0.725340    0.727059    0.628496    0.584786\n",
      "13                  NPV    0.935800    0.938500    0.918400    0.908000\n",
      "14              ROC_AUC    0.832849    0.838610    0.781047    0.749329\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_rf_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_rf_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_rf_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_rf_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_rf_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_rf_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_rf_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_rf_3)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 14:58:45,749] Trial 200 finished with value: 0.8120605299734345 and parameters: {'n_estimators': 298}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 14:59:15,539] Trial 201 finished with value: 0.8090189768402507 and parameters: {'n_estimators': 357}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:00:37,487] Trial 202 finished with value: 0.8128073976505533 and parameters: {'n_estimators': 999}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:01:03,665] Trial 203 finished with value: 0.8098884395995795 and parameters: {'n_estimators': 313}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:01:32,490] Trial 204 finished with value: 0.8090189768402507 and parameters: {'n_estimators': 345}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:02:03,977] Trial 205 finished with value: 0.8094799778594617 and parameters: {'n_estimators': 379}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:02:27,108] Trial 206 finished with value: 0.8085931369386156 and parameters: {'n_estimators': 277}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:02:54,926] Trial 207 finished with value: 0.8106649575741492 and parameters: {'n_estimators': 333}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:03:19,492] Trial 208 finished with value: 0.8102583627669023 and parameters: {'n_estimators': 292}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:03:46,019] Trial 209 finished with value: 0.809094698321189 and parameters: {'n_estimators': 316}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:04:15,481] Trial 210 finished with value: 0.8098127181186413 and parameters: {'n_estimators': 354}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:04:41,683] Trial 211 finished with value: 0.8100273529621884 and parameters: {'n_estimators': 312}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:05:09,262] Trial 212 finished with value: 0.81145869885254 and parameters: {'n_estimators': 329}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:05:53,927] Trial 213 finished with value: 0.813166269795126 and parameters: {'n_estimators': 541}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:06:18,529] Trial 214 finished with value: 0.8102583627669023 and parameters: {'n_estimators': 293}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:06:39,864] Trial 215 finished with value: 0.8106517207239922 and parameters: {'n_estimators': 253}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:07:10,978] Trial 216 finished with value: 0.8102737191378522 and parameters: {'n_estimators': 374}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:07:40,350] Trial 217 finished with value: 0.8098127181186413 and parameters: {'n_estimators': 352}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:08:08,111] Trial 218 finished with value: 0.8106649575741492 and parameters: {'n_estimators': 333}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:08:30,829] Trial 219 finished with value: 0.8094089496738526 and parameters: {'n_estimators': 270}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:08:56,708] Trial 220 finished with value: 0.8108210942405789 and parameters: {'n_estimators': 309}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:09:23,082] Trial 221 finished with value: 0.809094698321189 and parameters: {'n_estimators': 314}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:09:47,132] Trial 222 finished with value: 0.807641073663335 and parameters: {'n_estimators': 285}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:10:13,143] Trial 223 finished with value: 0.8108210942405789 and parameters: {'n_estimators': 309}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:10:41,400] Trial 224 finished with value: 0.8099364775865805 and parameters: {'n_estimators': 339}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:11:06,124] Trial 225 finished with value: 0.811266788695044 and parameters: {'n_estimators': 296}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:11:32,995] Trial 226 finished with value: 0.8108210942405789 and parameters: {'n_estimators': 322}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:12:02,571] Trial 227 finished with value: 0.8090189768402507 and parameters: {'n_estimators': 355}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:12:29,747] Trial 228 finished with value: 0.8108210942405789 and parameters: {'n_estimators': 325}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:12:55,206] Trial 229 finished with value: 0.8117805840328616 and parameters: {'n_estimators': 303}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:13:18,633] Trial 230 finished with value: 0.8085931369386156 and parameters: {'n_estimators': 279}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:13:42,823] Trial 231 finished with value: 0.8102583627669023 and parameters: {'n_estimators': 288}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:14:04,874] Trial 232 finished with value: 0.8084568863985722 and parameters: {'n_estimators': 261}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:14:30,892] Trial 233 finished with value: 0.8100273529621884 and parameters: {'n_estimators': 310}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:14:54,209] Trial 234 finished with value: 0.8085931369386156 and parameters: {'n_estimators': 277}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:15:23,001] Trial 235 finished with value: 0.8090189768402507 and parameters: {'n_estimators': 344}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:16:05,205] Trial 236 finished with value: 0.8115049478078662 and parameters: {'n_estimators': 510}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:16:32,720] Trial 237 finished with value: 0.81145869885254 and parameters: {'n_estimators': 330}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:16:57,552] Trial 238 finished with value: 0.8092988729746196 and parameters: {'n_estimators': 297}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:17:24,145] Trial 239 finished with value: 0.8106649575741492 and parameters: {'n_estimators': 317}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:17:52,793] Trial 240 finished with value: 0.8098127181186413 and parameters: {'n_estimators': 342}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:18:15,381] Trial 241 finished with value: 0.8109792089268131 and parameters: {'n_estimators': 268}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:18:38,405] Trial 242 finished with value: 0.8102734728908091 and parameters: {'n_estimators': 273}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:18:58,494] Trial 243 finished with value: 0.8099424620120128 and parameters: {'n_estimators': 238}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:19:23,203] Trial 244 finished with value: 0.8102583627669023 and parameters: {'n_estimators': 295}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:19:44,296] Trial 245 finished with value: 0.8105606579896929 and parameters: {'n_estimators': 251}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:20:44,441] Trial 246 finished with value: 0.81284025613821 and parameters: {'n_estimators': 730}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:21:11,112] Trial 247 finished with value: 0.81145869885254 and parameters: {'n_estimators': 318}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:21:35,097] Trial 248 finished with value: 0.807641073663335 and parameters: {'n_estimators': 285}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:22:01,092] Trial 249 finished with value: 0.8100273529621884 and parameters: {'n_estimators': 310}. Best is trial 104 with value: 0.8228073486234246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8228\n",
      "\tBest params:\n",
      "\t\tn_estimators: 327\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   47.000000   48.000000   40.000000   35.000000   \n",
      "1                    TN  306.000000  305.000000  304.000000  306.000000   \n",
      "2                    FP    8.000000    9.000000   11.000000   10.000000   \n",
      "3                    FN   21.000000   20.000000   27.000000   31.000000   \n",
      "4              Accuracy    0.924084    0.924084    0.900524    0.892670   \n",
      "5             Precision    0.854545    0.842105    0.784314    0.777778   \n",
      "6           Sensitivity    0.691176    0.705882    0.597015    0.530303   \n",
      "7           Specificity    0.974500    0.971300    0.965100    0.968400   \n",
      "8              F1 score    0.764228    0.768000    0.677966    0.630631   \n",
      "9   F1 score (weighted)    0.920842    0.921397    0.895011    0.884243   \n",
      "10     F1 score (macro)    0.859493    0.861308    0.809571    0.783922   \n",
      "11    Balanced Accuracy    0.832849    0.838610    0.781047    0.749329   \n",
      "12                  MCC    0.725340    0.727059    0.628496    0.584786   \n",
      "13                  NPV    0.935800    0.938500    0.918400    0.908000   \n",
      "14              ROC_AUC    0.832849    0.838610    0.781047    0.749329   \n",
      "\n",
      "          Set4  \n",
      "0    37.000000  \n",
      "1   307.000000  \n",
      "2     8.000000  \n",
      "3    30.000000  \n",
      "4     0.900524  \n",
      "5     0.822222  \n",
      "6     0.552239  \n",
      "7     0.974600  \n",
      "8     0.660714  \n",
      "9     0.892432  \n",
      "10    0.801216  \n",
      "11    0.763421  \n",
      "12    0.621515  \n",
      "13    0.911000  \n",
      "14    0.763421  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_rf_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_rf_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_rf_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_rf_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_rf_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_rf_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_rf_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_rf_4)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 15:22:32,411] Trial 250 finished with value: 0.8023443506388842 and parameters: {'n_estimators': 327}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:23:22,093] Trial 251 finished with value: 0.8022485910447097 and parameters: {'n_estimators': 586}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:23:48,129] Trial 252 finished with value: 0.8007548907759434 and parameters: {'n_estimators': 302}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:24:19,161] Trial 253 finished with value: 0.8044374245302812 and parameters: {'n_estimators': 362}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:24:48,147] Trial 254 finished with value: 0.8052422967461117 and parameters: {'n_estimators': 338}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:25:11,865] Trial 255 finished with value: 0.8040242450733326 and parameters: {'n_estimators': 274}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:25:37,754] Trial 256 finished with value: 0.8021748957067004 and parameters: {'n_estimators': 300}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:26:05,825] Trial 257 finished with value: 0.8023443506388842 and parameters: {'n_estimators': 326}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:26:28,180] Trial 258 finished with value: 0.8023016539489077 and parameters: {'n_estimators': 259}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:26:58,380] Trial 259 finished with value: 0.8037643555696412 and parameters: {'n_estimators': 352}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:27:22,783] Trial 260 finished with value: 0.8017429422465316 and parameters: {'n_estimators': 284}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:27:49,826] Trial 261 finished with value: 0.8006591733784966 and parameters: {'n_estimators': 313}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:28:22,451] Trial 262 finished with value: 0.8029594833538107 and parameters: {'n_estimators': 380}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:28:51,176] Trial 263 finished with value: 0.8052422967461117 and parameters: {'n_estimators': 333}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:29:16,749] Trial 264 finished with value: 0.8007548907759434 and parameters: {'n_estimators': 296}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:29:36,389] Trial 265 finished with value: 0.800527849506149 and parameters: {'n_estimators': 225}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:30:03,734] Trial 266 finished with value: 0.8006591733784966 and parameters: {'n_estimators': 316}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:30:33,714] Trial 267 finished with value: 0.8037643555696412 and parameters: {'n_estimators': 349}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:30:58,299] Trial 268 finished with value: 0.8021748957067004 and parameters: {'n_estimators': 283}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:31:27,073] Trial 269 finished with value: 0.8052422967461117 and parameters: {'n_estimators': 333}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:31:52,993] Trial 270 finished with value: 0.8021748957067004 and parameters: {'n_estimators': 300}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:32:15,843] Trial 271 finished with value: 0.8031629471772886 and parameters: {'n_estimators': 264}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:32:43,070] Trial 272 finished with value: 0.8006591733784966 and parameters: {'n_estimators': 316}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:33:14,825] Trial 273 finished with value: 0.8029594833538107 and parameters: {'n_estimators': 370}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:33:44,188] Trial 274 finished with value: 0.8052422967461117 and parameters: {'n_estimators': 342}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:34:05,883] Trial 275 finished with value: 0.8021748957067004 and parameters: {'n_estimators': 249}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:34:30,857] Trial 276 finished with value: 0.8021748957067004 and parameters: {'n_estimators': 289}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:34:57,221] Trial 277 finished with value: 0.7991298416068775 and parameters: {'n_estimators': 306}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:35:20,663] Trial 278 finished with value: 0.8040242450733326 and parameters: {'n_estimators': 272}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:35:48,722] Trial 279 finished with value: 0.8023443506388842 and parameters: {'n_estimators': 325}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:36:57,720] Trial 280 finished with value: 0.8006918598688442 and parameters: {'n_estimators': 811}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:37:28,328] Trial 281 finished with value: 0.8044374245302812 and parameters: {'n_estimators': 355}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:37:55,033] Trial 282 finished with value: 0.7991298416068775 and parameters: {'n_estimators': 308}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:38:23,597] Trial 283 finished with value: 0.8038222918153547 and parameters: {'n_estimators': 331}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:38:48,533] Trial 284 finished with value: 0.8017429422465316 and parameters: {'n_estimators': 288}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:39:22,127] Trial 285 finished with value: 0.8020964483760402 and parameters: {'n_estimators': 392}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:39:50,237] Trial 286 finished with value: 0.8023443506388842 and parameters: {'n_estimators': 324}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:40:13,467] Trial 287 finished with value: 0.8031629471772886 and parameters: {'n_estimators': 267}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:40:44,774] Trial 288 finished with value: 0.8044374245302812 and parameters: {'n_estimators': 365}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:41:10,759] Trial 289 finished with value: 0.8021748957067004 and parameters: {'n_estimators': 300}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:41:40,147] Trial 290 finished with value: 0.8052422967461117 and parameters: {'n_estimators': 341}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:42:07,588] Trial 291 finished with value: 0.8006591733784966 and parameters: {'n_estimators': 317}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:42:32,615] Trial 292 finished with value: 0.8021748957067004 and parameters: {'n_estimators': 287}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:43:02,127] Trial 293 finished with value: 0.8052422967461117 and parameters: {'n_estimators': 343}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:43:23,231] Trial 294 finished with value: 0.8023016539489077 and parameters: {'n_estimators': 242}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:43:49,493] Trial 295 finished with value: 0.8007548907759434 and parameters: {'n_estimators': 304}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:44:13,043] Trial 296 finished with value: 0.8040242450733326 and parameters: {'n_estimators': 272}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:44:40,779] Trial 297 finished with value: 0.8023443506388842 and parameters: {'n_estimators': 322}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:45:11,652] Trial 298 finished with value: 0.8044374245302812 and parameters: {'n_estimators': 358}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:45:36,884] Trial 299 finished with value: 0.7991298416068775 and parameters: {'n_estimators': 291}. Best is trial 104 with value: 0.8228073486234246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8228\n",
      "\tBest params:\n",
      "\t\tn_estimators: 327\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   47.000000   48.000000   40.000000   35.000000   \n",
      "1                    TN  306.000000  305.000000  304.000000  306.000000   \n",
      "2                    FP    8.000000    9.000000   11.000000   10.000000   \n",
      "3                    FN   21.000000   20.000000   27.000000   31.000000   \n",
      "4              Accuracy    0.924084    0.924084    0.900524    0.892670   \n",
      "5             Precision    0.854545    0.842105    0.784314    0.777778   \n",
      "6           Sensitivity    0.691176    0.705882    0.597015    0.530303   \n",
      "7           Specificity    0.974500    0.971300    0.965100    0.968400   \n",
      "8              F1 score    0.764228    0.768000    0.677966    0.630631   \n",
      "9   F1 score (weighted)    0.920842    0.921397    0.895011    0.884243   \n",
      "10     F1 score (macro)    0.859493    0.861308    0.809571    0.783922   \n",
      "11    Balanced Accuracy    0.832849    0.838610    0.781047    0.749329   \n",
      "12                  MCC    0.725340    0.727059    0.628496    0.584786   \n",
      "13                  NPV    0.935800    0.938500    0.918400    0.908000   \n",
      "14              ROC_AUC    0.832849    0.838610    0.781047    0.749329   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    37.000000   43.000000  \n",
      "1   307.000000  302.000000  \n",
      "2     8.000000   13.000000  \n",
      "3    30.000000   24.000000  \n",
      "4     0.900524    0.903141  \n",
      "5     0.822222    0.767857  \n",
      "6     0.552239    0.641791  \n",
      "7     0.974600    0.958700  \n",
      "8     0.660714    0.699187  \n",
      "9     0.892432    0.899641  \n",
      "10    0.801216    0.820732  \n",
      "11    0.763421    0.800261  \n",
      "12    0.621515    0.645681  \n",
      "13    0.911000    0.926400  \n",
      "14    0.763421    0.800261  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_rf_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_rf_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_rf_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_rf_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_rf_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_rf_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_rf_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_rf_5)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 15:46:06,301] Trial 300 finished with value: 0.7964060516789336 and parameters: {'n_estimators': 312}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:46:34,069] Trial 301 finished with value: 0.7932958682718183 and parameters: {'n_estimators': 333}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:46:56,869] Trial 302 finished with value: 0.7941917846651043 and parameters: {'n_estimators': 272}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:47:22,347] Trial 303 finished with value: 0.7961794242357774 and parameters: {'n_estimators': 305}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:47:53,614] Trial 304 finished with value: 0.7943666904924036 and parameters: {'n_estimators': 376}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:48:22,624] Trial 305 finished with value: 0.797028682111347 and parameters: {'n_estimators': 347}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:48:49,702] Trial 306 finished with value: 0.7941770302549293 and parameters: {'n_estimators': 322}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:49:13,733] Trial 307 finished with value: 0.7939570330887167 and parameters: {'n_estimators': 286}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:49:39,060] Trial 308 finished with value: 0.794728685233951 and parameters: {'n_estimators': 301}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:50:17,509] Trial 309 finished with value: 0.7932279526863937 and parameters: {'n_estimators': 462}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:50:38,750] Trial 310 finished with value: 0.796750313093198 and parameters: {'n_estimators': 252}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:51:06,846] Trial 311 finished with value: 0.7932958682718183 and parameters: {'n_estimators': 334}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:51:37,155] Trial 312 finished with value: 0.7926362706337229 and parameters: {'n_estimators': 362}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:52:03,877] Trial 313 finished with value: 0.7932266626078704 and parameters: {'n_estimators': 318}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:52:38,752] Trial 314 finished with value: 0.7950157719522888 and parameters: {'n_estimators': 420}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:53:02,379] Trial 315 finished with value: 0.7956343995336993 and parameters: {'n_estimators': 281}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:53:24,108] Trial 316 finished with value: 0.796750313093198 and parameters: {'n_estimators': 258}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:53:49,335] Trial 317 finished with value: 0.794728685233951 and parameters: {'n_estimators': 302}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:54:17,754] Trial 318 finished with value: 0.7923455006247594 and parameters: {'n_estimators': 341}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:54:44,822] Trial 319 finished with value: 0.7932958682718183 and parameters: {'n_estimators': 325}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:55:03,621] Trial 320 finished with value: 0.7967971486497787 and parameters: {'n_estimators': 224}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:55:27,739] Trial 321 finished with value: 0.7957617228766094 and parameters: {'n_estimators': 288}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:55:57,284] Trial 322 finished with value: 0.7913540572345531 and parameters: {'n_estimators': 354}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:56:23,257] Trial 323 finished with value: 0.7978567906807601 and parameters: {'n_estimators': 311}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:56:56,038] Trial 324 finished with value: 0.7951681894844879 and parameters: {'n_estimators': 396}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:57:18,999] Trial 325 finished with value: 0.7940644613221941 and parameters: {'n_estimators': 273}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:57:47,141] Trial 326 finished with value: 0.7923455006247594 and parameters: {'n_estimators': 338}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:58:11,687] Trial 327 finished with value: 0.7978567906807601 and parameters: {'n_estimators': 293}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:58:38,032] Trial 328 finished with value: 0.7964060516789336 and parameters: {'n_estimators': 315}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:59:08,901] Trial 329 finished with value: 0.7943666904924036 and parameters: {'n_estimators': 370}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 15:59:36,474] Trial 330 finished with value: 0.7947978908978988 and parameters: {'n_estimators': 331}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:00:01,541] Trial 331 finished with value: 0.794728685233951 and parameters: {'n_estimators': 300}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:00:23,963] Trial 332 finished with value: 0.7941917846651043 and parameters: {'n_estimators': 267}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:00:53,146] Trial 333 finished with value: 0.7961794242357775 and parameters: {'n_estimators': 351}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:01:19,582] Trial 334 finished with value: 0.7949040290528531 and parameters: {'n_estimators': 317}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:01:39,970] Trial 335 finished with value: 0.7958691511100869 and parameters: {'n_estimators': 243}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:02:03,606] Trial 336 finished with value: 0.7939570330887167 and parameters: {'n_estimators': 284}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:02:28,929] Trial 337 finished with value: 0.794728685233951 and parameters: {'n_estimators': 303}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:02:56,594] Trial 338 finished with value: 0.7932958682718183 and parameters: {'n_estimators': 332}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:03:25,774] Trial 339 finished with value: 0.7941770302549294 and parameters: {'n_estimators': 350}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:03:52,486] Trial 340 finished with value: 0.7932266626078704 and parameters: {'n_estimators': 321}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:04:15,840] Trial 341 finished with value: 0.7939570330887167 and parameters: {'n_estimators': 279}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:04:56,615] Trial 342 finished with value: 0.7914643041123236 and parameters: {'n_estimators': 494}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:05:28,319] Trial 343 finished with value: 0.7953965867170878 and parameters: {'n_estimators': 383}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:05:53,310] Trial 344 finished with value: 0.7960521008928673 and parameters: {'n_estimators': 299}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:06:14,950] Trial 345 finished with value: 0.796750313093198 and parameters: {'n_estimators': 257}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:06:43,010] Trial 346 finished with value: 0.7932958682718183 and parameters: {'n_estimators': 336}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:07:08,752] Trial 347 finished with value: 0.7964060516789336 and parameters: {'n_estimators': 309}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:07:32,825] Trial 348 finished with value: 0.794728685233951 and parameters: {'n_estimators': 289}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:08:02,740] Trial 349 finished with value: 0.7932279526863937 and parameters: {'n_estimators': 359}. Best is trial 104 with value: 0.8228073486234246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8228\n",
      "\tBest params:\n",
      "\t\tn_estimators: 327\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   47.000000   48.000000   40.000000   35.000000   \n",
      "1                    TN  306.000000  305.000000  304.000000  306.000000   \n",
      "2                    FP    8.000000    9.000000   11.000000   10.000000   \n",
      "3                    FN   21.000000   20.000000   27.000000   31.000000   \n",
      "4              Accuracy    0.924084    0.924084    0.900524    0.892670   \n",
      "5             Precision    0.854545    0.842105    0.784314    0.777778   \n",
      "6           Sensitivity    0.691176    0.705882    0.597015    0.530303   \n",
      "7           Specificity    0.974500    0.971300    0.965100    0.968400   \n",
      "8              F1 score    0.764228    0.768000    0.677966    0.630631   \n",
      "9   F1 score (weighted)    0.920842    0.921397    0.895011    0.884243   \n",
      "10     F1 score (macro)    0.859493    0.861308    0.809571    0.783922   \n",
      "11    Balanced Accuracy    0.832849    0.838610    0.781047    0.749329   \n",
      "12                  MCC    0.725340    0.727059    0.628496    0.584786   \n",
      "13                  NPV    0.935800    0.938500    0.918400    0.908000   \n",
      "14              ROC_AUC    0.832849    0.838610    0.781047    0.749329   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    37.000000   43.000000   46.000000  \n",
      "1   307.000000  302.000000  302.000000  \n",
      "2     8.000000   13.000000   11.000000  \n",
      "3    30.000000   24.000000   23.000000  \n",
      "4     0.900524    0.903141    0.910995  \n",
      "5     0.822222    0.767857    0.807018  \n",
      "6     0.552239    0.641791    0.666667  \n",
      "7     0.974600    0.958700    0.964900  \n",
      "8     0.660714    0.699187    0.730159  \n",
      "9     0.892432    0.899641    0.907593  \n",
      "10    0.801216    0.820732    0.838434  \n",
      "11    0.763421    0.800261    0.815761  \n",
      "12    0.621515    0.645681    0.681878  \n",
      "13    0.911000    0.926400    0.929200  \n",
      "14    0.763421    0.800261    0.815761  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_rf_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_rf_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_rf_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_rf_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_rf_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_rf_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_rf_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_rf_6)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 16:08:32,653] Trial 350 finished with value: 0.8122876546523556 and parameters: {'n_estimators': 319}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:09:24,468] Trial 351 finished with value: 0.8179731755241502 and parameters: {'n_estimators': 626}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:09:41,843] Trial 352 finished with value: 0.8130443664949223 and parameters: {'n_estimators': 204}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:10:04,819] Trial 353 finished with value: 0.8122876546523556 and parameters: {'n_estimators': 274}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:10:33,561] Trial 354 finished with value: 0.8172706669047078 and parameters: {'n_estimators': 344}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:10:59,217] Trial 355 finished with value: 0.8122876546523556 and parameters: {'n_estimators': 306}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:11:26,754] Trial 356 finished with value: 0.8130061846287566 and parameters: {'n_estimators': 329}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:11:51,060] Trial 357 finished with value: 0.8113870650390783 and parameters: {'n_estimators': 289}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:12:22,340] Trial 358 finished with value: 0.8196713822900286 and parameters: {'n_estimators': 374}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:12:53,997] Trial 359 finished with value: 0.8186965491937561 and parameters: {'n_estimators': 380}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:13:24,698] Trial 360 finished with value: 0.8181813469033058 and parameters: {'n_estimators': 368}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:13:54,588] Trial 361 finished with value: 0.8181813469033058 and parameters: {'n_estimators': 358}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:14:28,379] Trial 362 finished with value: 0.820392870890078 and parameters: {'n_estimators': 401}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:15:01,863] Trial 363 finished with value: 0.8187468901561793 and parameters: {'n_estimators': 400}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:15:38,553] Trial 364 finished with value: 0.8162958338084355 and parameters: {'n_estimators': 437}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:16:09,837] Trial 365 finished with value: 0.8196713822900286 and parameters: {'n_estimators': 376}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:16:44,192] Trial 366 finished with value: 0.8177858691951583 and parameters: {'n_estimators': 414}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:17:18,394] Trial 367 finished with value: 0.8186978701071592 and parameters: {'n_estimators': 412}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:17:51,146] Trial 368 finished with value: 0.820392870890078 and parameters: {'n_estimators': 393}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:18:23,547] Trial 369 finished with value: 0.820392870890078 and parameters: {'n_estimators': 389}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:18:56,353] Trial 370 finished with value: 0.820392870890078 and parameters: {'n_estimators': 394}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:19:29,327] Trial 371 finished with value: 0.820392870890078 and parameters: {'n_estimators': 396}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:20:02,066] Trial 372 finished with value: 0.820392870890078 and parameters: {'n_estimators': 395}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:20:38,092] Trial 373 finished with value: 0.8179744964375532 and parameters: {'n_estimators': 432}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:21:10,873] Trial 374 finished with value: 0.820392870890078 and parameters: {'n_estimators': 394}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:21:44,307] Trial 375 finished with value: 0.820392870890078 and parameters: {'n_estimators': 403}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:22:21,817] Trial 376 finished with value: 0.8162945128950325 and parameters: {'n_estimators': 452}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:22:55,159] Trial 377 finished with value: 0.8187468901561793 and parameters: {'n_estimators': 402}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:23:30,393] Trial 378 finished with value: 0.8179744964375532 and parameters: {'n_estimators': 423}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:24:03,872] Trial 379 finished with value: 0.820392870890078 and parameters: {'n_estimators': 403}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:24:37,464] Trial 380 finished with value: 0.8187468901561793 and parameters: {'n_estimators': 404}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:25:10,496] Trial 381 finished with value: 0.820392870890078 and parameters: {'n_estimators': 396}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:25:43,639] Trial 382 finished with value: 0.8187468901561793 and parameters: {'n_estimators': 398}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:26:19,279] Trial 383 finished with value: 0.8179744964375532 and parameters: {'n_estimators': 429}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:26:51,987] Trial 384 finished with value: 0.820392870890078 and parameters: {'n_estimators': 394}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:27:26,277] Trial 385 finished with value: 0.8186978701071592 and parameters: {'n_estimators': 412}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:27:58,728] Trial 386 finished with value: 0.8187468901561793 and parameters: {'n_estimators': 390}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:28:31,543] Trial 387 finished with value: 0.820392870890078 and parameters: {'n_estimators': 395}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:29:08,748] Trial 388 finished with value: 0.8146485321611339 and parameters: {'n_estimators': 447}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:29:42,467] Trial 389 finished with value: 0.8186978701071592 and parameters: {'n_estimators': 406}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:30:15,219] Trial 390 finished with value: 0.820392870890078 and parameters: {'n_estimators': 393}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:30:47,551] Trial 391 finished with value: 0.820392870890078 and parameters: {'n_estimators': 389}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:31:20,344] Trial 392 finished with value: 0.820392870890078 and parameters: {'n_estimators': 394}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:31:52,755] Trial 393 finished with value: 0.820392870890078 and parameters: {'n_estimators': 389}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:32:28,432] Trial 394 finished with value: 0.8179744964375532 and parameters: {'n_estimators': 428}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:33:01,220] Trial 395 finished with value: 0.820392870890078 and parameters: {'n_estimators': 394}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:33:36,108] Trial 396 finished with value: 0.8189041564167582 and parameters: {'n_estimators': 420}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:34:09,544] Trial 397 finished with value: 0.8187468901561793 and parameters: {'n_estimators': 402}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:34:41,904] Trial 398 finished with value: 0.820392870890078 and parameters: {'n_estimators': 389}. Best is trial 104 with value: 0.8228073486234246.\n",
      "[I 2023-12-05 16:35:21,436] Trial 399 finished with value: 0.8172065138070334 and parameters: {'n_estimators': 473}. Best is trial 104 with value: 0.8228073486234246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8228\n",
      "\tBest params:\n",
      "\t\tn_estimators: 327\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   47.000000   48.000000   40.000000   35.000000   \n",
      "1                    TN  306.000000  305.000000  304.000000  306.000000   \n",
      "2                    FP    8.000000    9.000000   11.000000   10.000000   \n",
      "3                    FN   21.000000   20.000000   27.000000   31.000000   \n",
      "4              Accuracy    0.924084    0.924084    0.900524    0.892670   \n",
      "5             Precision    0.854545    0.842105    0.784314    0.777778   \n",
      "6           Sensitivity    0.691176    0.705882    0.597015    0.530303   \n",
      "7           Specificity    0.974500    0.971300    0.965100    0.968400   \n",
      "8              F1 score    0.764228    0.768000    0.677966    0.630631   \n",
      "9   F1 score (weighted)    0.920842    0.921397    0.895011    0.884243   \n",
      "10     F1 score (macro)    0.859493    0.861308    0.809571    0.783922   \n",
      "11    Balanced Accuracy    0.832849    0.838610    0.781047    0.749329   \n",
      "12                  MCC    0.725340    0.727059    0.628496    0.584786   \n",
      "13                  NPV    0.935800    0.938500    0.918400    0.908000   \n",
      "14              ROC_AUC    0.832849    0.838610    0.781047    0.749329   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    37.000000   43.000000   46.000000   41.000000  \n",
      "1   307.000000  302.000000  302.000000  300.000000  \n",
      "2     8.000000   13.000000   11.000000   15.000000  \n",
      "3    30.000000   24.000000   23.000000   26.000000  \n",
      "4     0.900524    0.903141    0.910995    0.892670  \n",
      "5     0.822222    0.767857    0.807018    0.732143  \n",
      "6     0.552239    0.641791    0.666667    0.611940  \n",
      "7     0.974600    0.958700    0.964900    0.952400  \n",
      "8     0.660714    0.699187    0.730159    0.666667  \n",
      "9     0.892432    0.899641    0.907593    0.888792  \n",
      "10    0.801216    0.820732    0.838434    0.801352  \n",
      "11    0.763421    0.800261    0.815761    0.782161  \n",
      "12    0.621515    0.645681    0.681878    0.606759  \n",
      "13    0.911000    0.926400    0.929200    0.920200  \n",
      "14    0.763421    0.800261    0.815761    0.782161  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_rf_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_rf_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_rf_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_rf_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_rf_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_rf_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_rf_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_rf_7)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 16:36:00,194] Trial 400 finished with value: 0.8242596654303632 and parameters: {'n_estimators': 439}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:36:36,043] Trial 401 finished with value: 0.8242596654303632 and parameters: {'n_estimators': 443}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:37:11,938] Trial 402 finished with value: 0.8242596654303632 and parameters: {'n_estimators': 442}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:37:47,893] Trial 403 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 444}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:38:24,328] Trial 404 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 449}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:39:01,260] Trial 405 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 455}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:39:37,862] Trial 406 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 452}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:40:14,982] Trial 407 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 459}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:40:52,061] Trial 408 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 457}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:41:31,811] Trial 409 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 492}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:42:09,770] Trial 410 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 468}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:42:49,481] Trial 411 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 492}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:43:29,553] Trial 412 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 497}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:44:08,965] Trial 413 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 487}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:44:50,416] Trial 414 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 515}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:45:31,190] Trial 415 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 505}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:46:11,014] Trial 416 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 493}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:46:53,568] Trial 417 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 526}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:47:33,221] Trial 418 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 491}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:48:15,429] Trial 419 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 523}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:48:57,739] Trial 420 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 522}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:49:39,410] Trial 421 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 516}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:50:22,389] Trial 422 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 531}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:51:04,930] Trial 423 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 527}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:51:45,309] Trial 424 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 500}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:52:30,772] Trial 425 finished with value: 0.8212225291277843 and parameters: {'n_estimators': 562}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:53:12,166] Trial 426 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 512}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:53:50,756] Trial 427 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 477}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:54:34,430] Trial 428 finished with value: 0.8212225291277843 and parameters: {'n_estimators': 540}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:55:13,552] Trial 429 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 484}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:55:55,599] Trial 430 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 522}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:56:32,980] Trial 431 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 462}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:57:12,862] Trial 432 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 492}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:57:57,226] Trial 433 finished with value: 0.8212225291277843 and parameters: {'n_estimators': 549}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:58:38,389] Trial 434 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 510}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:59:16,797] Trial 435 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 475}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 16:59:54,100] Trial 436 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 461}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:00:34,178] Trial 437 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 496}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:01:16,519] Trial 438 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 523}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:01:57,538] Trial 439 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 505}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:02:35,986] Trial 440 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 474}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:03:12,382] Trial 441 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 449}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:03:55,495] Trial 442 finished with value: 0.8212225291277843 and parameters: {'n_estimators': 533}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:04:41,647] Trial 443 finished with value: 0.8197567508116123 and parameters: {'n_estimators': 570}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:05:21,021] Trial 444 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 484}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:06:02,302] Trial 445 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 509}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:06:40,269] Trial 446 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 468}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:07:19,760] Trial 447 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 490}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:07:56,729] Trial 448 finished with value: 0.8225469405729129 and parameters: {'n_estimators': 456}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:08:41,347] Trial 449 finished with value: 0.8212225291277843 and parameters: {'n_estimators': 553}. Best is trial 400 with value: 0.8242596654303632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8243\n",
      "\tBest params:\n",
      "\t\tn_estimators: 439\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   47.000000   48.000000   40.000000   35.000000   \n",
      "1                    TN  306.000000  305.000000  304.000000  306.000000   \n",
      "2                    FP    8.000000    9.000000   11.000000   10.000000   \n",
      "3                    FN   21.000000   20.000000   27.000000   31.000000   \n",
      "4              Accuracy    0.924084    0.924084    0.900524    0.892670   \n",
      "5             Precision    0.854545    0.842105    0.784314    0.777778   \n",
      "6           Sensitivity    0.691176    0.705882    0.597015    0.530303   \n",
      "7           Specificity    0.974500    0.971300    0.965100    0.968400   \n",
      "8              F1 score    0.764228    0.768000    0.677966    0.630631   \n",
      "9   F1 score (weighted)    0.920842    0.921397    0.895011    0.884243   \n",
      "10     F1 score (macro)    0.859493    0.861308    0.809571    0.783922   \n",
      "11    Balanced Accuracy    0.832849    0.838610    0.781047    0.749329   \n",
      "12                  MCC    0.725340    0.727059    0.628496    0.584786   \n",
      "13                  NPV    0.935800    0.938500    0.918400    0.908000   \n",
      "14              ROC_AUC    0.832849    0.838610    0.781047    0.749329   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    37.000000   43.000000   46.000000   41.000000   39.000000  \n",
      "1   307.000000  302.000000  302.000000  300.000000  307.000000  \n",
      "2     8.000000   13.000000   11.000000   15.000000    8.000000  \n",
      "3    30.000000   24.000000   23.000000   26.000000   28.000000  \n",
      "4     0.900524    0.903141    0.910995    0.892670    0.905759  \n",
      "5     0.822222    0.767857    0.807018    0.732143    0.829787  \n",
      "6     0.552239    0.641791    0.666667    0.611940    0.582090  \n",
      "7     0.974600    0.958700    0.964900    0.952400    0.974600  \n",
      "8     0.660714    0.699187    0.730159    0.666667    0.684211  \n",
      "9     0.892432    0.899641    0.907593    0.888792    0.898942  \n",
      "10    0.801216    0.820732    0.838434    0.801352    0.814413  \n",
      "11    0.763421    0.800261    0.815761    0.782161    0.778346  \n",
      "12    0.621515    0.645681    0.681878    0.606759    0.644521  \n",
      "13    0.911000    0.926400    0.929200    0.920200    0.916400  \n",
      "14    0.763421    0.800261    0.815761    0.782161    0.778346  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_rf_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_rf_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_rf_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_rf_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_rf_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_rf_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_rf_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_rf_8)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:09:29,152] Trial 450 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 524}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:10:10,307] Trial 451 finished with value: 0.7986377548057253 and parameters: {'n_estimators': 493}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:10:47,562] Trial 452 finished with value: 0.7981068085029166 and parameters: {'n_estimators': 444}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:11:30,570] Trial 453 finished with value: 0.7986377548057253 and parameters: {'n_estimators': 511}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:12:15,349] Trial 454 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 534}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:12:54,280] Trial 455 finished with value: 0.7977051001647258 and parameters: {'n_estimators': 464}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:13:34,723] Trial 456 finished with value: 0.7977051001647258 and parameters: {'n_estimators': 484}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:14:11,783] Trial 457 finished with value: 0.7981068085029166 and parameters: {'n_estimators': 443}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:15:01,240] Trial 458 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 592}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:15:43,434] Trial 459 finished with value: 0.8012682089119595 and parameters: {'n_estimators': 504}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:16:23,036] Trial 460 finished with value: 0.7960390421479981 and parameters: {'n_estimators': 472}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:17:06,269] Trial 461 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 517}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:17:51,667] Trial 462 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 545}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:18:29,702] Trial 463 finished with value: 0.800110831319339 and parameters: {'n_estimators': 454}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:19:10,185] Trial 464 finished with value: 0.7977051001647258 and parameters: {'n_estimators': 485}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:19:51,741] Trial 465 finished with value: 0.7995364870458082 and parameters: {'n_estimators': 497}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:20:35,367] Trial 466 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 524}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:21:14,390] Trial 467 finished with value: 0.7977051001647258 and parameters: {'n_estimators': 466}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:21:52,049] Trial 468 finished with value: 0.7972080762628336 and parameters: {'n_estimators': 449}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:22:32,131] Trial 469 finished with value: 0.7977051001647258 and parameters: {'n_estimators': 481}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:23:17,110] Trial 470 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 539}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:23:59,034] Trial 471 finished with value: 0.7995364870458082 and parameters: {'n_estimators': 501}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:24:45,751] Trial 472 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 562}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:25:22,553] Trial 473 finished with value: 0.7981068085029166 and parameters: {'n_estimators': 440}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:26:05,451] Trial 474 finished with value: 0.7986377548057253 and parameters: {'n_estimators': 514}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:26:44,932] Trial 475 finished with value: 0.7977707640141495 and parameters: {'n_estimators': 473}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:27:26,291] Trial 476 finished with value: 0.7986377548057253 and parameters: {'n_estimators': 493}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:28:04,587] Trial 477 finished with value: 0.8017768893360667 and parameters: {'n_estimators': 458}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:28:48,540] Trial 478 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 526}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:29:24,785] Trial 479 finished with value: 0.7947633840412063 and parameters: {'n_estimators': 433}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:30:05,379] Trial 480 finished with value: 0.7977051001647258 and parameters: {'n_estimators': 486}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:30:47,483] Trial 481 finished with value: 0.8003694766718766 and parameters: {'n_estimators': 506}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:31:26,779] Trial 482 finished with value: 0.7986038324048086 and parameters: {'n_estimators': 469}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:32:04,912] Trial 483 finished with value: 0.8017768893360667 and parameters: {'n_estimators': 456}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:32:50,089] Trial 484 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 540}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:33:31,658] Trial 485 finished with value: 0.7995364870458082 and parameters: {'n_estimators': 499}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:34:15,553] Trial 486 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 526}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:34:52,351] Trial 487 finished with value: 0.7981068085029166 and parameters: {'n_estimators': 440}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:35:31,841] Trial 488 finished with value: 0.7960390421479981 and parameters: {'n_estimators': 472}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:36:14,938] Trial 489 finished with value: 0.7986377548057253 and parameters: {'n_estimators': 516}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:37:00,924] Trial 490 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 550}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:37:41,687] Trial 491 finished with value: 0.7986377548057253 and parameters: {'n_estimators': 489}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:38:19,800] Trial 492 finished with value: 0.800866548762657 and parameters: {'n_estimators': 455}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:39:00,034] Trial 493 finished with value: 0.7977051001647258 and parameters: {'n_estimators': 482}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:39:36,284] Trial 494 finished with value: 0.7964407504861888 and parameters: {'n_estimators': 435}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:40:19,233] Trial 495 finished with value: 0.7986377548057253 and parameters: {'n_estimators': 513}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:41:06,594] Trial 496 finished with value: 0.7977565928226145 and parameters: {'n_estimators': 567}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:41:46,133] Trial 497 finished with value: 0.7960390421479981 and parameters: {'n_estimators': 472}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:42:27,994] Trial 498 finished with value: 0.7995364870458082 and parameters: {'n_estimators': 500}. Best is trial 400 with value: 0.8242596654303632.\n",
      "[I 2023-12-05 17:43:05,500] Trial 499 finished with value: 0.7972080762628336 and parameters: {'n_estimators': 449}. Best is trial 400 with value: 0.8242596654303632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8243\n",
      "\tBest params:\n",
      "\t\tn_estimators: 439\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   47.000000   48.000000   40.000000   35.000000   \n",
      "1                    TN  306.000000  305.000000  304.000000  306.000000   \n",
      "2                    FP    8.000000    9.000000   11.000000   10.000000   \n",
      "3                    FN   21.000000   20.000000   27.000000   31.000000   \n",
      "4              Accuracy    0.924084    0.924084    0.900524    0.892670   \n",
      "5             Precision    0.854545    0.842105    0.784314    0.777778   \n",
      "6           Sensitivity    0.691176    0.705882    0.597015    0.530303   \n",
      "7           Specificity    0.974500    0.971300    0.965100    0.968400   \n",
      "8              F1 score    0.764228    0.768000    0.677966    0.630631   \n",
      "9   F1 score (weighted)    0.920842    0.921397    0.895011    0.884243   \n",
      "10     F1 score (macro)    0.859493    0.861308    0.809571    0.783922   \n",
      "11    Balanced Accuracy    0.832849    0.838610    0.781047    0.749329   \n",
      "12                  MCC    0.725340    0.727059    0.628496    0.584786   \n",
      "13                  NPV    0.935800    0.938500    0.918400    0.908000   \n",
      "14              ROC_AUC    0.832849    0.838610    0.781047    0.749329   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    37.000000   43.000000   46.000000   41.000000   39.000000   47.000000  \n",
      "1   307.000000  302.000000  302.000000  300.000000  307.000000  303.000000  \n",
      "2     8.000000   13.000000   11.000000   15.000000    8.000000   10.000000  \n",
      "3    30.000000   24.000000   23.000000   26.000000   28.000000   22.000000  \n",
      "4     0.900524    0.903141    0.910995    0.892670    0.905759    0.916230  \n",
      "5     0.822222    0.767857    0.807018    0.732143    0.829787    0.824561  \n",
      "6     0.552239    0.641791    0.666667    0.611940    0.582090    0.681159  \n",
      "7     0.974600    0.958700    0.964900    0.952400    0.974600    0.968100  \n",
      "8     0.660714    0.699187    0.730159    0.666667    0.684211    0.746032  \n",
      "9     0.892432    0.899641    0.907593    0.888792    0.898942    0.913029  \n",
      "10    0.801216    0.820732    0.838434    0.801352    0.814413    0.847938  \n",
      "11    0.763421    0.800261    0.815761    0.782161    0.778346    0.824605  \n",
      "12    0.621515    0.645681    0.681878    0.606759    0.644521    0.700976  \n",
      "13    0.911000    0.926400    0.929200    0.920200    0.916400    0.932300  \n",
      "14    0.763421    0.800261    0.815761    0.782161    0.778346    0.824605  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_rf_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_rf_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_rf_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_rf_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_rf_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_rf_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_rf_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_rf_9)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })  \n",
    "\n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8243\n",
      "\tBest params:\n",
      "\t\tn_estimators: 439\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>20.600000</td>\n",
       "      <td>2.988868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>151.600000</td>\n",
       "      <td>2.065591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.309401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>2.973961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.901571</td>\n",
       "      <td>0.015374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.778154</td>\n",
       "      <td>0.062597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.616829</td>\n",
       "      <td>0.087541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.961970</td>\n",
       "      <td>0.014537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.683886</td>\n",
       "      <td>0.064213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.896614</td>\n",
       "      <td>0.017919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.812766</td>\n",
       "      <td>0.036118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.789396</td>\n",
       "      <td>0.041731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.635352</td>\n",
       "      <td>0.065035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.922460</td>\n",
       "      <td>0.015956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.789396</td>\n",
       "      <td>0.041731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP        20.600000     2.988868\n",
       "1                    TN       151.600000     2.065591\n",
       "2                    FP         6.000000     2.309401\n",
       "3                    FN        12.800000     2.973961\n",
       "4              Accuracy         0.901571     0.015374\n",
       "5             Precision         0.778154     0.062597\n",
       "6           Sensitivity         0.616829     0.087541\n",
       "7           Specificity         0.961970     0.014537\n",
       "8              F1 score         0.683886     0.064213\n",
       "9   F1 score (weighted)         0.896614     0.017919\n",
       "10     F1 score (macro)         0.812766     0.036118\n",
       "11    Balanced Accuracy         0.789396     0.041731\n",
       "12                  MCC         0.635352     0.065035\n",
       "13                  NPV         0.922460     0.015956\n",
       "14              ROC_AUC         0.789396     0.041731"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>42.300000</td>\n",
       "      <td>4.595892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>304.200000</td>\n",
       "      <td>2.394438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>2.311805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>25.200000</td>\n",
       "      <td>3.794733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.924084</td>\n",
       "      <td>0.924084</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.916230</td>\n",
       "      <td>0.907068</td>\n",
       "      <td>0.011527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.804233</td>\n",
       "      <td>0.037977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.530303</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.641791</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.626026</td>\n",
       "      <td>0.060681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.974500</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>0.968400</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.958700</td>\n",
       "      <td>0.964900</td>\n",
       "      <td>0.952400</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.968100</td>\n",
       "      <td>0.967260</td>\n",
       "      <td>0.007324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.630631</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.702779</td>\n",
       "      <td>0.047025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.920842</td>\n",
       "      <td>0.921397</td>\n",
       "      <td>0.895011</td>\n",
       "      <td>0.884243</td>\n",
       "      <td>0.892432</td>\n",
       "      <td>0.899641</td>\n",
       "      <td>0.907593</td>\n",
       "      <td>0.888792</td>\n",
       "      <td>0.898942</td>\n",
       "      <td>0.913029</td>\n",
       "      <td>0.902192</td>\n",
       "      <td>0.013034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.859493</td>\n",
       "      <td>0.861308</td>\n",
       "      <td>0.809571</td>\n",
       "      <td>0.783922</td>\n",
       "      <td>0.801216</td>\n",
       "      <td>0.820732</td>\n",
       "      <td>0.838434</td>\n",
       "      <td>0.801352</td>\n",
       "      <td>0.814413</td>\n",
       "      <td>0.847938</td>\n",
       "      <td>0.823838</td>\n",
       "      <td>0.026629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.832849</td>\n",
       "      <td>0.838610</td>\n",
       "      <td>0.781047</td>\n",
       "      <td>0.749329</td>\n",
       "      <td>0.763421</td>\n",
       "      <td>0.800261</td>\n",
       "      <td>0.815761</td>\n",
       "      <td>0.782161</td>\n",
       "      <td>0.778346</td>\n",
       "      <td>0.824605</td>\n",
       "      <td>0.796639</td>\n",
       "      <td>0.030483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.725340</td>\n",
       "      <td>0.727059</td>\n",
       "      <td>0.628496</td>\n",
       "      <td>0.584786</td>\n",
       "      <td>0.621515</td>\n",
       "      <td>0.645681</td>\n",
       "      <td>0.681878</td>\n",
       "      <td>0.606759</td>\n",
       "      <td>0.644521</td>\n",
       "      <td>0.700976</td>\n",
       "      <td>0.656701</td>\n",
       "      <td>0.049694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.935800</td>\n",
       "      <td>0.938500</td>\n",
       "      <td>0.918400</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.911000</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>0.929200</td>\n",
       "      <td>0.920200</td>\n",
       "      <td>0.916400</td>\n",
       "      <td>0.932300</td>\n",
       "      <td>0.923620</td>\n",
       "      <td>0.010426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.832849</td>\n",
       "      <td>0.838610</td>\n",
       "      <td>0.781047</td>\n",
       "      <td>0.749329</td>\n",
       "      <td>0.763421</td>\n",
       "      <td>0.800261</td>\n",
       "      <td>0.815761</td>\n",
       "      <td>0.782161</td>\n",
       "      <td>0.778346</td>\n",
       "      <td>0.824605</td>\n",
       "      <td>0.796639</td>\n",
       "      <td>0.030483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   47.000000   48.000000   40.000000   35.000000   \n",
       "1                    TN  306.000000  305.000000  304.000000  306.000000   \n",
       "2                    FP    8.000000    9.000000   11.000000   10.000000   \n",
       "3                    FN   21.000000   20.000000   27.000000   31.000000   \n",
       "4              Accuracy    0.924084    0.924084    0.900524    0.892670   \n",
       "5             Precision    0.854545    0.842105    0.784314    0.777778   \n",
       "6           Sensitivity    0.691176    0.705882    0.597015    0.530303   \n",
       "7           Specificity    0.974500    0.971300    0.965100    0.968400   \n",
       "8              F1 score    0.764228    0.768000    0.677966    0.630631   \n",
       "9   F1 score (weighted)    0.920842    0.921397    0.895011    0.884243   \n",
       "10     F1 score (macro)    0.859493    0.861308    0.809571    0.783922   \n",
       "11    Balanced Accuracy    0.832849    0.838610    0.781047    0.749329   \n",
       "12                  MCC    0.725340    0.727059    0.628496    0.584786   \n",
       "13                  NPV    0.935800    0.938500    0.918400    0.908000   \n",
       "14              ROC_AUC    0.832849    0.838610    0.781047    0.749329   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    37.000000   43.000000   46.000000   41.000000   39.000000   47.000000   \n",
       "1   307.000000  302.000000  302.000000  300.000000  307.000000  303.000000   \n",
       "2     8.000000   13.000000   11.000000   15.000000    8.000000   10.000000   \n",
       "3    30.000000   24.000000   23.000000   26.000000   28.000000   22.000000   \n",
       "4     0.900524    0.903141    0.910995    0.892670    0.905759    0.916230   \n",
       "5     0.822222    0.767857    0.807018    0.732143    0.829787    0.824561   \n",
       "6     0.552239    0.641791    0.666667    0.611940    0.582090    0.681159   \n",
       "7     0.974600    0.958700    0.964900    0.952400    0.974600    0.968100   \n",
       "8     0.660714    0.699187    0.730159    0.666667    0.684211    0.746032   \n",
       "9     0.892432    0.899641    0.907593    0.888792    0.898942    0.913029   \n",
       "10    0.801216    0.820732    0.838434    0.801352    0.814413    0.847938   \n",
       "11    0.763421    0.800261    0.815761    0.782161    0.778346    0.824605   \n",
       "12    0.621515    0.645681    0.681878    0.606759    0.644521    0.700976   \n",
       "13    0.911000    0.926400    0.929200    0.920200    0.916400    0.932300   \n",
       "14    0.763421    0.800261    0.815761    0.782161    0.778346    0.824605   \n",
       "\n",
       "           ave       std  \n",
       "0    42.300000  4.595892  \n",
       "1   304.200000  2.394438  \n",
       "2    10.300000  2.311805  \n",
       "3    25.200000  3.794733  \n",
       "4     0.907068  0.011527  \n",
       "5     0.804233  0.037977  \n",
       "6     0.626026  0.060681  \n",
       "7     0.967260  0.007324  \n",
       "8     0.702779  0.047025  \n",
       "9     0.902192  0.013034  \n",
       "10    0.823838  0.026629  \n",
       "11    0.796639  0.030483  \n",
       "12    0.656701  0.049694  \n",
       "13    0.923620  0.010426  \n",
       "14    0.796639  0.030483  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.902408</td>\n",
       "      <td>0.015716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.781280</td>\n",
       "      <td>0.063987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.619703</td>\n",
       "      <td>0.076392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>0.014269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.687890</td>\n",
       "      <td>0.056048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.897661</td>\n",
       "      <td>0.017110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.814998</td>\n",
       "      <td>0.032255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.791004</td>\n",
       "      <td>0.037181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.639137</td>\n",
       "      <td>0.061950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.922976</td>\n",
       "      <td>0.014276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.791004</td>\n",
       "      <td>0.037181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.902408     0.015716\n",
       "1             Precision         0.781280     0.063987\n",
       "2           Sensitivity         0.619703     0.076392\n",
       "3           Specificity         0.962312     0.014269\n",
       "4              F1 score         0.687890     0.056048\n",
       "5   F1 score (weighted)         0.897661     0.017110\n",
       "6      F1 score (macro)         0.814998     0.032255\n",
       "7     Balanced Accuracy         0.791004     0.037181\n",
       "8                   MCC         0.639137     0.061950\n",
       "9                   NPV         0.922976     0.014276\n",
       "10              ROC_AUC         0.791004     0.037181"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "   \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_rf)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "    \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_rf))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_rf))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_rf))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_rf))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_rf))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model f1_score 0.8097 with a standard deviation of 0.0435\n",
      "RF optimized model f1_score 0.8112 with a standard deviation of 0.0377\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"RF baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf_clf_withSemiSel.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_clf, \"OUTPUT/rf_clf_withSemiSel.joblib\")\n",
    "#joblib.dump(optimized_rf, \"OUTPUT/optimized_rf_withSemiSel.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf_clf_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        21.200000     2.440401\n",
      "1                    TN       150.700000     1.636392\n",
      "2                    FP         6.900000     2.078995\n",
      "3                    FN        12.200000     2.347576\n",
      "4              Accuracy         0.900000     0.016269\n",
      "5             Precision         0.755931     0.062577\n",
      "6           Sensitivity         0.634649     0.069260\n",
      "7           Specificity         0.956270     0.013022\n",
      "8              F1 score         0.688120     0.056352\n",
      "9   F1 score (weighted)         0.896377     0.017163\n",
      "10     F1 score (macro)         0.814275     0.032793\n",
      "11    Balanced Accuracy         0.795456     0.035788\n",
      "12                  MCC         0.633857     0.064282\n",
      "13                  NPV         0.925300     0.013402\n",
      "14              ROC_AUC         0.795456     0.035788\n",
      "CPU times: user 11.4 s, sys: 92 ms, total: 11.5 s\n",
      "Wall time: 1.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_clf = lgbm.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=8,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_clf.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"logloss\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_clf.predict(X_test) \n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "  \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:48:35,824] A new study created in memory with name: LGBMClassifier\n",
      "[I 2023-12-05 17:48:36,852] Trial 0 finished with value: 0.8126660921062351 and parameters: {'n_estimators': 475, 'learning_rate': 0.15507938422246212, 'max_depth': 9, 'max_bin': 226, 'num_leaves': 576}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:38,661] Trial 1 finished with value: 0.7969453964342158 and parameters: {'n_estimators': 256, 'learning_rate': 0.02842033094223168, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 202}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:39,081] Trial 2 finished with value: 0.7906978691801592 and parameters: {'n_estimators': 54, 'learning_rate': 0.15788856639567778, 'max_depth': 5, 'max_bin': 151, 'num_leaves': 193}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:40,054] Trial 3 finished with value: 0.8091246511377601 and parameters: {'n_estimators': 158, 'learning_rate': 0.10991921153151461, 'max_depth': 10, 'max_bin': 299, 'num_leaves': 48}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:40,667] Trial 4 finished with value: 0.8023609559692767 and parameters: {'n_estimators': 334, 'learning_rate': 0.17804839241358755, 'max_depth': 5, 'max_bin': 150, 'num_leaves': 136}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:41,376] Trial 5 finished with value: 0.7939928392728568 and parameters: {'n_estimators': 449, 'learning_rate': 0.1898560699716184, 'max_depth': 8, 'max_bin': 235, 'num_leaves': 169}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:42,473] Trial 6 finished with value: 0.7609345163830442 and parameters: {'n_estimators': 346, 'learning_rate': 0.035663668272285336, 'max_depth': 4, 'max_bin': 191, 'num_leaves': 665}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:43,713] Trial 7 finished with value: 0.8108999531058754 and parameters: {'n_estimators': 651, 'learning_rate': 0.08513001703744212, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 604}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:44,782] Trial 8 finished with value: 0.8003284672366224 and parameters: {'n_estimators': 111, 'learning_rate': 0.06120553935761209, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 471}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:45,496] Trial 9 finished with value: 0.7845086373638368 and parameters: {'n_estimators': 539, 'learning_rate': 0.12220956692869021, 'max_depth': 4, 'max_bin': 166, 'num_leaves': 618}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:46,311] Trial 10 finished with value: 0.8017124587827078 and parameters: {'n_estimators': 871, 'learning_rate': 0.14108323078518412, 'max_depth': 7, 'max_bin': 247, 'num_leaves': 438}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:47,568] Trial 11 finished with value: 0.8067044275902114 and parameters: {'n_estimators': 662, 'learning_rate': 0.0831656854303835, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 559}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:48,623] Trial 12 finished with value: 0.8047128924334258 and parameters: {'n_estimators': 685, 'learning_rate': 0.08795814013640607, 'max_depth': 8, 'max_bin': 244, 'num_leaves': 737}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:49,515] Trial 13 finished with value: 0.8076940933834624 and parameters: {'n_estimators': 611, 'learning_rate': 0.13746073701520806, 'max_depth': 9, 'max_bin': 186, 'num_leaves': 331}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:50,407] Trial 14 finished with value: 0.7997525098068398 and parameters: {'n_estimators': 824, 'learning_rate': 0.10523571395277874, 'max_depth': 7, 'max_bin': 218, 'num_leaves': 531}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:54,618] Trial 15 finished with value: 0.7029113375962635 and parameters: {'n_estimators': 508, 'learning_rate': 0.002665277751211531, 'max_depth': 10, 'max_bin': 261, 'num_leaves': 337}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:55,705] Trial 16 finished with value: 0.811327149572963 and parameters: {'n_estimators': 757, 'learning_rate': 0.1677139235295755, 'max_depth': 12, 'max_bin': 190, 'num_leaves': 741}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:56,707] Trial 17 finished with value: 0.7920216850912456 and parameters: {'n_estimators': 768, 'learning_rate': 0.19619072655576097, 'max_depth': 12, 'max_bin': 185, 'num_leaves': 750}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:57,744] Trial 18 finished with value: 0.8074713181027209 and parameters: {'n_estimators': 398, 'learning_rate': 0.1664224750835575, 'max_depth': 12, 'max_bin': 228, 'num_leaves': 667}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:58,462] Trial 19 finished with value: 0.8018655729995139 and parameters: {'n_estimators': 743, 'learning_rate': 0.1548550111973521, 'max_depth': 6, 'max_bin': 172, 'num_leaves': 503}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:48:59,349] Trial 20 finished with value: 0.7897148091919369 and parameters: {'n_estimators': 551, 'learning_rate': 0.17413313809602532, 'max_depth': 9, 'max_bin': 197, 'num_leaves': 673}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:00,466] Trial 21 finished with value: 0.8052846910848294 and parameters: {'n_estimators': 638, 'learning_rate': 0.13051601403965707, 'max_depth': 11, 'max_bin': 204, 'num_leaves': 600}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:01,497] Trial 22 finished with value: 0.803572832515024 and parameters: {'n_estimators': 787, 'learning_rate': 0.1410035837193489, 'max_depth': 11, 'max_bin': 226, 'num_leaves': 592}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:02,304] Trial 23 finished with value: 0.7850655782682274 and parameters: {'n_estimators': 585, 'learning_rate': 0.19996263699598502, 'max_depth': 9, 'max_bin': 171, 'num_leaves': 418}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:03,512] Trial 24 finished with value: 0.8030272469037086 and parameters: {'n_estimators': 469, 'learning_rate': 0.11458817854214243, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 701}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:04,511] Trial 25 finished with value: 0.7957808490911031 and parameters: {'n_estimators': 693, 'learning_rate': 0.15035695238613106, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 635}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:05,297] Trial 26 finished with value: 0.7946893798399544 and parameters: {'n_estimators': 896, 'learning_rate': 0.17578745844912194, 'max_depth': 9, 'max_bin': 243, 'num_leaves': 368}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:06,402] Trial 27 finished with value: 0.8044389228933152 and parameters: {'n_estimators': 746, 'learning_rate': 0.12672299593615544, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 559}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:07,363] Trial 28 finished with value: 0.8021317584400943 and parameters: {'n_estimators': 587, 'learning_rate': 0.09799617121792939, 'max_depth': 8, 'max_bin': 276, 'num_leaves': 716}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:08,297] Trial 29 finished with value: 0.7962446315417268 and parameters: {'n_estimators': 433, 'learning_rate': 0.16356472009240486, 'max_depth': 11, 'max_bin': 179, 'num_leaves': 484}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:09,154] Trial 30 finished with value: 0.7935343264493852 and parameters: {'n_estimators': 262, 'learning_rate': 0.15006945769345817, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 243}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:10,076] Trial 31 finished with value: 0.8000713725370204 and parameters: {'n_estimators': 201, 'learning_rate': 0.11441571661726835, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 58}. Best is trial 0 with value: 0.8126660921062351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:49:11,226] Trial 32 finished with value: 0.8105587923865393 and parameters: {'n_estimators': 234, 'learning_rate': 0.07465090486168496, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 31}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:12,467] Trial 33 finished with value: 0.8119903268040989 and parameters: {'n_estimators': 278, 'learning_rate': 0.07565578378527377, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 254}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:13,805] Trial 34 finished with value: 0.8084790181825756 and parameters: {'n_estimators': 310, 'learning_rate': 0.0626072091441671, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 242}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:14,969] Trial 35 finished with value: 0.8041265769748627 and parameters: {'n_estimators': 399, 'learning_rate': 0.0978436529690251, 'max_depth': 12, 'max_bin': 258, 'num_leaves': 303}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:15,757] Trial 36 finished with value: 0.8008433431046875 and parameters: {'n_estimators': 113, 'learning_rate': 0.1823350718317516, 'max_depth': 9, 'max_bin': 273, 'num_leaves': 107}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:16,326] Trial 37 finished with value: 0.7704267739296593 and parameters: {'n_estimators': 836, 'learning_rate': 0.18357531491964896, 'max_depth': 3, 'max_bin': 236, 'num_leaves': 261}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:17,211] Trial 38 finished with value: 0.8089434071209534 and parameters: {'n_estimators': 717, 'learning_rate': 0.16575338350102536, 'max_depth': 10, 'max_bin': 160, 'num_leaves': 192}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:18,107] Trial 39 finished with value: 0.7943958644466366 and parameters: {'n_estimators': 522, 'learning_rate': 0.11928821310600657, 'max_depth': 8, 'max_bin': 192, 'num_leaves': 643}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:18,839] Trial 40 finished with value: 0.807262022918709 and parameters: {'n_estimators': 53, 'learning_rate': 0.10721224891790027, 'max_depth': 11, 'max_bin': 211, 'num_leaves': 445}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:19,963] Trial 41 finished with value: 0.8083422495089196 and parameters: {'n_estimators': 231, 'learning_rate': 0.07684570428120961, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 32}. Best is trial 0 with value: 0.8126660921062351.\n",
      "[I 2023-12-05 17:49:21,230] Trial 42 finished with value: 0.8164937960220016 and parameters: {'n_estimators': 311, 'learning_rate': 0.06855485984623058, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 102}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:22,625] Trial 43 finished with value: 0.8019819619385007 and parameters: {'n_estimators': 308, 'learning_rate': 0.05663687547137511, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 137}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:23,725] Trial 44 finished with value: 0.7945377482934612 and parameters: {'n_estimators': 342, 'learning_rate': 0.09237892222553118, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 130}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:24,711] Trial 45 finished with value: 0.7997957493116415 and parameters: {'n_estimators': 160, 'learning_rate': 0.0869360371102028, 'max_depth': 10, 'max_bin': 286, 'num_leaves': 88}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:26,086] Trial 46 finished with value: 0.8054227610917548 and parameters: {'n_estimators': 406, 'learning_rate': 0.06690010467105101, 'max_depth': 12, 'max_bin': 268, 'num_leaves': 385}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:27,512] Trial 47 finished with value: 0.7948455944854331 and parameters: {'n_estimators': 280, 'learning_rate': 0.051110600387295986, 'max_depth': 10, 'max_bin': 234, 'num_leaves': 576}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:28,847] Trial 48 finished with value: 0.8013161158248142 and parameters: {'n_estimators': 354, 'learning_rate': 0.07992146758421398, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 691}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:30,299] Trial 49 finished with value: 0.7928119740567696 and parameters: {'n_estimators': 646, 'learning_rate': 0.04470965914033991, 'max_depth': 9, 'max_bin': 205, 'num_leaves': 524}. Best is trial 42 with value: 0.8164937960220016.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8165\n",
      "\tBest params:\n",
      "\t\tn_estimators: 311\n",
      "\t\tlearning_rate: 0.06855485984623058\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 281\n",
      "\t\tnum_leaves: 102\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   42.000000\n",
      "1                    TN  309.000000\n",
      "2                    FP    5.000000\n",
      "3                    FN   26.000000\n",
      "4              Accuracy    0.918848\n",
      "5             Precision    0.893617\n",
      "6           Sensitivity    0.617647\n",
      "7           Specificity    0.984100\n",
      "8              F1 score    0.730435\n",
      "9   F1 score (weighted)    0.912752\n",
      "10     F1 score (macro)    0.841334\n",
      "11    Balanced Accuracy    0.800862\n",
      "12                  MCC    0.700721\n",
      "13                  NPV    0.922400\n",
      "14              ROC_AUC    0.800862\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_lgbm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_lgbm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_lgbm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_lgbm_0)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:49:31,357] Trial 50 finished with value: 0.7789413007796374 and parameters: {'n_estimators': 482, 'learning_rate': 0.0715428979428422, 'max_depth': 6, 'max_bin': 179, 'num_leaves': 172}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:32,435] Trial 51 finished with value: 0.7793687677314829 and parameters: {'n_estimators': 207, 'learning_rate': 0.0744999146630336, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 71}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:33,485] Trial 52 finished with value: 0.7806878207816053 and parameters: {'n_estimators': 186, 'learning_rate': 0.08332277420537246, 'max_depth': 11, 'max_bin': 280, 'num_leaves': 290}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:34,433] Trial 53 finished with value: 0.7777868166337831 and parameters: {'n_estimators': 234, 'learning_rate': 0.09369573477971067, 'max_depth': 11, 'max_bin': 264, 'num_leaves': 32}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:35,543] Trial 54 finished with value: 0.7803505103407408 and parameters: {'n_estimators': 112, 'learning_rate': 0.06768155077644769, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 92}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:36,581] Trial 55 finished with value: 0.7798219131000478 and parameters: {'n_estimators': 380, 'learning_rate': 0.07674688201927535, 'max_depth': 10, 'max_bin': 286, 'num_leaves': 138}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:37,682] Trial 56 finished with value: 0.7843880263403882 and parameters: {'n_estimators': 795, 'learning_rate': 0.10361721132835813, 'max_depth': 11, 'max_bin': 196, 'num_leaves': 724}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:38,763] Trial 57 finished with value: 0.7824061193208031 and parameters: {'n_estimators': 293, 'learning_rate': 0.08814370401902158, 'max_depth': 10, 'max_bin': 281, 'num_leaves': 627}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:40,028] Trial 58 finished with value: 0.7880456340536897 and parameters: {'n_estimators': 153, 'learning_rate': 0.05943687331138961, 'max_depth': 12, 'max_bin': 188, 'num_leaves': 221}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:41,229] Trial 59 finished with value: 0.7757101649354212 and parameters: {'n_estimators': 686, 'learning_rate': 0.06931389239182903, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 429}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:41,958] Trial 60 finished with value: 0.7825840765772509 and parameters: {'n_estimators': 447, 'learning_rate': 0.19176055093902994, 'max_depth': 7, 'max_bin': 181, 'num_leaves': 341}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:42,902] Trial 61 finished with value: 0.7838513993264709 and parameters: {'n_estimators': 156, 'learning_rate': 0.08448084043729029, 'max_depth': 9, 'max_bin': 300, 'num_leaves': 63}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:43,959] Trial 62 finished with value: 0.7808415865701079 and parameters: {'n_estimators': 250, 'learning_rate': 0.07787456833116733, 'max_depth': 10, 'max_bin': 292, 'num_leaves': 163}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:44,802] Trial 63 finished with value: 0.786483829145441 and parameters: {'n_estimators': 550, 'learning_rate': 0.1320332597574912, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 48}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:45,562] Trial 64 finished with value: 0.7943173032947559 and parameters: {'n_estimators': 95, 'learning_rate': 0.14489945549260316, 'max_depth': 9, 'max_bin': 297, 'num_leaves': 92}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:46,355] Trial 65 finished with value: 0.7799697366436174 and parameters: {'n_estimators': 603, 'learning_rate': 0.13702176218467074, 'max_depth': 8, 'max_bin': 276, 'num_leaves': 550}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:47,196] Trial 66 finished with value: 0.7944515179209604 and parameters: {'n_estimators': 360, 'learning_rate': 0.15953848562200496, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 114}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:48,063] Trial 67 finished with value: 0.7773692557723281 and parameters: {'n_estimators': 824, 'learning_rate': 0.17491847958272444, 'max_depth': 11, 'max_bin': 205, 'num_leaves': 489}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:48,929] Trial 68 finished with value: 0.7938579065021494 and parameters: {'n_estimators': 84, 'learning_rate': 0.169788930543532, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 678}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:49,767] Trial 69 finished with value: 0.7868409693993157 and parameters: {'n_estimators': 727, 'learning_rate': 0.15602495666396232, 'max_depth': 9, 'max_bin': 282, 'num_leaves': 606}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:50,667] Trial 70 finished with value: 0.7745080618109005 and parameters: {'n_estimators': 321, 'learning_rate': 0.11040175259193075, 'max_depth': 10, 'max_bin': 234, 'num_leaves': 161}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:51,437] Trial 71 finished with value: 0.7766506124181953 and parameters: {'n_estimators': 741, 'learning_rate': 0.16374676359818607, 'max_depth': 10, 'max_bin': 154, 'num_leaves': 83}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:52,247] Trial 72 finished with value: 0.7802531950368127 and parameters: {'n_estimators': 778, 'learning_rate': 0.16996109120874217, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 200}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:53,270] Trial 73 finished with value: 0.7878386633420005 and parameters: {'n_estimators': 717, 'learning_rate': 0.09408925407547, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 189}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:54,098] Trial 74 finished with value: 0.7761459332670695 and parameters: {'n_estimators': 672, 'learning_rate': 0.1834360516501669, 'max_depth': 10, 'max_bin': 223, 'num_leaves': 459}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:55,097] Trial 75 finished with value: 0.7911073454476145 and parameters: {'n_estimators': 272, 'learning_rate': 0.09955546269530482, 'max_depth': 11, 'max_bin': 174, 'num_leaves': 112}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:55,849] Trial 76 finished with value: 0.7856578914796324 and parameters: {'n_estimators': 631, 'learning_rate': 0.14952353588749048, 'max_depth': 9, 'max_bin': 161, 'num_leaves': 54}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:56,848] Trial 77 finished with value: 0.7791629425697247 and parameters: {'n_estimators': 181, 'learning_rate': 0.12357078953040587, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 749}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:57,919] Trial 78 finished with value: 0.7754256723492186 and parameters: {'n_estimators': 135, 'learning_rate': 0.082305711235865, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 277}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:59,168] Trial 79 finished with value: 0.7771983835207117 and parameters: {'n_estimators': 227, 'learning_rate': 0.0646053859024861, 'max_depth': 11, 'max_bin': 297, 'num_leaves': 643}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:49:59,974] Trial 80 finished with value: 0.7772289329807245 and parameters: {'n_estimators': 498, 'learning_rate': 0.15834343290793612, 'max_depth': 10, 'max_bin': 244, 'num_leaves': 222}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:01,115] Trial 81 finished with value: 0.784303516355034 and parameters: {'n_estimators': 301, 'learning_rate': 0.07515784825729853, 'max_depth': 11, 'max_bin': 289, 'num_leaves': 258}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:02,397] Trial 82 finished with value: 0.7887512934410725 and parameters: {'n_estimators': 248, 'learning_rate': 0.06199171494116865, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 318}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:03,663] Trial 83 finished with value: 0.7881861603456519 and parameters: {'n_estimators': 856, 'learning_rate': 0.056264977317953456, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 218}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:04,817] Trial 84 finished with value: 0.7862485514006432 and parameters: {'n_estimators': 332, 'learning_rate': 0.07161888478183545, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 182}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:05,793] Trial 85 finished with value: 0.7864607626765081 and parameters: {'n_estimators': 712, 'learning_rate': 0.08768119661516938, 'max_depth': 11, 'max_bin': 296, 'num_leaves': 32}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:07,000] Trial 86 finished with value: 0.7767504060238997 and parameters: {'n_estimators': 212, 'learning_rate': 0.06403797296785348, 'max_depth': 10, 'max_bin': 268, 'num_leaves': 407}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:07,940] Trial 87 finished with value: 0.7761713930764476 and parameters: {'n_estimators': 805, 'learning_rate': 0.07184798412600621, 'max_depth': 7, 'max_bin': 285, 'num_leaves': 145}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:08,955] Trial 88 finished with value: 0.7852361125591878 and parameters: {'n_estimators': 384, 'learning_rate': 0.08243263836558053, 'max_depth': 9, 'max_bin': 193, 'num_leaves': 361}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:09,529] Trial 89 finished with value: 0.7668798797846705 and parameters: {'n_estimators': 574, 'learning_rate': 0.17948898705663147, 'max_depth': 4, 'max_bin': 277, 'num_leaves': 242}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:10,343] Trial 90 finished with value: 0.7765780005968266 and parameters: {'n_estimators': 322, 'learning_rate': 0.07841912702882495, 'max_depth': 5, 'max_bin': 291, 'num_leaves': 517}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:11,464] Trial 91 finished with value: 0.7824234933000118 and parameters: {'n_estimators': 756, 'learning_rate': 0.06752009828448816, 'max_depth': 11, 'max_bin': 295, 'num_leaves': 44}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:12,390] Trial 92 finished with value: 0.7696174408709159 and parameters: {'n_estimators': 191, 'learning_rate': 0.08972579488458296, 'max_depth': 11, 'max_bin': 300, 'num_leaves': 62}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:13,444] Trial 93 finished with value: 0.773955950526216 and parameters: {'n_estimators': 271, 'learning_rate': 0.07444375110319719, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 70}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:14,593] Trial 94 finished with value: 0.7745347324745631 and parameters: {'n_estimators': 235, 'learning_rate': 0.07867016310400574, 'max_depth': 10, 'max_bin': 291, 'num_leaves': 579}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:15,839] Trial 95 finished with value: 0.7837492890913104 and parameters: {'n_estimators': 285, 'learning_rate': 0.05321079610859619, 'max_depth': 11, 'max_bin': 184, 'num_leaves': 33}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:16,714] Trial 96 finished with value: 0.7782003970866622 and parameters: {'n_estimators': 699, 'learning_rate': 0.09575766871801371, 'max_depth': 8, 'max_bin': 288, 'num_leaves': 77}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:17,834] Trial 97 finished with value: 0.7832034211179526 and parameters: {'n_estimators': 174, 'learning_rate': 0.09200136005942951, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 657}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:19,035] Trial 98 finished with value: 0.7942584355683352 and parameters: {'n_estimators': 218, 'learning_rate': 0.06069004938270645, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 97}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:20,177] Trial 99 finished with value: 0.7874180465771625 and parameters: {'n_estimators': 412, 'learning_rate': 0.0694270173923131, 'max_depth': 12, 'max_bin': 260, 'num_leaves': 124}. Best is trial 42 with value: 0.8164937960220016.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8165\n",
      "\tBest params:\n",
      "\t\tn_estimators: 311\n",
      "\t\tlearning_rate: 0.06855485984623058\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 281\n",
      "\t\tnum_leaves: 102\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   42.000000   49.000000\n",
      "1                    TN  309.000000  307.000000\n",
      "2                    FP    5.000000    7.000000\n",
      "3                    FN   26.000000   19.000000\n",
      "4              Accuracy    0.918848    0.931937\n",
      "5             Precision    0.893617    0.875000\n",
      "6           Sensitivity    0.617647    0.720588\n",
      "7           Specificity    0.984100    0.977700\n",
      "8              F1 score    0.730435    0.790323\n",
      "9   F1 score (weighted)    0.912752    0.929282\n",
      "10     F1 score (macro)    0.841334    0.874849\n",
      "11    Balanced Accuracy    0.800862    0.849148\n",
      "12                  MCC    0.700721    0.755189\n",
      "13                  NPV    0.922400    0.941700\n",
      "14              ROC_AUC    0.800862    0.849148\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_lgbm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_lgbm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_lgbm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_lgbm_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:50:21,278] Trial 100 finished with value: 0.8107829181022919 and parameters: {'n_estimators': 131, 'learning_rate': 0.1036123063335545, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 47}. Best is trial 42 with value: 0.8164937960220016.\n",
      "[I 2023-12-05 17:50:22,301] Trial 101 finished with value: 0.822432089792845 and parameters: {'n_estimators': 247, 'learning_rate': 0.08325386235216893, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 49}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:23,251] Trial 102 finished with value: 0.8195221099293398 and parameters: {'n_estimators': 133, 'learning_rate': 0.1033109304785258, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 50}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:24,191] Trial 103 finished with value: 0.814079253537521 and parameters: {'n_estimators': 134, 'learning_rate': 0.10204205577605423, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 76}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:24,934] Trial 104 finished with value: 0.8189420763831874 and parameters: {'n_estimators': 71, 'learning_rate': 0.10242612978821687, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 47}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:25,735] Trial 105 finished with value: 0.8148693405437358 and parameters: {'n_estimators': 89, 'learning_rate': 0.10287872638760323, 'max_depth': 9, 'max_bin': 208, 'num_leaves': 47}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:26,512] Trial 106 finished with value: 0.8219150520895429 and parameters: {'n_estimators': 81, 'learning_rate': 0.10631032315644043, 'max_depth': 9, 'max_bin': 215, 'num_leaves': 106}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:27,284] Trial 107 finished with value: 0.8110352506284318 and parameters: {'n_estimators': 80, 'learning_rate': 0.10130744630843948, 'max_depth': 9, 'max_bin': 208, 'num_leaves': 101}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:27,971] Trial 108 finished with value: 0.8154367757106378 and parameters: {'n_estimators': 74, 'learning_rate': 0.10179842148440268, 'max_depth': 8, 'max_bin': 208, 'num_leaves': 76}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:28,583] Trial 109 finished with value: 0.8078734076800188 and parameters: {'n_estimators': 61, 'learning_rate': 0.10976880811387356, 'max_depth': 8, 'max_bin': 217, 'num_leaves': 81}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:29,241] Trial 110 finished with value: 0.8123795092049331 and parameters: {'n_estimators': 68, 'learning_rate': 0.11668933264414107, 'max_depth': 8, 'max_bin': 209, 'num_leaves': 123}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:29,929] Trial 111 finished with value: 0.8177525649224133 and parameters: {'n_estimators': 72, 'learning_rate': 0.11571801623959112, 'max_depth': 8, 'max_bin': 202, 'num_leaves': 148}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:30,723] Trial 112 finished with value: 0.8121984408955315 and parameters: {'n_estimators': 99, 'learning_rate': 0.1158843073575615, 'max_depth': 8, 'max_bin': 208, 'num_leaves': 148}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:31,396] Trial 113 finished with value: 0.8043730612864955 and parameters: {'n_estimators': 69, 'learning_rate': 0.11535964683941795, 'max_depth': 8, 'max_bin': 202, 'num_leaves': 154}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:32,204] Trial 114 finished with value: 0.8180389895676974 and parameters: {'n_estimators': 99, 'learning_rate': 0.10739180439903868, 'max_depth': 8, 'max_bin': 211, 'num_leaves': 111}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:33,045] Trial 115 finished with value: 0.8103432056074441 and parameters: {'n_estimators': 136, 'learning_rate': 0.10719978520304047, 'max_depth': 7, 'max_bin': 211, 'num_leaves': 121}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:33,606] Trial 116 finished with value: 0.8061207806020505 and parameters: {'n_estimators': 50, 'learning_rate': 0.09889712899925501, 'max_depth': 8, 'max_bin': 221, 'num_leaves': 108}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:34,458] Trial 117 finished with value: 0.8158260323067598 and parameters: {'n_estimators': 107, 'learning_rate': 0.11064153419744881, 'max_depth': 9, 'max_bin': 225, 'num_leaves': 65}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:35,300] Trial 118 finished with value: 0.8132840238724037 and parameters: {'n_estimators': 124, 'learning_rate': 0.11145309898354198, 'max_depth': 9, 'max_bin': 226, 'num_leaves': 64}. Best is trial 101 with value: 0.822432089792845.\n",
      "[I 2023-12-05 17:50:36,164] Trial 119 finished with value: 0.822893760075328 and parameters: {'n_estimators': 119, 'learning_rate': 0.11124765093309455, 'max_depth': 9, 'max_bin': 226, 'num_leaves': 68}. Best is trial 119 with value: 0.822893760075328.\n",
      "[I 2023-12-05 17:50:37,000] Trial 120 finished with value: 0.8240512221088103 and parameters: {'n_estimators': 99, 'learning_rate': 0.10189959615102016, 'max_depth': 9, 'max_bin': 214, 'num_leaves': 80}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:37,857] Trial 121 finished with value: 0.8168949545052977 and parameters: {'n_estimators': 98, 'learning_rate': 0.1072168333260319, 'max_depth': 9, 'max_bin': 215, 'num_leaves': 82}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:38,693] Trial 122 finished with value: 0.8118259645886037 and parameters: {'n_estimators': 98, 'learning_rate': 0.10710154837638843, 'max_depth': 9, 'max_bin': 215, 'num_leaves': 58}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:39,483] Trial 123 finished with value: 0.8148938805203235 and parameters: {'n_estimators': 84, 'learning_rate': 0.10524291330266257, 'max_depth': 9, 'max_bin': 219, 'num_leaves': 91}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:40,325] Trial 124 finished with value: 0.8167345984831453 and parameters: {'n_estimators': 112, 'learning_rate': 0.12092883644164112, 'max_depth': 9, 'max_bin': 229, 'num_leaves': 91}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:41,129] Trial 125 finished with value: 0.8035833877286853 and parameters: {'n_estimators': 111, 'learning_rate': 0.12015313371219015, 'max_depth': 8, 'max_bin': 228, 'num_leaves': 100}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:42,030] Trial 126 finished with value: 0.8152565866854269 and parameters: {'n_estimators': 150, 'learning_rate': 0.11158957836299989, 'max_depth': 9, 'max_bin': 223, 'num_leaves': 83}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:42,924] Trial 127 finished with value: 0.8102321180883767 and parameters: {'n_estimators': 110, 'learning_rate': 0.0963790378258343, 'max_depth': 9, 'max_bin': 204, 'num_leaves': 108}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:43,813] Trial 128 finished with value: 0.8211922750731183 and parameters: {'n_estimators': 165, 'learning_rate': 0.12515682595868627, 'max_depth': 8, 'max_bin': 237, 'num_leaves': 132}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:44,570] Trial 129 finished with value: 0.8027875224960722 and parameters: {'n_estimators': 119, 'learning_rate': 0.11931653959630521, 'max_depth': 7, 'max_bin': 231, 'num_leaves': 130}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:45,477] Trial 130 finished with value: 0.8144258907424096 and parameters: {'n_estimators': 164, 'learning_rate': 0.12206174271305748, 'max_depth': 9, 'max_bin': 239, 'num_leaves': 140}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:46,138] Trial 131 finished with value: 0.8041450989558546 and parameters: {'n_estimators': 72, 'learning_rate': 0.12539720716431557, 'max_depth': 8, 'max_bin': 213, 'num_leaves': 71}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:47,009] Trial 132 finished with value: 0.8144318402993831 and parameters: {'n_estimators': 145, 'learning_rate': 0.11043475837076619, 'max_depth': 8, 'max_bin': 223, 'num_leaves': 91}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:47,582] Trial 133 finished with value: 0.8135717872427743 and parameters: {'n_estimators': 50, 'learning_rate': 0.11428219835206828, 'max_depth': 9, 'max_bin': 218, 'num_leaves': 54}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:48,378] Trial 134 finished with value: 0.8126423061891851 and parameters: {'n_estimators': 96, 'learning_rate': 0.10006126154133994, 'max_depth': 8, 'max_bin': 198, 'num_leaves': 113}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:49,180] Trial 135 finished with value: 0.8116988049351773 and parameters: {'n_estimators': 108, 'learning_rate': 0.10795501352676912, 'max_depth': 8, 'max_bin': 216, 'num_leaves': 68}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:49,916] Trial 136 finished with value: 0.81284970048905 and parameters: {'n_estimators': 77, 'learning_rate': 0.10497483540893397, 'max_depth': 9, 'max_bin': 239, 'num_leaves': 45}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:50,737] Trial 137 finished with value: 0.820343461241236 and parameters: {'n_estimators': 167, 'learning_rate': 0.12667737134469448, 'max_depth': 7, 'max_bin': 212, 'num_leaves': 83}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:51,543] Trial 138 finished with value: 0.8229844448421497 and parameters: {'n_estimators': 160, 'learning_rate': 0.12269205893421621, 'max_depth': 6, 'max_bin': 226, 'num_leaves': 176}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:52,269] Trial 139 finished with value: 0.8099641892747842 and parameters: {'n_estimators': 169, 'learning_rate': 0.12670808266443462, 'max_depth': 5, 'max_bin': 231, 'num_leaves': 167}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:53,019] Trial 140 finished with value: 0.8075302773503324 and parameters: {'n_estimators': 201, 'learning_rate': 0.1298969522681519, 'max_depth': 6, 'max_bin': 221, 'num_leaves': 93}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:53,831] Trial 141 finished with value: 0.8172197348581923 and parameters: {'n_estimators': 147, 'learning_rate': 0.11909929854313063, 'max_depth': 7, 'max_bin': 226, 'num_leaves': 133}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:54,646] Trial 142 finished with value: 0.8101878163032161 and parameters: {'n_estimators': 146, 'learning_rate': 0.11904537690923589, 'max_depth': 7, 'max_bin': 213, 'num_leaves': 133}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:55,444] Trial 143 finished with value: 0.8197586007676865 and parameters: {'n_estimators': 127, 'learning_rate': 0.12111699792829633, 'max_depth': 7, 'max_bin': 227, 'num_leaves': 117}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:56,235] Trial 144 finished with value: 0.8043350265675961 and parameters: {'n_estimators': 122, 'learning_rate': 0.12439671708014588, 'max_depth': 7, 'max_bin': 228, 'num_leaves': 177}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:57,021] Trial 145 finished with value: 0.8114351989525336 and parameters: {'n_estimators': 191, 'learning_rate': 0.13017262321739825, 'max_depth': 7, 'max_bin': 237, 'num_leaves': 127}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:57,822] Trial 146 finished with value: 0.8105744044030828 and parameters: {'n_estimators': 158, 'learning_rate': 0.12198738529222564, 'max_depth': 7, 'max_bin': 219, 'num_leaves': 158}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:58,597] Trial 147 finished with value: 0.8096424496864405 and parameters: {'n_estimators': 132, 'learning_rate': 0.1134415654181943, 'max_depth': 6, 'max_bin': 234, 'num_leaves': 112}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:50:59,410] Trial 148 finished with value: 0.8047411394881303 and parameters: {'n_estimators': 179, 'learning_rate': 0.1173750648031758, 'max_depth': 6, 'max_bin': 228, 'num_leaves': 30}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:00,070] Trial 149 finished with value: 0.8142551180087862 and parameters: {'n_estimators': 94, 'learning_rate': 0.12088275884416846, 'max_depth': 6, 'max_bin': 225, 'num_leaves': 140}. Best is trial 120 with value: 0.8240512221088103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8241\n",
      "\tBest params:\n",
      "\t\tn_estimators: 99\n",
      "\t\tlearning_rate: 0.10189959615102016\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 214\n",
      "\t\tnum_leaves: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   42.000000   49.000000   39.000000\n",
      "1                    TN  309.000000  307.000000  306.000000\n",
      "2                    FP    5.000000    7.000000    9.000000\n",
      "3                    FN   26.000000   19.000000   28.000000\n",
      "4              Accuracy    0.918848    0.931937    0.903141\n",
      "5             Precision    0.893617    0.875000    0.812500\n",
      "6           Sensitivity    0.617647    0.720588    0.582090\n",
      "7           Specificity    0.984100    0.977700    0.971400\n",
      "8              F1 score    0.730435    0.790323    0.678261\n",
      "9   F1 score (weighted)    0.912752    0.929282    0.896558\n",
      "10     F1 score (macro)    0.841334    0.874849    0.810625\n",
      "11    Balanced Accuracy    0.800862    0.849148    0.776759\n",
      "12                  MCC    0.700721    0.755189    0.635083\n",
      "13                  NPV    0.922400    0.941700    0.916200\n",
      "14              ROC_AUC    0.800862    0.849148    0.776759\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_lgbm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_lgbm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_lgbm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_lgbm_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:51:01,000] Trial 150 finished with value: 0.805855273211083 and parameters: {'n_estimators': 127, 'learning_rate': 0.13321682836618015, 'max_depth': 7, 'max_bin': 204, 'num_leaves': 101}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:01,833] Trial 151 finished with value: 0.8048589466735789 and parameters: {'n_estimators': 149, 'learning_rate': 0.11500255758270399, 'max_depth': 7, 'max_bin': 212, 'num_leaves': 87}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:02,556] Trial 152 finished with value: 0.8057822326197002 and parameters: {'n_estimators': 118, 'learning_rate': 0.12695513424451263, 'max_depth': 6, 'max_bin': 222, 'num_leaves': 114}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:03,438] Trial 153 finished with value: 0.8067056143990602 and parameters: {'n_estimators': 167, 'learning_rate': 0.10686294279025334, 'max_depth': 7, 'max_bin': 216, 'num_leaves': 150}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:04,194] Trial 154 finished with value: 0.8169190727610779 and parameters: {'n_estimators': 90, 'learning_rate': 0.11822402065222418, 'max_depth': 8, 'max_bin': 219, 'num_leaves': 81}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:04,920] Trial 155 finished with value: 0.8093007412984333 and parameters: {'n_estimators': 86, 'learning_rate': 0.11797910273490915, 'max_depth': 8, 'max_bin': 232, 'num_leaves': 54}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:05,519] Trial 156 finished with value: 0.800442996881135 and parameters: {'n_estimators': 59, 'learning_rate': 0.11119746478037232, 'max_depth': 8, 'max_bin': 218, 'num_leaves': 82}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:06,297] Trial 157 finished with value: 0.8176063861050717 and parameters: {'n_estimators': 100, 'learning_rate': 0.12216260483569077, 'max_depth': 8, 'max_bin': 225, 'num_leaves': 41}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:07,037] Trial 158 finished with value: 0.8063863210946186 and parameters: {'n_estimators': 91, 'learning_rate': 0.1259493474733162, 'max_depth': 8, 'max_bin': 201, 'num_leaves': 44}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:07,865] Trial 159 finished with value: 0.8097079429354661 and parameters: {'n_estimators': 143, 'learning_rate': 0.1144935549056187, 'max_depth': 7, 'max_bin': 225, 'num_leaves': 38}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:08,502] Trial 160 finished with value: 0.8067378287064635 and parameters: {'n_estimators': 68, 'learning_rate': 0.1340591450355584, 'max_depth': 8, 'max_bin': 215, 'num_leaves': 65}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:09,135] Trial 161 finished with value: 0.7948934614597889 and parameters: {'n_estimators': 111, 'learning_rate': 0.1214034020054225, 'max_depth': 5, 'max_bin': 220, 'num_leaves': 77}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:09,965] Trial 162 finished with value: 0.802020471208398 and parameters: {'n_estimators': 99, 'learning_rate': 0.12898913386209623, 'max_depth': 9, 'max_bin': 211, 'num_leaves': 125}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:10,763] Trial 163 finished with value: 0.8088286222230388 and parameters: {'n_estimators': 124, 'learning_rate': 0.12400646845229725, 'max_depth': 8, 'max_bin': 227, 'num_leaves': 95}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:11,504] Trial 164 finished with value: 0.813422012455054 and parameters: {'n_estimators': 78, 'learning_rate': 0.11719999372099175, 'max_depth': 9, 'max_bin': 222, 'num_leaves': 55}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:12,391] Trial 165 finished with value: 0.8078085853846384 and parameters: {'n_estimators': 133, 'learning_rate': 0.11147038948185956, 'max_depth': 8, 'max_bin': 242, 'num_leaves': 106}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:13,325] Trial 166 finished with value: 0.8165631388807532 and parameters: {'n_estimators': 108, 'learning_rate': 0.09704917523786599, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 77}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:14,162] Trial 167 finished with value: 0.807228785046467 and parameters: {'n_estimators': 154, 'learning_rate': 0.10874191551347778, 'max_depth': 7, 'max_bin': 230, 'num_leaves': 66}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:14,800] Trial 168 finished with value: 0.8024041016541025 and parameters: {'n_estimators': 57, 'learning_rate': 0.10438072224080601, 'max_depth': 9, 'max_bin': 236, 'num_leaves': 122}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:15,534] Trial 169 finished with value: 0.8091721655688721 and parameters: {'n_estimators': 86, 'learning_rate': 0.12176613573887653, 'max_depth': 8, 'max_bin': 224, 'num_leaves': 87}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:16,423] Trial 170 finished with value: 0.8131245841993451 and parameters: {'n_estimators': 167, 'learning_rate': 0.1288701701729931, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 51}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:17,327] Trial 171 finished with value: 0.8135167133553207 and parameters: {'n_estimators': 98, 'learning_rate': 0.09664271097785944, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 79}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:18,281] Trial 172 finished with value: 0.8115107195522878 and parameters: {'n_estimators': 111, 'learning_rate': 0.09321827953193271, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 97}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:19,282] Trial 173 finished with value: 0.8132357925398539 and parameters: {'n_estimators': 140, 'learning_rate': 0.09962216167421911, 'max_depth': 10, 'max_bin': 220, 'num_leaves': 71}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:20,137] Trial 174 finished with value: 0.8006404250250526 and parameters: {'n_estimators': 111, 'learning_rate': 0.11433448016717017, 'max_depth': 9, 'max_bin': 211, 'num_leaves': 142}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:20,848] Trial 175 finished with value: 0.8065147647887374 and parameters: {'n_estimators': 81, 'learning_rate': 0.10620199679459459, 'max_depth': 8, 'max_bin': 217, 'num_leaves': 44}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:21,812] Trial 176 finished with value: 0.8095347577231301 and parameters: {'n_estimators': 185, 'learning_rate': 0.10239965896691282, 'max_depth': 9, 'max_bin': 206, 'num_leaves': 110}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:22,735] Trial 177 finished with value: 0.809296725800657 and parameters: {'n_estimators': 120, 'learning_rate': 0.11869027182709241, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 59}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:23,311] Trial 178 finished with value: 0.7880008628883173 and parameters: {'n_estimators': 63, 'learning_rate': 0.09648979496640542, 'max_depth': 7, 'max_bin': 232, 'num_leaves': 30}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:24,070] Trial 179 finished with value: 0.8121079845829302 and parameters: {'n_estimators': 100, 'learning_rate': 0.1367926595200499, 'max_depth': 8, 'max_bin': 220, 'num_leaves': 83}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:24,977] Trial 180 finished with value: 0.8031034612080505 and parameters: {'n_estimators': 129, 'learning_rate': 0.1082168688527238, 'max_depth': 9, 'max_bin': 227, 'num_leaves': 121}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:25,561] Trial 181 finished with value: 0.7961768755575249 and parameters: {'n_estimators': 145, 'learning_rate': 0.12322859087581967, 'max_depth': 3, 'max_bin': 210, 'num_leaves': 99}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:26,457] Trial 182 finished with value: 0.8059003765389937 and parameters: {'n_estimators': 203, 'learning_rate': 0.09221384975918025, 'max_depth': 7, 'max_bin': 223, 'num_leaves': 71}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:27,167] Trial 183 finished with value: 0.8097136496244112 and parameters: {'n_estimators': 79, 'learning_rate': 0.11325443176947117, 'max_depth': 8, 'max_bin': 217, 'num_leaves': 107}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:28,049] Trial 184 finished with value: 0.8128666253388553 and parameters: {'n_estimators': 106, 'learning_rate': 0.08824598561206914, 'max_depth': 9, 'max_bin': 214, 'num_leaves': 136}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:28,966] Trial 185 finished with value: 0.8113912837818293 and parameters: {'n_estimators': 159, 'learning_rate': 0.11896526268934766, 'max_depth': 8, 'max_bin': 196, 'num_leaves': 162}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:29,885] Trial 186 finished with value: 0.807169163848976 and parameters: {'n_estimators': 123, 'learning_rate': 0.09950908874489, 'max_depth': 9, 'max_bin': 225, 'num_leaves': 93}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:30,726] Trial 187 finished with value: 0.8189271708505655 and parameters: {'n_estimators': 91, 'learning_rate': 0.12580319799214884, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 61}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:31,453] Trial 188 finished with value: 0.8062735838208843 and parameters: {'n_estimators': 70, 'learning_rate': 0.1262158130713018, 'max_depth': 10, 'max_bin': 229, 'num_leaves': 55}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:32,294] Trial 189 finished with value: 0.8060643389236833 and parameters: {'n_estimators': 97, 'learning_rate': 0.11644349233515595, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 42}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:32,931] Trial 190 finished with value: 0.8024998128087482 and parameters: {'n_estimators': 55, 'learning_rate': 0.12942024980250552, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 71}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:33,756] Trial 191 finished with value: 0.8035434611505016 and parameters: {'n_estimators': 94, 'learning_rate': 0.13237784186201906, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 80}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:34,554] Trial 192 finished with value: 0.8146335486368347 and parameters: {'n_estimators': 141, 'learning_rate': 0.12402333753687561, 'max_depth': 7, 'max_bin': 222, 'num_leaves': 56}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:35,278] Trial 193 finished with value: 0.8089912283706701 and parameters: {'n_estimators': 116, 'learning_rate': 0.11073809536127838, 'max_depth': 6, 'max_bin': 207, 'num_leaves': 115}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:36,219] Trial 194 finished with value: 0.8124888411900114 and parameters: {'n_estimators': 179, 'learning_rate': 0.10554472378722068, 'max_depth': 8, 'max_bin': 218, 'num_leaves': 99}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:37,016] Trial 195 finished with value: 0.8095571998257874 and parameters: {'n_estimators': 85, 'learning_rate': 0.08509812344038406, 'max_depth': 9, 'max_bin': 212, 'num_leaves': 83}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:37,926] Trial 196 finished with value: 0.8073186285914161 and parameters: {'n_estimators': 131, 'learning_rate': 0.12105103162410721, 'max_depth': 9, 'max_bin': 203, 'num_leaves': 129}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:38,689] Trial 197 finished with value: 0.8066420017033851 and parameters: {'n_estimators': 75, 'learning_rate': 0.11406718404429532, 'max_depth': 10, 'max_bin': 222, 'num_leaves': 66}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:39,502] Trial 198 finished with value: 0.8153520184320939 and parameters: {'n_estimators': 106, 'learning_rate': 0.10256476765724792, 'max_depth': 8, 'max_bin': 225, 'num_leaves': 92}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:40,095] Trial 199 finished with value: 0.7981296192746049 and parameters: {'n_estimators': 50, 'learning_rate': 0.12583959113643076, 'max_depth': 9, 'max_bin': 220, 'num_leaves': 147}. Best is trial 120 with value: 0.8240512221088103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8241\n",
      "\tBest params:\n",
      "\t\tn_estimators: 99\n",
      "\t\tlearning_rate: 0.10189959615102016\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 214\n",
      "\t\tnum_leaves: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   42.000000   49.000000   39.000000   38.000000\n",
      "1                    TN  309.000000  307.000000  306.000000  309.000000\n",
      "2                    FP    5.000000    7.000000    9.000000    7.000000\n",
      "3                    FN   26.000000   19.000000   28.000000   28.000000\n",
      "4              Accuracy    0.918848    0.931937    0.903141    0.908377\n",
      "5             Precision    0.893617    0.875000    0.812500    0.844444\n",
      "6           Sensitivity    0.617647    0.720588    0.582090    0.575758\n",
      "7           Specificity    0.984100    0.977700    0.971400    0.977800\n",
      "8              F1 score    0.730435    0.790323    0.678261    0.684685\n",
      "9   F1 score (weighted)    0.912752    0.929282    0.896558    0.901183\n",
      "10     F1 score (macro)    0.841334    0.874849    0.810625    0.815543\n",
      "11    Balanced Accuracy    0.800862    0.849148    0.776759    0.776803\n",
      "12                  MCC    0.700721    0.755189    0.635083    0.649224\n",
      "13                  NPV    0.922400    0.941700    0.916200    0.916900\n",
      "14              ROC_AUC    0.800862    0.849148    0.776759    0.776803\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                    max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_lgbm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_lgbm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_lgbm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_lgbm_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:51:40,997] Trial 200 finished with value: 0.7974566092663969 and parameters: {'n_estimators': 159, 'learning_rate': 0.10897694044404531, 'max_depth': 7, 'max_bin': 209, 'num_leaves': 48}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:41,856] Trial 201 finished with value: 0.8094179964696104 and parameters: {'n_estimators': 108, 'learning_rate': 0.11209498070293998, 'max_depth': 9, 'max_bin': 226, 'num_leaves': 64}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:42,631] Trial 202 finished with value: 0.79841586345858 and parameters: {'n_estimators': 90, 'learning_rate': 0.1176153170856464, 'max_depth': 9, 'max_bin': 214, 'num_leaves': 30}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:43,587] Trial 203 finished with value: 0.8125311798361409 and parameters: {'n_estimators': 122, 'learning_rate': 0.10878382525253111, 'max_depth': 10, 'max_bin': 233, 'num_leaves': 74}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:44,420] Trial 204 finished with value: 0.8057975453533945 and parameters: {'n_estimators': 102, 'learning_rate': 0.12028016111632224, 'max_depth': 9, 'max_bin': 228, 'num_leaves': 107}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:45,226] Trial 205 finished with value: 0.8084099441016823 and parameters: {'n_estimators': 145, 'learning_rate': 0.10410767360077837, 'max_depth': 8, 'max_bin': 216, 'num_leaves': 62}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:45,972] Trial 206 finished with value: 0.8076591941872332 and parameters: {'n_estimators': 75, 'learning_rate': 0.09842156039572283, 'max_depth': 9, 'max_bin': 224, 'num_leaves': 92}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:46,804] Trial 207 finished with value: 0.7992021371953213 and parameters: {'n_estimators': 121, 'learning_rate': 0.0900160332133359, 'max_depth': 8, 'max_bin': 221, 'num_leaves': 47}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:47,690] Trial 208 finished with value: 0.8015302768744685 and parameters: {'n_estimators': 135, 'learning_rate': 0.11512417956459756, 'max_depth': 10, 'max_bin': 230, 'num_leaves': 121}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:48,568] Trial 209 finished with value: 0.8043778268406292 and parameters: {'n_estimators': 170, 'learning_rate': 0.11134305023149739, 'max_depth': 9, 'max_bin': 218, 'num_leaves': 76}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:49,290] Trial 210 finished with value: 0.8008801842965039 and parameters: {'n_estimators': 87, 'learning_rate': 0.1346689059700439, 'max_depth': 8, 'max_bin': 214, 'num_leaves': 57}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:49,967] Trial 211 finished with value: 0.7957788512912836 and parameters: {'n_estimators': 72, 'learning_rate': 0.10282386072598056, 'max_depth': 8, 'max_bin': 208, 'num_leaves': 84}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:50,625] Trial 212 finished with value: 0.8045071582600016 and parameters: {'n_estimators': 68, 'learning_rate': 0.09682119727199447, 'max_depth': 8, 'max_bin': 210, 'num_leaves': 74}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:51,384] Trial 213 finished with value: 0.7927984997506483 and parameters: {'n_estimators': 105, 'learning_rate': 0.10608337334037372, 'max_depth': 7, 'max_bin': 204, 'num_leaves': 105}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:52,128] Trial 214 finished with value: 0.8002424478994028 and parameters: {'n_estimators': 89, 'learning_rate': 0.12308616389156202, 'max_depth': 8, 'max_bin': 213, 'num_leaves': 87}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:52,949] Trial 215 finished with value: 0.8155030180219814 and parameters: {'n_estimators': 118, 'learning_rate': 0.10133633309736231, 'max_depth': 8, 'max_bin': 199, 'num_leaves': 57}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:53,808] Trial 216 finished with value: 0.810759820984708 and parameters: {'n_estimators': 116, 'learning_rate': 0.0947558942578428, 'max_depth': 9, 'max_bin': 194, 'num_leaves': 42}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:54,574] Trial 217 finished with value: 0.7983071635764876 and parameters: {'n_estimators': 151, 'learning_rate': 0.12971893297765125, 'max_depth': 7, 'max_bin': 190, 'num_leaves': 61}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:55,605] Trial 218 finished with value: 0.8131059510643442 and parameters: {'n_estimators': 257, 'learning_rate': 0.10014910666067495, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 131}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:56,380] Trial 219 finished with value: 0.7973565994412839 and parameters: {'n_estimators': 130, 'learning_rate': 0.1395847222554456, 'max_depth': 8, 'max_bin': 225, 'num_leaves': 46}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:57,202] Trial 220 finished with value: 0.7955353089649273 and parameters: {'n_estimators': 104, 'learning_rate': 0.11740830455676138, 'max_depth': 9, 'max_bin': 217, 'num_leaves': 66}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:57,934] Trial 221 finished with value: 0.813965827504515 and parameters: {'n_estimators': 83, 'learning_rate': 0.10207425646395606, 'max_depth': 8, 'max_bin': 199, 'num_leaves': 78}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:58,596] Trial 222 finished with value: 0.7958019893094317 and parameters: {'n_estimators': 69, 'learning_rate': 0.10756338567755969, 'max_depth': 8, 'max_bin': 206, 'num_leaves': 101}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:51:59,385] Trial 223 finished with value: 0.8046778020859134 and parameters: {'n_estimators': 98, 'learning_rate': 0.09369567998586253, 'max_depth': 8, 'max_bin': 211, 'num_leaves': 56}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:00,242] Trial 224 finished with value: 0.8042871039435754 and parameters: {'n_estimators': 115, 'learning_rate': 0.1098750031431812, 'max_depth': 8, 'max_bin': 215, 'num_leaves': 116}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:00,994] Trial 225 finished with value: 0.7936055596244064 and parameters: {'n_estimators': 193, 'learning_rate': 0.1004335029494068, 'max_depth': 6, 'max_bin': 221, 'num_leaves': 86}. Best is trial 120 with value: 0.8240512221088103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:52:01,867] Trial 226 finished with value: 0.818352922474662 and parameters: {'n_estimators': 144, 'learning_rate': 0.10485840208384388, 'max_depth': 8, 'max_bin': 255, 'num_leaves': 31}. Best is trial 120 with value: 0.8240512221088103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:52:02,559] Trial 227 finished with value: 0.8080804536831685 and parameters: {'n_estimators': 137, 'learning_rate': 0.11331026894717469, 'max_depth': 5, 'max_bin': 228, 'num_leaves': 36}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:03,385] Trial 228 finished with value: 0.8051043885600999 and parameters: {'n_estimators': 156, 'learning_rate': 0.10560879745944829, 'max_depth': 7, 'max_bin': 249, 'num_leaves': 33}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:04,386] Trial 229 finished with value: 0.8151021544143877 and parameters: {'n_estimators': 177, 'learning_rate': 0.12171505480156562, 'max_depth': 9, 'max_bin': 241, 'num_leaves': 48}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:05,235] Trial 230 finished with value: 0.8097455228050399 and parameters: {'n_estimators': 132, 'learning_rate': 0.11910476296595233, 'max_depth': 8, 'max_bin': 235, 'num_leaves': 30}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:05,986] Trial 231 finished with value: 0.8029496981399935 and parameters: {'n_estimators': 93, 'learning_rate': 0.1033421215121477, 'max_depth': 8, 'max_bin': 265, 'num_leaves': 69}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:06,831] Trial 232 finished with value: 0.8084648816689024 and parameters: {'n_estimators': 115, 'learning_rate': 0.09854289523495156, 'max_depth': 8, 'max_bin': 210, 'num_leaves': 58}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:07,526] Trial 233 finished with value: 0.802793823736504 and parameters: {'n_estimators': 61, 'learning_rate': 0.10887893609837522, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 97}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:08,265] Trial 234 finished with value: 0.8037711955129513 and parameters: {'n_estimators': 87, 'learning_rate': 0.1262021560872707, 'max_depth': 8, 'max_bin': 202, 'num_leaves': 74}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:09,110] Trial 235 finished with value: 0.8102126536777362 and parameters: {'n_estimators': 109, 'learning_rate': 0.08158422551798761, 'max_depth': 8, 'max_bin': 246, 'num_leaves': 49}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:09,975] Trial 236 finished with value: 0.7981489318192326 and parameters: {'n_estimators': 141, 'learning_rate': 0.10581786443524616, 'max_depth': 9, 'max_bin': 258, 'num_leaves': 138}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:10,976] Trial 237 finished with value: 0.7981765879976963 and parameters: {'n_estimators': 124, 'learning_rate': 0.08991848844114507, 'max_depth': 10, 'max_bin': 224, 'num_leaves': 155}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:11,851] Trial 238 finished with value: 0.8083779568541638 and parameters: {'n_estimators': 164, 'learning_rate': 0.11426726216820822, 'max_depth': 8, 'max_bin': 213, 'num_leaves': 86}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:12,514] Trial 239 finished with value: 0.8099033863520052 and parameters: {'n_estimators': 78, 'learning_rate': 0.08654203421173043, 'max_depth': 7, 'max_bin': 254, 'num_leaves': 66}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:13,406] Trial 240 finished with value: 0.8075398036128464 and parameters: {'n_estimators': 222, 'learning_rate': 0.0955916242629351, 'max_depth': 8, 'max_bin': 207, 'num_leaves': 112}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:14,212] Trial 241 finished with value: 0.8135332290233261 and parameters: {'n_estimators': 104, 'learning_rate': 0.10256860552174514, 'max_depth': 8, 'max_bin': 225, 'num_leaves': 88}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:15,020] Trial 242 finished with value: 0.8128128975953972 and parameters: {'n_estimators': 103, 'learning_rate': 0.10224853031958285, 'max_depth': 8, 'max_bin': 227, 'num_leaves': 98}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:15,575] Trial 243 finished with value: 0.7993820384066056 and parameters: {'n_estimators': 50, 'learning_rate': 0.11122199259374542, 'max_depth': 8, 'max_bin': 230, 'num_leaves': 80}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:16,463] Trial 244 finished with value: 0.8168480896363896 and parameters: {'n_estimators': 115, 'learning_rate': 0.09846470194622806, 'max_depth': 9, 'max_bin': 220, 'num_leaves': 60}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:17,384] Trial 245 finished with value: 0.814456308521228 and parameters: {'n_estimators': 125, 'learning_rate': 0.0980572847072927, 'max_depth': 9, 'max_bin': 220, 'num_leaves': 58}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:18,166] Trial 246 finished with value: 0.8053291735103232 and parameters: {'n_estimators': 91, 'learning_rate': 0.12573758563456247, 'max_depth': 9, 'max_bin': 216, 'num_leaves': 45}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:18,889] Trial 247 finished with value: 0.795049554213089 and parameters: {'n_estimators': 70, 'learning_rate': 0.06497723844879184, 'max_depth': 9, 'max_bin': 219, 'num_leaves': 69}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:20,118] Trial 248 finished with value: 0.8036434297100803 and parameters: {'n_estimators': 144, 'learning_rate': 0.02677042326815343, 'max_depth': 9, 'max_bin': 222, 'num_leaves': 42}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:21,133] Trial 249 finished with value: 0.8111343547484843 and parameters: {'n_estimators': 118, 'learning_rate': 0.09148141158661605, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 174}. Best is trial 120 with value: 0.8240512221088103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8241\n",
      "\tBest params:\n",
      "\t\tn_estimators: 99\n",
      "\t\tlearning_rate: 0.10189959615102016\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 214\n",
      "\t\tnum_leaves: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   42.000000   49.000000   39.000000   38.000000   \n",
      "1                    TN  309.000000  307.000000  306.000000  309.000000   \n",
      "2                    FP    5.000000    7.000000    9.000000    7.000000   \n",
      "3                    FN   26.000000   19.000000   28.000000   28.000000   \n",
      "4              Accuracy    0.918848    0.931937    0.903141    0.908377   \n",
      "5             Precision    0.893617    0.875000    0.812500    0.844444   \n",
      "6           Sensitivity    0.617647    0.720588    0.582090    0.575758   \n",
      "7           Specificity    0.984100    0.977700    0.971400    0.977800   \n",
      "8              F1 score    0.730435    0.790323    0.678261    0.684685   \n",
      "9   F1 score (weighted)    0.912752    0.929282    0.896558    0.901183   \n",
      "10     F1 score (macro)    0.841334    0.874849    0.810625    0.815543   \n",
      "11    Balanced Accuracy    0.800862    0.849148    0.776759    0.776803   \n",
      "12                  MCC    0.700721    0.755189    0.635083    0.649224   \n",
      "13                  NPV    0.922400    0.941700    0.916200    0.916900   \n",
      "14              ROC_AUC    0.800862    0.849148    0.776759    0.776803   \n",
      "\n",
      "          Set4  \n",
      "0    40.000000  \n",
      "1   308.000000  \n",
      "2     7.000000  \n",
      "3    27.000000  \n",
      "4     0.910995  \n",
      "5     0.851064  \n",
      "6     0.597015  \n",
      "7     0.977800  \n",
      "8     0.701754  \n",
      "9     0.904557  \n",
      "10    0.824723  \n",
      "11    0.787396  \n",
      "12    0.665476  \n",
      "13    0.919400  \n",
      "14    0.787396  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_lgbm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_lgbm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_lgbm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_lgbm_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:52:22,161] Trial 250 finished with value: 0.8044093308971604 and parameters: {'n_estimators': 150, 'learning_rate': 0.11788255709953029, 'max_depth': 9, 'max_bin': 211, 'num_leaves': 58}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:22,965] Trial 251 finished with value: 0.7968556215518585 and parameters: {'n_estimators': 86, 'learning_rate': 0.10554426289139911, 'max_depth': 9, 'max_bin': 217, 'num_leaves': 112}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:23,728] Trial 252 finished with value: 0.8016507165200389 and parameters: {'n_estimators': 104, 'learning_rate': 0.10872790525598919, 'max_depth': 7, 'max_bin': 196, 'num_leaves': 75}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:24,677] Trial 253 finished with value: 0.8061755689727848 and parameters: {'n_estimators': 131, 'learning_rate': 0.12179538557429143, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 53}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:25,399] Trial 254 finished with value: 0.8046229294102508 and parameters: {'n_estimators': 73, 'learning_rate': 0.13166964018130897, 'max_depth': 9, 'max_bin': 215, 'num_leaves': 125}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:26,288] Trial 255 finished with value: 0.7934883185787386 and parameters: {'n_estimators': 120, 'learning_rate': 0.0997675483004117, 'max_depth': 8, 'max_bin': 223, 'num_leaves': 100}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:27,279] Trial 256 finished with value: 0.8068710772409127 and parameters: {'n_estimators': 166, 'learning_rate': 0.11413117602558741, 'max_depth': 10, 'max_bin': 232, 'num_leaves': 67}. Best is trial 120 with value: 0.8240512221088103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:52:28,154] Trial 257 finished with value: 0.783870923818214 and parameters: {'n_estimators': 98, 'learning_rate': 0.0737261946809102, 'max_depth': 9, 'max_bin': 208, 'num_leaves': 31}. Best is trial 120 with value: 0.8240512221088103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:52:29,098] Trial 258 finished with value: 0.7991975193776436 and parameters: {'n_estimators': 142, 'learning_rate': 0.0953994683443598, 'max_depth': 8, 'max_bin': 212, 'num_leaves': 83}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:29,611] Trial 259 finished with value: 0.7826097452201012 and parameters: {'n_estimators': 83, 'learning_rate': 0.12841395167385455, 'max_depth': 4, 'max_bin': 218, 'num_leaves': 45}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:30,494] Trial 260 finished with value: 0.803554959141986 and parameters: {'n_estimators': 430, 'learning_rate': 0.12320383873248147, 'max_depth': 7, 'max_bin': 227, 'num_leaves': 137}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:31,427] Trial 261 finished with value: 0.7952239190669129 and parameters: {'n_estimators': 359, 'learning_rate': 0.11663257590714987, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 95}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:32,275] Trial 262 finished with value: 0.7967657508258252 and parameters: {'n_estimators': 108, 'learning_rate': 0.11096773245972207, 'max_depth': 9, 'max_bin': 223, 'num_leaves': 63}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:33,240] Trial 263 finished with value: 0.794742648798882 and parameters: {'n_estimators': 133, 'learning_rate': 0.0823284753089285, 'max_depth': 8, 'max_bin': 213, 'num_leaves': 77}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:34,254] Trial 264 finished with value: 0.8006230523504303 and parameters: {'n_estimators': 180, 'learning_rate': 0.10617949263207396, 'max_depth': 9, 'max_bin': 229, 'num_leaves': 114}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:34,781] Trial 265 finished with value: 0.7857060329605023 and parameters: {'n_estimators': 59, 'learning_rate': 0.10094873966038403, 'max_depth': 6, 'max_bin': 219, 'num_leaves': 54}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:35,651] Trial 266 finished with value: 0.7941272838406978 and parameters: {'n_estimators': 204, 'learning_rate': 0.12009269468807458, 'max_depth': 7, 'max_bin': 210, 'num_leaves': 150}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:36,655] Trial 267 finished with value: 0.8166596642573628 and parameters: {'n_estimators': 155, 'learning_rate': 0.09248303699623608, 'max_depth': 9, 'max_bin': 216, 'num_leaves': 90}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:37,680] Trial 268 finished with value: 0.8091766813251938 and parameters: {'n_estimators': 168, 'learning_rate': 0.09206328083486538, 'max_depth': 9, 'max_bin': 216, 'num_leaves': 123}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:38,731] Trial 269 finished with value: 0.8038188195593936 and parameters: {'n_estimators': 149, 'learning_rate': 0.07606466266819056, 'max_depth': 9, 'max_bin': 220, 'num_leaves': 104}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:39,747] Trial 270 finished with value: 0.793722614433908 and parameters: {'n_estimators': 160, 'learning_rate': 0.0946333809084639, 'max_depth': 9, 'max_bin': 225, 'num_leaves': 98}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:40,725] Trial 271 finished with value: 0.7951913255313657 and parameters: {'n_estimators': 125, 'learning_rate': 0.0845799690895804, 'max_depth': 9, 'max_bin': 216, 'num_leaves': 85}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:41,680] Trial 272 finished with value: 0.7963730287082242 and parameters: {'n_estimators': 117, 'learning_rate': 0.08935395784385787, 'max_depth': 10, 'max_bin': 221, 'num_leaves': 42}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:42,574] Trial 273 finished with value: 0.8137961644470568 and parameters: {'n_estimators': 148, 'learning_rate': 0.1277906184257371, 'max_depth': 9, 'max_bin': 226, 'num_leaves': 69}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:43,776] Trial 274 finished with value: 0.7995564931312736 and parameters: {'n_estimators': 188, 'learning_rate': 0.06899481915338525, 'max_depth': 10, 'max_bin': 232, 'num_leaves': 130}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:44,456] Trial 275 finished with value: 0.7950803725980167 and parameters: {'n_estimators': 97, 'learning_rate': 0.09729290081196217, 'max_depth': 6, 'max_bin': 223, 'num_leaves': 30}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:45,353] Trial 276 finished with value: 0.7943039054665582 and parameters: {'n_estimators': 134, 'learning_rate': 0.11608287643657351, 'max_depth': 9, 'max_bin': 214, 'num_leaves': 86}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:46,235] Trial 277 finished with value: 0.803510190674461 and parameters: {'n_estimators': 243, 'learning_rate': 0.12317052907511909, 'max_depth': 7, 'max_bin': 237, 'num_leaves': 58}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:47,186] Trial 278 finished with value: 0.7925183795597799 and parameters: {'n_estimators': 113, 'learning_rate': 0.07859381665484884, 'max_depth': 9, 'max_bin': 218, 'num_leaves': 113}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:48,054] Trial 279 finished with value: 0.7983583619128838 and parameters: {'n_estimators': 92, 'learning_rate': 0.11127870680699632, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 70}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:48,966] Trial 280 finished with value: 0.8057735906183625 and parameters: {'n_estimators': 152, 'learning_rate': 0.10439035316006913, 'max_depth': 8, 'max_bin': 229, 'num_leaves': 97}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:49,814] Trial 281 finished with value: 0.7940583637870232 and parameters: {'n_estimators': 114, 'learning_rate': 0.13390714229665635, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 46}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:50,731] Trial 282 finished with value: 0.7950320690106235 and parameters: {'n_estimators': 130, 'learning_rate': 0.107979030380072, 'max_depth': 9, 'max_bin': 224, 'num_leaves': 78}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:51,590] Trial 283 finished with value: 0.8072838303199698 and parameters: {'n_estimators': 174, 'learning_rate': 0.11335246755075847, 'max_depth': 8, 'max_bin': 212, 'num_leaves': 57}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:52,344] Trial 284 finished with value: 0.8060503253464347 and parameters: {'n_estimators': 79, 'learning_rate': 0.14267835450822006, 'max_depth': 9, 'max_bin': 222, 'num_leaves': 141}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:53,077] Trial 285 finished with value: 0.8036826248073222 and parameters: {'n_estimators': 96, 'learning_rate': 0.11962429306149054, 'max_depth': 7, 'max_bin': 227, 'num_leaves': 163}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:54,023] Trial 286 finished with value: 0.8016586561097835 and parameters: {'n_estimators': 139, 'learning_rate': 0.09875065455555893, 'max_depth': 8, 'max_bin': 217, 'num_leaves': 107}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:54,960] Trial 287 finished with value: 0.8136101201490883 and parameters: {'n_estimators': 310, 'learning_rate': 0.12802083836363626, 'max_depth': 10, 'max_bin': 234, 'num_leaves': 90}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:55,796] Trial 288 finished with value: 0.8015176318373938 and parameters: {'n_estimators': 110, 'learning_rate': 0.12469814003125534, 'max_depth': 9, 'max_bin': 211, 'num_leaves': 41}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:56,462] Trial 289 finished with value: 0.7878023110348312 and parameters: {'n_estimators': 69, 'learning_rate': 0.08608695972030211, 'max_depth': 8, 'max_bin': 220, 'num_leaves': 65}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:57,460] Trial 290 finished with value: 0.7972632817228067 and parameters: {'n_estimators': 163, 'learning_rate': 0.103384685785171, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 211}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:58,198] Trial 291 finished with value: 0.7804670108491572 and parameters: {'n_estimators': 89, 'learning_rate': 0.05922597161270481, 'max_depth': 7, 'max_bin': 193, 'num_leaves': 79}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:52:59,204] Trial 292 finished with value: 0.8123965653895707 and parameters: {'n_estimators': 121, 'learning_rate': 0.1083585279456874, 'max_depth': 11, 'max_bin': 214, 'num_leaves': 124}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:00,140] Trial 293 finished with value: 0.8094842490734212 and parameters: {'n_estimators': 104, 'learning_rate': 0.09333261520224174, 'max_depth': 10, 'max_bin': 230, 'num_leaves': 48}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:01,054] Trial 294 finished with value: 0.7958162994392495 and parameters: {'n_estimators': 186, 'learning_rate': 0.11677728060882088, 'max_depth': 8, 'max_bin': 272, 'num_leaves': 189}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:02,464] Trial 295 finished with value: 0.7934165156683246 and parameters: {'n_estimators': 208, 'learning_rate': 0.04937327757063543, 'max_depth': 9, 'max_bin': 225, 'num_leaves': 400}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:03,378] Trial 296 finished with value: 0.8022035949115777 and parameters: {'n_estimators': 144, 'learning_rate': 0.10075514170846296, 'max_depth': 8, 'max_bin': 209, 'num_leaves': 105}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:04,140] Trial 297 finished with value: 0.7858877887206498 and parameters: {'n_estimators': 82, 'learning_rate': 0.12075646841151388, 'max_depth': 9, 'max_bin': 201, 'num_leaves': 65}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:05,062] Trial 298 finished with value: 0.7999671828867856 and parameters: {'n_estimators': 125, 'learning_rate': 0.11121125206903902, 'max_depth': 10, 'max_bin': 218, 'num_leaves': 30}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:05,971] Trial 299 finished with value: 0.8039337279891863 and parameters: {'n_estimators': 472, 'learning_rate': 0.10569318081853728, 'max_depth': 7, 'max_bin': 215, 'num_leaves': 86}. Best is trial 120 with value: 0.8240512221088103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8241\n",
      "\tBest params:\n",
      "\t\tn_estimators: 99\n",
      "\t\tlearning_rate: 0.10189959615102016\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 214\n",
      "\t\tnum_leaves: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   42.000000   49.000000   39.000000   38.000000   \n",
      "1                    TN  309.000000  307.000000  306.000000  309.000000   \n",
      "2                    FP    5.000000    7.000000    9.000000    7.000000   \n",
      "3                    FN   26.000000   19.000000   28.000000   28.000000   \n",
      "4              Accuracy    0.918848    0.931937    0.903141    0.908377   \n",
      "5             Precision    0.893617    0.875000    0.812500    0.844444   \n",
      "6           Sensitivity    0.617647    0.720588    0.582090    0.575758   \n",
      "7           Specificity    0.984100    0.977700    0.971400    0.977800   \n",
      "8              F1 score    0.730435    0.790323    0.678261    0.684685   \n",
      "9   F1 score (weighted)    0.912752    0.929282    0.896558    0.901183   \n",
      "10     F1 score (macro)    0.841334    0.874849    0.810625    0.815543   \n",
      "11    Balanced Accuracy    0.800862    0.849148    0.776759    0.776803   \n",
      "12                  MCC    0.700721    0.755189    0.635083    0.649224   \n",
      "13                  NPV    0.922400    0.941700    0.916200    0.916900   \n",
      "14              ROC_AUC    0.800862    0.849148    0.776759    0.776803   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    40.000000   41.000000  \n",
      "1   308.000000  308.000000  \n",
      "2     7.000000    7.000000  \n",
      "3    27.000000   26.000000  \n",
      "4     0.910995    0.913613  \n",
      "5     0.851064    0.854167  \n",
      "6     0.597015    0.611940  \n",
      "7     0.977800    0.977800  \n",
      "8     0.701754    0.713043  \n",
      "9     0.904557    0.907741  \n",
      "10    0.824723    0.831098  \n",
      "11    0.787396    0.794859  \n",
      "12    0.665476    0.676618  \n",
      "13    0.919400    0.922200  \n",
      "14    0.787396    0.794859  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_lgbm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_lgbm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_lgbm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_lgbm_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:53:06,912] Trial 300 finished with value: 0.7898174659848138 and parameters: {'n_estimators': 534, 'learning_rate': 0.13639111545320765, 'max_depth': 8, 'max_bin': 223, 'num_leaves': 124}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:07,467] Trial 301 finished with value: 0.7897047924072691 and parameters: {'n_estimators': 66, 'learning_rate': 0.13185644077061592, 'max_depth': 6, 'max_bin': 212, 'num_leaves': 57}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:08,414] Trial 302 finished with value: 0.7853639599197197 and parameters: {'n_estimators': 157, 'learning_rate': 0.09643245456669865, 'max_depth': 9, 'max_bin': 227, 'num_leaves': 149}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:08,977] Trial 303 finished with value: 0.7798599111435662 and parameters: {'n_estimators': 101, 'learning_rate': 0.18903809635637897, 'max_depth': 5, 'max_bin': 220, 'num_leaves': 75}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:09,922] Trial 304 finished with value: 0.7826307320181425 and parameters: {'n_estimators': 133, 'learning_rate': 0.11669971895673809, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 319}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:10,502] Trial 305 finished with value: 0.7884232016899528 and parameters: {'n_estimators': 53, 'learning_rate': 0.12462228288224898, 'max_depth': 8, 'max_bin': 198, 'num_leaves': 93}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:11,510] Trial 306 finished with value: 0.7972471492183203 and parameters: {'n_estimators': 281, 'learning_rate': 0.08917678631236332, 'max_depth': 9, 'max_bin': 246, 'num_leaves': 112}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:12,217] Trial 307 finished with value: 0.7878188527881523 and parameters: {'n_estimators': 111, 'learning_rate': 0.15209640859846857, 'max_depth': 7, 'max_bin': 216, 'num_leaves': 44}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:13,126] Trial 308 finished with value: 0.7940807085380246 and parameters: {'n_estimators': 89, 'learning_rate': 0.07971225093976345, 'max_depth': 11, 'max_bin': 222, 'num_leaves': 64}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:14,038] Trial 309 finished with value: 0.8064775643766972 and parameters: {'n_estimators': 139, 'learning_rate': 0.10067474606671417, 'max_depth': 8, 'max_bin': 218, 'num_leaves': 96}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:14,884] Trial 310 finished with value: 0.8041633687682687 and parameters: {'n_estimators': 120, 'learning_rate': 0.11263529701453863, 'max_depth': 9, 'max_bin': 254, 'num_leaves': 137}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:15,957] Trial 311 finished with value: 0.786954818341741 and parameters: {'n_estimators': 174, 'learning_rate': 0.0715468462744527, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 76}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:16,734] Trial 312 finished with value: 0.795024167511877 and parameters: {'n_estimators': 83, 'learning_rate': 0.10614055369965195, 'max_depth': 9, 'max_bin': 230, 'num_leaves': 56}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:17,503] Trial 313 finished with value: 0.7917966343883917 and parameters: {'n_estimators': 102, 'learning_rate': 0.11954169167278123, 'max_depth': 8, 'max_bin': 214, 'num_leaves': 110}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:18,545] Trial 314 finished with value: 0.7962421598389102 and parameters: {'n_estimators': 155, 'learning_rate': 0.06341707675781974, 'max_depth': 9, 'max_bin': 224, 'num_leaves': 88}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:19,226] Trial 315 finished with value: 0.7998516412582012 and parameters: {'n_estimators': 72, 'learning_rate': 0.09409784273595008, 'max_depth': 8, 'max_bin': 233, 'num_leaves': 48}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:19,986] Trial 316 finished with value: 0.7898129618653333 and parameters: {'n_estimators': 387, 'learning_rate': 0.129247707049965, 'max_depth': 7, 'max_bin': 220, 'num_leaves': 72}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:20,890] Trial 317 finished with value: 0.7861394948188121 and parameters: {'n_estimators': 125, 'learning_rate': 0.11448081437007347, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 118}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:21,717] Trial 318 finished with value: 0.7924183008991837 and parameters: {'n_estimators': 100, 'learning_rate': 0.12345913617621342, 'max_depth': 9, 'max_bin': 240, 'num_leaves': 99}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:22,584] Trial 319 finished with value: 0.7991517191011857 and parameters: {'n_estimators': 140, 'learning_rate': 0.10923076191667085, 'max_depth': 8, 'max_bin': 213, 'num_leaves': 361}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:23,489] Trial 320 finished with value: 0.7934339779320367 and parameters: {'n_estimators': 193, 'learning_rate': 0.1022163381522559, 'max_depth': 9, 'max_bin': 218, 'num_leaves': 39}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:24,359] Trial 321 finished with value: 0.8005377757439 and parameters: {'n_estimators': 109, 'learning_rate': 0.11869144743057627, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 60}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:25,204] Trial 322 finished with value: 0.7869821555808493 and parameters: {'n_estimators': 162, 'learning_rate': 0.09886498666783708, 'max_depth': 8, 'max_bin': 226, 'num_leaves': 78}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:26,052] Trial 323 finished with value: 0.7904505333096719 and parameters: {'n_estimators': 338, 'learning_rate': 0.08616171961753716, 'max_depth': 5, 'max_bin': 211, 'num_leaves': 30}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:26,750] Trial 324 finished with value: 0.7933035822565657 and parameters: {'n_estimators': 82, 'learning_rate': 0.10495394794022765, 'max_depth': 7, 'max_bin': 216, 'num_leaves': 292}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:27,682] Trial 325 finished with value: 0.7994773883852068 and parameters: {'n_estimators': 221, 'learning_rate': 0.1255859195259101, 'max_depth': 11, 'max_bin': 258, 'num_leaves': 168}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:28,542] Trial 326 finished with value: 0.8008708490287404 and parameters: {'n_estimators': 122, 'learning_rate': 0.1093324455452973, 'max_depth': 8, 'max_bin': 222, 'num_leaves': 130}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:29,368] Trial 327 finished with value: 0.7882576700479874 and parameters: {'n_estimators': 98, 'learning_rate': 0.11407600108399506, 'max_depth': 9, 'max_bin': 228, 'num_leaves': 90}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:29,944] Trial 328 finished with value: 0.7837633875184296 and parameters: {'n_estimators': 59, 'learning_rate': 0.09162284623952849, 'max_depth': 7, 'max_bin': 236, 'num_leaves': 53}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:31,061] Trial 329 finished with value: 0.7890124986935783 and parameters: {'n_estimators': 147, 'learning_rate': 0.0680990670611489, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 68}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:31,889] Trial 330 finished with value: 0.7953418172139173 and parameters: {'n_estimators': 117, 'learning_rate': 0.1969501705900623, 'max_depth': 9, 'max_bin': 269, 'num_leaves': 468}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:32,614] Trial 331 finished with value: 0.7939701388601172 and parameters: {'n_estimators': 77, 'learning_rate': 0.09695781844950328, 'max_depth': 8, 'max_bin': 284, 'num_leaves': 112}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:33,336] Trial 332 finished with value: 0.7944038762478751 and parameters: {'n_estimators': 176, 'learning_rate': 0.1393031288840939, 'max_depth': 6, 'max_bin': 217, 'num_leaves': 85}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:34,176] Trial 333 finished with value: 0.8069375722343161 and parameters: {'n_estimators': 134, 'learning_rate': 0.12259459753176111, 'max_depth': 9, 'max_bin': 221, 'num_leaves': 46}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:34,915] Trial 334 finished with value: 0.7801213457858743 and parameters: {'n_estimators': 94, 'learning_rate': 0.13171710320103333, 'max_depth': 8, 'max_bin': 212, 'num_leaves': 145}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:35,830] Trial 335 finished with value: 0.7826833924985659 and parameters: {'n_estimators': 112, 'learning_rate': 0.10332349723919604, 'max_depth': 9, 'max_bin': 250, 'num_leaves': 441}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:36,585] Trial 336 finished with value: 0.8019032383684479 and parameters: {'n_estimators': 69, 'learning_rate': 0.11141054658939786, 'max_depth': 10, 'max_bin': 231, 'num_leaves': 101}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:37,407] Trial 337 finished with value: 0.7939054976517896 and parameters: {'n_estimators': 153, 'learning_rate': 0.10793888067091574, 'max_depth': 7, 'max_bin': 226, 'num_leaves': 66}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:38,394] Trial 338 finished with value: 0.7951246819191449 and parameters: {'n_estimators': 131, 'learning_rate': 0.1163121493480222, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 122}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:38,969] Trial 339 finished with value: 0.7865742784222938 and parameters: {'n_estimators': 52, 'learning_rate': 0.12029226398598207, 'max_depth': 8, 'max_bin': 215, 'num_leaves': 79}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:39,927] Trial 340 finished with value: 0.7914003588928862 and parameters: {'n_estimators': 501, 'learning_rate': 0.08234054153517566, 'max_depth': 9, 'max_bin': 219, 'num_leaves': 51}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:41,305] Trial 341 finished with value: 0.794360812100815 and parameters: {'n_estimators': 264, 'learning_rate': 0.03828571532658198, 'max_depth': 8, 'max_bin': 224, 'num_leaves': 30}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:42,160] Trial 342 finished with value: 0.7958702765482455 and parameters: {'n_estimators': 88, 'learning_rate': 0.09773372819955167, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 103}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:42,986] Trial 343 finished with value: 0.7982161051049866 and parameters: {'n_estimators': 108, 'learning_rate': 0.12819729156843288, 'max_depth': 9, 'max_bin': 221, 'num_leaves': 68}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:43,826] Trial 344 finished with value: 0.7985386212937823 and parameters: {'n_estimators': 166, 'learning_rate': 0.10374752161508125, 'max_depth': 7, 'max_bin': 216, 'num_leaves': 84}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:44,593] Trial 345 finished with value: 0.795653295464773 and parameters: {'n_estimators': 130, 'learning_rate': 0.08987012477215914, 'max_depth': 6, 'max_bin': 196, 'num_leaves': 157}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:45,127] Trial 346 finished with value: 0.7666904203042025 and parameters: {'n_estimators': 85, 'learning_rate': 0.07200290257010698, 'max_depth': 4, 'max_bin': 210, 'num_leaves': 57}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:45,882] Trial 347 finished with value: 0.7807427062621615 and parameters: {'n_estimators': 193, 'learning_rate': 0.17032271803677967, 'max_depth': 8, 'max_bin': 228, 'num_leaves': 99}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:46,754] Trial 348 finished with value: 0.7881732564462098 and parameters: {'n_estimators': 116, 'learning_rate': 0.10073458308784831, 'max_depth': 9, 'max_bin': 224, 'num_leaves': 126}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:47,649] Trial 349 finished with value: 0.7922687501786864 and parameters: {'n_estimators': 141, 'learning_rate': 0.11420769915038995, 'max_depth': 10, 'max_bin': 214, 'num_leaves': 44}. Best is trial 120 with value: 0.8240512221088103.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.824051\n",
      "\tBest params:\n",
      "\t\tn_estimators: 99\n",
      "\t\tlearning_rate: 0.10189959615102016\n",
      "\t\tmax_depth: 9\n",
      "\t\tmax_bin: 214\n",
      "\t\tnum_leaves: 80\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   42.000000   49.000000   39.000000   38.000000   \n",
      "1                    TN  309.000000  307.000000  306.000000  309.000000   \n",
      "2                    FP    5.000000    7.000000    9.000000    7.000000   \n",
      "3                    FN   26.000000   19.000000   28.000000   28.000000   \n",
      "4              Accuracy    0.918848    0.931937    0.903141    0.908377   \n",
      "5             Precision    0.893617    0.875000    0.812500    0.844444   \n",
      "6           Sensitivity    0.617647    0.720588    0.582090    0.575758   \n",
      "7           Specificity    0.984100    0.977700    0.971400    0.977800   \n",
      "8              F1 score    0.730435    0.790323    0.678261    0.684685   \n",
      "9   F1 score (weighted)    0.912752    0.929282    0.896558    0.901183   \n",
      "10     F1 score (macro)    0.841334    0.874849    0.810625    0.815543   \n",
      "11    Balanced Accuracy    0.800862    0.849148    0.776759    0.776803   \n",
      "12                  MCC    0.700721    0.755189    0.635083    0.649224   \n",
      "13                  NPV    0.922400    0.941700    0.916200    0.916900   \n",
      "14              ROC_AUC    0.800862    0.849148    0.776759    0.776803   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    40.000000   41.000000   39.000000  \n",
      "1   308.000000  308.000000  306.000000  \n",
      "2     7.000000    7.000000    7.000000  \n",
      "3    27.000000   26.000000   30.000000  \n",
      "4     0.910995    0.913613    0.903141  \n",
      "5     0.851064    0.854167    0.847826  \n",
      "6     0.597015    0.611940    0.565217  \n",
      "7     0.977800    0.977800    0.977600  \n",
      "8     0.701754    0.713043    0.678261  \n",
      "9     0.904557    0.907741    0.895172  \n",
      "10    0.824723    0.831098    0.810625  \n",
      "11    0.787396    0.794859    0.771427  \n",
      "12    0.665476    0.676618    0.641698  \n",
      "13    0.919400    0.922200    0.910700  \n",
      "14    0.787396    0.794859    0.771427  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_lgbm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_lgbm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_lgbm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_lgbm_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:53:48,720] Trial 350 finished with value: 0.6900650073694713 and parameters: {'n_estimators': 95, 'learning_rate': 0.012388338369440352, 'max_depth': 9, 'max_bin': 280, 'num_leaves': 73}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:49,372] Trial 351 finished with value: 0.7999094747857305 and parameters: {'n_estimators': 65, 'learning_rate': 0.1450656016939398, 'max_depth': 8, 'max_bin': 219, 'num_leaves': 139}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:50,814] Trial 352 finished with value: 0.8125645123752188 and parameters: {'n_estimators': 151, 'learning_rate': 0.0551092375697559, 'max_depth': 11, 'max_bin': 201, 'num_leaves': 536}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:51,780] Trial 353 finished with value: 0.8062875965326887 and parameters: {'n_estimators': 106, 'learning_rate': 0.062275429903159894, 'max_depth': 9, 'max_bin': 232, 'num_leaves': 93}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:52,785] Trial 354 finished with value: 0.806271873880886 and parameters: {'n_estimators': 292, 'learning_rate': 0.09399008097859177, 'max_depth': 10, 'max_bin': 216, 'num_leaves': 58}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:53,621] Trial 355 finished with value: 0.8003742149142246 and parameters: {'n_estimators': 125, 'learning_rate': 0.10779022508205724, 'max_depth': 7, 'max_bin': 208, 'num_leaves': 382}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:54,343] Trial 356 finished with value: 0.8064506120655356 and parameters: {'n_estimators': 78, 'learning_rate': 0.1261343274655775, 'max_depth': 8, 'max_bin': 264, 'num_leaves': 115}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:55,197] Trial 357 finished with value: 0.8028957665029631 and parameters: {'n_estimators': 97, 'learning_rate': 0.12014655614925175, 'max_depth': 9, 'max_bin': 226, 'num_leaves': 83}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:55,990] Trial 358 finished with value: 0.808271424540982 and parameters: {'n_estimators': 178, 'learning_rate': 0.15925323686863346, 'max_depth': 8, 'max_bin': 223, 'num_leaves': 43}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:57,208] Trial 359 finished with value: 0.8163292546355416 and parameters: {'n_estimators': 145, 'learning_rate': 0.058168523677018136, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 67}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:58,407] Trial 360 finished with value: 0.8114488888348497 and parameters: {'n_estimators': 167, 'learning_rate': 0.06725583557224163, 'max_depth': 10, 'max_bin': 219, 'num_leaves': 105}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:53:59,683] Trial 361 finished with value: 0.8033572223397194 and parameters: {'n_estimators': 150, 'learning_rate': 0.050078455098755754, 'max_depth': 10, 'max_bin': 222, 'num_leaves': 72}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:00,803] Trial 362 finished with value: 0.8206838566526263 and parameters: {'n_estimators': 139, 'learning_rate': 0.07590079738086526, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 88}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:01,926] Trial 363 finished with value: 0.8170178961605187 and parameters: {'n_estimators': 157, 'learning_rate': 0.07658166056892068, 'max_depth': 10, 'max_bin': 213, 'num_leaves': 89}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:03,067] Trial 364 finished with value: 0.8175890151130124 and parameters: {'n_estimators': 179, 'learning_rate': 0.07586476861678274, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 116}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:04,254] Trial 365 finished with value: 0.8199879847666081 and parameters: {'n_estimators': 205, 'learning_rate': 0.07688119527659562, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 130}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:05,451] Trial 366 finished with value: 0.8070249885866245 and parameters: {'n_estimators': 187, 'learning_rate': 0.07550031273124518, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 128}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:06,548] Trial 367 finished with value: 0.804120962703346 and parameters: {'n_estimators': 177, 'learning_rate': 0.07879349936349427, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 145}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:07,645] Trial 368 finished with value: 0.8140467452819697 and parameters: {'n_estimators': 167, 'learning_rate': 0.07759404772020503, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 135}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:08,830] Trial 369 finished with value: 0.8155254007399126 and parameters: {'n_estimators': 216, 'learning_rate': 0.07391027677073662, 'max_depth': 10, 'max_bin': 212, 'num_leaves': 180}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:09,969] Trial 370 finished with value: 0.8198491794979288 and parameters: {'n_estimators': 200, 'learning_rate': 0.07598221655868949, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 152}. Best is trial 120 with value: 0.8240512221088103.\n",
      "[I 2023-12-05 17:54:11,088] Trial 371 finished with value: 0.8246395761493097 and parameters: {'n_estimators': 193, 'learning_rate': 0.08119223817993149, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 150}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:12,202] Trial 372 finished with value: 0.8141190278715185 and parameters: {'n_estimators': 236, 'learning_rate': 0.08073891828232589, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 176}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:13,316] Trial 373 finished with value: 0.8207209369676859 and parameters: {'n_estimators': 192, 'learning_rate': 0.07827582002944773, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 154}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:14,469] Trial 374 finished with value: 0.8055801226459807 and parameters: {'n_estimators': 211, 'learning_rate': 0.07571557123187325, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 147}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:15,639] Trial 375 finished with value: 0.8137894054250362 and parameters: {'n_estimators': 218, 'learning_rate': 0.07124513165818903, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 165}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:16,784] Trial 376 finished with value: 0.8030470285993531 and parameters: {'n_estimators': 198, 'learning_rate': 0.0729880643296765, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 161}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:17,974] Trial 377 finished with value: 0.8149764130115921 and parameters: {'n_estimators': 193, 'learning_rate': 0.07995160045156642, 'max_depth': 10, 'max_bin': 203, 'num_leaves': 155}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:19,091] Trial 378 finished with value: 0.8186854862241482 and parameters: {'n_estimators': 213, 'learning_rate': 0.0766764561250601, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 199}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:20,244] Trial 379 finished with value: 0.8074058506514241 and parameters: {'n_estimators': 234, 'learning_rate': 0.07725985111778085, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 198}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:21,329] Trial 380 finished with value: 0.8054002109527743 and parameters: {'n_estimators': 205, 'learning_rate': 0.08234767293869863, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 189}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:22,510] Trial 381 finished with value: 0.8140447671284093 and parameters: {'n_estimators': 208, 'learning_rate': 0.07594804684160078, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 166}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:23,669] Trial 382 finished with value: 0.8175704301384655 and parameters: {'n_estimators': 202, 'learning_rate': 0.08330727446869114, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 169}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:24,807] Trial 383 finished with value: 0.8094204158794159 and parameters: {'n_estimators': 203, 'learning_rate': 0.08551642141736068, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 175}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:25,950] Trial 384 finished with value: 0.8070785988608098 and parameters: {'n_estimators': 190, 'learning_rate': 0.08264003102867487, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 186}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:27,174] Trial 385 finished with value: 0.8153727856780929 and parameters: {'n_estimators': 213, 'learning_rate': 0.07112177261948326, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 148}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:28,292] Trial 386 finished with value: 0.8231897341976756 and parameters: {'n_estimators': 184, 'learning_rate': 0.08105672302423872, 'max_depth': 10, 'max_bin': 208, 'num_leaves': 156}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:29,413] Trial 387 finished with value: 0.8085945706474924 and parameters: {'n_estimators': 189, 'learning_rate': 0.08292448363490487, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 158}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:30,508] Trial 388 finished with value: 0.8031034768015155 and parameters: {'n_estimators': 245, 'learning_rate': 0.08064387853650791, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 206}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:31,593] Trial 389 finished with value: 0.8143073456815632 and parameters: {'n_estimators': 232, 'learning_rate': 0.07937942412363508, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 178}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:32,758] Trial 390 finished with value: 0.8008970126336703 and parameters: {'n_estimators': 202, 'learning_rate': 0.07286673542666351, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 168}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:33,946] Trial 391 finished with value: 0.804522311680668 and parameters: {'n_estimators': 218, 'learning_rate': 0.06710441056616076, 'max_depth': 10, 'max_bin': 209, 'num_leaves': 153}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:34,996] Trial 392 finished with value: 0.8126726493650003 and parameters: {'n_estimators': 200, 'learning_rate': 0.08423698068540147, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 140}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:36,085] Trial 393 finished with value: 0.8067699398342739 and parameters: {'n_estimators': 178, 'learning_rate': 0.08639430622677652, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 196}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:37,192] Trial 394 finished with value: 0.8060068809413965 and parameters: {'n_estimators': 183, 'learning_rate': 0.07541976979715032, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 158}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:38,320] Trial 395 finished with value: 0.8206820309751232 and parameters: {'n_estimators': 179, 'learning_rate': 0.07771732449577581, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 178}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:39,441] Trial 396 finished with value: 0.8134141938803365 and parameters: {'n_estimators': 235, 'learning_rate': 0.0743063551574826, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 176}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:40,514] Trial 397 finished with value: 0.8056278734626074 and parameters: {'n_estimators': 188, 'learning_rate': 0.08237570081081483, 'max_depth': 10, 'max_bin': 207, 'num_leaves': 217}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:41,635] Trial 398 finished with value: 0.8127351566072598 and parameters: {'n_estimators': 214, 'learning_rate': 0.07800130489171908, 'max_depth': 11, 'max_bin': 203, 'num_leaves': 196}. Best is trial 371 with value: 0.8246395761493097.\n",
      "[I 2023-12-05 17:54:42,895] Trial 399 finished with value: 0.8177594266810301 and parameters: {'n_estimators': 223, 'learning_rate': 0.0719212070223464, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 233}. Best is trial 371 with value: 0.8246395761493097.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8246396\n",
      "\tBest params:\n",
      "\t\tn_estimators: 193\n",
      "\t\tlearning_rate: 0.08119223817993149\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 204\n",
      "\t\tnum_leaves: 150\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   42.000000   49.000000   39.000000   38.000000   \n",
      "1                    TN  309.000000  307.000000  306.000000  309.000000   \n",
      "2                    FP    5.000000    7.000000    9.000000    7.000000   \n",
      "3                    FN   26.000000   19.000000   28.000000   28.000000   \n",
      "4              Accuracy    0.918848    0.931937    0.903141    0.908377   \n",
      "5             Precision    0.893617    0.875000    0.812500    0.844444   \n",
      "6           Sensitivity    0.617647    0.720588    0.582090    0.575758   \n",
      "7           Specificity    0.984100    0.977700    0.971400    0.977800   \n",
      "8              F1 score    0.730435    0.790323    0.678261    0.684685   \n",
      "9   F1 score (weighted)    0.912752    0.929282    0.896558    0.901183   \n",
      "10     F1 score (macro)    0.841334    0.874849    0.810625    0.815543   \n",
      "11    Balanced Accuracy    0.800862    0.849148    0.776759    0.776803   \n",
      "12                  MCC    0.700721    0.755189    0.635083    0.649224   \n",
      "13                  NPV    0.922400    0.941700    0.916200    0.916900   \n",
      "14              ROC_AUC    0.800862    0.849148    0.776759    0.776803   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    40.000000   41.000000   39.000000   38.000000  \n",
      "1   308.000000  308.000000  306.000000  306.000000  \n",
      "2     7.000000    7.000000    7.000000    9.000000  \n",
      "3    27.000000   26.000000   30.000000   29.000000  \n",
      "4     0.910995    0.913613    0.903141    0.900524  \n",
      "5     0.851064    0.854167    0.847826    0.808511  \n",
      "6     0.597015    0.611940    0.565217    0.567164  \n",
      "7     0.977800    0.977800    0.977600    0.971400  \n",
      "8     0.701754    0.713043    0.678261    0.666667  \n",
      "9     0.904557    0.907741    0.895172    0.893328  \n",
      "10    0.824723    0.831098    0.810625    0.804103  \n",
      "11    0.787396    0.794859    0.771427    0.769296  \n",
      "12    0.665476    0.676618    0.641698    0.623565  \n",
      "13    0.919400    0.922200    0.910700    0.913400  \n",
      "14    0.787396    0.794859    0.771427    0.769296  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_lgbm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_lgbm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_lgbm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_lgbm_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:54:44,255] Trial 400 finished with value: 0.8289436443515321 and parameters: {'n_estimators': 220, 'learning_rate': 0.06771770912453205, 'max_depth': 10, 'max_bin': 211, 'num_leaves': 245}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:45,471] Trial 401 finished with value: 0.8270129993240223 and parameters: {'n_estimators': 254, 'learning_rate': 0.06983469248117388, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 270}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:46,698] Trial 402 finished with value: 0.8149358139429191 and parameters: {'n_estimators': 258, 'learning_rate': 0.06877126188944199, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 254}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:47,938] Trial 403 finished with value: 0.8251782777824396 and parameters: {'n_estimators': 232, 'learning_rate': 0.07002861175234286, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 194}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:49,204] Trial 404 finished with value: 0.81911538484463 and parameters: {'n_estimators': 229, 'learning_rate': 0.0656968701991257, 'max_depth': 10, 'max_bin': 210, 'num_leaves': 280}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:50,591] Trial 405 finished with value: 0.8211119439610822 and parameters: {'n_estimators': 253, 'learning_rate': 0.06411521500901667, 'max_depth': 10, 'max_bin': 206, 'num_leaves': 287}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:51,890] Trial 406 finished with value: 0.8288331765370602 and parameters: {'n_estimators': 252, 'learning_rate': 0.06428053145447743, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 228}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:53,198] Trial 407 finished with value: 0.8256168808679492 and parameters: {'n_estimators': 254, 'learning_rate': 0.06425942174480802, 'max_depth': 10, 'max_bin': 205, 'num_leaves': 267}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:54,464] Trial 408 finished with value: 0.8219196174311548 and parameters: {'n_estimators': 264, 'learning_rate': 0.06471169537157577, 'max_depth': 10, 'max_bin': 204, 'num_leaves': 274}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:55,742] Trial 409 finished with value: 0.8257050952175243 and parameters: {'n_estimators': 255, 'learning_rate': 0.0644777082121825, 'max_depth': 10, 'max_bin': 202, 'num_leaves': 260}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:57,057] Trial 410 finished with value: 0.8164512672388193 and parameters: {'n_estimators': 258, 'learning_rate': 0.06370511914558269, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 279}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:58,269] Trial 411 finished with value: 0.8199118783651353 and parameters: {'n_estimators': 254, 'learning_rate': 0.06606837112093666, 'max_depth': 10, 'max_bin': 201, 'num_leaves': 267}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:54:59,563] Trial 412 finished with value: 0.8213233731071012 and parameters: {'n_estimators': 276, 'learning_rate': 0.060483353993209046, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 264}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:55:00,891] Trial 413 finished with value: 0.8237607074581241 and parameters: {'n_estimators': 277, 'learning_rate': 0.0602627666381351, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 255}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:55:02,261] Trial 414 finished with value: 0.8151006210538927 and parameters: {'n_estimators': 263, 'learning_rate': 0.058244507594015504, 'max_depth': 10, 'max_bin': 192, 'num_leaves': 258}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:55:03,526] Trial 415 finished with value: 0.8252795251545079 and parameters: {'n_estimators': 280, 'learning_rate': 0.060434390378644724, 'max_depth': 10, 'max_bin': 198, 'num_leaves': 238}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:55:04,829] Trial 416 finished with value: 0.8265399543656585 and parameters: {'n_estimators': 281, 'learning_rate': 0.06056512243746456, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 236}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:55:06,143] Trial 417 finished with value: 0.8204945882929039 and parameters: {'n_estimators': 316, 'learning_rate': 0.061132208314195316, 'max_depth': 10, 'max_bin': 195, 'num_leaves': 250}. Best is trial 400 with value: 0.8289436443515321.\n",
      "[I 2023-12-05 17:55:07,422] Trial 418 finished with value: 0.8289701877469691 and parameters: {'n_estimators': 281, 'learning_rate': 0.06052087522800964, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 239}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:08,799] Trial 419 finished with value: 0.8171019237662765 and parameters: {'n_estimators': 289, 'learning_rate': 0.05913884371291907, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 236}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:10,164] Trial 420 finished with value: 0.8200267117386332 and parameters: {'n_estimators': 277, 'learning_rate': 0.06357838493674453, 'max_depth': 11, 'max_bin': 195, 'num_leaves': 245}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:11,470] Trial 421 finished with value: 0.8211071029144513 and parameters: {'n_estimators': 273, 'learning_rate': 0.06078148893763039, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 296}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:12,891] Trial 422 finished with value: 0.813342300338145 and parameters: {'n_estimators': 302, 'learning_rate': 0.055862018527757136, 'max_depth': 10, 'max_bin': 190, 'num_leaves': 301}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:14,246] Trial 423 finished with value: 0.8174916751068173 and parameters: {'n_estimators': 274, 'learning_rate': 0.06171898160326304, 'max_depth': 10, 'max_bin': 196, 'num_leaves': 277}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:15,574] Trial 424 finished with value: 0.8135218994987623 and parameters: {'n_estimators': 271, 'learning_rate': 0.05890945464400247, 'max_depth': 10, 'max_bin': 197, 'num_leaves': 225}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:16,992] Trial 425 finished with value: 0.8189028623354095 and parameters: {'n_estimators': 250, 'learning_rate': 0.05324810923321321, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 265}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:18,305] Trial 426 finished with value: 0.8206729566317008 and parameters: {'n_estimators': 292, 'learning_rate': 0.06455407614323463, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 234}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:19,563] Trial 427 finished with value: 0.8259641582410906 and parameters: {'n_estimators': 276, 'learning_rate': 0.06756702571027492, 'max_depth': 10, 'max_bin': 199, 'num_leaves': 263}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:20,986] Trial 428 finished with value: 0.8217374897736558 and parameters: {'n_estimators': 279, 'learning_rate': 0.0607112321846234, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 272}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:22,380] Trial 429 finished with value: 0.8181784017983936 and parameters: {'n_estimators': 280, 'learning_rate': 0.06141681640835107, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 292}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:23,788] Trial 430 finished with value: 0.8169754250628556 and parameters: {'n_estimators': 267, 'learning_rate': 0.05697997013990891, 'max_depth': 11, 'max_bin': 194, 'num_leaves': 244}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:25,105] Trial 431 finished with value: 0.8197924962079929 and parameters: {'n_estimators': 248, 'learning_rate': 0.06755348404970832, 'max_depth': 11, 'max_bin': 198, 'num_leaves': 272}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:26,487] Trial 432 finished with value: 0.8245034565667495 and parameters: {'n_estimators': 290, 'learning_rate': 0.06119893927754886, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 258}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:27,932] Trial 433 finished with value: 0.8174793778521015 and parameters: {'n_estimators': 297, 'learning_rate': 0.05416677411988743, 'max_depth': 11, 'max_bin': 183, 'num_leaves': 261}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:29,241] Trial 434 finished with value: 0.8179909840522633 and parameters: {'n_estimators': 287, 'learning_rate': 0.06010946404017982, 'max_depth': 10, 'max_bin': 174, 'num_leaves': 310}. Best is trial 418 with value: 0.8289701877469691.\n",
      "[I 2023-12-05 17:55:30,509] Trial 435 finished with value: 0.8334758811559657 and parameters: {'n_estimators': 326, 'learning_rate': 0.06443090203789681, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 251}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:31,790] Trial 436 finished with value: 0.8223550842485711 and parameters: {'n_estimators': 311, 'learning_rate': 0.06624456628301147, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 225}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:33,012] Trial 437 finished with value: 0.8266446570534949 and parameters: {'n_estimators': 341, 'learning_rate': 0.0668073406775366, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 221}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:34,292] Trial 438 finished with value: 0.8265108600459461 and parameters: {'n_estimators': 329, 'learning_rate': 0.06799999272890257, 'max_depth': 10, 'max_bin': 180, 'num_leaves': 221}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:35,508] Trial 439 finished with value: 0.8151871378571502 and parameters: {'n_estimators': 331, 'learning_rate': 0.06840304774795439, 'max_depth': 11, 'max_bin': 181, 'num_leaves': 223}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:36,796] Trial 440 finished with value: 0.8324697696893633 and parameters: {'n_estimators': 318, 'learning_rate': 0.06564826935810622, 'max_depth': 10, 'max_bin': 174, 'num_leaves': 233}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:38,021] Trial 441 finished with value: 0.8283381945092133 and parameters: {'n_estimators': 312, 'learning_rate': 0.06630538451185285, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 227}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:39,366] Trial 442 finished with value: 0.8202357632943839 and parameters: {'n_estimators': 327, 'learning_rate': 0.06255861623055851, 'max_depth': 10, 'max_bin': 177, 'num_leaves': 228}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:40,615] Trial 443 finished with value: 0.8186213950613294 and parameters: {'n_estimators': 311, 'learning_rate': 0.06868853627602922, 'max_depth': 10, 'max_bin': 176, 'num_leaves': 248}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:41,863] Trial 444 finished with value: 0.8191554587828186 and parameters: {'n_estimators': 350, 'learning_rate': 0.06554212177931837, 'max_depth': 10, 'max_bin': 179, 'num_leaves': 214}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:43,115] Trial 445 finished with value: 0.83327840122103 and parameters: {'n_estimators': 311, 'learning_rate': 0.06931712868536949, 'max_depth': 10, 'max_bin': 186, 'num_leaves': 240}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:44,355] Trial 446 finished with value: 0.8265714755705545 and parameters: {'n_estimators': 317, 'learning_rate': 0.06851140416858899, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 238}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:45,584] Trial 447 finished with value: 0.8237579482506245 and parameters: {'n_estimators': 342, 'learning_rate': 0.07071228954990473, 'max_depth': 10, 'max_bin': 168, 'num_leaves': 241}. Best is trial 435 with value: 0.8334758811559657.\n",
      "[I 2023-12-05 17:55:46,818] Trial 448 finished with value: 0.8359965382704905 and parameters: {'n_estimators': 373, 'learning_rate': 0.07048752776926567, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 247}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:48,013] Trial 449 finished with value: 0.8239812117209524 and parameters: {'n_estimators': 377, 'learning_rate': 0.06969880766136671, 'max_depth': 10, 'max_bin': 170, 'num_leaves': 242}. Best is trial 448 with value: 0.8359965382704905.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.83599654\n",
      "\tBest params:\n",
      "\t\tn_estimators: 373\n",
      "\t\tlearning_rate: 0.07048752776926567\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 185\n",
      "\t\tnum_leaves: 247\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   42.000000   49.000000   39.000000   38.000000   \n",
      "1                    TN  309.000000  307.000000  306.000000  309.000000   \n",
      "2                    FP    5.000000    7.000000    9.000000    7.000000   \n",
      "3                    FN   26.000000   19.000000   28.000000   28.000000   \n",
      "4              Accuracy    0.918848    0.931937    0.903141    0.908377   \n",
      "5             Precision    0.893617    0.875000    0.812500    0.844444   \n",
      "6           Sensitivity    0.617647    0.720588    0.582090    0.575758   \n",
      "7           Specificity    0.984100    0.977700    0.971400    0.977800   \n",
      "8              F1 score    0.730435    0.790323    0.678261    0.684685   \n",
      "9   F1 score (weighted)    0.912752    0.929282    0.896558    0.901183   \n",
      "10     F1 score (macro)    0.841334    0.874849    0.810625    0.815543   \n",
      "11    Balanced Accuracy    0.800862    0.849148    0.776759    0.776803   \n",
      "12                  MCC    0.700721    0.755189    0.635083    0.649224   \n",
      "13                  NPV    0.922400    0.941700    0.916200    0.916900   \n",
      "14              ROC_AUC    0.800862    0.849148    0.776759    0.776803   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    40.000000   41.000000   39.000000   38.000000   33.000000  \n",
      "1   308.000000  308.000000  306.000000  306.000000  311.000000  \n",
      "2     7.000000    7.000000    7.000000    9.000000    4.000000  \n",
      "3    27.000000   26.000000   30.000000   29.000000   34.000000  \n",
      "4     0.910995    0.913613    0.903141    0.900524    0.900524  \n",
      "5     0.851064    0.854167    0.847826    0.808511    0.891892  \n",
      "6     0.597015    0.611940    0.565217    0.567164    0.492537  \n",
      "7     0.977800    0.977800    0.977600    0.971400    0.987300  \n",
      "8     0.701754    0.713043    0.678261    0.666667    0.634615  \n",
      "9     0.904557    0.907741    0.895172    0.893328    0.888437  \n",
      "10    0.824723    0.831098    0.810625    0.804103    0.788520  \n",
      "11    0.787396    0.794859    0.771427    0.769296    0.739919  \n",
      "12    0.665476    0.676618    0.641698    0.623565    0.616989  \n",
      "13    0.919400    0.922200    0.910700    0.913400    0.901400  \n",
      "14    0.787396    0.794859    0.771427    0.769296    0.739919  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_lgbm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_lgbm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_lgbm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_lgbm_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:55:49,290] Trial 450 finished with value: 0.7985808042204546 and parameters: {'n_estimators': 344, 'learning_rate': 0.06968069422334146, 'max_depth': 10, 'max_bin': 169, 'num_leaves': 244}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:50,439] Trial 451 finished with value: 0.8136335850127617 and parameters: {'n_estimators': 368, 'learning_rate': 0.07023418683577953, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 237}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:51,599] Trial 452 finished with value: 0.8049737313959702 and parameters: {'n_estimators': 332, 'learning_rate': 0.06866458368498166, 'max_depth': 10, 'max_bin': 169, 'num_leaves': 252}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:52,824] Trial 453 finished with value: 0.8023369034775929 and parameters: {'n_estimators': 373, 'learning_rate': 0.05637983344559588, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 214}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:53,992] Trial 454 finished with value: 0.803901436140656 and parameters: {'n_estimators': 351, 'learning_rate': 0.06556876589154224, 'max_depth': 10, 'max_bin': 165, 'num_leaves': 236}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:55,128] Trial 455 finished with value: 0.8022825338003571 and parameters: {'n_estimators': 318, 'learning_rate': 0.06981602186838463, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 254}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:56,304] Trial 456 finished with value: 0.8042799349874727 and parameters: {'n_estimators': 326, 'learning_rate': 0.0653710076651162, 'max_depth': 10, 'max_bin': 172, 'num_leaves': 225}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:57,449] Trial 457 finished with value: 0.7963727855804561 and parameters: {'n_estimators': 299, 'learning_rate': 0.07084375020070321, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 243}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:58,535] Trial 458 finished with value: 0.8022796311601402 and parameters: {'n_estimators': 391, 'learning_rate': 0.06372894689639788, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 216}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:55:59,767] Trial 459 finished with value: 0.7942747762997104 and parameters: {'n_estimators': 356, 'learning_rate': 0.05694453525763629, 'max_depth': 10, 'max_bin': 184, 'num_leaves': 257}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:01,094] Trial 460 finished with value: 0.8075331551390514 and parameters: {'n_estimators': 305, 'learning_rate': 0.051834925625817964, 'max_depth': 10, 'max_bin': 175, 'num_leaves': 238}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:02,240] Trial 461 finished with value: 0.8119259707056337 and parameters: {'n_estimators': 336, 'learning_rate': 0.06815125027126782, 'max_depth': 10, 'max_bin': 170, 'num_leaves': 211}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:03,468] Trial 462 finished with value: 0.8017890887143716 and parameters: {'n_estimators': 404, 'learning_rate': 0.06287213023676677, 'max_depth': 10, 'max_bin': 180, 'num_leaves': 234}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:04,580] Trial 463 finished with value: 0.8012240377757032 and parameters: {'n_estimators': 316, 'learning_rate': 0.0712517977009838, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 255}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:05,714] Trial 464 finished with value: 0.8054452029065535 and parameters: {'n_estimators': 299, 'learning_rate': 0.0669921700715599, 'max_depth': 10, 'max_bin': 172, 'num_leaves': 225}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:06,922] Trial 465 finished with value: 0.8026539327269899 and parameters: {'n_estimators': 367, 'learning_rate': 0.058929295036667266, 'max_depth': 10, 'max_bin': 160, 'num_leaves': 249}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:08,132] Trial 466 finished with value: 0.7953933243617806 and parameters: {'n_estimators': 344, 'learning_rate': 0.07128723500718405, 'max_depth': 10, 'max_bin': 166, 'num_leaves': 267}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:09,269] Trial 467 finished with value: 0.8031598769192 and parameters: {'n_estimators': 324, 'learning_rate': 0.06369582444344948, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 208}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:10,498] Trial 468 finished with value: 0.8075138572082488 and parameters: {'n_estimators': 295, 'learning_rate': 0.05587247257539969, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 234}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:11,718] Trial 469 finished with value: 0.7988973700304196 and parameters: {'n_estimators': 426, 'learning_rate': 0.06731028088354879, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 259}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:12,941] Trial 470 finished with value: 0.8018459899805015 and parameters: {'n_estimators': 304, 'learning_rate': 0.06137769563432929, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 241}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:14,128] Trial 471 finished with value: 0.8065038327531526 and parameters: {'n_estimators': 289, 'learning_rate': 0.05387212258522085, 'max_depth': 10, 'max_bin': 173, 'num_leaves': 222}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:15,329] Trial 472 finished with value: 0.8099856697788279 and parameters: {'n_estimators': 325, 'learning_rate': 0.07193172971970782, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 208}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:16,495] Trial 473 finished with value: 0.8025391546778173 and parameters: {'n_estimators': 341, 'learning_rate': 0.0658225240698865, 'max_depth': 10, 'max_bin': 166, 'num_leaves': 242}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:17,689] Trial 474 finished with value: 0.8018774383392728 and parameters: {'n_estimators': 313, 'learning_rate': 0.06193169495977039, 'max_depth': 10, 'max_bin': 175, 'num_leaves': 263}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:18,960] Trial 475 finished with value: 0.8104659958986016 and parameters: {'n_estimators': 375, 'learning_rate': 0.058461344849645075, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 227}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:20,078] Trial 476 finished with value: 0.8031495483826425 and parameters: {'n_estimators': 243, 'learning_rate': 0.06901552399191536, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 281}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:21,405] Trial 477 finished with value: 0.7978895519141849 and parameters: {'n_estimators': 288, 'learning_rate': 0.049064342392602545, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 247}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:22,531] Trial 478 finished with value: 0.8029777935322866 and parameters: {'n_estimators': 263, 'learning_rate': 0.07195632284845441, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 221}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:23,658] Trial 479 finished with value: 0.8017135930156568 and parameters: {'n_estimators': 358, 'learning_rate': 0.06546647052386277, 'max_depth': 10, 'max_bin': 186, 'num_leaves': 266}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:24,767] Trial 480 finished with value: 0.805383208738028 and parameters: {'n_estimators': 278, 'learning_rate': 0.06213972483367044, 'max_depth': 10, 'max_bin': 170, 'num_leaves': 201}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:25,971] Trial 481 finished with value: 0.8039525481461964 and parameters: {'n_estimators': 310, 'learning_rate': 0.06843400940818058, 'max_depth': 10, 'max_bin': 163, 'num_leaves': 252}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:27,257] Trial 482 finished with value: 0.7956578767842106 and parameters: {'n_estimators': 245, 'learning_rate': 0.057829777291592076, 'max_depth': 10, 'max_bin': 188, 'num_leaves': 231}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:28,446] Trial 483 finished with value: 0.8045351012000598 and parameters: {'n_estimators': 333, 'learning_rate': 0.06436155679812851, 'max_depth': 11, 'max_bin': 193, 'num_leaves': 342}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:29,568] Trial 484 finished with value: 0.8014958976782782 and parameters: {'n_estimators': 300, 'learning_rate': 0.07268655341987212, 'max_depth': 10, 'max_bin': 182, 'num_leaves': 237}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:30,731] Trial 485 finished with value: 0.7973841029422932 and parameters: {'n_estimators': 282, 'learning_rate': 0.06831909504258962, 'max_depth': 10, 'max_bin': 176, 'num_leaves': 214}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:31,919] Trial 486 finished with value: 0.8016159610798328 and parameters: {'n_estimators': 263, 'learning_rate': 0.06056569067291597, 'max_depth': 10, 'max_bin': 191, 'num_leaves': 276}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:33,034] Trial 487 finished with value: 0.7993500225729201 and parameters: {'n_estimators': 348, 'learning_rate': 0.07269398422538886, 'max_depth': 10, 'max_bin': 168, 'num_leaves': 249}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:34,304] Trial 488 finished with value: 0.8070170512266653 and parameters: {'n_estimators': 323, 'learning_rate': 0.05507728524780863, 'max_depth': 10, 'max_bin': 154, 'num_leaves': 260}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:35,517] Trial 489 finished with value: 0.8032244849512823 and parameters: {'n_estimators': 895, 'learning_rate': 0.06509190805453476, 'max_depth': 11, 'max_bin': 174, 'num_leaves': 229}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:36,764] Trial 490 finished with value: 0.7925900567908688 and parameters: {'n_estimators': 248, 'learning_rate': 0.051689435201231676, 'max_depth': 10, 'max_bin': 180, 'num_leaves': 209}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:38,013] Trial 491 finished with value: 0.8021096304979546 and parameters: {'n_estimators': 270, 'learning_rate': 0.059402120690354436, 'max_depth': 10, 'max_bin': 185, 'num_leaves': 243}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:39,191] Trial 492 finished with value: 0.7973644087086372 and parameters: {'n_estimators': 296, 'learning_rate': 0.07056109200188809, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 282}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:40,552] Trial 493 finished with value: 0.8135040249751718 and parameters: {'n_estimators': 315, 'learning_rate': 0.0466394038924195, 'max_depth': 10, 'max_bin': 200, 'num_leaves': 263}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:41,684] Trial 494 finished with value: 0.8075018160358421 and parameters: {'n_estimators': 237, 'learning_rate': 0.06664929444796837, 'max_depth': 10, 'max_bin': 189, 'num_leaves': 225}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:42,505] Trial 495 finished with value: 0.7667904197471094 and parameters: {'n_estimators': 336, 'learning_rate': 0.06239815765778018, 'max_depth': 3, 'max_bin': 173, 'num_leaves': 245}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:43,830] Trial 496 finished with value: 0.8085554403542128 and parameters: {'n_estimators': 384, 'learning_rate': 0.05534411002427225, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 205}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:44,923] Trial 497 finished with value: 0.8053339688883986 and parameters: {'n_estimators': 282, 'learning_rate': 0.06896611867139332, 'max_depth': 10, 'max_bin': 187, 'num_leaves': 233}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:45,963] Trial 498 finished with value: 0.8053424010425847 and parameters: {'n_estimators': 259, 'learning_rate': 0.0734720451017287, 'max_depth': 10, 'max_bin': 193, 'num_leaves': 256}. Best is trial 448 with value: 0.8359965382704905.\n",
      "[I 2023-12-05 17:56:47,170] Trial 499 finished with value: 0.8019965451659342 and parameters: {'n_estimators': 358, 'learning_rate': 0.06337511979477418, 'max_depth': 11, 'max_bin': 191, 'num_leaves': 272}. Best is trial 448 with value: 0.8359965382704905.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.835996538\n",
      "\tBest params:\n",
      "\t\tn_estimators: 373\n",
      "\t\tlearning_rate: 0.07048752776926567\n",
      "\t\tmax_depth: 10\n",
      "\t\tmax_bin: 185\n",
      "\t\tnum_leaves: 247\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   42.000000   49.000000   39.000000   38.000000   \n",
      "1                    TN  309.000000  307.000000  306.000000  309.000000   \n",
      "2                    FP    5.000000    7.000000    9.000000    7.000000   \n",
      "3                    FN   26.000000   19.000000   28.000000   28.000000   \n",
      "4              Accuracy    0.918848    0.931937    0.903141    0.908377   \n",
      "5             Precision    0.893617    0.875000    0.812500    0.844444   \n",
      "6           Sensitivity    0.617647    0.720588    0.582090    0.575758   \n",
      "7           Specificity    0.984100    0.977700    0.971400    0.977800   \n",
      "8              F1 score    0.730435    0.790323    0.678261    0.684685   \n",
      "9   F1 score (weighted)    0.912752    0.929282    0.896558    0.901183   \n",
      "10     F1 score (macro)    0.841334    0.874849    0.810625    0.815543   \n",
      "11    Balanced Accuracy    0.800862    0.849148    0.776759    0.776803   \n",
      "12                  MCC    0.700721    0.755189    0.635083    0.649224   \n",
      "13                  NPV    0.922400    0.941700    0.916200    0.916900   \n",
      "14              ROC_AUC    0.800862    0.849148    0.776759    0.776803   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    40.000000   41.000000   39.000000   38.000000   33.000000   45.000000  \n",
      "1   308.000000  308.000000  306.000000  306.000000  311.000000  306.000000  \n",
      "2     7.000000    7.000000    7.000000    9.000000    4.000000    7.000000  \n",
      "3    27.000000   26.000000   30.000000   29.000000   34.000000   24.000000  \n",
      "4     0.910995    0.913613    0.903141    0.900524    0.900524    0.918848  \n",
      "5     0.851064    0.854167    0.847826    0.808511    0.891892    0.865385  \n",
      "6     0.597015    0.611940    0.565217    0.567164    0.492537    0.652174  \n",
      "7     0.977800    0.977800    0.977600    0.971400    0.987300    0.977600  \n",
      "8     0.701754    0.713043    0.678261    0.666667    0.634615    0.743802  \n",
      "9     0.904557    0.907741    0.895172    0.893328    0.888437    0.914220  \n",
      "10    0.824723    0.831098    0.810625    0.804103    0.788520    0.847795  \n",
      "11    0.787396    0.794859    0.771427    0.769296    0.739919    0.814905  \n",
      "12    0.665476    0.676618    0.641698    0.623565    0.616989    0.706557  \n",
      "13    0.919400    0.922200    0.910700    0.913400    0.901400    0.927300  \n",
      "14    0.787396    0.794859    0.771427    0.769296    0.739919    0.814905  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_lgbm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_lgbm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_lgbm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_lgbm_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        20.100000     1.911951\n",
      "1                    TN       151.800000     1.873796\n",
      "2                    FP         5.800000     1.873796\n",
      "3                    FN        13.300000     2.057507\n",
      "4              Accuracy         0.900000     0.015104\n",
      "5             Precision         0.777849     0.059111\n",
      "6           Sensitivity         0.602123     0.058298\n",
      "7           Specificity         0.963230     0.011835\n",
      "8              F1 score         0.677365     0.050767\n",
      "9   F1 score (weighted)         0.894737     0.016186\n",
      "10     F1 score (macro)         0.809087     0.029658\n",
      "11    Balanced Accuracy         0.782669     0.030521\n",
      "12                  MCC         0.627433     0.058039\n",
      "13                  NPV         0.919540     0.011710\n",
      "14              ROC_AUC         0.782669     0.030521\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>40.400000</td>\n",
       "      <td>4.325634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>311.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>307.600000</td>\n",
       "      <td>1.712698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>1.523884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>27.100000</td>\n",
       "      <td>3.928528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.931937</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.913613</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.010101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.891892</td>\n",
       "      <td>0.865385</td>\n",
       "      <td>0.854441</td>\n",
       "      <td>0.028874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.598213</td>\n",
       "      <td>0.060036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.984100</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.971400</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.978050</td>\n",
       "      <td>0.004846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.730435</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.678261</td>\n",
       "      <td>0.684685</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.678261</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.702185</td>\n",
       "      <td>0.044340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.912752</td>\n",
       "      <td>0.929282</td>\n",
       "      <td>0.896558</td>\n",
       "      <td>0.901183</td>\n",
       "      <td>0.904557</td>\n",
       "      <td>0.907741</td>\n",
       "      <td>0.895172</td>\n",
       "      <td>0.893328</td>\n",
       "      <td>0.888437</td>\n",
       "      <td>0.914220</td>\n",
       "      <td>0.904323</td>\n",
       "      <td>0.012160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.841334</td>\n",
       "      <td>0.874849</td>\n",
       "      <td>0.810625</td>\n",
       "      <td>0.815543</td>\n",
       "      <td>0.824723</td>\n",
       "      <td>0.831098</td>\n",
       "      <td>0.810625</td>\n",
       "      <td>0.804103</td>\n",
       "      <td>0.788520</td>\n",
       "      <td>0.847795</td>\n",
       "      <td>0.824922</td>\n",
       "      <td>0.024897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.800862</td>\n",
       "      <td>0.849148</td>\n",
       "      <td>0.776759</td>\n",
       "      <td>0.776803</td>\n",
       "      <td>0.787396</td>\n",
       "      <td>0.794859</td>\n",
       "      <td>0.771427</td>\n",
       "      <td>0.769296</td>\n",
       "      <td>0.739919</td>\n",
       "      <td>0.814905</td>\n",
       "      <td>0.788137</td>\n",
       "      <td>0.029559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.700721</td>\n",
       "      <td>0.755189</td>\n",
       "      <td>0.635083</td>\n",
       "      <td>0.649224</td>\n",
       "      <td>0.665476</td>\n",
       "      <td>0.676618</td>\n",
       "      <td>0.641698</td>\n",
       "      <td>0.623565</td>\n",
       "      <td>0.616989</td>\n",
       "      <td>0.706557</td>\n",
       "      <td>0.667112</td>\n",
       "      <td>0.043380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.922400</td>\n",
       "      <td>0.941700</td>\n",
       "      <td>0.916200</td>\n",
       "      <td>0.916900</td>\n",
       "      <td>0.919400</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.910700</td>\n",
       "      <td>0.913400</td>\n",
       "      <td>0.901400</td>\n",
       "      <td>0.927300</td>\n",
       "      <td>0.919160</td>\n",
       "      <td>0.010688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.800862</td>\n",
       "      <td>0.849148</td>\n",
       "      <td>0.776759</td>\n",
       "      <td>0.776803</td>\n",
       "      <td>0.787396</td>\n",
       "      <td>0.794859</td>\n",
       "      <td>0.771427</td>\n",
       "      <td>0.769296</td>\n",
       "      <td>0.739919</td>\n",
       "      <td>0.814905</td>\n",
       "      <td>0.788137</td>\n",
       "      <td>0.029559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   42.000000   49.000000   39.000000   38.000000   \n",
       "1                    TN  309.000000  307.000000  306.000000  309.000000   \n",
       "2                    FP    5.000000    7.000000    9.000000    7.000000   \n",
       "3                    FN   26.000000   19.000000   28.000000   28.000000   \n",
       "4              Accuracy    0.918848    0.931937    0.903141    0.908377   \n",
       "5             Precision    0.893617    0.875000    0.812500    0.844444   \n",
       "6           Sensitivity    0.617647    0.720588    0.582090    0.575758   \n",
       "7           Specificity    0.984100    0.977700    0.971400    0.977800   \n",
       "8              F1 score    0.730435    0.790323    0.678261    0.684685   \n",
       "9   F1 score (weighted)    0.912752    0.929282    0.896558    0.901183   \n",
       "10     F1 score (macro)    0.841334    0.874849    0.810625    0.815543   \n",
       "11    Balanced Accuracy    0.800862    0.849148    0.776759    0.776803   \n",
       "12                  MCC    0.700721    0.755189    0.635083    0.649224   \n",
       "13                  NPV    0.922400    0.941700    0.916200    0.916900   \n",
       "14              ROC_AUC    0.800862    0.849148    0.776759    0.776803   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    40.000000   41.000000   39.000000   38.000000   33.000000   45.000000   \n",
       "1   308.000000  308.000000  306.000000  306.000000  311.000000  306.000000   \n",
       "2     7.000000    7.000000    7.000000    9.000000    4.000000    7.000000   \n",
       "3    27.000000   26.000000   30.000000   29.000000   34.000000   24.000000   \n",
       "4     0.910995    0.913613    0.903141    0.900524    0.900524    0.918848   \n",
       "5     0.851064    0.854167    0.847826    0.808511    0.891892    0.865385   \n",
       "6     0.597015    0.611940    0.565217    0.567164    0.492537    0.652174   \n",
       "7     0.977800    0.977800    0.977600    0.971400    0.987300    0.977600   \n",
       "8     0.701754    0.713043    0.678261    0.666667    0.634615    0.743802   \n",
       "9     0.904557    0.907741    0.895172    0.893328    0.888437    0.914220   \n",
       "10    0.824723    0.831098    0.810625    0.804103    0.788520    0.847795   \n",
       "11    0.787396    0.794859    0.771427    0.769296    0.739919    0.814905   \n",
       "12    0.665476    0.676618    0.641698    0.623565    0.616989    0.706557   \n",
       "13    0.919400    0.922200    0.910700    0.913400    0.901400    0.927300   \n",
       "14    0.787396    0.794859    0.771427    0.769296    0.739919    0.814905   \n",
       "\n",
       "           ave       std  \n",
       "0    40.400000  4.325634  \n",
       "1   307.600000  1.712698  \n",
       "2     6.900000  1.523884  \n",
       "3    27.100000  3.928528  \n",
       "4     0.910995  0.010101  \n",
       "5     0.854441  0.028874  \n",
       "6     0.598213  0.060036  \n",
       "7     0.978050  0.004846  \n",
       "8     0.702185  0.044340  \n",
       "9     0.904323  0.012160  \n",
       "10    0.824922  0.024897  \n",
       "11    0.788137  0.029559  \n",
       "12    0.667112  0.043380  \n",
       "13    0.919160  0.010688  \n",
       "14    0.788137  0.029559  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.906597</td>\n",
       "      <td>0.017068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.815328</td>\n",
       "      <td>0.072369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.609318</td>\n",
       "      <td>0.087851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.969534</td>\n",
       "      <td>0.014964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.692840</td>\n",
       "      <td>0.063238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.900811</td>\n",
       "      <td>0.018828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.818848</td>\n",
       "      <td>0.036171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.789424</td>\n",
       "      <td>0.042617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.651571</td>\n",
       "      <td>0.068393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.921686</td>\n",
       "      <td>0.016075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.789424</td>\n",
       "      <td>0.042617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.906597     0.017068\n",
       "1             Precision         0.815328     0.072369\n",
       "2           Sensitivity         0.609318     0.087851\n",
       "3           Specificity         0.969534     0.014964\n",
       "4              F1 score         0.692840     0.063238\n",
       "5   F1 score (weighted)         0.900811     0.018828\n",
       "6      F1 score (macro)         0.818848     0.036171\n",
       "7     Balanced Accuracy         0.789424     0.042617\n",
       "8                   MCC         0.651571     0.068393\n",
       "9                   NPV         0.921686     0.016075\n",
       "10              ROC_AUC         0.789424     0.042617"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_lgbm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "       \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_lgbm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_lgbm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_lgbm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_lgbm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_lgbm))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "mat_met_optimized_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model f1_score 0.8119 with a standard deviation of 0.0403\n",
      "LightGBM optimized model f1_score 0.8156 with a standard deviation of 0.0428\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#f1_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "f1_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"f1_macro\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)f1_score %0.4f with a standard deviation of %0.4f\" % (f1_cv_lgbm_opt_testSet.mean(), f1_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(f1_cv_lgbm_opt), np.std(f1_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm_clf_withSemiSel.joblib']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_clf, \"OUTPUT/lgbm_clf_withSemiSel.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"OUTPUT/optimized_lgbm_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm_clf_withSemiSel.joblib\") \n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        19.500000     2.068279\n",
      "1                    TN       152.300000     1.418136\n",
      "2                    FP         5.300000     1.494434\n",
      "3                    FN        13.900000     2.330951\n",
      "4              Accuracy         0.899476     0.014349\n",
      "5             Precision         0.786895     0.054572\n",
      "6           Sensitivity         0.584377     0.064874\n",
      "7           Specificity         0.966410     0.009379\n",
      "8              F1 score         0.669028     0.054844\n",
      "9   F1 score (weighted)         0.893183     0.016589\n",
      "10     F1 score (macro)         0.804874     0.031429\n",
      "11    Balanced Accuracy         0.775383     0.033025\n",
      "12                  MCC         0.621865     0.059378\n",
      "13                  NPV         0.916490     0.012945\n",
      "14              ROC_AUC         0.775383     0.033025\n",
      "CPU times: user 8.25 s, sys: 60 ms, total: 8.31 s\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    n_jobs=8,\n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_clf.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"logloss\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        \n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:57:10,062] A new study created in memory with name: XGBClassifier\n",
      "[I 2023-12-05 17:57:17,423] Trial 0 finished with value: 0.7999411502594839 and parameters: {'n_estimators': 852, 'eta': 0.03851698696514652, 'max_depth': 10, 'alpha': 0.43310000000000004, 'lambda': 36.9013937572624, 'max_bin': 439}. Best is trial 0 with value: 0.7999411502594839.\n",
      "[I 2023-12-05 17:57:23,642] Trial 1 finished with value: 0.7986185058600851 and parameters: {'n_estimators': 794, 'eta': 0.03175297536886935, 'max_depth': 10, 'alpha': 0.38920000000000005, 'lambda': 12.772731198485477, 'max_bin': 471}. Best is trial 0 with value: 0.7999411502594839.\n",
      "[I 2023-12-05 17:57:27,698] Trial 2 finished with value: 0.7954878417427051 and parameters: {'n_estimators': 773, 'eta': 0.05873893343998224, 'max_depth': 8, 'alpha': 0.4189, 'lambda': 16.5725710232487, 'max_bin': 395}. Best is trial 0 with value: 0.7999411502594839.\n",
      "[I 2023-12-05 17:57:31,362] Trial 3 finished with value: 0.7939168773062659 and parameters: {'n_estimators': 405, 'eta': 0.07105088540985934, 'max_depth': 10, 'alpha': 0.8769, 'lambda': 21.300083062289374, 'max_bin': 372}. Best is trial 0 with value: 0.7999411502594839.\n",
      "[I 2023-12-05 17:57:33,696] Trial 4 finished with value: 0.7909969129523189 and parameters: {'n_estimators': 294, 'eta': 0.07057510203866048, 'max_depth': 9, 'alpha': 0.5353, 'lambda': 2.326451714127712, 'max_bin': 287}. Best is trial 0 with value: 0.7999411502594839.\n",
      "[I 2023-12-05 17:57:36,385] Trial 5 finished with value: 0.804943702137938 and parameters: {'n_estimators': 261, 'eta': 0.09713166734064149, 'max_depth': 11, 'alpha': 0.7981, 'lambda': 11.649565911390543, 'max_bin': 435}. Best is trial 5 with value: 0.804943702137938.\n",
      "[I 2023-12-05 17:57:43,632] Trial 6 finished with value: 0.7530806967894766 and parameters: {'n_estimators': 557, 'eta': 0.011173142677990204, 'max_depth': 9, 'alpha': 0.6555000000000001, 'lambda': 28.430942754030642, 'max_bin': 262}. Best is trial 5 with value: 0.804943702137938.\n",
      "[I 2023-12-05 17:57:46,277] Trial 7 finished with value: 0.7993423381313762 and parameters: {'n_estimators': 598, 'eta': 0.061084951323012976, 'max_depth': 9, 'alpha': 0.1688, 'lambda': 3.274760081462412, 'max_bin': 320}. Best is trial 5 with value: 0.804943702137938.\n",
      "[I 2023-12-05 17:57:48,712] Trial 8 finished with value: 0.8099560941939572 and parameters: {'n_estimators': 509, 'eta': 0.09203488600961084, 'max_depth': 12, 'alpha': 0.8726, 'lambda': 6.146804136542333, 'max_bin': 403}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:57:51,190] Trial 9 finished with value: 0.7950615285878724 and parameters: {'n_estimators': 196, 'eta': 0.07326905399221424, 'max_depth': 8, 'alpha': 0.8662000000000001, 'lambda': 23.11877869023212, 'max_bin': 377}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:57:53,509] Trial 10 finished with value: 0.789898371966592 and parameters: {'n_estimators': 461, 'eta': 0.09850339261125754, 'max_depth': 5, 'alpha': 0.6775, 'lambda': 8.094719353400214, 'max_bin': 497}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:57:54,587] Trial 11 finished with value: 0.7938043696792696 and parameters: {'n_estimators': 64, 'eta': 0.09965897178965318, 'max_depth': 12, 'alpha': 0.999, 'lambda': 10.551269087998145, 'max_bin': 425}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:57:57,049] Trial 12 finished with value: 0.8057024414690088 and parameters: {'n_estimators': 309, 'eta': 0.08795022618558034, 'max_depth': 12, 'alpha': 0.7814, 'lambda': 7.054686830432036, 'max_bin': 335}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:57:59,376] Trial 13 finished with value: 0.8029124166972625 and parameters: {'n_estimators': 635, 'eta': 0.08478049547616837, 'max_depth': 12, 'alpha': 0.9829, 'lambda': 1.4184581043017888, 'max_bin': 329}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:01,955] Trial 14 finished with value: 0.79608083946627 and parameters: {'n_estimators': 387, 'eta': 0.08500584244712903, 'max_depth': 6, 'alpha': 0.6985, 'lambda': 7.1061452881529314, 'max_bin': 334}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:03,900] Trial 15 finished with value: 0.8008520433975341 and parameters: {'n_estimators': 128, 'eta': 0.08467563862851629, 'max_depth': 12, 'alpha': 0.2205, 'lambda': 16.68073326793961, 'max_bin': 357}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:07,409] Trial 16 finished with value: 0.8007269358969117 and parameters: {'n_estimators': 338, 'eta': 0.04931071151201881, 'max_depth': 11, 'alpha': 0.7947000000000001, 'lambda': 6.676862897534128, 'max_bin': 408}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:09,700] Trial 17 finished with value: 0.8046617022369876 and parameters: {'n_estimators': 521, 'eta': 0.08643246456048133, 'max_depth': 11, 'alpha': 0.5575, 'lambda': 4.621849062035812, 'max_bin': 287}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:12,433] Trial 18 finished with value: 0.8035697818557821 and parameters: {'n_estimators': 696, 'eta': 0.07738973411017251, 'max_depth': 7, 'alpha': 0.055, 'lambda': 8.87070822251898, 'max_bin': 348}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:14,711] Trial 19 finished with value: 0.7975569603946671 and parameters: {'n_estimators': 444, 'eta': 0.09122557245783192, 'max_depth': 12, 'alpha': 0.8777, 'lambda': 1.4623515104693832, 'max_bin': 299}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:17,140] Trial 20 finished with value: 0.7997830908571106 and parameters: {'n_estimators': 223, 'eta': 0.07906479861552439, 'max_depth': 11, 'alpha': 0.7422000000000001, 'lambda': 5.707115845502077, 'max_bin': 390}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:19,862] Trial 21 finished with value: 0.7975002324439856 and parameters: {'n_estimators': 282, 'eta': 0.0989503932692975, 'max_depth': 11, 'alpha': 0.8125, 'lambda': 12.591240980036819, 'max_bin': 460}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:22,501] Trial 22 finished with value: 0.8026630156243824 and parameters: {'n_estimators': 209, 'eta': 0.09203171582489762, 'max_depth': 12, 'alpha': 0.9304, 'lambda': 11.410788657514932, 'max_bin': 420}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:25,569] Trial 23 finished with value: 0.8021905104408606 and parameters: {'n_estimators': 369, 'eta': 0.09273052765756482, 'max_depth': 11, 'alpha': 0.5999, 'lambda': 15.252942972503199, 'max_bin': 444}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:28,347] Trial 24 finished with value: 0.806242653804685 and parameters: {'n_estimators': 310, 'eta': 0.07874099844532127, 'max_depth': 10, 'alpha': 0.7861, 'lambda': 8.208342565795455, 'max_bin': 405}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:30,887] Trial 25 finished with value: 0.7968949984604119 and parameters: {'n_estimators': 496, 'eta': 0.07973096410892486, 'max_depth': 10, 'alpha': 0.7477, 'lambda': 5.1389334944313765, 'max_bin': 357}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:32,987] Trial 26 finished with value: 0.8001165531473283 and parameters: {'n_estimators': 134, 'eta': 0.06330378340244566, 'max_depth': 12, 'alpha': 0.6307, 'lambda': 8.65076135793721, 'max_bin': 409}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:35,848] Trial 27 finished with value: 0.8008601338689294 and parameters: {'n_estimators': 330, 'eta': 0.06826310315376875, 'max_depth': 10, 'alpha': 0.8803000000000001, 'lambda': 4.237635374300458, 'max_bin': 388}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:38,553] Trial 28 finished with value: 0.7922841786865026 and parameters: {'n_estimators': 422, 'eta': 0.07694147559455919, 'max_depth': 7, 'alpha': 0.29760000000000003, 'lambda': 9.3170327565074, 'max_bin': 314}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:41,131] Trial 29 finished with value: 0.8011265884839069 and parameters: {'n_estimators': 895, 'eta': 0.0913987138731222, 'max_depth': 10, 'alpha': 0.9285, 'lambda': 6.6437961396829905, 'max_bin': 453}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:44,297] Trial 30 finished with value: 0.8081410421015001 and parameters: {'n_estimators': 647, 'eta': 0.08369447361698586, 'max_depth': 11, 'alpha': 0.7279, 'lambda': 14.485428485867436, 'max_bin': 347}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:47,349] Trial 31 finished with value: 0.8015373675012368 and parameters: {'n_estimators': 599, 'eta': 0.08244794406204446, 'max_depth': 11, 'alpha': 0.7322000000000001, 'lambda': 15.375553441238672, 'max_bin': 345}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:50,271] Trial 32 finished with value: 0.8022704945532751 and parameters: {'n_estimators': 760, 'eta': 0.089245388218448, 'max_depth': 12, 'alpha': 0.47650000000000003, 'lambda': 13.144367340036911, 'max_bin': 364}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:53,288] Trial 33 finished with value: 0.8011639056165014 and parameters: {'n_estimators': 687, 'eta': 0.07615193476307464, 'max_depth': 11, 'alpha': 0.7898000000000001, 'lambda': 9.258738799820204, 'max_bin': 404}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:55,696] Trial 34 finished with value: 0.7999352307171572 and parameters: {'n_estimators': 531, 'eta': 0.08122786374993314, 'max_depth': 10, 'alpha': 0.5921000000000001, 'lambda': 4.308701105138391, 'max_bin': 380}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:58:59,129] Trial 35 finished with value: 0.8066616474956234 and parameters: {'n_estimators': 687, 'eta': 0.06791376448636906, 'max_depth': 10, 'alpha': 0.8370000000000001, 'lambda': 13.368255559290416, 'max_bin': 340}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:02,800] Trial 36 finished with value: 0.7994654003002585 and parameters: {'n_estimators': 815, 'eta': 0.07026738586721192, 'max_depth': 9, 'alpha': 0.8442000000000001, 'lambda': 13.74888574334903, 'max_bin': 368}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:07,155] Trial 37 finished with value: 0.8024741912431292 and parameters: {'n_estimators': 704, 'eta': 0.05533665560865744, 'max_depth': 10, 'alpha': 0.9107000000000001, 'lambda': 18.435533758015534, 'max_bin': 425}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:10,304] Trial 38 finished with value: 0.8029105806444317 and parameters: {'n_estimators': 657, 'eta': 0.06864711267902912, 'max_depth': 9, 'alpha': 0.364, 'lambda': 10.837555259483402, 'max_bin': 308}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:15,257] Trial 39 finished with value: 0.8005752671821289 and parameters: {'n_estimators': 741, 'eta': 0.06445069340129828, 'max_depth': 8, 'alpha': 0.4888, 'lambda': 38.864816002057026, 'max_bin': 484}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:18,442] Trial 40 finished with value: 0.8048210517482657 and parameters: {'n_estimators': 587, 'eta': 0.07371349460851222, 'max_depth': 10, 'alpha': 0.6945, 'lambda': 13.940214557299866, 'max_bin': 256}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:21,271] Trial 41 finished with value: 0.8030944867360311 and parameters: {'n_estimators': 470, 'eta': 0.08722661838878379, 'max_depth': 11, 'alpha': 0.8288000000000001, 'lambda': 10.707843199771927, 'max_bin': 335}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:23,579] Trial 42 finished with value: 0.8047181330988821 and parameters: {'n_estimators': 831, 'eta': 0.09419293403423151, 'max_depth': 12, 'alpha': 0.9545, 'lambda': 3.272425844107965, 'max_bin': 340}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:26,196] Trial 43 finished with value: 0.8036666264968101 and parameters: {'n_estimators': 643, 'eta': 0.08228131503900954, 'max_depth': 11, 'alpha': 0.7702, 'lambda': 7.206460979370917, 'max_bin': 323}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:29,478] Trial 44 finished with value: 0.8051557072233562 and parameters: {'n_estimators': 335, 'eta': 0.07353406756340121, 'max_depth': 12, 'alpha': 0.8486, 'lambda': 10.08838853176359, 'max_bin': 275}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:32,443] Trial 45 finished with value: 0.8000982115935017 and parameters: {'n_estimators': 561, 'eta': 0.08905620952133296, 'max_depth': 10, 'alpha': 0.6537000000000001, 'lambda': 12.034312679861252, 'max_bin': 397}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:34,664] Trial 46 finished with value: 0.7972158707111853 and parameters: {'n_estimators': 407, 'eta': 0.0972754448629984, 'max_depth': 9, 'alpha': 0.7101000000000001, 'lambda': 8.20782950317781, 'max_bin': 377}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:37,092] Trial 47 finished with value: 0.8025073297432417 and parameters: {'n_estimators': 248, 'eta': 0.09513351252588292, 'max_depth': 11, 'alpha': 0.9045000000000001, 'lambda': 5.857944274284806, 'max_bin': 353}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:39,257] Trial 48 finished with value: 0.7956387499375346 and parameters: {'n_estimators': 168, 'eta': 0.08601360029519156, 'max_depth': 8, 'alpha': 0.9653, 'lambda': 19.209641803030276, 'max_bin': 301}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:41,826] Trial 49 finished with value: 0.8054518313907639 and parameters: {'n_estimators': 294, 'eta': 0.08017162817076566, 'max_depth': 12, 'alpha': 0.7688, 'lambda': 2.6312545043993314, 'max_bin': 364}. Best is trial 8 with value: 0.8099560941939572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8100\n",
      "\tBest params:\n",
      "\t\tn_estimators: 509\n",
      "\t\teta: 0.09203488600961084\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.8726\n",
      "\t\tlambda: 6.146804136542333\n",
      "\t\tmax_bin: 403\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBClassifier\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   44.000000\n",
      "1                    TN  308.000000\n",
      "2                    FP    6.000000\n",
      "3                    FN   24.000000\n",
      "4              Accuracy    0.921466\n",
      "5             Precision    0.880000\n",
      "6           Sensitivity    0.647059\n",
      "7           Specificity    0.980900\n",
      "8              F1 score    0.745763\n",
      "9   F1 score (weighted)    0.916570\n",
      "10     F1 score (macro)    0.849662\n",
      "11    Balanced Accuracy    0.813975\n",
      "12                  MCC    0.712181\n",
      "13                  NPV    0.927700\n",
      "14              ROC_AUC    0.813975\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "   \n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_xgb_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_xgb_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_xgb_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_xgb_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_xgb_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_xgb_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_xgb_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_xgb_0)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 17:59:44,471] Trial 50 finished with value: 0.7958074326077166 and parameters: {'n_estimators': 723, 'eta': 0.07609463666415263, 'max_depth': 11, 'alpha': 0.5423, 'lambda': 7.854897421040233, 'max_bin': 326}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:46,705] Trial 51 finished with value: 0.7947170849681214 and parameters: {'n_estimators': 370, 'eta': 0.08207281037182226, 'max_depth': 12, 'alpha': 0.7819, 'lambda': 2.4759040255008786, 'max_bin': 385}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:48,652] Trial 52 finished with value: 0.7911881639269805 and parameters: {'n_estimators': 279, 'eta': 0.08621718877356799, 'max_depth': 12, 'alpha': 0.8129000000000001, 'lambda': 1.4346126764664318, 'max_bin': 368}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:50,974] Trial 53 finished with value: 0.7969223425791698 and parameters: {'n_estimators': 306, 'eta': 0.08036309454137971, 'max_depth': 12, 'alpha': 0.7513000000000001, 'lambda': 3.262128122875988, 'max_bin': 415}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:53,240] Trial 54 finished with value: 0.7923905172356212 and parameters: {'n_estimators': 778, 'eta': 0.08891555419324536, 'max_depth': 12, 'alpha': 0.6614, 'lambda': 5.7994628705042, 'max_bin': 435}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:55,742] Trial 55 finished with value: 0.7958024217481883 and parameters: {'n_estimators': 430, 'eta': 0.07195433054355677, 'max_depth': 11, 'alpha': 0.8664000000000001, 'lambda': 7.1625733293574685, 'max_bin': 362}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 17:59:58,175] Trial 56 finished with value: 0.7898582194325214 and parameters: {'n_estimators': 497, 'eta': 0.09464312193561387, 'max_depth': 12, 'alpha': 0.8166, 'lambda': 9.94062147104334, 'max_bin': 396}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:00,452] Trial 57 finished with value: 0.7849298441485598 and parameters: {'n_estimators': 251, 'eta': 0.0844962158606941, 'max_depth': 5, 'alpha': 0.7275, 'lambda': 11.684921397126258, 'max_bin': 349}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:02,765] Trial 58 finished with value: 0.7884350357744484 and parameters: {'n_estimators': 622, 'eta': 0.07916777430403206, 'max_depth': 11, 'alpha': 0.5998, 'lambda': 5.2324898110258395, 'max_bin': 338}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:04,854] Trial 59 finished with value: 0.7892466193922256 and parameters: {'n_estimators': 309, 'eta': 0.09889605734585573, 'max_depth': 11, 'alpha': 0.77, 'lambda': 3.95144943659019, 'max_bin': 318}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:07,685] Trial 60 finished with value: 0.7847380241055973 and parameters: {'n_estimators': 671, 'eta': 0.06673370771804202, 'max_depth': 10, 'alpha': 0.8913000000000001, 'lambda': 8.066387646861036, 'max_bin': 373}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:10,530] Trial 61 finished with value: 0.7882416590671684 and parameters: {'n_estimators': 344, 'eta': 0.07377131851413604, 'max_depth': 12, 'alpha': 0.8420000000000001, 'lambda': 10.253777712029907, 'max_bin': 289}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:12,906] Trial 62 finished with value: 0.789623183979739 and parameters: {'n_estimators': 172, 'eta': 0.07496252721119623, 'max_depth': 12, 'alpha': 0.8437, 'lambda': 9.374992073466483, 'max_bin': 281}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:15,401] Trial 63 finished with value: 0.7891054749032718 and parameters: {'n_estimators': 363, 'eta': 0.07816840653287901, 'max_depth': 12, 'alpha': 0.6896, 'lambda': 6.508224038971099, 'max_bin': 271}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:18,075] Trial 64 finished with value: 0.7880762564785767 and parameters: {'n_estimators': 396, 'eta': 0.07258595651598775, 'max_depth': 12, 'alpha': 0.8677, 'lambda': 12.375504015631632, 'max_bin': 401}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:20,529] Trial 65 finished with value: 0.7888531345856025 and parameters: {'n_estimators': 270, 'eta': 0.08376902508928788, 'max_depth': 12, 'alpha': 0.9857, 'lambda': 8.784771906680632, 'max_bin': 358}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:23,000] Trial 66 finished with value: 0.7812676672084864 and parameters: {'n_estimators': 227, 'eta': 0.08973873703328436, 'max_depth': 11, 'alpha': 0.9433, 'lambda': 10.766796066796106, 'max_bin': 413}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:25,356] Trial 67 finished with value: 0.7905138089312305 and parameters: {'n_estimators': 320, 'eta': 0.07785218697770432, 'max_depth': 11, 'alpha': 0.040100000000000004, 'lambda': 4.947331146371682, 'max_bin': 385}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:26,314] Trial 68 finished with value: 0.7637268681086057 and parameters: {'n_estimators': 76, 'eta': 0.07090244408305602, 'max_depth': 6, 'alpha': 0.6297, 'lambda': 2.3106619026022344, 'max_bin': 425}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:28,840] Trial 69 finished with value: 0.7854694629291293 and parameters: {'n_estimators': 443, 'eta': 0.09269955869412216, 'max_depth': 12, 'alpha': 0.7977000000000001, 'lambda': 14.43158891690811, 'max_bin': 345}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:32,296] Trial 70 finished with value: 0.7824307003246943 and parameters: {'n_estimators': 347, 'eta': 0.058500279137485886, 'max_depth': 11, 'alpha': 0.7221000000000001, 'lambda': 16.573111183335758, 'max_bin': 444}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:34,702] Trial 71 finished with value: 0.786597371157782 and parameters: {'n_estimators': 251, 'eta': 0.08760367848026575, 'max_depth': 10, 'alpha': 0.7736000000000001, 'lambda': 12.853640369778564, 'max_bin': 456}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:36,943] Trial 72 finished with value: 0.7936332992715271 and parameters: {'n_estimators': 196, 'eta': 0.09617417774334602, 'max_depth': 12, 'alpha': 0.8474, 'lambda': 11.649465580888634, 'max_bin': 433}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:39,178] Trial 73 finished with value: 0.7899660737621778 and parameters: {'n_estimators': 297, 'eta': 0.09155235491636907, 'max_depth': 9, 'alpha': 0.9165000000000001, 'lambda': 10.063558095055544, 'max_bin': 334}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:41,683] Trial 74 finished with value: 0.7880145688904502 and parameters: {'n_estimators': 555, 'eta': 0.08349521420115906, 'max_depth': 10, 'alpha': 0.8011, 'lambda': 15.258512904996305, 'max_bin': 310}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:44,041] Trial 75 finished with value: 0.7923433738845144 and parameters: {'n_estimators': 282, 'eta': 0.0965536466874039, 'max_depth': 11, 'alpha': 0.7528, 'lambda': 7.672234661852956, 'max_bin': 469}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:46,462] Trial 76 finished with value: 0.7943856635614194 and parameters: {'n_estimators': 616, 'eta': 0.07972102388791354, 'max_depth': 12, 'alpha': 0.8865000000000001, 'lambda': 6.608660820184806, 'max_bin': 405}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:48,685] Trial 77 finished with value: 0.7930028136082463 and parameters: {'n_estimators': 230, 'eta': 0.09036019581634902, 'max_depth': 11, 'alpha': 0.8202, 'lambda': 9.267947929576895, 'max_bin': 328}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:51,069] Trial 78 finished with value: 0.7883984853848268 and parameters: {'n_estimators': 325, 'eta': 0.09991851379703078, 'max_depth': 12, 'alpha': 0.6729, 'lambda': 13.30107317365997, 'max_bin': 391}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:53,388] Trial 79 finished with value: 0.7840362842091566 and parameters: {'n_estimators': 383, 'eta': 0.08754794215317152, 'max_depth': 10, 'alpha': 0.7102, 'lambda': 11.099429127716146, 'max_bin': 353}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:55,782] Trial 80 finished with value: 0.7882402972290872 and parameters: {'n_estimators': 492, 'eta': 0.08119452827258303, 'max_depth': 9, 'alpha': 0.4173, 'lambda': 8.427398292340488, 'max_bin': 381}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:00:58,843] Trial 81 finished with value: 0.7813054519129274 and parameters: {'n_estimators': 556, 'eta': 0.07475545647517456, 'max_depth': 10, 'alpha': 0.7504000000000001, 'lambda': 14.434915746436843, 'max_bin': 254}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:01,728] Trial 82 finished with value: 0.7847867152708018 and parameters: {'n_estimators': 602, 'eta': 0.0763365411265906, 'max_depth': 10, 'alpha': 0.8562000000000001, 'lambda': 13.884127656348578, 'max_bin': 254}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:04,658] Trial 83 finished with value: 0.7854607038468223 and parameters: {'n_estimators': 578, 'eta': 0.08534352627234411, 'max_depth': 11, 'alpha': 0.627, 'lambda': 17.584696550541718, 'max_bin': 272}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:06,967] Trial 84 finished with value: 0.7905197559158842 and parameters: {'n_estimators': 515, 'eta': 0.09345964315814032, 'max_depth': 10, 'alpha': 0.1416, 'lambda': 12.242313421344932, 'max_bin': 299}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:09,998] Trial 85 finished with value: 0.7868908822262133 and parameters: {'n_estimators': 670, 'eta': 0.07004681598623559, 'max_depth': 9, 'alpha': 0.7852, 'lambda': 15.966160075928553, 'max_bin': 266}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:12,837] Trial 86 finished with value: 0.7890787537465396 and parameters: {'n_estimators': 694, 'eta': 0.07354742750828047, 'max_depth': 11, 'alpha': 0.6839000000000001, 'lambda': 12.902981619869733, 'max_bin': 262}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:15,461] Trial 87 finished with value: 0.7868021162068697 and parameters: {'n_estimators': 645, 'eta': 0.06685878746158241, 'max_depth': 12, 'alpha': 0.5709000000000001, 'lambda': 6.022964120959026, 'max_bin': 368}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:17,921] Trial 88 finished with value: 0.7897276602222012 and parameters: {'n_estimators': 722, 'eta': 0.08416645547640733, 'max_depth': 12, 'alpha': 0.8255, 'lambda': 10.095211650560989, 'max_bin': 281}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:20,283] Trial 89 finished with value: 0.7860867210727598 and parameters: {'n_estimators': 263, 'eta': 0.0778675013901501, 'max_depth': 10, 'alpha': 0.7325, 'lambda': 7.505352318800982, 'max_bin': 342}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:22,962] Trial 90 finished with value: 0.7936782020101456 and parameters: {'n_estimators': 416, 'eta': 0.08264466562499964, 'max_depth': 11, 'alpha': 0.8943000000000001, 'lambda': 11.080894443469747, 'max_bin': 443}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:25,000] Trial 91 finished with value: 0.7859444994175757 and parameters: {'n_estimators': 814, 'eta': 0.09377591533262879, 'max_depth': 12, 'alpha': 0.9510000000000001, 'lambda': 3.5985787889735077, 'max_bin': 338}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:27,056] Trial 92 finished with value: 0.7913831916710122 and parameters: {'n_estimators': 541, 'eta': 0.0970481665367475, 'max_depth': 12, 'alpha': 0.9592, 'lambda': 4.684921068055394, 'max_bin': 354}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:29,178] Trial 93 finished with value: 0.7878758071536438 and parameters: {'n_estimators': 827, 'eta': 0.09095188771281408, 'max_depth': 12, 'alpha': 0.9188000000000001, 'lambda': 3.1076297560893513, 'max_bin': 362}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:31,124] Trial 94 finished with value: 0.7950237611741878 and parameters: {'n_estimators': 584, 'eta': 0.08795472552041013, 'max_depth': 12, 'alpha': 0.8728, 'lambda': 1.1983811442502594, 'max_bin': 317}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:33,410] Trial 95 finished with value: 0.7894244907344531 and parameters: {'n_estimators': 349, 'eta': 0.09451107229735589, 'max_depth': 12, 'alpha': 0.9981000000000001, 'lambda': 5.759728505079932, 'max_bin': 329}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:35,655] Trial 96 finished with value: 0.7883665446328433 and parameters: {'n_estimators': 878, 'eta': 0.08060237689012997, 'max_depth': 11, 'alpha': 0.7628, 'lambda': 4.316533864372589, 'max_bin': 430}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:38,019] Trial 97 finished with value: 0.7900635072152864 and parameters: {'n_estimators': 753, 'eta': 0.08559214212124064, 'max_depth': 7, 'alpha': 0.8002, 'lambda': 6.960898927534873, 'max_bin': 342}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:40,270] Trial 98 finished with value: 0.7897851987862935 and parameters: {'n_estimators': 205, 'eta': 0.07497196343276097, 'max_depth': 12, 'alpha': 0.7013, 'lambda': 2.485062791145479, 'max_bin': 415}. Best is trial 8 with value: 0.8099560941939572.\n",
      "[I 2023-12-05 18:01:42,662] Trial 99 finished with value: 0.7889226310215114 and parameters: {'n_estimators': 854, 'eta': 0.08966584151307813, 'max_depth': 11, 'alpha': 0.8286, 'lambda': 8.590411750406325, 'max_bin': 496}. Best is trial 8 with value: 0.8099560941939572.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8100\n",
      "\tBest params:\n",
      "\t\tn_estimators: 509\n",
      "\t\teta: 0.09203488600961084\n",
      "\t\tmax_depth: 12\n",
      "\t\talpha: 0.8726\n",
      "\t\tlambda: 6.146804136542333\n",
      "\t\tmax_bin: 403\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   44.000000   50.000000\n",
      "1                    TN  308.000000  307.000000\n",
      "2                    FP    6.000000    7.000000\n",
      "3                    FN   24.000000   18.000000\n",
      "4              Accuracy    0.921466    0.934555\n",
      "5             Precision    0.880000    0.877193\n",
      "6           Sensitivity    0.647059    0.735294\n",
      "7           Specificity    0.980900    0.977700\n",
      "8              F1 score    0.745763    0.800000\n",
      "9   F1 score (weighted)    0.916570    0.932239\n",
      "10     F1 score (macro)    0.849662    0.880438\n",
      "11    Balanced Accuracy    0.813975    0.856501\n",
      "12                  MCC    0.712181    0.765474\n",
      "13                  NPV    0.927700    0.944600\n",
      "14              ROC_AUC    0.813975    0.856501\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_xgb_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_xgb_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_xgb_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_xgb_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_xgb_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_xgb_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_xgb_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_xgb_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:01:46,021] Trial 100 finished with value: 0.8318984198495217 and parameters: {'n_estimators': 464, 'eta': 0.09814566475841854, 'max_depth': 10, 'alpha': 0.9313, 'lambda': 11.831333803478168, 'max_bin': 347}. Best is trial 100 with value: 0.8318984198495217.\n",
      "[I 2023-12-05 18:01:48,774] Trial 101 finished with value: 0.8276207934343056 and parameters: {'n_estimators': 464, 'eta': 0.09897678481015779, 'max_depth': 10, 'alpha': 0.927, 'lambda': 11.772909470402963, 'max_bin': 375}. Best is trial 100 with value: 0.8318984198495217.\n",
      "[I 2023-12-05 18:01:51,560] Trial 102 finished with value: 0.8341420627094955 and parameters: {'n_estimators': 452, 'eta': 0.09749861254895852, 'max_depth': 10, 'alpha': 0.931, 'lambda': 12.113245645578496, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:01:54,454] Trial 103 finished with value: 0.8270563445327419 and parameters: {'n_estimators': 458, 'eta': 0.09890806697652167, 'max_depth': 10, 'alpha': 0.9387000000000001, 'lambda': 12.238713789450898, 'max_bin': 370}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:01:57,276] Trial 104 finished with value: 0.8262841625326061 and parameters: {'n_estimators': 443, 'eta': 0.0973096323449734, 'max_depth': 10, 'alpha': 0.9342, 'lambda': 11.623341127723211, 'max_bin': 373}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:00,094] Trial 105 finished with value: 0.829544353838795 and parameters: {'n_estimators': 455, 'eta': 0.09796314853862371, 'max_depth': 10, 'alpha': 0.9294, 'lambda': 11.394690260286929, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:02,965] Trial 106 finished with value: 0.8300048208957858 and parameters: {'n_estimators': 454, 'eta': 0.0988084615555228, 'max_depth': 10, 'alpha': 0.9349000000000001, 'lambda': 12.178750601744238, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:05,695] Trial 107 finished with value: 0.8297659093772468 and parameters: {'n_estimators': 457, 'eta': 0.09833666842443549, 'max_depth': 10, 'alpha': 0.9212, 'lambda': 11.950138339872193, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:08,556] Trial 108 finished with value: 0.8266033467059518 and parameters: {'n_estimators': 457, 'eta': 0.09835766454546335, 'max_depth': 10, 'alpha': 0.9753000000000001, 'lambda': 11.855129372790453, 'max_bin': 351}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:11,392] Trial 109 finished with value: 0.8267879325764913 and parameters: {'n_estimators': 455, 'eta': 0.09988427506384197, 'max_depth': 10, 'alpha': 0.9786, 'lambda': 11.748687136501546, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:14,261] Trial 110 finished with value: 0.8338524960537159 and parameters: {'n_estimators': 461, 'eta': 0.09820962799758731, 'max_depth': 10, 'alpha': 0.9735, 'lambda': 11.744609328649348, 'max_bin': 373}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:17,191] Trial 111 finished with value: 0.8248601408823969 and parameters: {'n_estimators': 457, 'eta': 0.09864661402576844, 'max_depth': 10, 'alpha': 0.9753000000000001, 'lambda': 11.818373893016972, 'max_bin': 350}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:19,979] Trial 112 finished with value: 0.8277191659286467 and parameters: {'n_estimators': 458, 'eta': 0.09834342314096217, 'max_depth': 10, 'alpha': 0.9738, 'lambda': 11.76439118513989, 'max_bin': 374}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:22,706] Trial 113 finished with value: 0.8280640567557999 and parameters: {'n_estimators': 481, 'eta': 0.0996795602856579, 'max_depth': 10, 'alpha': 0.9375, 'lambda': 12.388687539728599, 'max_bin': 375}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:25,526] Trial 114 finished with value: 0.8323353542913724 and parameters: {'n_estimators': 485, 'eta': 0.09958383733234985, 'max_depth': 10, 'alpha': 0.9824, 'lambda': 12.458639068949076, 'max_bin': 359}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:28,220] Trial 115 finished with value: 0.8283417598142725 and parameters: {'n_estimators': 475, 'eta': 0.09995562975796167, 'max_depth': 10, 'alpha': 0.9968, 'lambda': 13.04518323880033, 'max_bin': 378}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:31,099] Trial 116 finished with value: 0.8274165270978997 and parameters: {'n_estimators': 482, 'eta': 0.09591208157733493, 'max_depth': 9, 'alpha': 0.9505, 'lambda': 12.734056910836536, 'max_bin': 379}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:34,030] Trial 117 finished with value: 0.8279507150299612 and parameters: {'n_estimators': 485, 'eta': 0.09594552185722002, 'max_depth': 9, 'alpha': 0.9966, 'lambda': 13.280462105210743, 'max_bin': 377}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:37,180] Trial 118 finished with value: 0.8276464740758067 and parameters: {'n_estimators': 480, 'eta': 0.09587180414260343, 'max_depth': 10, 'alpha': 0.9996, 'lambda': 13.746596332499506, 'max_bin': 359}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:40,173] Trial 119 finished with value: 0.8223684253380202 and parameters: {'n_estimators': 484, 'eta': 0.09568125822128082, 'max_depth': 9, 'alpha': 0.9994000000000001, 'lambda': 13.544826914124556, 'max_bin': 358}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:43,425] Trial 120 finished with value: 0.8314911930444285 and parameters: {'n_estimators': 504, 'eta': 0.09366108878044216, 'max_depth': 9, 'alpha': 0.9077000000000001, 'lambda': 14.553073225561041, 'max_bin': 364}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:46,649] Trial 121 finished with value: 0.8245676868736765 and parameters: {'n_estimators': 506, 'eta': 0.09289315908833369, 'max_depth': 9, 'alpha': 0.9111, 'lambda': 14.937680865504905, 'max_bin': 362}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:49,789] Trial 122 finished with value: 0.8263134210840125 and parameters: {'n_estimators': 529, 'eta': 0.09550497199018373, 'max_depth': 9, 'alpha': 0.9659000000000001, 'lambda': 14.319903130464713, 'max_bin': 358}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:52,800] Trial 123 finished with value: 0.8293609076366746 and parameters: {'n_estimators': 430, 'eta': 0.09734905471412267, 'max_depth': 10, 'alpha': 0.9941000000000001, 'lambda': 13.408037380549636, 'max_bin': 365}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:55,619] Trial 124 finished with value: 0.8300049073933747 and parameters: {'n_estimators': 431, 'eta': 0.09982627768047354, 'max_depth': 8, 'alpha': 0.9026000000000001, 'lambda': 13.02865829707341, 'max_bin': 384}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:02:58,607] Trial 125 finished with value: 0.8220808011903127 and parameters: {'n_estimators': 430, 'eta': 0.09211357787624469, 'max_depth': 8, 'alpha': 0.9039, 'lambda': 13.02778613345655, 'max_bin': 383}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:01,551] Trial 126 finished with value: 0.8317581354631638 and parameters: {'n_estimators': 407, 'eta': 0.09718884081871657, 'max_depth': 8, 'alpha': 0.9344, 'lambda': 10.753353391093498, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:04,351] Trial 127 finished with value: 0.8239810455884703 and parameters: {'n_estimators': 404, 'eta': 0.09970907341575883, 'max_depth': 8, 'alpha': 0.9352, 'lambda': 15.855843695048549, 'max_bin': 365}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:07,099] Trial 128 finished with value: 0.8223796477023188 and parameters: {'n_estimators': 433, 'eta': 0.09413125053334223, 'max_depth': 8, 'alpha': 0.8897, 'lambda': 10.80812077706671, 'max_bin': 388}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:09,878] Trial 129 finished with value: 0.8334773817232566 and parameters: {'n_estimators': 393, 'eta': 0.09752915982548313, 'max_depth': 8, 'alpha': 0.9552, 'lambda': 9.617105496844342, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:12,711] Trial 130 finished with value: 0.833533011377037 and parameters: {'n_estimators': 383, 'eta': 0.09677734503172641, 'max_depth': 8, 'alpha': 0.9587, 'lambda': 10.58370922874501, 'max_bin': 393}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:15,525] Trial 131 finished with value: 0.8316854717183497 and parameters: {'n_estimators': 383, 'eta': 0.09765763467669886, 'max_depth': 8, 'alpha': 0.9596, 'lambda': 9.695465250656845, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:18,320] Trial 132 finished with value: 0.8307808678589508 and parameters: {'n_estimators': 382, 'eta': 0.09715815673019076, 'max_depth': 8, 'alpha': 0.9158000000000001, 'lambda': 9.987868127545106, 'max_bin': 394}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:21,042] Trial 133 finished with value: 0.8314172408560095 and parameters: {'n_estimators': 384, 'eta': 0.09368253949926487, 'max_depth': 8, 'alpha': 0.9124, 'lambda': 9.368126480593247, 'max_bin': 399}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:23,816] Trial 134 finished with value: 0.8262223208601179 and parameters: {'n_estimators': 382, 'eta': 0.09224593400090134, 'max_depth': 8, 'alpha': 0.9542, 'lambda': 9.571955671366652, 'max_bin': 394}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:26,645] Trial 135 finished with value: 0.8249460621405419 and parameters: {'n_estimators': 394, 'eta': 0.09416516115392506, 'max_depth': 7, 'alpha': 0.9065000000000001, 'lambda': 10.452295608200794, 'max_bin': 400}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:29,323] Trial 136 finished with value: 0.8247033705435045 and parameters: {'n_estimators': 368, 'eta': 0.0968742812837055, 'max_depth': 8, 'alpha': 0.8799, 'lambda': 9.177119337340109, 'max_bin': 390}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:32,129] Trial 137 finished with value: 0.8276169033761593 and parameters: {'n_estimators': 418, 'eta': 0.09088920480328576, 'max_depth': 8, 'alpha': 0.9604, 'lambda': 9.67416712871806, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:34,944] Trial 138 finished with value: 0.826075486082769 and parameters: {'n_estimators': 412, 'eta': 0.09370469184988106, 'max_depth': 8, 'alpha': 0.9147000000000001, 'lambda': 10.5282928855896, 'max_bin': 384}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:37,792] Trial 139 finished with value: 0.826926738329122 and parameters: {'n_estimators': 381, 'eta': 0.09698701076880602, 'max_depth': 7, 'alpha': 0.8661000000000001, 'lambda': 10.766749131844467, 'max_bin': 410}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:40,625] Trial 140 finished with value: 0.826608080382055 and parameters: {'n_estimators': 397, 'eta': 0.09507255928200313, 'max_depth': 8, 'alpha': 0.9482, 'lambda': 8.802805908631887, 'max_bin': 398}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:43,501] Trial 141 finished with value: 0.8264815582250866 and parameters: {'n_estimators': 361, 'eta': 0.09740587273334103, 'max_depth': 8, 'alpha': 0.9249, 'lambda': 11.045704090635745, 'max_bin': 344}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:46,467] Trial 142 finished with value: 0.8330932716089448 and parameters: {'n_estimators': 446, 'eta': 0.0926424497595037, 'max_depth': 8, 'alpha': 0.8994000000000001, 'lambda': 10.210345117735997, 'max_bin': 355}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:49,382] Trial 143 finished with value: 0.8301961151432968 and parameters: {'n_estimators': 503, 'eta': 0.09227902191880866, 'max_depth': 8, 'alpha': 0.8992, 'lambda': 9.744450959536312, 'max_bin': 355}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:52,381] Trial 144 finished with value: 0.8313000287151867 and parameters: {'n_estimators': 511, 'eta': 0.09193583894109564, 'max_depth': 8, 'alpha': 0.893, 'lambda': 9.89367491169047, 'max_bin': 393}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:55,271] Trial 145 finished with value: 0.8333070431332503 and parameters: {'n_estimators': 519, 'eta': 0.09148363250974464, 'max_depth': 8, 'alpha': 0.8838, 'lambda': 9.976679306303598, 'max_bin': 385}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:03:58,237] Trial 146 finished with value: 0.8309892982878558 and parameters: {'n_estimators': 511, 'eta': 0.08982621902704141, 'max_depth': 8, 'alpha': 0.8803000000000001, 'lambda': 10.155507281445297, 'max_bin': 355}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:00,970] Trial 147 finished with value: 0.8255459094212139 and parameters: {'n_estimators': 525, 'eta': 0.0897173836711804, 'max_depth': 8, 'alpha': 0.8779, 'lambda': 8.353103562513786, 'max_bin': 390}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:04,011] Trial 148 finished with value: 0.8318864688684968 and parameters: {'n_estimators': 549, 'eta': 0.09072807076018422, 'max_depth': 8, 'alpha': 0.8563000000000001, 'lambda': 10.027355477194673, 'max_bin': 403}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:06,830] Trial 149 finished with value: 0.828145233354457 and parameters: {'n_estimators': 551, 'eta': 0.08862088667572912, 'max_depth': 8, 'alpha': 0.8591000000000001, 'lambda': 9.156808308553217, 'max_bin': 405}. Best is trial 102 with value: 0.8341420627094955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8341\n",
      "\tBest params:\n",
      "\t\tn_estimators: 452\n",
      "\t\teta: 0.09749861254895852\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.931\n",
      "\t\tlambda: 12.113245645578496\n",
      "\t\tmax_bin: 349\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   44.000000   50.000000   37.000000\n",
      "1                    TN  308.000000  307.000000  305.000000\n",
      "2                    FP    6.000000    7.000000   10.000000\n",
      "3                    FN   24.000000   18.000000   30.000000\n",
      "4              Accuracy    0.921466    0.934555    0.895288\n",
      "5             Precision    0.880000    0.877193    0.787234\n",
      "6           Sensitivity    0.647059    0.735294    0.552239\n",
      "7           Specificity    0.980900    0.977700    0.968300\n",
      "8              F1 score    0.745763    0.800000    0.649123\n",
      "9   F1 score (weighted)    0.916570    0.932239    0.887714\n",
      "10     F1 score (macro)    0.849662    0.880438    0.793792\n",
      "11    Balanced Accuracy    0.813975    0.856501    0.760246\n",
      "12                  MCC    0.712181    0.765474    0.602610\n",
      "13                  NPV    0.927700    0.944600    0.910400\n",
      "14              ROC_AUC    0.813975    0.856501    0.760246\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_xgb_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_xgb_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_xgb_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_xgb_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_xgb_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_xgb_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_xgb_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_xgb_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:04:09,676] Trial 150 finished with value: 0.8184798743241618 and parameters: {'n_estimators': 517, 'eta': 0.0906179244906319, 'max_depth': 7, 'alpha': 0.9682000000000001, 'lambda': 7.818410352395962, 'max_bin': 419}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:12,353] Trial 151 finished with value: 0.8214408359667313 and parameters: {'n_estimators': 412, 'eta': 0.09411168684812997, 'max_depth': 8, 'alpha': 0.8933000000000001, 'lambda': 9.826818964956592, 'max_bin': 396}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:15,205] Trial 152 finished with value: 0.8206646857913651 and parameters: {'n_estimators': 540, 'eta': 0.09204183760726144, 'max_depth': 8, 'alpha': 0.9498000000000001, 'lambda': 10.383787306678556, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:17,704] Trial 153 finished with value: 0.8197825699957093 and parameters: {'n_estimators': 506, 'eta': 0.09490124638155197, 'max_depth': 8, 'alpha': 0.8759, 'lambda': 8.849516129070777, 'max_bin': 393}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:20,226] Trial 154 finished with value: 0.8178785439700139 and parameters: {'n_estimators': 569, 'eta': 0.09274884003823884, 'max_depth': 8, 'alpha': 0.3448, 'lambda': 10.094413918510394, 'max_bin': 364}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:23,075] Trial 155 finished with value: 0.8141070126983136 and parameters: {'n_estimators': 496, 'eta': 0.08865938019057328, 'max_depth': 8, 'alpha': 0.9187000000000001, 'lambda': 10.693421131566234, 'max_bin': 407}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:25,911] Trial 156 finished with value: 0.8179650541071879 and parameters: {'n_estimators': 353, 'eta': 0.08994200955064498, 'max_depth': 8, 'alpha': 0.9758, 'lambda': 9.493907084151903, 'max_bin': 387}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:28,814] Trial 157 finished with value: 0.8143797169256064 and parameters: {'n_estimators': 531, 'eta': 0.08689505264580029, 'max_depth': 8, 'alpha': 0.8926000000000001, 'lambda': 11.138412048123687, 'max_bin': 401}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:31,213] Trial 158 finished with value: 0.818923422818715 and parameters: {'n_estimators': 384, 'eta': 0.09572915429819354, 'max_depth': 8, 'alpha': 0.8543000000000001, 'lambda': 8.186876292730297, 'max_bin': 371}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:33,954] Trial 159 finished with value: 0.8178428949260826 and parameters: {'n_estimators': 441, 'eta': 0.09144240466492261, 'max_depth': 8, 'alpha': 0.9490000000000001, 'lambda': 7.422197619493513, 'max_bin': 356}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:36,911] Trial 160 finished with value: 0.8294536361489758 and parameters: {'n_estimators': 510, 'eta': 0.09362455084340711, 'max_depth': 7, 'alpha': 0.9186000000000001, 'lambda': 10.134484550507317, 'max_bin': 378}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:39,992] Trial 161 finished with value: 0.8202090144929655 and parameters: {'n_estimators': 505, 'eta': 0.09162007015119435, 'max_depth': 8, 'alpha': 0.905, 'lambda': 9.02640111329574, 'max_bin': 356}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:42,860] Trial 162 finished with value: 0.8233400229333909 and parameters: {'n_estimators': 474, 'eta': 0.09557713366919733, 'max_depth': 8, 'alpha': 0.8828, 'lambda': 11.119747199555466, 'max_bin': 360}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:45,657] Trial 163 finished with value: 0.8204785722744792 and parameters: {'n_estimators': 498, 'eta': 0.0931632568530775, 'max_depth': 8, 'alpha': 0.9342, 'lambda': 9.771742455075369, 'max_bin': 354}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:48,338] Trial 164 finished with value: 0.8245745963684218 and parameters: {'n_estimators': 403, 'eta': 0.0969462425445282, 'max_depth': 8, 'alpha': 0.9602, 'lambda': 9.728286774216354, 'max_bin': 335}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:51,197] Trial 165 finished with value: 0.8186456010005887 and parameters: {'n_estimators': 544, 'eta': 0.08694809468083045, 'max_depth': 8, 'alpha': 0.8999, 'lambda': 8.858829742796742, 'max_bin': 366}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:53,888] Trial 166 finished with value: 0.8147652045109826 and parameters: {'n_estimators': 420, 'eta': 0.09691445104633556, 'max_depth': 8, 'alpha': 0.8509, 'lambda': 11.217996032180611, 'max_bin': 383}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:56,560] Trial 167 finished with value: 0.8246109158319979 and parameters: {'n_estimators': 374, 'eta': 0.09309826077267343, 'max_depth': 7, 'alpha': 0.9740000000000001, 'lambda': 8.030984832203313, 'max_bin': 372}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:04:59,399] Trial 168 finished with value: 0.8250999458956908 and parameters: {'n_estimators': 335, 'eta': 0.08971362062968892, 'max_depth': 8, 'alpha': 0.9373, 'lambda': 10.274101888961049, 'max_bin': 345}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:02,232] Trial 169 finished with value: 0.8237702875959666 and parameters: {'n_estimators': 567, 'eta': 0.09512291078922251, 'max_depth': 8, 'alpha': 0.9141, 'lambda': 12.585550642042058, 'max_bin': 362}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:04,858] Trial 170 finished with value: 0.8182067141956976 and parameters: {'n_estimators': 517, 'eta': 0.09759327030868034, 'max_depth': 9, 'alpha': 0.8771, 'lambda': 11.12510267777866, 'max_bin': 392}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:07,549] Trial 171 finished with value: 0.8206544950869915 and parameters: {'n_estimators': 434, 'eta': 0.09955737835745682, 'max_depth': 8, 'alpha': 0.8975000000000001, 'lambda': 12.719368029952436, 'max_bin': 382}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:10,422] Trial 172 finished with value: 0.8174212470434681 and parameters: {'n_estimators': 470, 'eta': 0.0940746495122696, 'max_depth': 8, 'alpha': 0.9011, 'lambda': 9.524062272577707, 'max_bin': 388}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:13,167] Trial 173 finished with value: 0.8247655546214941 and parameters: {'n_estimators': 495, 'eta': 0.09976923777737992, 'max_depth': 8, 'alpha': 0.5193, 'lambda': 10.525647085109616, 'max_bin': 398}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:16,191] Trial 174 finished with value: 0.8192856850325374 and parameters: {'n_estimators': 396, 'eta': 0.09680174991630495, 'max_depth': 8, 'alpha': 0.9459000000000001, 'lambda': 12.438232802209797, 'max_bin': 355}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:19,158] Trial 175 finished with value: 0.8194122278506122 and parameters: {'n_estimators': 443, 'eta': 0.09168466974526931, 'max_depth': 8, 'alpha': 0.8402000000000001, 'lambda': 11.63270544457059, 'max_bin': 380}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:22,020] Trial 176 finished with value: 0.8187500815466479 and parameters: {'n_estimators': 426, 'eta': 0.09492212336940786, 'max_depth': 8, 'alpha': 0.9252, 'lambda': 14.045884798750544, 'max_bin': 371}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:24,720] Trial 177 finished with value: 0.8140826434012706 and parameters: {'n_estimators': 474, 'eta': 0.09681303559377767, 'max_depth': 8, 'alpha': 0.9778, 'lambda': 8.524765912201458, 'max_bin': 403}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:27,129] Trial 178 finished with value: 0.8227444149235694 and parameters: {'n_estimators': 413, 'eta': 0.09994781327626684, 'max_depth': 8, 'alpha': 0.8617, 'lambda': 10.065541076107007, 'max_bin': 366}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:29,948] Trial 179 finished with value: 0.821217722043458 and parameters: {'n_estimators': 367, 'eta': 0.08837538291790696, 'max_depth': 7, 'alpha': 0.25680000000000003, 'lambda': 11.409556869856235, 'max_bin': 411}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:32,749] Trial 180 finished with value: 0.8197169221088625 and parameters: {'n_estimators': 521, 'eta': 0.09112379644113376, 'max_depth': 9, 'alpha': 0.9596, 'lambda': 9.223234366436817, 'max_bin': 395}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:35,680] Trial 181 finished with value: 0.8235147424513667 and parameters: {'n_estimators': 444, 'eta': 0.09807675857425217, 'max_depth': 8, 'alpha': 0.9315, 'lambda': 12.151349177678501, 'max_bin': 350}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:38,401] Trial 182 finished with value: 0.8217829708101657 and parameters: {'n_estimators': 467, 'eta': 0.0979096423443158, 'max_depth': 8, 'alpha': 0.9117000000000001, 'lambda': 10.964469914638777, 'max_bin': 344}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:41,284] Trial 183 finished with value: 0.8180766927517183 and parameters: {'n_estimators': 447, 'eta': 0.09520407674587815, 'max_depth': 8, 'alpha': 0.9455, 'lambda': 12.047510824060161, 'max_bin': 339}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:44,148] Trial 184 finished with value: 0.8216616609415646 and parameters: {'n_estimators': 397, 'eta': 0.09336845570323091, 'max_depth': 8, 'alpha': 0.8908, 'lambda': 10.084927796096421, 'max_bin': 386}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:47,067] Trial 185 finished with value: 0.8207035516836619 and parameters: {'n_estimators': 488, 'eta': 0.0976572866459511, 'max_depth': 8, 'alpha': 0.9284, 'lambda': 13.516806834135764, 'max_bin': 360}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:50,081] Trial 186 finished with value: 0.8248268446954592 and parameters: {'n_estimators': 420, 'eta': 0.09561679230240633, 'max_depth': 8, 'alpha': 0.9780000000000001, 'lambda': 12.595133690677876, 'max_bin': 353}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:52,960] Trial 187 finished with value: 0.8189575604475452 and parameters: {'n_estimators': 540, 'eta': 0.09353323719827161, 'max_depth': 8, 'alpha': 0.9584, 'lambda': 14.673872790250645, 'max_bin': 373}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:55,540] Trial 188 finished with value: 0.823881958797903 and parameters: {'n_estimators': 385, 'eta': 0.09961167647945575, 'max_depth': 9, 'alpha': 0.8768, 'lambda': 10.742178520308851, 'max_bin': 361}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:05:59,889] Trial 189 finished with value: 0.8190860056960064 and parameters: {'n_estimators': 492, 'eta': 0.04436561656342229, 'max_depth': 8, 'alpha': 0.9071, 'lambda': 9.349267164862336, 'max_bin': 377}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:02,600] Trial 190 finished with value: 0.8220097139524303 and parameters: {'n_estimators': 467, 'eta': 0.09100422555491239, 'max_depth': 8, 'alpha': 0.9427000000000001, 'lambda': 7.64987803134121, 'max_bin': 352}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:05,493] Trial 191 finished with value: 0.8234984722481629 and parameters: {'n_estimators': 454, 'eta': 0.09688577487567926, 'max_depth': 10, 'alpha': 0.9143, 'lambda': 11.626893326130766, 'max_bin': 347}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:08,093] Trial 192 finished with value: 0.825065833689058 and parameters: {'n_estimators': 437, 'eta': 0.09781876265404363, 'max_depth': 10, 'alpha': 0.9236000000000001, 'lambda': 12.25377907146331, 'max_bin': 348}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:10,947] Trial 193 finished with value: 0.8165224257696269 and parameters: {'n_estimators': 506, 'eta': 0.098361734471351, 'max_depth': 10, 'alpha': 0.892, 'lambda': 10.55243597129261, 'max_bin': 357}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:13,876] Trial 194 finished with value: 0.8206220392758468 and parameters: {'n_estimators': 469, 'eta': 0.09452350942741622, 'max_depth': 10, 'alpha': 0.9629000000000001, 'lambda': 13.144107065780581, 'max_bin': 367}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:16,922] Trial 195 finished with value: 0.8142605226515551 and parameters: {'n_estimators': 529, 'eta': 0.09582375931497986, 'max_depth': 8, 'alpha': 0.8647, 'lambda': 22.747380123869917, 'max_bin': 331}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:19,760] Trial 196 finished with value: 0.825234985454367 and parameters: {'n_estimators': 356, 'eta': 0.09279930815856341, 'max_depth': 10, 'alpha': 0.9357000000000001, 'lambda': 11.375849315128779, 'max_bin': 391}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:22,351] Trial 197 finished with value: 0.8256935308982545 and parameters: {'n_estimators': 423, 'eta': 0.09835106006457002, 'max_depth': 8, 'alpha': 0.9148000000000001, 'lambda': 8.519760838511976, 'max_bin': 341}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:25,286] Trial 198 finished with value: 0.8200744246563781 and parameters: {'n_estimators': 401, 'eta': 0.08971027459730362, 'max_depth': 10, 'alpha': 0.8925000000000001, 'lambda': 9.925525901667077, 'max_bin': 362}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:28,367] Trial 199 finished with value: 0.8204473471319043 and parameters: {'n_estimators': 454, 'eta': 0.08549698562030787, 'max_depth': 8, 'alpha': 0.9816, 'lambda': 14.057017214312278, 'max_bin': 401}. Best is trial 102 with value: 0.8341420627094955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8341\n",
      "\tBest params:\n",
      "\t\tn_estimators: 452\n",
      "\t\teta: 0.09749861254895852\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.931\n",
      "\t\tlambda: 12.113245645578496\n",
      "\t\tmax_bin: 349\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   44.000000   50.000000   37.000000   36.000000\n",
      "1                    TN  308.000000  307.000000  305.000000  305.000000\n",
      "2                    FP    6.000000    7.000000   10.000000   11.000000\n",
      "3                    FN   24.000000   18.000000   30.000000   30.000000\n",
      "4              Accuracy    0.921466    0.934555    0.895288    0.892670\n",
      "5             Precision    0.880000    0.877193    0.787234    0.765957\n",
      "6           Sensitivity    0.647059    0.735294    0.552239    0.545455\n",
      "7           Specificity    0.980900    0.977700    0.968300    0.965200\n",
      "8              F1 score    0.745763    0.800000    0.649123    0.637168\n",
      "9   F1 score (weighted)    0.916570    0.932239    0.887714    0.885213\n",
      "10     F1 score (macro)    0.849662    0.880438    0.793792    0.787094\n",
      "11    Balanced Accuracy    0.813975    0.856501    0.760246    0.755322\n",
      "12                  MCC    0.712181    0.765474    0.602610    0.587710\n",
      "13                  NPV    0.927700    0.944600    0.910400    0.910400\n",
      "14              ROC_AUC    0.813975    0.856501    0.760246    0.755322\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_xgb_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_xgb_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_xgb_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_xgb_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_xgb_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_xgb_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_xgb_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_xgb_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:06:31,670] Trial 200 finished with value: 0.8217919660656768 and parameters: {'n_estimators': 486, 'eta': 0.09998786847884027, 'max_depth': 8, 'alpha': 0.9500000000000001, 'lambda': 12.486606342675124, 'max_bin': 381}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:34,198] Trial 201 finished with value: 0.8065791694344163 and parameters: {'n_estimators': 456, 'eta': 0.09634900031563112, 'max_depth': 10, 'alpha': 0.9343, 'lambda': 11.498753711168188, 'max_bin': 345}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:36,444] Trial 202 finished with value: 0.8135924178830735 and parameters: {'n_estimators': 443, 'eta': 0.0979996709471553, 'max_depth': 10, 'alpha': 0.1091, 'lambda': 10.715445276321638, 'max_bin': 351}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:42,481] Trial 203 finished with value: 0.8107916868874001 and parameters: {'n_estimators': 508, 'eta': 0.029131904619370215, 'max_depth': 10, 'alpha': 0.9225000000000001, 'lambda': 9.541009679671689, 'max_bin': 357}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:45,165] Trial 204 finished with value: 0.8154892894101119 and parameters: {'n_estimators': 474, 'eta': 0.0942977835566435, 'max_depth': 10, 'alpha': 0.9067000000000001, 'lambda': 11.871351984326848, 'max_bin': 348}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:47,724] Trial 205 finished with value: 0.8074289806735561 and parameters: {'n_estimators': 374, 'eta': 0.09996780407835727, 'max_depth': 10, 'alpha': 0.8755000000000001, 'lambda': 10.374555681266186, 'max_bin': 368}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:50,640] Trial 206 finished with value: 0.8177933542714036 and parameters: {'n_estimators': 408, 'eta': 0.09195774126921993, 'max_depth': 8, 'alpha': 0.9333, 'lambda': 11.141692516335258, 'max_bin': 354}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:53,237] Trial 207 finished with value: 0.8130284114463302 and parameters: {'n_estimators': 431, 'eta': 0.09587846818498333, 'max_depth': 9, 'alpha': 0.9587, 'lambda': 9.011006524985913, 'max_bin': 395}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:56,244] Trial 208 finished with value: 0.8208429311583874 and parameters: {'n_estimators': 525, 'eta': 0.09772148772863755, 'max_depth': 8, 'alpha': 0.9848, 'lambda': 13.437672963207614, 'max_bin': 339}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:06:59,130] Trial 209 finished with value: 0.8251471622068346 and parameters: {'n_estimators': 460, 'eta': 0.09362293605177187, 'max_depth': 8, 'alpha': 0.8971, 'lambda': 12.38062224551173, 'max_bin': 362}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:01,863] Trial 210 finished with value: 0.8058766432343191 and parameters: {'n_estimators': 386, 'eta': 0.0959713417735734, 'max_depth': 10, 'alpha': 0.9248000000000001, 'lambda': 15.164878503063902, 'max_bin': 385}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:04,500] Trial 211 finished with value: 0.8133195157047265 and parameters: {'n_estimators': 508, 'eta': 0.09285263762749954, 'max_depth': 6, 'alpha': 0.915, 'lambda': 9.996130891648063, 'max_bin': 376}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:07,414] Trial 212 finished with value: 0.8213716967449105 and parameters: {'n_estimators': 494, 'eta': 0.09463962591711969, 'max_depth': 7, 'alpha': 0.9456, 'lambda': 10.330673076231612, 'max_bin': 376}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:10,188] Trial 213 finished with value: 0.819367596577264 and parameters: {'n_estimators': 512, 'eta': 0.09821779485277227, 'max_depth': 7, 'alpha': 0.8864000000000001, 'lambda': 11.290714484076144, 'max_bin': 370}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:12,947] Trial 214 finished with value: 0.8125643497366323 and parameters: {'n_estimators': 559, 'eta': 0.08878165733962246, 'max_depth': 7, 'alpha': 0.9219, 'lambda': 9.581307820152732, 'max_bin': 350}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:15,831] Trial 215 finished with value: 0.8234870737461761 and parameters: {'n_estimators': 487, 'eta': 0.0913433721794813, 'max_depth': 8, 'alpha': 0.9654, 'lambda': 8.280475472231839, 'max_bin': 383}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:18,555] Trial 216 finished with value: 0.8075708317868793 and parameters: {'n_estimators': 545, 'eta': 0.09611050980680205, 'max_depth': 9, 'alpha': 0.8656, 'lambda': 10.64955581717518, 'max_bin': 358}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:22,260] Trial 217 finished with value: 0.8092431620816735 and parameters: {'n_estimators': 427, 'eta': 0.05966410338178416, 'max_depth': 5, 'alpha': 0.9034000000000001, 'lambda': 11.798785130231375, 'max_bin': 365}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:24,972] Trial 218 finished with value: 0.8106698769719684 and parameters: {'n_estimators': 448, 'eta': 0.09412855323332023, 'max_depth': 10, 'alpha': 0.8385, 'lambda': 8.931854727806183, 'max_bin': 389}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:28,594] Trial 219 finished with value: 0.8199263312137672 and parameters: {'n_estimators': 478, 'eta': 0.06270455417543581, 'max_depth': 8, 'alpha': 0.9453, 'lambda': 9.775717323300404, 'max_bin': 406}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:31,457] Trial 220 finished with value: 0.8102509314755026 and parameters: {'n_estimators': 528, 'eta': 0.09797494044398433, 'max_depth': 8, 'alpha': 0.9297000000000001, 'lambda': 12.818983734157499, 'max_bin': 378}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:34,286] Trial 221 finished with value: 0.8182134761069481 and parameters: {'n_estimators': 426, 'eta': 0.0999040972572526, 'max_depth': 10, 'alpha': 0.9849, 'lambda': 19.4367306905104, 'max_bin': 365}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:37,001] Trial 222 finished with value: 0.8096405080422626 and parameters: {'n_estimators': 410, 'eta': 0.0969764892362254, 'max_depth': 10, 'alpha': 0.9730000000000001, 'lambda': 13.961317628230185, 'max_bin': 353}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:39,685] Trial 223 finished with value: 0.8122008151001081 and parameters: {'n_estimators': 467, 'eta': 0.09309983547950308, 'max_depth': 10, 'alpha': 0.9607, 'lambda': 11.269122028608889, 'max_bin': 370}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:42,778] Trial 224 finished with value: 0.8147252890767025 and parameters: {'n_estimators': 394, 'eta': 0.09743626517975337, 'max_depth': 10, 'alpha': 0.9884000000000001, 'lambda': 17.155199106863634, 'max_bin': 398}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:45,761] Trial 225 finished with value: 0.8107087264656544 and parameters: {'n_estimators': 441, 'eta': 0.09529129927716358, 'max_depth': 6, 'alpha': 0.9953000000000001, 'lambda': 13.155558666040562, 'max_bin': 362}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:48,585] Trial 226 finished with value: 0.8083103822021439 and parameters: {'n_estimators': 496, 'eta': 0.09054957435836083, 'max_depth': 10, 'alpha': 0.9039, 'lambda': 12.060221246893601, 'max_bin': 345}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:51,846] Trial 227 finished with value: 0.8180345177779434 and parameters: {'n_estimators': 458, 'eta': 0.09999657319780844, 'max_depth': 10, 'alpha': 0.9402, 'lambda': 27.360175689139513, 'max_bin': 357}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:54,787] Trial 228 finished with value: 0.8091112347447293 and parameters: {'n_estimators': 362, 'eta': 0.08774103652811675, 'max_depth': 8, 'alpha': 0.8852000000000001, 'lambda': 10.50499905496436, 'max_bin': 373}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:07:57,692] Trial 229 finished with value: 0.8151550702428072 and parameters: {'n_estimators': 414, 'eta': 0.0978500070201332, 'max_depth': 8, 'alpha': 0.9600000000000001, 'lambda': 13.326141263802157, 'max_bin': 394}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:00,901] Trial 230 finished with value: 0.8175476532607495 and parameters: {'n_estimators': 436, 'eta': 0.09288939081860745, 'max_depth': 8, 'alpha': 0.9207000000000001, 'lambda': 14.54983516750542, 'max_bin': 351}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:03,619] Trial 231 finished with value: 0.8076465237363903 and parameters: {'n_estimators': 478, 'eta': 0.09839254488881409, 'max_depth': 10, 'alpha': 0.9807, 'lambda': 12.846085336588718, 'max_bin': 377}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:06,447] Trial 232 finished with value: 0.8113658717058186 and parameters: {'n_estimators': 518, 'eta': 0.09544717944854757, 'max_depth': 10, 'alpha': 0.9989, 'lambda': 12.106586060377783, 'max_bin': 380}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:09,102] Trial 233 finished with value: 0.8131419247547896 and parameters: {'n_estimators': 476, 'eta': 0.09645360764695511, 'max_depth': 10, 'alpha': 0.9649000000000001, 'lambda': 10.931838986296349, 'max_bin': 387}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:11,507] Trial 234 finished with value: 0.8114649528629956 and parameters: {'n_estimators': 501, 'eta': 0.09981945007067554, 'max_depth': 10, 'alpha': 0.9452, 'lambda': 9.201640410662508, 'max_bin': 364}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:13,963] Trial 235 finished with value: 0.8031708801142722 and parameters: {'n_estimators': 458, 'eta': 0.09801948652965208, 'max_depth': 10, 'alpha': 0.9122, 'lambda': 10.081500459700244, 'max_bin': 372}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:16,986] Trial 236 finished with value: 0.8147545294699577 and parameters: {'n_estimators': 391, 'eta': 0.09489458318629493, 'max_depth': 8, 'alpha': 0.9975, 'lambda': 13.65047818071615, 'max_bin': 380}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:19,386] Trial 237 finished with value: 0.8039350941806337 and parameters: {'n_estimators': 487, 'eta': 0.09668636853087907, 'max_depth': 10, 'alpha': 0.9363, 'lambda': 11.517023853504067, 'max_bin': 358}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:22,403] Trial 238 finished with value: 0.8233626826797964 and parameters: {'n_estimators': 439, 'eta': 0.09230030982973764, 'max_depth': 8, 'alpha': 0.8888, 'lambda': 10.08585550802001, 'max_bin': 366}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:25,442] Trial 239 finished with value: 0.8200073442619026 and parameters: {'n_estimators': 468, 'eta': 0.09982209213768367, 'max_depth': 8, 'alpha': 0.9695, 'lambda': 12.3213555224399, 'max_bin': 392}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:27,900] Trial 240 finished with value: 0.8055671546286801 and parameters: {'n_estimators': 512, 'eta': 0.0938889992763695, 'max_depth': 10, 'alpha': 0.8628, 'lambda': 11.071478432651153, 'max_bin': 347}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:30,681] Trial 241 finished with value: 0.8157619426954568 and parameters: {'n_estimators': 547, 'eta': 0.08999992353816157, 'max_depth': 8, 'alpha': 0.8616, 'lambda': 8.93275653660995, 'max_bin': 399}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:33,518] Trial 242 finished with value: 0.8126243449759096 and parameters: {'n_estimators': 559, 'eta': 0.08899535269472209, 'max_depth': 8, 'alpha': 0.9047000000000001, 'lambda': 9.496745991424053, 'max_bin': 355}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:36,524] Trial 243 finished with value: 0.815327257272544 and parameters: {'n_estimators': 576, 'eta': 0.08757212843803001, 'max_depth': 8, 'alpha': 0.8831, 'lambda': 10.465708799431845, 'max_bin': 403}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:39,165] Trial 244 finished with value: 0.8153184818795994 and parameters: {'n_estimators': 528, 'eta': 0.09664710563835505, 'max_depth': 8, 'alpha': 0.8464, 'lambda': 7.619255927770245, 'max_bin': 411}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:42,204] Trial 245 finished with value: 0.8175362833324679 and parameters: {'n_estimators': 541, 'eta': 0.09133023066560389, 'max_depth': 8, 'alpha': 0.9253, 'lambda': 15.764988127133105, 'max_bin': 407}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:45,224] Trial 246 finished with value: 0.814280535667554 and parameters: {'n_estimators': 375, 'eta': 0.08579361599059887, 'max_depth': 8, 'alpha': 0.9580000000000001, 'lambda': 8.660393563114106, 'max_bin': 389}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:47,665] Trial 247 finished with value: 0.8176743121134822 and parameters: {'n_estimators': 420, 'eta': 0.09831894139448229, 'max_depth': 10, 'alpha': 0.9087000000000001, 'lambda': 9.414324148789246, 'max_bin': 419}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:50,570] Trial 248 finished with value: 0.8226844097004664 and parameters: {'n_estimators': 497, 'eta': 0.09481891286964855, 'max_depth': 8, 'alpha': 0.9435, 'lambda': 12.744022130976864, 'max_bin': 342}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:08:53,724] Trial 249 finished with value: 0.8101458851547587 and parameters: {'n_estimators': 453, 'eta': 0.08222322005276557, 'max_depth': 8, 'alpha': 0.871, 'lambda': 11.506256902320713, 'max_bin': 384}. Best is trial 102 with value: 0.8341420627094955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8341\n",
      "\tBest params:\n",
      "\t\tn_estimators: 452\n",
      "\t\teta: 0.09749861254895852\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.931\n",
      "\t\tlambda: 12.113245645578496\n",
      "\t\tmax_bin: 349\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   50.000000   37.000000   36.000000   \n",
      "1                    TN  308.000000  307.000000  305.000000  305.000000   \n",
      "2                    FP    6.000000    7.000000   10.000000   11.000000   \n",
      "3                    FN   24.000000   18.000000   30.000000   30.000000   \n",
      "4              Accuracy    0.921466    0.934555    0.895288    0.892670   \n",
      "5             Precision    0.880000    0.877193    0.787234    0.765957   \n",
      "6           Sensitivity    0.647059    0.735294    0.552239    0.545455   \n",
      "7           Specificity    0.980900    0.977700    0.968300    0.965200   \n",
      "8              F1 score    0.745763    0.800000    0.649123    0.637168   \n",
      "9   F1 score (weighted)    0.916570    0.932239    0.887714    0.885213   \n",
      "10     F1 score (macro)    0.849662    0.880438    0.793792    0.787094   \n",
      "11    Balanced Accuracy    0.813975    0.856501    0.760246    0.755322   \n",
      "12                  MCC    0.712181    0.765474    0.602610    0.587710   \n",
      "13                  NPV    0.927700    0.944600    0.910400    0.910400   \n",
      "14              ROC_AUC    0.813975    0.856501    0.760246    0.755322   \n",
      "\n",
      "          Set4  \n",
      "0    39.000000  \n",
      "1   312.000000  \n",
      "2     3.000000  \n",
      "3    28.000000  \n",
      "4     0.918848  \n",
      "5     0.928571  \n",
      "6     0.582090  \n",
      "7     0.990500  \n",
      "8     0.715596  \n",
      "9     0.911090  \n",
      "10    0.834134  \n",
      "11    0.786283  \n",
      "12    0.696072  \n",
      "13    0.917600  \n",
      "14    0.786283  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_xgb_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_xgb_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_xgb_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_xgb_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_xgb_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_xgb_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_xgb_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_xgb_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:08:56,402] Trial 250 finished with value: 0.8119269861686114 and parameters: {'n_estimators': 405, 'eta': 0.09239529335845437, 'max_depth': 10, 'alpha': 0.9992000000000001, 'lambda': 6.827736631543209, 'max_bin': 403}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:00,906] Trial 251 finished with value: 0.801397412002426 and parameters: {'n_estimators': 481, 'eta': 0.05048050219772108, 'max_depth': 7, 'alpha': 0.925, 'lambda': 10.091426265868726, 'max_bin': 368}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:03,549] Trial 252 finished with value: 0.8143034031746849 and parameters: {'n_estimators': 513, 'eta': 0.09997990111418441, 'max_depth': 8, 'alpha': 0.8942, 'lambda': 8.260415001126344, 'max_bin': 395}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:06,937] Trial 253 finished with value: 0.8131128209401022 and parameters: {'n_estimators': 601, 'eta': 0.06686712902954407, 'max_depth': 9, 'alpha': 0.9715, 'lambda': 10.729146691520773, 'max_bin': 375}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:09,605] Trial 254 finished with value: 0.8008939246417632 and parameters: {'n_estimators': 434, 'eta': 0.09638340644186745, 'max_depth': 8, 'alpha': 0.9481, 'lambda': 14.87320310442822, 'max_bin': 361}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:12,463] Trial 255 finished with value: 0.8151560030662693 and parameters: {'n_estimators': 386, 'eta': 0.09451184444879403, 'max_depth': 10, 'alpha': 0.9243, 'lambda': 11.752524502124036, 'max_bin': 352}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:15,218] Trial 256 finished with value: 0.8094291503323635 and parameters: {'n_estimators': 345, 'eta': 0.09800976492106564, 'max_depth': 8, 'alpha': 0.9015000000000001, 'lambda': 13.452187943976188, 'max_bin': 348}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:17,902] Trial 257 finished with value: 0.8133509714547709 and parameters: {'n_estimators': 468, 'eta': 0.0898472589229776, 'max_depth': 10, 'alpha': 0.8781, 'lambda': 9.314723823957399, 'max_bin': 336}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:20,806] Trial 258 finished with value: 0.8091376453538576 and parameters: {'n_estimators': 533, 'eta': 0.09608121130971355, 'max_depth': 8, 'alpha': 0.9542, 'lambda': 12.559598238221547, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:23,401] Trial 259 finished with value: 0.8093638332969387 and parameters: {'n_estimators': 450, 'eta': 0.09341639558373808, 'max_depth': 8, 'alpha': 0.8198000000000001, 'lambda': 10.860281883346024, 'max_bin': 358}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:26,426] Trial 260 finished with value: 0.8049886215877505 and parameters: {'n_estimators': 503, 'eta': 0.07838009719228137, 'max_depth': 10, 'alpha': 0.9832000000000001, 'lambda': 10.051508137037592, 'max_bin': 381}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:30,204] Trial 261 finished with value: 0.8074760569668756 and parameters: {'n_estimators': 426, 'eta': 0.09799743803599585, 'max_depth': 9, 'alpha': 0.8519, 'lambda': 38.49676704821148, 'max_bin': 398}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:34,222] Trial 262 finished with value: 0.8073103397977309 and parameters: {'n_estimators': 556, 'eta': 0.09177161365546994, 'max_depth': 8, 'alpha': 0.9312, 'lambda': 35.01628221405666, 'max_bin': 387}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:37,947] Trial 263 finished with value: 0.8060551668664321 and parameters: {'n_estimators': 409, 'eta': 0.06450765309092106, 'max_depth': 8, 'alpha': 0.9107000000000001, 'lambda': 11.803574697345736, 'max_bin': 364}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:40,802] Trial 264 finished with value: 0.8043792725389298 and parameters: {'n_estimators': 486, 'eta': 0.09470727763498095, 'max_depth': 10, 'alpha': 0.9671000000000001, 'lambda': 14.021516166330585, 'max_bin': 373}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:43,997] Trial 265 finished with value: 0.8008456576875853 and parameters: {'n_estimators': 371, 'eta': 0.07149070914805662, 'max_depth': 7, 'alpha': 0.8906000000000001, 'lambda': 8.67797306939508, 'max_bin': 342}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:47,007] Trial 266 finished with value: 0.8072770424212983 and parameters: {'n_estimators': 465, 'eta': 0.09839165733278117, 'max_depth': 8, 'alpha': 0.9494, 'lambda': 12.980932189742113, 'max_bin': 354}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:49,622] Trial 267 finished with value: 0.8120924147688035 and parameters: {'n_estimators': 442, 'eta': 0.0999911879516418, 'max_depth': 10, 'alpha': 0.9226000000000001, 'lambda': 9.792777339860036, 'max_bin': 407}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:52,957] Trial 268 finished with value: 0.8015864544218603 and parameters: {'n_estimators': 517, 'eta': 0.08675585213711122, 'max_depth': 8, 'alpha': 0.9760000000000001, 'lambda': 16.034867227320063, 'max_bin': 377}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:56,739] Trial 269 finished with value: 0.8014909191581119 and parameters: {'n_estimators': 401, 'eta': 0.057395408210496046, 'max_depth': 8, 'alpha': 0.8758, 'lambda': 11.152242454839167, 'max_bin': 392}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:09:59,453] Trial 270 finished with value: 0.8152538799717549 and parameters: {'n_estimators': 480, 'eta': 0.09626946744624962, 'max_depth': 10, 'alpha': 0.9394, 'lambda': 10.519171359192695, 'max_bin': 360}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:03,186] Trial 271 finished with value: 0.8146397791441041 and parameters: {'n_estimators': 423, 'eta': 0.0888822901814665, 'max_depth': 6, 'alpha': 0.4335, 'lambda': 18.571930020857273, 'max_bin': 350}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:05,636] Trial 272 finished with value: 0.8151688651717015 and parameters: {'n_estimators': 499, 'eta': 0.09346778543313895, 'max_depth': 10, 'alpha': 0.9117000000000001, 'lambda': 7.8199949710832035, 'max_bin': 367}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:08,164] Trial 273 finished with value: 0.8114598017744845 and parameters: {'n_estimators': 462, 'eta': 0.09710846876932251, 'max_depth': 8, 'alpha': 0.8977, 'lambda': 9.227006394334031, 'max_bin': 382}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:11,751] Trial 274 finished with value: 0.8096083843899073 and parameters: {'n_estimators': 530, 'eta': 0.07514120000299135, 'max_depth': 8, 'alpha': 0.9338000000000001, 'lambda': 12.01742713413558, 'max_bin': 347}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:14,682] Trial 275 finished with value: 0.8096365708347069 and parameters: {'n_estimators': 358, 'eta': 0.0907397468894663, 'max_depth': 9, 'alpha': 0.9844, 'lambda': 11.19207802536027, 'max_bin': 373}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:17,490] Trial 276 finished with value: 0.8078101135422339 and parameters: {'n_estimators': 582, 'eta': 0.09519802698191322, 'max_depth': 10, 'alpha': 0.8601000000000001, 'lambda': 14.42207159644883, 'max_bin': 402}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:20,436] Trial 277 finished with value: 0.8124173848252022 and parameters: {'n_estimators': 385, 'eta': 0.08511929008064247, 'max_depth': 8, 'alpha': 0.9557, 'lambda': 9.928604309270016, 'max_bin': 357}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:23,116] Trial 278 finished with value: 0.8104859733512917 and parameters: {'n_estimators': 453, 'eta': 0.09792677858516423, 'max_depth': 10, 'alpha': 0.9092, 'lambda': 13.09566480893032, 'max_bin': 386}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:26,083] Trial 279 finished with value: 0.8127033470354317 and parameters: {'n_estimators': 491, 'eta': 0.09213693917685517, 'max_depth': 8, 'alpha': 0.9984000000000001, 'lambda': 12.22493288625541, 'max_bin': 395}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:28,729] Trial 280 finished with value: 0.8086229099508666 and parameters: {'n_estimators': 542, 'eta': 0.08318419807724706, 'max_depth': 8, 'alpha': 0.8331000000000001, 'lambda': 8.456478901822, 'max_bin': 361}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:31,484] Trial 281 finished with value: 0.8077040797152406 and parameters: {'n_estimators': 436, 'eta': 0.09439109891268377, 'max_depth': 10, 'alpha': 0.9362, 'lambda': 10.609149748029179, 'max_bin': 414}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:34,217] Trial 282 finished with value: 0.8089236451699042 and parameters: {'n_estimators': 510, 'eta': 0.09984973412007025, 'max_depth': 8, 'alpha': 0.8819, 'lambda': 9.281839855586542, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:37,132] Trial 283 finished with value: 0.805107787766388 and parameters: {'n_estimators': 413, 'eta': 0.09639262059683398, 'max_depth': 9, 'alpha': 0.9618000000000001, 'lambda': 11.627004836025638, 'max_bin': 377}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:42,820] Trial 284 finished with value: 0.7704892051899004 and parameters: {'n_estimators': 473, 'eta': 0.01025465317717756, 'max_depth': 7, 'alpha': 0.913, 'lambda': 13.69338549901216, 'max_bin': 345}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:45,124] Trial 285 finished with value: 0.8060462597032645 and parameters: {'n_estimators': 396, 'eta': 0.09804690663841151, 'max_depth': 8, 'alpha': 0.9762000000000001, 'lambda': 7.223256682134378, 'max_bin': 354}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:47,665] Trial 286 finished with value: 0.8164438376827171 and parameters: {'n_estimators': 455, 'eta': 0.09286595371620684, 'max_depth': 10, 'alpha': 0.8917, 'lambda': 10.255800015306805, 'max_bin': 363}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:50,838] Trial 287 finished with value: 0.8086487817390869 and parameters: {'n_estimators': 489, 'eta': 0.08905990727004719, 'max_depth': 8, 'alpha': 0.9284, 'lambda': 12.481076434236474, 'max_bin': 390}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:53,679] Trial 288 finished with value: 0.8150130430869661 and parameters: {'n_estimators': 374, 'eta': 0.09576131132580486, 'max_depth': 10, 'alpha': 0.9500000000000001, 'lambda': 11.257393033075138, 'max_bin': 351}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:10:56,161] Trial 289 finished with value: 0.8091714427928982 and parameters: {'n_estimators': 329, 'eta': 0.09813927338274951, 'max_depth': 8, 'alpha': 0.8697, 'lambda': 6.354950796758617, 'max_bin': 401}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:01,837] Trial 290 finished with value: 0.8072320741121237 and parameters: {'n_estimators': 523, 'eta': 0.03475859592714001, 'max_depth': 8, 'alpha': 0.9173, 'lambda': 15.055747292555761, 'max_bin': 340}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:07,950] Trial 291 finished with value: 0.7992582407214139 and parameters: {'n_estimators': 431, 'eta': 0.014197725167062816, 'max_depth': 10, 'alpha': 0.9628000000000001, 'lambda': 9.405802410071017, 'max_bin': 372}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:10,592] Trial 292 finished with value: 0.814646982101036 and parameters: {'n_estimators': 555, 'eta': 0.09399806412451027, 'max_depth': 8, 'alpha': 0.8992, 'lambda': 10.760872820912233, 'max_bin': 383}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:14,081] Trial 293 finished with value: 0.810713216597185 and parameters: {'n_estimators': 444, 'eta': 0.0908831825409964, 'max_depth': 10, 'alpha': 0.9438000000000001, 'lambda': 20.6463446532803, 'max_bin': 366}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:17,013] Trial 294 finished with value: 0.8097204108977781 and parameters: {'n_estimators': 477, 'eta': 0.09603296137448351, 'max_depth': 8, 'alpha': 0.1985, 'lambda': 17.0881285912799, 'max_bin': 379}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:20,340] Trial 295 finished with value: 0.810745810910716 and parameters: {'n_estimators': 504, 'eta': 0.0685107501278017, 'max_depth': 8, 'alpha': 0.9832000000000001, 'lambda': 8.426386261311004, 'max_bin': 356}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:23,295] Trial 296 finished with value: 0.8097783816260641 and parameters: {'n_estimators': 417, 'eta': 0.0869548733940009, 'max_depth': 9, 'alpha': 0.8597, 'lambda': 13.154465748392505, 'max_bin': 396}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:25,904] Trial 297 finished with value: 0.8077945817812857 and parameters: {'n_estimators': 397, 'eta': 0.09999478562179284, 'max_depth': 10, 'alpha': 0.9293, 'lambda': 11.744456960756853, 'max_bin': 406}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:30,273] Trial 298 finished with value: 0.8096540669797925 and parameters: {'n_estimators': 466, 'eta': 0.05462104252859134, 'max_depth': 8, 'alpha': 0.8869, 'lambda': 10.234936291138345, 'max_bin': 391}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:31,369] Trial 299 finished with value: 0.783727392087872 and parameters: {'n_estimators': 80, 'eta': 0.09192654255388459, 'max_depth': 7, 'alpha': 0.9136000000000001, 'lambda': 17.953226215450215, 'max_bin': 361}. Best is trial 102 with value: 0.8341420627094955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8341\n",
      "\tBest params:\n",
      "\t\tn_estimators: 452\n",
      "\t\teta: 0.09749861254895852\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.931\n",
      "\t\tlambda: 12.113245645578496\n",
      "\t\tmax_bin: 349\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   50.000000   37.000000   36.000000   \n",
      "1                    TN  308.000000  307.000000  305.000000  305.000000   \n",
      "2                    FP    6.000000    7.000000   10.000000   11.000000   \n",
      "3                    FN   24.000000   18.000000   30.000000   30.000000   \n",
      "4              Accuracy    0.921466    0.934555    0.895288    0.892670   \n",
      "5             Precision    0.880000    0.877193    0.787234    0.765957   \n",
      "6           Sensitivity    0.647059    0.735294    0.552239    0.545455   \n",
      "7           Specificity    0.980900    0.977700    0.968300    0.965200   \n",
      "8              F1 score    0.745763    0.800000    0.649123    0.637168   \n",
      "9   F1 score (weighted)    0.916570    0.932239    0.887714    0.885213   \n",
      "10     F1 score (macro)    0.849662    0.880438    0.793792    0.787094   \n",
      "11    Balanced Accuracy    0.813975    0.856501    0.760246    0.755322   \n",
      "12                  MCC    0.712181    0.765474    0.602610    0.587710   \n",
      "13                  NPV    0.927700    0.944600    0.910400    0.910400   \n",
      "14              ROC_AUC    0.813975    0.856501    0.760246    0.755322   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    39.000000   44.000000  \n",
      "1   312.000000  304.000000  \n",
      "2     3.000000   11.000000  \n",
      "3    28.000000   23.000000  \n",
      "4     0.918848    0.910995  \n",
      "5     0.928571    0.800000  \n",
      "6     0.582090    0.656716  \n",
      "7     0.990500    0.965100  \n",
      "8     0.715596    0.721311  \n",
      "9     0.911090    0.907449  \n",
      "10    0.834134    0.834176  \n",
      "11    0.786283    0.810898  \n",
      "12    0.696072    0.673574  \n",
      "13    0.917600    0.929700  \n",
      "14    0.786283    0.810898  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_xgb_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_xgb_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_xgb_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_xgb_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_xgb_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_xgb_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_xgb_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_xgb_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:11:34,943] Trial 300 finished with value: 0.8064592745573289 and parameters: {'n_estimators': 350, 'eta': 0.06980450896768217, 'max_depth': 8, 'alpha': 0.9683, 'lambda': 9.665116077680212, 'max_bin': 354}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:37,301] Trial 301 finished with value: 0.8065731921624135 and parameters: {'n_estimators': 516, 'eta': 0.09768481434004284, 'max_depth': 10, 'alpha': 0.9460000000000001, 'lambda': 12.496225752599637, 'max_bin': 347}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:39,916] Trial 302 finished with value: 0.8052418264285486 and parameters: {'n_estimators': 445, 'eta': 0.09411519561873027, 'max_depth': 8, 'alpha': 0.9006000000000001, 'lambda': 11.129170592141325, 'max_bin': 370}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:42,248] Trial 303 finished with value: 0.8126121522944214 and parameters: {'n_estimators': 535, 'eta': 0.09659897723505542, 'max_depth': 10, 'alpha': 0.9312, 'lambda': 8.889055318444512, 'max_bin': 385}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:45,254] Trial 304 finished with value: 0.8023523735368523 and parameters: {'n_estimators': 486, 'eta': 0.08014528447207936, 'max_depth': 8, 'alpha': 0.8480000000000001, 'lambda': 13.729190472201472, 'max_bin': 376}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:47,743] Trial 305 finished with value: 0.8085571439052515 and parameters: {'n_estimators': 380, 'eta': 0.09996188498327647, 'max_depth': 10, 'alpha': 0.9838, 'lambda': 7.988306338709919, 'max_bin': 366}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:50,425] Trial 306 finished with value: 0.8094493564891035 and parameters: {'n_estimators': 426, 'eta': 0.09050834648385066, 'max_depth': 8, 'alpha': 0.87, 'lambda': 10.17200793562454, 'max_bin': 351}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:53,467] Trial 307 finished with value: 0.8091468358285265 and parameters: {'n_estimators': 472, 'eta': 0.09460296959646042, 'max_depth': 8, 'alpha': 0.9550000000000001, 'lambda': 16.653001617968012, 'max_bin': 333}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:56,461] Trial 308 finished with value: 0.8025782786858071 and parameters: {'n_estimators': 494, 'eta': 0.08411173962344998, 'max_depth': 8, 'alpha': 0.9184, 'lambda': 11.969428186814174, 'max_bin': 343}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:11:58,817] Trial 309 finished with value: 0.8099959231007994 and parameters: {'n_estimators': 456, 'eta': 0.09802002118382372, 'max_depth': 9, 'alpha': 0.8869, 'lambda': 10.860972284283639, 'max_bin': 399}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:01,010] Trial 310 finished with value: 0.8105816427679263 and parameters: {'n_estimators': 406, 'eta': 0.09273050870557928, 'max_depth': 10, 'alpha': 0.9369000000000001, 'lambda': 5.4550421178809, 'max_bin': 358}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:03,563] Trial 311 finished with value: 0.8121515032576132 and parameters: {'n_estimators': 575, 'eta': 0.08860385383640741, 'max_depth': 8, 'alpha': 0.9942000000000001, 'lambda': 12.812018942198202, 'max_bin': 388}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:06,480] Trial 312 finished with value: 0.8058743864044189 and parameters: {'n_estimators': 547, 'eta': 0.09595403266584225, 'max_depth': 8, 'alpha': 0.9649000000000001, 'lambda': 14.391788483015299, 'max_bin': 411}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:08,825] Trial 313 finished with value: 0.8093980391933732 and parameters: {'n_estimators': 365, 'eta': 0.0761354892601801, 'max_depth': 10, 'alpha': 0.0171, 'lambda': 9.50994491640096, 'max_bin': 363}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:11,469] Trial 314 finished with value: 0.8063903370068365 and parameters: {'n_estimators': 515, 'eta': 0.09771064635625899, 'max_depth': 6, 'alpha': 0.9032, 'lambda': 11.496887567499149, 'max_bin': 380}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:14,429] Trial 315 finished with value: 0.8107347958725526 and parameters: {'n_estimators': 432, 'eta': 0.09515693644189153, 'max_depth': 10, 'alpha': 0.8331000000000001, 'lambda': 22.432947513147987, 'max_bin': 374}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:16,866] Trial 316 finished with value: 0.8073918293047997 and parameters: {'n_estimators': 503, 'eta': 0.09279125870879003, 'max_depth': 8, 'alpha': 0.9209, 'lambda': 8.86279416744478, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:20,362] Trial 317 finished with value: 0.7983178787219188 and parameters: {'n_estimators': 448, 'eta': 0.060639654015506736, 'max_depth': 7, 'alpha': 0.9999, 'lambda': 10.501394978614611, 'max_bin': 393}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:23,195] Trial 318 finished with value: 0.8065235573969689 and parameters: {'n_estimators': 389, 'eta': 0.09997900528155479, 'max_depth': 8, 'alpha': 0.9428000000000001, 'lambda': 15.682244408234192, 'max_bin': 359}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:25,671] Trial 319 finished with value: 0.8065197666697017 and parameters: {'n_estimators': 470, 'eta': 0.08985503775046146, 'max_depth': 10, 'alpha': 0.8779, 'lambda': 12.16455273728378, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:28,964] Trial 320 finished with value: 0.8011692145571946 and parameters: {'n_estimators': 411, 'eta': 0.06569391660296395, 'max_depth': 8, 'alpha': 0.9618000000000001, 'lambda': 9.808978455353751, 'max_bin': 402}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:33,027] Trial 321 finished with value: 0.8055188568648605 and parameters: {'n_estimators': 532, 'eta': 0.045387383300676966, 'max_depth': 10, 'alpha': 0.8947, 'lambda': 7.115657989870588, 'max_bin': 353}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:36,676] Trial 322 finished with value: 0.8060113317212403 and parameters: {'n_estimators': 484, 'eta': 0.06146988207729294, 'max_depth': 9, 'alpha': 0.9198000000000001, 'lambda': 13.146795091532814, 'max_bin': 383}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:39,049] Trial 323 finished with value: 0.8106375384807268 and parameters: {'n_estimators': 457, 'eta': 0.0974482075072323, 'max_depth': 8, 'alpha': 0.5693, 'lambda': 10.88889512802372, 'max_bin': 344}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:41,757] Trial 324 finished with value: 0.8067188327612567 and parameters: {'n_estimators': 424, 'eta': 0.09475419186957942, 'max_depth': 8, 'alpha': 0.9738, 'lambda': 11.71268042818583, 'max_bin': 365}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:44,054] Trial 325 finished with value: 0.8127631848210463 and parameters: {'n_estimators': 503, 'eta': 0.09167070905335967, 'max_depth': 10, 'alpha': 0.29650000000000004, 'lambda': 8.047564286928555, 'max_bin': 338}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:47,214] Trial 326 finished with value: 0.8019884421058243 and parameters: {'n_estimators': 564, 'eta': 0.06907526220723281, 'max_depth': 8, 'alpha': 0.9449000000000001, 'lambda': 9.9555740916608, 'max_bin': 396}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:50,813] Trial 327 finished with value: 0.8074610957514015 and parameters: {'n_estimators': 524, 'eta': 0.08745690554943943, 'max_depth': 10, 'alpha': 0.8581000000000001, 'lambda': 25.612226515611994, 'max_bin': 374}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:54,149] Trial 328 finished with value: 0.8043094296495686 and parameters: {'n_estimators': 439, 'eta': 0.07176654468543463, 'max_depth': 8, 'alpha': 0.9002, 'lambda': 13.955692328012532, 'max_bin': 357}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:56,631] Trial 329 finished with value: 0.8044792187263032 and parameters: {'n_estimators': 475, 'eta': 0.09655314853950872, 'max_depth': 8, 'alpha': 0.931, 'lambda': 9.128310232803923, 'max_bin': 387}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:12:59,190] Trial 330 finished with value: 0.8114851890292831 and parameters: {'n_estimators': 393, 'eta': 0.09833322355989155, 'max_depth': 10, 'alpha': 0.8036000000000001, 'lambda': 11.259193128509722, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:02,117] Trial 331 finished with value: 0.8091649968075005 and parameters: {'n_estimators': 362, 'eta': 0.09534771264693255, 'max_depth': 8, 'alpha': 0.9780000000000001, 'lambda': 12.637203229353085, 'max_bin': 379}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:04,842] Trial 332 finished with value: 0.8014930564036054 and parameters: {'n_estimators': 460, 'eta': 0.09327882269122416, 'max_depth': 7, 'alpha': 0.8749, 'lambda': 10.459990190241909, 'max_bin': 370}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:07,592] Trial 333 finished with value: 0.8057755361032525 and parameters: {'n_estimators': 495, 'eta': 0.09789214782079884, 'max_depth': 10, 'alpha': 0.9184, 'lambda': 15.069068610071088, 'max_bin': 361}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:09,650] Trial 334 finished with value: 0.8042474301383621 and parameters: {'n_estimators': 407, 'eta': 0.0901478452576283, 'max_depth': 8, 'alpha': 0.9508000000000001, 'lambda': 1.411193480367178, 'max_bin': 405}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:12,022] Trial 335 finished with value: 0.8104009874087007 and parameters: {'n_estimators': 426, 'eta': 0.09986882217043307, 'max_depth': 9, 'alpha': 0.9072, 'lambda': 8.78356145258341, 'max_bin': 391}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:14,855] Trial 336 finished with value: 0.8047654503071971 and parameters: {'n_estimators': 377, 'eta': 0.07799731875919974, 'max_depth': 8, 'alpha': 0.9651000000000001, 'lambda': 11.90333463544, 'max_bin': 354}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:17,261] Trial 337 finished with value: 0.7995617283590977 and parameters: {'n_estimators': 445, 'eta': 0.0962115555824601, 'max_depth': 10, 'alpha': 0.8868, 'lambda': 13.512260123685236, 'max_bin': 416}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:20,323] Trial 338 finished with value: 0.807969204866801 and parameters: {'n_estimators': 551, 'eta': 0.09362438691732582, 'max_depth': 8, 'alpha': 0.9304, 'lambda': 18.699792789480405, 'max_bin': 366}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:22,935] Trial 339 finished with value: 0.8051169724777505 and parameters: {'n_estimators': 481, 'eta': 0.08596976893733266, 'max_depth': 8, 'alpha': 0.9981000000000001, 'lambda': 9.55730235527367, 'max_bin': 399}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:25,124] Trial 340 finished with value: 0.814635394451968 and parameters: {'n_estimators': 518, 'eta': 0.09172876505716912, 'max_depth': 10, 'alpha': 0.8471000000000001, 'lambda': 4.760530975227866, 'max_bin': 372}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:27,625] Trial 341 finished with value: 0.8024704578273234 and parameters: {'n_estimators': 461, 'eta': 0.09845237569200324, 'max_depth': 8, 'alpha': 0.9471, 'lambda': 10.892605168029357, 'max_bin': 344}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:30,253] Trial 342 finished with value: 0.8107375536262748 and parameters: {'n_estimators': 505, 'eta': 0.08088939986179262, 'max_depth': 9, 'alpha': 0.9004000000000001, 'lambda': 7.711028963459263, 'max_bin': 379}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:33,926] Trial 343 finished with value: 0.8058221910734865 and parameters: {'n_estimators': 538, 'eta': 0.05424941608973102, 'max_depth': 10, 'alpha': 0.9759000000000001, 'lambda': 12.559146970499157, 'max_bin': 362}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:36,332] Trial 344 finished with value: 0.8120431306395213 and parameters: {'n_estimators': 418, 'eta': 0.09615361098056491, 'max_depth': 11, 'alpha': 0.4806, 'lambda': 10.244872733659458, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:39,281] Trial 345 finished with value: 0.8064835539055404 and parameters: {'n_estimators': 441, 'eta': 0.088049773586629, 'max_depth': 8, 'alpha': 0.927, 'lambda': 11.38092419834669, 'max_bin': 409}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:42,832] Trial 346 finished with value: 0.7947757299125057 and parameters: {'n_estimators': 485, 'eta': 0.08267814445736635, 'max_depth': 7, 'alpha': 0.8696, 'lambda': 21.420739513611583, 'max_bin': 482}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:45,634] Trial 347 finished with value: 0.8070034638457703 and parameters: {'n_estimators': 400, 'eta': 0.09489394746226561, 'max_depth': 8, 'alpha': 0.6483, 'lambda': 19.72156942137189, 'max_bin': 385}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:47,961] Trial 348 finished with value: 0.8075284826811311 and parameters: {'n_estimators': 470, 'eta': 0.09992254996172506, 'max_depth': 10, 'alpha': 0.9568000000000001, 'lambda': 8.744977171747006, 'max_bin': 356}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:50,903] Trial 349 finished with value: 0.8016046464894571 and parameters: {'n_estimators': 388, 'eta': 0.09327743954321703, 'max_depth': 8, 'alpha': 0.9135000000000001, 'lambda': 14.467014263495784, 'max_bin': 395}. Best is trial 102 with value: 0.8341420627094955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8341\n",
      "\tBest params:\n",
      "\t\tn_estimators: 452\n",
      "\t\teta: 0.09749861254895852\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.931\n",
      "\t\tlambda: 12.113245645578496\n",
      "\t\tmax_bin: 349\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   50.000000   37.000000   36.000000   \n",
      "1                    TN  308.000000  307.000000  305.000000  305.000000   \n",
      "2                    FP    6.000000    7.000000   10.000000   11.000000   \n",
      "3                    FN   24.000000   18.000000   30.000000   30.000000   \n",
      "4              Accuracy    0.921466    0.934555    0.895288    0.892670   \n",
      "5             Precision    0.880000    0.877193    0.787234    0.765957   \n",
      "6           Sensitivity    0.647059    0.735294    0.552239    0.545455   \n",
      "7           Specificity    0.980900    0.977700    0.968300    0.965200   \n",
      "8              F1 score    0.745763    0.800000    0.649123    0.637168   \n",
      "9   F1 score (weighted)    0.916570    0.932239    0.887714    0.885213   \n",
      "10     F1 score (macro)    0.849662    0.880438    0.793792    0.787094   \n",
      "11    Balanced Accuracy    0.813975    0.856501    0.760246    0.755322   \n",
      "12                  MCC    0.712181    0.765474    0.602610    0.587710   \n",
      "13                  NPV    0.927700    0.944600    0.910400    0.910400   \n",
      "14              ROC_AUC    0.813975    0.856501    0.760246    0.755322   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    39.000000   44.000000   44.000000  \n",
      "1   312.000000  304.000000  302.000000  \n",
      "2     3.000000   11.000000   11.000000  \n",
      "3    28.000000   23.000000   25.000000  \n",
      "4     0.918848    0.910995    0.905759  \n",
      "5     0.928571    0.800000    0.800000  \n",
      "6     0.582090    0.656716    0.637681  \n",
      "7     0.990500    0.965100    0.964900  \n",
      "8     0.715596    0.721311    0.709677  \n",
      "9     0.911090    0.907449    0.901470  \n",
      "10    0.834134    0.834176    0.826714  \n",
      "11    0.786283    0.810898    0.801269  \n",
      "12    0.696072    0.673574    0.660276  \n",
      "13    0.917600    0.929700    0.923500  \n",
      "14    0.786283    0.810898    0.801269  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_xgb_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_xgb_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_xgb_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_xgb_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_xgb_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_xgb_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_xgb_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_xgb_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:13:53,923] Trial 350 finished with value: 0.8180250450852722 and parameters: {'n_estimators': 496, 'eta': 0.09718480948169668, 'max_depth': 10, 'alpha': 0.8904000000000001, 'lambda': 9.583783319383938, 'max_bin': 376}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:13:56,896] Trial 351 finished with value: 0.8122065064438477 and parameters: {'n_estimators': 519, 'eta': 0.08981661458006637, 'max_depth': 8, 'alpha': 0.9406, 'lambda': 13.088537092815251, 'max_bin': 365}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:00,075] Trial 352 finished with value: 0.8146148627287028 and parameters: {'n_estimators': 337, 'eta': 0.09827924331081099, 'max_depth': 8, 'alpha': 0.9801000000000001, 'lambda': 16.523636539198623, 'max_bin': 337}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:02,893] Trial 353 finished with value: 0.8166513719929958 and parameters: {'n_estimators': 454, 'eta': 0.09147948109491763, 'max_depth': 8, 'alpha': 0.913, 'lambda': 12.01357753023662, 'max_bin': 390}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:08,276] Trial 354 finished with value: 0.8072113424722908 and parameters: {'n_estimators': 430, 'eta': 0.028422097402733586, 'max_depth': 10, 'alpha': 0.9557, 'lambda': 10.69250829885438, 'max_bin': 360}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:11,681] Trial 355 finished with value: 0.8168028029810342 and parameters: {'n_estimators': 594, 'eta': 0.07195775924324418, 'max_depth': 9, 'alpha': 0.8807, 'lambda': 10.030960816037503, 'max_bin': 355}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:14,584] Trial 356 finished with value: 0.812341854594204 and parameters: {'n_estimators': 373, 'eta': 0.09453236651172596, 'max_depth': 6, 'alpha': 0.9379000000000001, 'lambda': 11.384224583937476, 'max_bin': 370}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:17,610] Trial 357 finished with value: 0.8124409721471757 and parameters: {'n_estimators': 411, 'eta': 0.09644106433903128, 'max_depth': 10, 'alpha': 0.8254, 'lambda': 12.34513834328485, 'max_bin': 347}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:20,162] Trial 358 finished with value: 0.8127077665163249 and parameters: {'n_estimators': 564, 'eta': 0.08456892523797173, 'max_depth': 8, 'alpha': 0.8520000000000001, 'lambda': 3.48758362974578, 'max_bin': 381}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:22,681] Trial 359 finished with value: 0.8068891500319092 and parameters: {'n_estimators': 349, 'eta': 0.09994855158643602, 'max_depth': 8, 'alpha': 0.9027000000000001, 'lambda': 9.211392956292425, 'max_bin': 403}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:25,074] Trial 360 finished with value: 0.8189221420536166 and parameters: {'n_estimators': 478, 'eta': 0.09313308401522032, 'max_depth': 10, 'alpha': 0.9670000000000001, 'lambda': 6.632277591316575, 'max_bin': 351}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:27,646] Trial 361 finished with value: 0.8133257083391158 and parameters: {'n_estimators': 534, 'eta': 0.09660879375537987, 'max_depth': 8, 'alpha': 0.9255, 'lambda': 8.403975647642392, 'max_bin': 385}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:31,315] Trial 362 finished with value: 0.8112323747806695 and parameters: {'n_estimators': 447, 'eta': 0.06509159408781856, 'max_depth': 10, 'alpha': 0.8733000000000001, 'lambda': 13.7371933606831, 'max_bin': 341}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:35,005] Trial 363 finished with value: 0.807235241675689 and parameters: {'n_estimators': 505, 'eta': 0.06838543388239444, 'max_depth': 8, 'alpha': 0.9804, 'lambda': 17.912320708146616, 'max_bin': 375}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:38,687] Trial 364 finished with value: 0.8149263716984685 and parameters: {'n_estimators': 623, 'eta': 0.0980929592643056, 'max_depth': 8, 'alpha': 0.9459000000000001, 'lambda': 32.8145652055654, 'max_bin': 394}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:41,425] Trial 365 finished with value: 0.8086315480854559 and parameters: {'n_estimators': 465, 'eta': 0.09464804094364235, 'max_depth': 7, 'alpha': 0.998, 'lambda': 10.575137164406556, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:44,167] Trial 366 finished with value: 0.8131758733210221 and parameters: {'n_estimators': 432, 'eta': 0.09118425620527867, 'max_depth': 10, 'alpha': 0.9012, 'lambda': 11.348991914261221, 'max_bin': 364}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:49,213] Trial 367 finished with value: 0.8076848217922409 and parameters: {'n_estimators': 488, 'eta': 0.05134977337918935, 'max_depth': 8, 'alpha': 0.9149, 'lambda': 24.304001820769283, 'max_bin': 357}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:51,952] Trial 368 finished with value: 0.814714930445881 and parameters: {'n_estimators': 393, 'eta': 0.09597532452669867, 'max_depth': 10, 'alpha': 0.9326000000000001, 'lambda': 9.920088572363968, 'max_bin': 352}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:54,944] Trial 369 finished with value: 0.8197696111323923 and parameters: {'n_estimators': 417, 'eta': 0.08885769031109005, 'max_depth': 9, 'alpha': 0.9641000000000001, 'lambda': 13.175925019566074, 'max_bin': 397}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:14:58,224] Trial 370 finished with value: 0.8202797557305029 and parameters: {'n_estimators': 513, 'eta': 0.07522450956033012, 'max_depth': 8, 'alpha': 0.4389, 'lambda': 12.239880289552044, 'max_bin': 387}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:01,455] Trial 371 finished with value: 0.8184628105093964 and parameters: {'n_estimators': 548, 'eta': 0.07877404462146458, 'max_depth': 8, 'alpha': 0.8997, 'lambda': 15.230364154266514, 'max_bin': 408}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:04,172] Trial 372 finished with value: 0.8185846663803659 and parameters: {'n_estimators': 449, 'eta': 0.09816217749659067, 'max_depth': 10, 'alpha': 1.0, 'lambda': 10.802538749878583, 'max_bin': 359}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:07,061] Trial 373 finished with value: 0.8169153730972869 and parameters: {'n_estimators': 474, 'eta': 0.09280010436363328, 'max_depth': 8, 'alpha': 0.8844000000000001, 'lambda': 7.567221370165686, 'max_bin': 367}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:10,503] Trial 374 finished with value: 0.8058791051782158 and parameters: {'n_estimators': 379, 'eta': 0.0664542159159627, 'max_depth': 8, 'alpha': 0.9551000000000001, 'lambda': 9.251554687042347, 'max_bin': 378}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:13,528] Trial 375 finished with value: 0.8196317286582699 and parameters: {'n_estimators': 528, 'eta': 0.09485675819791456, 'max_depth': 10, 'alpha': 0.9235000000000001, 'lambda': 11.44909431174197, 'max_bin': 344}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:19,869] Trial 376 finished with value: 0.7680731441073502 and parameters: {'n_estimators': 502, 'eta': 0.007100135562979094, 'max_depth': 8, 'alpha': 0.8573000000000001, 'lambda': 10.266947871720706, 'max_bin': 373}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:22,605] Trial 377 finished with value: 0.8140835100473611 and parameters: {'n_estimators': 438, 'eta': 0.09995698717963941, 'max_depth': 7, 'alpha': 0.9764, 'lambda': 8.223537178850096, 'max_bin': 402}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:25,815] Trial 378 finished with value: 0.8213448779934387 and parameters: {'n_estimators': 405, 'eta': 0.08693152390935202, 'max_depth': 10, 'alpha': 0.5139, 'lambda': 12.76786402210681, 'max_bin': 347}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:28,665] Trial 379 finished with value: 0.8146531237098804 and parameters: {'n_estimators': 463, 'eta': 0.0967549023691841, 'max_depth': 8, 'alpha': 0.9314, 'lambda': 14.554647065723039, 'max_bin': 390}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:32,648] Trial 380 finished with value: 0.8124593430487022 and parameters: {'n_estimators': 367, 'eta': 0.09069968392180475, 'max_depth': 10, 'alpha': 0.9457000000000001, 'lambda': 39.60314422470633, 'max_bin': 327}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:35,090] Trial 381 finished with value: 0.8186817731032487 and parameters: {'n_estimators': 490, 'eta': 0.09399384004449095, 'max_depth': 11, 'alpha': 0.8915000000000001, 'lambda': 6.001206862300725, 'max_bin': 361}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:39,040] Trial 382 finished with value: 0.804509339750167 and parameters: {'n_estimators': 418, 'eta': 0.05813310360778516, 'max_depth': 5, 'alpha': 0.9105000000000001, 'lambda': 11.930849609926982, 'max_bin': 353}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:44,022] Trial 383 finished with value: 0.8110102641098859 and parameters: {'n_estimators': 454, 'eta': 0.04517445671426561, 'max_depth': 8, 'alpha': 0.8745, 'lambda': 19.552437494697145, 'max_bin': 383}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:46,730] Trial 384 finished with value: 0.8132437778131777 and parameters: {'n_estimators': 475, 'eta': 0.0982972823204434, 'max_depth': 8, 'alpha': 0.9644, 'lambda': 9.648904743463273, 'max_bin': 399}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:49,485] Trial 385 finished with value: 0.8142658464974863 and parameters: {'n_estimators': 522, 'eta': 0.09232784658715014, 'max_depth': 9, 'alpha': 0.9239, 'lambda': 10.92262275236179, 'max_bin': 365}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:53,344] Trial 386 finished with value: 0.8077100462226421 and parameters: {'n_estimators': 395, 'eta': 0.06079907810051634, 'max_depth': 10, 'alpha': 0.8384, 'lambda': 13.875852999807208, 'max_bin': 371}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:56,144] Trial 387 finished with value: 0.82586485223037 and parameters: {'n_estimators': 430, 'eta': 0.07060625293569922, 'max_depth': 8, 'alpha': 0.9829, 'lambda': 2.0561465701188055, 'max_bin': 380}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:15:58,636] Trial 388 finished with value: 0.8124554118610327 and parameters: {'n_estimators': 495, 'eta': 0.09656129079310727, 'max_depth': 8, 'alpha': 0.9427000000000001, 'lambda': 8.758871271771982, 'max_bin': 392}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:01,643] Trial 389 finished with value: 0.8185049721048495 and parameters: {'n_estimators': 541, 'eta': 0.0883690340560186, 'max_depth': 10, 'alpha': 0.9068, 'lambda': 10.279277191235778, 'max_bin': 334}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:04,555] Trial 390 finished with value: 0.8158569664737783 and parameters: {'n_estimators': 579, 'eta': 0.09842985033767451, 'max_depth': 8, 'alpha': 0.9562, 'lambda': 12.853270096690988, 'max_bin': 357}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:07,363] Trial 391 finished with value: 0.8218267253098468 and parameters: {'n_estimators': 467, 'eta': 0.09522232422833277, 'max_depth': 10, 'alpha': 0.8645, 'lambda': 11.805066183507869, 'max_bin': 340}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:10,277] Trial 392 finished with value: 0.8180749924174666 and parameters: {'n_estimators': 446, 'eta': 0.09354632665833587, 'max_depth': 8, 'alpha': 0.9345, 'lambda': 9.574622273656223, 'max_bin': 376}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:13,277] Trial 393 finished with value: 0.8214164102411988 and parameters: {'n_estimators': 507, 'eta': 0.08999522046827824, 'max_depth': 8, 'alpha': 0.8884000000000001, 'lambda': 10.736518818959821, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:15,771] Trial 394 finished with value: 0.8224295188430484 and parameters: {'n_estimators': 358, 'eta': 0.09675892307249359, 'max_depth': 10, 'alpha': 0.9825, 'lambda': 4.1083872907872045, 'max_bin': 413}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:18,672] Trial 395 finished with value: 0.81752549297313 and parameters: {'n_estimators': 487, 'eta': 0.09833698855547267, 'max_depth': 9, 'alpha': 0.9224, 'lambda': 8.789342663192095, 'max_bin': 362}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:22,886] Trial 396 finished with value: 0.8100050947551626 and parameters: {'n_estimators': 410, 'eta': 0.07778451872262981, 'max_depth': 8, 'alpha': 0.9592, 'lambda': 30.647012376444064, 'max_bin': 368}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:26,779] Trial 397 finished with value: 0.8034187955884129 and parameters: {'n_estimators': 553, 'eta': 0.06341286753283312, 'max_depth': 7, 'alpha': 0.3446, 'lambda': 15.624609170850782, 'max_bin': 355}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:30,354] Trial 398 finished with value: 0.8166968796415945 and parameters: {'n_estimators': 379, 'eta': 0.08209099387719283, 'max_depth': 8, 'alpha': 0.8986000000000001, 'lambda': 16.901127241070075, 'max_bin': 386}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:33,879] Trial 399 finished with value: 0.8173847879681986 and parameters: {'n_estimators': 432, 'eta': 0.09210106432818428, 'max_depth': 10, 'alpha': 0.9146000000000001, 'lambda': 20.37972896639612, 'max_bin': 396}. Best is trial 102 with value: 0.8341420627094955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8341\n",
      "\tBest params:\n",
      "\t\tn_estimators: 452\n",
      "\t\teta: 0.09749861254895852\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.931\n",
      "\t\tlambda: 12.113245645578496\n",
      "\t\tmax_bin: 349\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   50.000000   37.000000   36.000000   \n",
      "1                    TN  308.000000  307.000000  305.000000  305.000000   \n",
      "2                    FP    6.000000    7.000000   10.000000   11.000000   \n",
      "3                    FN   24.000000   18.000000   30.000000   30.000000   \n",
      "4              Accuracy    0.921466    0.934555    0.895288    0.892670   \n",
      "5             Precision    0.880000    0.877193    0.787234    0.765957   \n",
      "6           Sensitivity    0.647059    0.735294    0.552239    0.545455   \n",
      "7           Specificity    0.980900    0.977700    0.968300    0.965200   \n",
      "8              F1 score    0.745763    0.800000    0.649123    0.637168   \n",
      "9   F1 score (weighted)    0.916570    0.932239    0.887714    0.885213   \n",
      "10     F1 score (macro)    0.849662    0.880438    0.793792    0.787094   \n",
      "11    Balanced Accuracy    0.813975    0.856501    0.760246    0.755322   \n",
      "12                  MCC    0.712181    0.765474    0.602610    0.587710   \n",
      "13                  NPV    0.927700    0.944600    0.910400    0.910400   \n",
      "14              ROC_AUC    0.813975    0.856501    0.760246    0.755322   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    39.000000   44.000000   44.000000   44.000000  \n",
      "1   312.000000  304.000000  302.000000  307.000000  \n",
      "2     3.000000   11.000000   11.000000    8.000000  \n",
      "3    28.000000   23.000000   25.000000   23.000000  \n",
      "4     0.918848    0.910995    0.905759    0.918848  \n",
      "5     0.928571    0.800000    0.800000    0.846154  \n",
      "6     0.582090    0.656716    0.637681    0.656716  \n",
      "7     0.990500    0.965100    0.964900    0.974600  \n",
      "8     0.715596    0.721311    0.709677    0.739496  \n",
      "9     0.911090    0.907449    0.901470    0.914677  \n",
      "10    0.834134    0.834176    0.826714    0.845717  \n",
      "11    0.786283    0.810898    0.801269    0.815660  \n",
      "12    0.696072    0.673574    0.660276    0.700137  \n",
      "13    0.917600    0.929700    0.923500    0.930300  \n",
      "14    0.786283    0.810898    0.801269    0.815660  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_xgb_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_xgb_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_xgb_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_xgb_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_xgb_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_xgb_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_xgb_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_xgb_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:16:39,419] Trial 400 finished with value: 0.8217536367841571 and parameters: {'n_estimators': 520, 'eta': 0.04219423125403686, 'max_depth': 8, 'alpha': 0.9411, 'lambda': 11.405363206696855, 'max_bin': 404}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:42,262] Trial 401 finished with value: 0.8216368472548888 and parameters: {'n_estimators': 459, 'eta': 0.09985467498631342, 'max_depth': 10, 'alpha': 0.8684000000000001, 'lambda': 12.179763018897424, 'max_bin': 347}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:46,115] Trial 402 finished with value: 0.8213428658326419 and parameters: {'n_estimators': 394, 'eta': 0.09532826106782892, 'max_depth': 9, 'alpha': 0.8141, 'lambda': 18.44872191784692, 'max_bin': 373}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:50,790] Trial 403 finished with value: 0.817320233231766 and parameters: {'n_estimators': 484, 'eta': 0.05634429233991743, 'max_depth': 8, 'alpha': 0.6002000000000001, 'lambda': 13.220304924609588, 'max_bin': 351}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:16:56,926] Trial 404 finished with value: 0.8115235937121114 and parameters: {'n_estimators': 443, 'eta': 0.01958677698757194, 'max_depth': 10, 'alpha': 0.9790000000000001, 'lambda': 22.018959781761534, 'max_bin': 359}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:00,775] Trial 405 finished with value: 0.8217911843745732 and parameters: {'n_estimators': 419, 'eta': 0.06453758751499808, 'max_depth': 8, 'alpha': 0.0786, 'lambda': 10.191201113483068, 'max_bin': 380}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:06,436] Trial 406 finished with value: 0.7509796674548898 and parameters: {'n_estimators': 504, 'eta': 0.0022776917007205633, 'max_depth': 8, 'alpha': 0.8863000000000001, 'lambda': 14.142812330794799, 'max_bin': 364}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:09,347] Trial 407 finished with value: 0.8158046541702374 and parameters: {'n_estimators': 458, 'eta': 0.08617020103188175, 'max_depth': 10, 'alpha': 0.9492, 'lambda': 7.233329992156852, 'max_bin': 391}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:12,140] Trial 408 finished with value: 0.8135351040352334 and parameters: {'n_estimators': 530, 'eta': 0.09393372342402914, 'max_depth': 8, 'alpha': 0.9138000000000001, 'lambda': 9.355077386899175, 'max_bin': 344}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:14,892] Trial 409 finished with value: 0.8164392513458463 and parameters: {'n_estimators': 318, 'eta': 0.09676239816555468, 'max_depth': 8, 'alpha': 0.9690000000000001, 'lambda': 8.164912320407904, 'max_bin': 369}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:20,343] Trial 410 finished with value: 0.8269184374852017 and parameters: {'n_estimators': 480, 'eta': 0.0397169091267804, 'max_depth': 11, 'alpha': 0.8472000000000001, 'lambda': 11.23596446184795, 'max_bin': 426}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:24,097] Trial 411 finished with value: 0.8191652755307748 and parameters: {'n_estimators': 567, 'eta': 0.0999990130458368, 'max_depth': 10, 'alpha': 0.9296000000000001, 'lambda': 28.45305868668741, 'max_bin': 354}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:27,349] Trial 412 finished with value: 0.8225025162567204 and parameters: {'n_estimators': 338, 'eta': 0.09080254169886486, 'max_depth': 8, 'alpha': 0.8929, 'lambda': 10.019428202004926, 'max_bin': 383}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:31,948] Trial 413 finished with value: 0.8201975964775068 and parameters: {'n_estimators': 379, 'eta': 0.06055272976973596, 'max_depth': 8, 'alpha': 0.9971000000000001, 'lambda': 23.081746681016178, 'max_bin': 400}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:36,220] Trial 414 finished with value: 0.8241486321350792 and parameters: {'n_estimators': 405, 'eta': 0.055980973609631425, 'max_depth': 10, 'alpha': 0.9451, 'lambda': 12.392638127894843, 'max_bin': 375}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:40,633] Trial 415 finished with value: 0.8215047208292242 and parameters: {'n_estimators': 469, 'eta': 0.05303580861001522, 'max_depth': 6, 'alpha': 0.9096000000000001, 'lambda': 10.770955024968599, 'max_bin': 360}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:44,723] Trial 416 finished with value: 0.8091633936920275 and parameters: {'n_estimators': 512, 'eta': 0.06746558523179283, 'max_depth': 7, 'alpha': 0.8703000000000001, 'lambda': 13.477081830211677, 'max_bin': 388}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:47,685] Trial 417 finished with value: 0.8203747118255867 and parameters: {'n_estimators': 430, 'eta': 0.09789370255893032, 'max_depth': 8, 'alpha': 0.9666, 'lambda': 9.412386521342988, 'max_bin': 407}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:51,205] Trial 418 finished with value: 0.8250095795890985 and parameters: {'n_estimators': 494, 'eta': 0.0734589835059064, 'max_depth': 10, 'alpha': 0.9359000000000001, 'lambda': 11.765666513782898, 'max_bin': 366}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:54,460] Trial 419 finished with value: 0.8251806978613286 and parameters: {'n_estimators': 355, 'eta': 0.0950638505139898, 'max_depth': 8, 'alpha': 0.8861, 'lambda': 12.678375172358988, 'max_bin': 352}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:17:57,752] Trial 420 finished with value: 0.8278448633451847 and parameters: {'n_estimators': 443, 'eta': 0.09275606403900773, 'max_depth': 10, 'alpha': 0.9561000000000001, 'lambda': 10.852731996720877, 'max_bin': 341}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:00,773] Trial 421 finished with value: 0.8141875541959127 and parameters: {'n_estimators': 533, 'eta': 0.08847560882330689, 'max_depth': 8, 'alpha': 0.9184, 'lambda': 8.81675222104411, 'max_bin': 378}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:02,842] Trial 422 finished with value: 0.8229943719809902 and parameters: {'n_estimators': 148, 'eta': 0.09651999360352748, 'max_depth': 9, 'alpha': 0.9772000000000001, 'lambda': 10.005282162480416, 'max_bin': 395}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:07,158] Trial 423 finished with value: 0.8200857255361317 and parameters: {'n_estimators': 469, 'eta': 0.05178497706482767, 'max_depth': 8, 'alpha': 0.9068, 'lambda': 7.843398337811034, 'max_bin': 371}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:10,353] Trial 424 finished with value: 0.8259815308404743 and parameters: {'n_estimators': 416, 'eta': 0.0939503320122706, 'max_depth': 10, 'alpha': 0.8554, 'lambda': 11.597423590326779, 'max_bin': 360}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:15,329] Trial 425 finished with value: 0.8176696425572836 and parameters: {'n_estimators': 546, 'eta': 0.08442924233036006, 'max_depth': 8, 'alpha': 0.9974000000000001, 'lambda': 37.845002893867374, 'max_bin': 348}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:19,070] Trial 426 finished with value: 0.8180981097897855 and parameters: {'n_estimators': 451, 'eta': 0.09776402400594006, 'max_depth': 10, 'alpha': 0.9282, 'lambda': 20.803528227429428, 'max_bin': 356}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:22,174] Trial 427 finished with value: 0.8241454430031169 and parameters: {'n_estimators': 495, 'eta': 0.09123962515997634, 'max_depth': 8, 'alpha': 0.9475, 'lambda': 10.59749938946377, 'max_bin': 384}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:26,046] Trial 428 finished with value: 0.8208154633595266 and parameters: {'n_estimators': 399, 'eta': 0.08062479316444376, 'max_depth': 11, 'alpha': 0.8844000000000001, 'lambda': 14.173641153448425, 'max_bin': 366}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:29,965] Trial 429 finished with value: 0.820777809261404 and parameters: {'n_estimators': 515, 'eta': 0.07239677494801812, 'max_depth': 8, 'alpha': 0.9041, 'lambda': 14.92635809100857, 'max_bin': 400}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:33,201] Trial 430 finished with value: 0.8200137184633945 and parameters: {'n_estimators': 387, 'eta': 0.09564742805780689, 'max_depth': 9, 'alpha': 0.8307, 'lambda': 12.120825283835192, 'max_bin': 391}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:36,591] Trial 431 finished with value: 0.8223388818922087 and parameters: {'n_estimators': 473, 'eta': 0.07669377960457531, 'max_depth': 10, 'alpha': 0.9643, 'lambda': 9.043590579770509, 'max_bin': 352}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:40,705] Trial 432 finished with value: 0.8153815680856591 and parameters: {'n_estimators': 445, 'eta': 0.09814772277247935, 'max_depth': 8, 'alpha': 0.9311, 'lambda': 34.69502639907629, 'max_bin': 374}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:45,450] Trial 433 finished with value: 0.7987967588224469 and parameters: {'n_estimators': 367, 'eta': 0.023612466535738508, 'max_depth': 8, 'alpha': 0.9797, 'lambda': 23.680676974321862, 'max_bin': 344}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:48,766] Trial 434 finished with value: 0.8238583193742081 and parameters: {'n_estimators': 429, 'eta': 0.09305752583693948, 'max_depth': 10, 'alpha': 0.8713000000000001, 'lambda': 13.199397041520964, 'max_bin': 364}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:52,177] Trial 435 finished with value: 0.8184662912737366 and parameters: {'n_estimators': 490, 'eta': 0.08942504780165722, 'max_depth': 7, 'alpha': 0.8977, 'lambda': 16.101307941054028, 'max_bin': 418}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:18:57,331] Trial 436 finished with value: 0.8191000643638466 and parameters: {'n_estimators': 519, 'eta': 0.03938341972975254, 'max_depth': 8, 'alpha': 0.9492, 'lambda': 9.838948922400672, 'max_bin': 380}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:00,698] Trial 437 finished with value: 0.8245391201943029 and parameters: {'n_estimators': 462, 'eta': 0.062304428926854644, 'max_depth': 10, 'alpha': 0.9216000000000001, 'lambda': 5.412592565796887, 'max_bin': 358}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:03,744] Trial 438 finished with value: 0.8196482889553298 and parameters: {'n_estimators': 414, 'eta': 0.09987157526096535, 'max_depth': 8, 'alpha': 0.937, 'lambda': 10.897769988415808, 'max_bin': 411}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:06,932] Trial 439 finished with value: 0.8184732221098947 and parameters: {'n_estimators': 480, 'eta': 0.09576521007224627, 'max_depth': 8, 'alpha': 0.5428000000000001, 'lambda': 17.290243608026458, 'max_bin': 405}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:11,474] Trial 440 finished with value: 0.8255058234272757 and parameters: {'n_estimators': 790, 'eta': 0.043976970686178625, 'max_depth': 10, 'alpha': 0.9124, 'lambda': 6.987382237615426, 'max_bin': 387}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:14,459] Trial 441 finished with value: 0.8259576199523281 and parameters: {'n_estimators': 559, 'eta': 0.0981869722255693, 'max_depth': 9, 'alpha': 0.9617, 'lambda': 11.505574205685697, 'max_bin': 371}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:17,176] Trial 442 finished with value: 0.8220428501414823 and parameters: {'n_estimators': 432, 'eta': 0.09459382440986086, 'max_depth': 8, 'alpha': 0.8857, 'lambda': 8.16830935614703, 'max_bin': 395}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:23,200] Trial 443 finished with value: 0.82511460828488 and parameters: {'n_estimators': 500, 'eta': 0.03139754617209345, 'max_depth': 10, 'alpha': 0.9836, 'lambda': 12.556363812103369, 'max_bin': 349}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:26,224] Trial 444 finished with value: 0.813374806185237 and parameters: {'n_estimators': 538, 'eta': 0.09101411132744064, 'max_depth': 8, 'alpha': 0.9375, 'lambda': 10.239434737150996, 'max_bin': 362}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:29,769] Trial 445 finished with value: 0.8218944325773364 and parameters: {'n_estimators': 388, 'eta': 0.0865862287245174, 'max_depth': 8, 'alpha': 0.8685, 'lambda': 21.256957586096053, 'max_bin': 292}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:33,810] Trial 446 finished with value: 0.8225796582042021 and parameters: {'n_estimators': 455, 'eta': 0.05836614904886913, 'max_depth': 11, 'alpha': 0.8413, 'lambda': 9.366757885864278, 'max_bin': 450}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:37,020] Trial 447 finished with value: 0.8200653945264541 and parameters: {'n_estimators': 477, 'eta': 0.0966992754827673, 'max_depth': 8, 'alpha': 0.9036000000000001, 'lambda': 13.606263298843052, 'max_bin': 336}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:39,544] Trial 448 finished with value: 0.8233362582401726 and parameters: {'n_estimators': 407, 'eta': 0.09990158827990772, 'max_depth': 10, 'alpha': 0.9610000000000001, 'lambda': 11.128805424799944, 'max_bin': 355}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:40,278] Trial 449 finished with value: 0.7849420667657591 and parameters: {'n_estimators': 50, 'eta': 0.09240596571946959, 'max_depth': 7, 'alpha': 0.4567, 'lambda': 12.117007691621104, 'max_bin': 376}. Best is trial 102 with value: 0.8341420627094955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8341\n",
      "\tBest params:\n",
      "\t\tn_estimators: 452\n",
      "\t\teta: 0.09749861254895852\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.931\n",
      "\t\tlambda: 12.113245645578496\n",
      "\t\tmax_bin: 349\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   50.000000   37.000000   36.000000   \n",
      "1                    TN  308.000000  307.000000  305.000000  305.000000   \n",
      "2                    FP    6.000000    7.000000   10.000000   11.000000   \n",
      "3                    FN   24.000000   18.000000   30.000000   30.000000   \n",
      "4              Accuracy    0.921466    0.934555    0.895288    0.892670   \n",
      "5             Precision    0.880000    0.877193    0.787234    0.765957   \n",
      "6           Sensitivity    0.647059    0.735294    0.552239    0.545455   \n",
      "7           Specificity    0.980900    0.977700    0.968300    0.965200   \n",
      "8              F1 score    0.745763    0.800000    0.649123    0.637168   \n",
      "9   F1 score (weighted)    0.916570    0.932239    0.887714    0.885213   \n",
      "10     F1 score (macro)    0.849662    0.880438    0.793792    0.787094   \n",
      "11    Balanced Accuracy    0.813975    0.856501    0.760246    0.755322   \n",
      "12                  MCC    0.712181    0.765474    0.602610    0.587710   \n",
      "13                  NPV    0.927700    0.944600    0.910400    0.910400   \n",
      "14              ROC_AUC    0.813975    0.856501    0.760246    0.755322   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    39.000000   44.000000   44.000000   44.000000   39.000000  \n",
      "1   312.000000  304.000000  302.000000  307.000000  309.000000  \n",
      "2     3.000000   11.000000   11.000000    8.000000    6.000000  \n",
      "3    28.000000   23.000000   25.000000   23.000000   28.000000  \n",
      "4     0.918848    0.910995    0.905759    0.918848    0.910995  \n",
      "5     0.928571    0.800000    0.800000    0.846154    0.866667  \n",
      "6     0.582090    0.656716    0.637681    0.656716    0.582090  \n",
      "7     0.990500    0.965100    0.964900    0.974600    0.981000  \n",
      "8     0.715596    0.721311    0.709677    0.739496    0.696429  \n",
      "9     0.911090    0.907449    0.901470    0.914677    0.903755  \n",
      "10    0.834134    0.834176    0.826714    0.845717    0.822141  \n",
      "11    0.786283    0.810898    0.801269    0.815660    0.781521  \n",
      "12    0.696072    0.673574    0.660276    0.700137    0.664220  \n",
      "13    0.917600    0.929700    0.923500    0.930300    0.916900  \n",
      "14    0.786283    0.810898    0.801269    0.815660    0.781521  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_xgb_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_xgb_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_xgb_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_xgb_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_xgb_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_xgb_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_xgb_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_xgb_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:19:43,539] Trial 450 finished with value: 0.8028297919261178 and parameters: {'n_estimators': 371, 'eta': 0.09464716224173335, 'max_depth': 10, 'alpha': 0.3899, 'lambda': 17.485920674776708, 'max_bin': 367}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:46,197] Trial 451 finished with value: 0.8089938423250829 and parameters: {'n_estimators': 515, 'eta': 0.09799923535691224, 'max_depth': 8, 'alpha': 0.9208000000000001, 'lambda': 8.66765613164701, 'max_bin': 340}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:48,965] Trial 452 finished with value: 0.8067864937451169 and parameters: {'n_estimators': 438, 'eta': 0.087847499134118, 'max_depth': 8, 'alpha': 0.9848, 'lambda': 10.083774481993574, 'max_bin': 383}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:51,876] Trial 453 finished with value: 0.8065942335673977 and parameters: {'n_estimators': 498, 'eta': 0.09656253061111109, 'max_depth': 10, 'alpha': 0.8949, 'lambda': 12.852563329875675, 'max_bin': 398}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:54,937] Trial 454 finished with value: 0.8076927493844968 and parameters: {'n_estimators': 421, 'eta': 0.07400064646225792, 'max_depth': 9, 'alpha': 0.9495, 'lambda': 6.433833623134687, 'max_bin': 391}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:19:58,160] Trial 455 finished with value: 0.8046263498227189 and parameters: {'n_estimators': 463, 'eta': 0.09322025049126206, 'max_depth': 8, 'alpha': 0.9205000000000001, 'lambda': 22.02128653693514, 'max_bin': 347}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:01,025] Trial 456 finished with value: 0.8009970559784116 and parameters: {'n_estimators': 398, 'eta': 0.05514858442533145, 'max_depth': 8, 'alpha': 0.8603000000000001, 'lambda': 1.897736873634777, 'max_bin': 359}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:06,238] Trial 457 finished with value: 0.8087958894562151 and parameters: {'n_estimators': 595, 'eta': 0.03668782621605575, 'max_depth': 10, 'alpha': 0.9969, 'lambda': 11.167357677304455, 'max_bin': 370}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:08,948] Trial 458 finished with value: 0.8051934366500578 and parameters: {'n_estimators': 536, 'eta': 0.08321535810233165, 'max_depth': 7, 'alpha': 0.9678, 'lambda': 9.387914506450802, 'max_bin': 499}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:11,894] Trial 459 finished with value: 0.805674951286972 and parameters: {'n_estimators': 347, 'eta': 0.07928131627326501, 'max_depth': 8, 'alpha': 0.9368000000000001, 'lambda': 10.607670061615185, 'max_bin': 377}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:15,137] Trial 460 finished with value: 0.8083951104151605 and parameters: {'n_estimators': 450, 'eta': 0.09999631046372269, 'max_depth': 10, 'alpha': 0.884, 'lambda': 24.777557433533943, 'max_bin': 352}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:19,383] Trial 461 finished with value: 0.8090806892873514 and parameters: {'n_estimators': 477, 'eta': 0.06213643250208541, 'max_depth': 8, 'alpha': 0.9093, 'lambda': 19.880939593766968, 'max_bin': 403}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:22,261] Trial 462 finished with value: 0.807208502581813 and parameters: {'n_estimators': 506, 'eta': 0.0896494967978293, 'max_depth': 8, 'alpha': 0.9495, 'lambda': 12.167828447454085, 'max_bin': 363}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:27,401] Trial 463 finished with value: 0.8035916464414953 and parameters: {'n_estimators': 524, 'eta': 0.05221660902201046, 'max_depth': 10, 'alpha': 0.9312, 'lambda': 26.126728126459003, 'max_bin': 382}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:29,489] Trial 464 finished with value: 0.8086839182984971 and parameters: {'n_estimators': 423, 'eta': 0.09566997042640529, 'max_depth': 9, 'alpha': 0.9722000000000001, 'lambda': 3.027408949991891, 'max_bin': 389}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:32,412] Trial 465 finished with value: 0.8107321438240878 and parameters: {'n_estimators': 487, 'eta': 0.09802967622687411, 'max_depth': 8, 'alpha': 0.8799, 'lambda': 14.609605123975289, 'max_bin': 345}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:35,210] Trial 466 finished with value: 0.8043033086481117 and parameters: {'n_estimators': 572, 'eta': 0.09220869777676936, 'max_depth': 10, 'alpha': 0.9068, 'lambda': 11.555990451253118, 'max_bin': 353}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:37,910] Trial 467 finished with value: 0.8068409172247689 and parameters: {'n_estimators': 382, 'eta': 0.09427759099838458, 'max_depth': 8, 'alpha': 0.7912, 'lambda': 10.098509963195617, 'max_bin': 373}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:40,633] Trial 468 finished with value: 0.8094666143025014 and parameters: {'n_estimators': 444, 'eta': 0.09743763862698425, 'max_depth': 8, 'alpha': 0.8522000000000001, 'lambda': 7.870409128996714, 'max_bin': 395}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:43,642] Trial 469 finished with value: 0.8087946779137457 and parameters: {'n_estimators': 464, 'eta': 0.09023622871812711, 'max_depth': 10, 'alpha': 1.0, 'lambda': 13.42321717989563, 'max_bin': 358}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:46,161] Trial 470 finished with value: 0.8050171921517357 and parameters: {'n_estimators': 546, 'eta': 0.09610650042737655, 'max_depth': 8, 'alpha': 0.9540000000000001, 'lambda': 9.0836035144667, 'max_bin': 368}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:49,415] Trial 471 finished with value: 0.8096578870674491 and parameters: {'n_estimators': 403, 'eta': 0.059157854699663684, 'max_depth': 10, 'alpha': 0.2477, 'lambda': 10.707469437244017, 'max_bin': 385}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:53,370] Trial 472 finished with value: 0.8052129786675462 and parameters: {'n_estimators': 359, 'eta': 0.047025106158008215, 'max_depth': 8, 'alpha': 0.9242, 'lambda': 12.75544323414684, 'max_bin': 362}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:56,806] Trial 473 finished with value: 0.8044554453084848 and parameters: {'n_estimators': 437, 'eta': 0.06638059657947169, 'max_depth': 8, 'alpha': 0.8974000000000001, 'lambda': 9.884343472584428, 'max_bin': 407}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:20:59,262] Trial 474 finished with value: 0.8010625108378271 and parameters: {'n_estimators': 500, 'eta': 0.09848965174127688, 'max_depth': 10, 'alpha': 0.1564, 'lambda': 11.818242860209168, 'max_bin': 348}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:03,924] Trial 475 finished with value: 0.8090957326061534 and parameters: {'n_estimators': 475, 'eta': 0.04084651026798898, 'max_depth': 8, 'alpha': 0.9729000000000001, 'lambda': 8.647055496376822, 'max_bin': 377}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:07,046] Trial 476 finished with value: 0.8072383303335396 and parameters: {'n_estimators': 456, 'eta': 0.08559045697544199, 'max_depth': 10, 'alpha': 0.9393, 'lambda': 11.152634899829568, 'max_bin': 356}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:10,087] Trial 477 finished with value: 0.8000639784639929 and parameters: {'n_estimators': 524, 'eta': 0.09428292331159503, 'max_depth': 6, 'alpha': 0.9182, 'lambda': 15.592488262115127, 'max_bin': 331}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:12,943] Trial 478 finished with value: 0.7977340667779295 and parameters: {'n_estimators': 421, 'eta': 0.07671753361874191, 'max_depth': 9, 'alpha': 0.8728, 'lambda': 9.569209369472288, 'max_bin': 399}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:17,471] Trial 479 finished with value: 0.8084671246715368 and parameters: {'n_estimators': 488, 'eta': 0.03395074181216936, 'max_depth': 8, 'alpha': 0.9533, 'lambda': 3.757543666984798, 'max_bin': 371}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:21,776] Trial 480 finished with value: 0.8059437890195534 and parameters: {'n_estimators': 558, 'eta': 0.053981528629066106, 'max_depth': 8, 'alpha': 0.8918, 'lambda': 14.477652316036746, 'max_bin': 343}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:25,024] Trial 481 finished with value: 0.7993412591177339 and parameters: {'n_estimators': 385, 'eta': 0.096269109094879, 'max_depth': 8, 'alpha': 0.9307000000000001, 'lambda': 28.973683380080388, 'max_bin': 366}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:32,063] Trial 482 finished with value: 0.8076916538670217 and parameters: {'n_estimators': 513, 'eta': 0.016844606259904304, 'max_depth': 10, 'alpha': 0.9821000000000001, 'lambda': 7.376635000978947, 'max_bin': 390}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:34,556] Trial 483 finished with value: 0.8067376628891234 and parameters: {'n_estimators': 462, 'eta': 0.09230493413747384, 'max_depth': 7, 'alpha': 0.9016000000000001, 'lambda': 5.138432527866762, 'max_bin': 380}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:39,639] Trial 484 finished with value: 0.7961759082404205 and parameters: {'n_estimators': 405, 'eta': 0.028027960929016913, 'max_depth': 8, 'alpha': 0.8162, 'lambda': 19.28093886200758, 'max_bin': 350}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:45,109] Trial 485 finished with value: 0.8114412105453216 and parameters: {'n_estimators': 445, 'eta': 0.0510427050916539, 'max_depth': 10, 'alpha': 0.9998, 'lambda': 36.842993575746476, 'max_bin': 411}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:48,112] Trial 486 finished with value: 0.8039836164199338 and parameters: {'n_estimators': 426, 'eta': 0.09994554255964513, 'max_depth': 8, 'alpha': 0.9609000000000001, 'lambda': 16.545993740794685, 'max_bin': 361}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:52,812] Trial 487 finished with value: 0.8105140928421471 and parameters: {'n_estimators': 875, 'eta': 0.058027562548662495, 'max_depth': 9, 'alpha': 0.8601000000000001, 'lambda': 23.7254146341379, 'max_bin': 355}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:21:57,324] Trial 488 finished with value: 0.8037531021831109 and parameters: {'n_estimators': 497, 'eta': 0.048220619295819535, 'max_depth': 10, 'alpha': 0.9188000000000001, 'lambda': 13.809669364164845, 'max_bin': 394}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:01,932] Trial 489 finished with value: 0.805021315537376 and parameters: {'n_estimators': 371, 'eta': 0.03781489535130233, 'max_depth': 8, 'alpha': 0.9399000000000001, 'lambda': 12.15676452620602, 'max_bin': 339}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:04,841] Trial 490 finished with value: 0.8140834795767917 and parameters: {'n_estimators': 478, 'eta': 0.08788741320662043, 'max_depth': 10, 'alpha': 0.8923000000000001, 'lambda': 10.48362255600224, 'max_bin': 386}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:07,691] Trial 491 finished with value: 0.8090874497978288 and parameters: {'n_estimators': 509, 'eta': 0.09094410849595759, 'max_depth': 8, 'alpha': 0.9707, 'lambda': 12.874962730574527, 'max_bin': 374}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:10,415] Trial 492 finished with value: 0.8108427088283422 and parameters: {'n_estimators': 536, 'eta': 0.09799940428740653, 'max_depth': 8, 'alpha': 0.8728, 'lambda': 11.268950905601216, 'max_bin': 365}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:13,071] Trial 493 finished with value: 0.8067788765391546 and parameters: {'n_estimators': 436, 'eta': 0.09509301765130443, 'max_depth': 10, 'alpha': 0.8381000000000001, 'lambda': 9.15406375888962, 'max_bin': 404}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:15,702] Trial 494 finished with value: 0.8100141389962806 and parameters: {'n_estimators': 455, 'eta': 0.09343867449191598, 'max_depth': 7, 'alpha': 0.9177000000000001, 'lambda': 8.353716428357147, 'max_bin': 353}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:18,479] Trial 495 finished with value: 0.8140023663777629 and parameters: {'n_estimators': 394, 'eta': 0.09645422213087798, 'max_depth': 8, 'alpha': 0.9478000000000001, 'lambda': 10.406721613696696, 'max_bin': 380}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:25,127] Trial 496 finished with value: 0.8060478801655174 and parameters: {'n_estimators': 488, 'eta': 0.023105382929017603, 'max_depth': 10, 'alpha': 0.9045000000000001, 'lambda': 17.786421173279184, 'max_bin': 359}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:28,024] Trial 497 finished with value: 0.8089500564693635 and parameters: {'n_estimators': 411, 'eta': 0.09832334264346551, 'max_depth': 11, 'alpha': 0.9327000000000001, 'lambda': 11.685147624487607, 'max_bin': 370}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:30,104] Trial 498 finished with value: 0.8073717303462239 and parameters: {'n_estimators': 469, 'eta': 0.0999669941502066, 'max_depth': 8, 'alpha': 0.8813000000000001, 'lambda': 1.0189929294687925, 'max_bin': 398}. Best is trial 102 with value: 0.8341420627094955.\n",
      "[I 2023-12-05 18:22:34,596] Trial 499 finished with value: 0.8055900956240547 and parameters: {'n_estimators': 548, 'eta': 0.042521407005057854, 'max_depth': 8, 'alpha': 0.9705, 'lambda': 9.847614990696531, 'max_bin': 345}. Best is trial 102 with value: 0.8341420627094955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8341\n",
      "\tBest params:\n",
      "\t\tn_estimators: 452\n",
      "\t\teta: 0.09749861254895852\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.931\n",
      "\t\tlambda: 12.113245645578496\n",
      "\t\tmax_bin: 349\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   50.000000   37.000000   36.000000   \n",
      "1                    TN  308.000000  307.000000  305.000000  305.000000   \n",
      "2                    FP    6.000000    7.000000   10.000000   11.000000   \n",
      "3                    FN   24.000000   18.000000   30.000000   30.000000   \n",
      "4              Accuracy    0.921466    0.934555    0.895288    0.892670   \n",
      "5             Precision    0.880000    0.877193    0.787234    0.765957   \n",
      "6           Sensitivity    0.647059    0.735294    0.552239    0.545455   \n",
      "7           Specificity    0.980900    0.977700    0.968300    0.965200   \n",
      "8              F1 score    0.745763    0.800000    0.649123    0.637168   \n",
      "9   F1 score (weighted)    0.916570    0.932239    0.887714    0.885213   \n",
      "10     F1 score (macro)    0.849662    0.880438    0.793792    0.787094   \n",
      "11    Balanced Accuracy    0.813975    0.856501    0.760246    0.755322   \n",
      "12                  MCC    0.712181    0.765474    0.602610    0.587710   \n",
      "13                  NPV    0.927700    0.944600    0.910400    0.910400   \n",
      "14              ROC_AUC    0.813975    0.856501    0.760246    0.755322   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    39.000000   44.000000   44.000000   44.000000   39.000000   44.000000  \n",
      "1   312.000000  304.000000  302.000000  307.000000  309.000000  305.000000  \n",
      "2     3.000000   11.000000   11.000000    8.000000    6.000000    8.000000  \n",
      "3    28.000000   23.000000   25.000000   23.000000   28.000000   25.000000  \n",
      "4     0.918848    0.910995    0.905759    0.918848    0.910995    0.913613  \n",
      "5     0.928571    0.800000    0.800000    0.846154    0.866667    0.846154  \n",
      "6     0.582090    0.656716    0.637681    0.656716    0.582090    0.637681  \n",
      "7     0.990500    0.965100    0.964900    0.974600    0.981000    0.974400  \n",
      "8     0.715596    0.721311    0.709677    0.739496    0.696429    0.727273  \n",
      "9     0.911090    0.907449    0.901470    0.914677    0.903755    0.908686  \n",
      "10    0.834134    0.834176    0.826714    0.845717    0.822141    0.837975  \n",
      "11    0.786283    0.810898    0.801269    0.815660    0.781521    0.806061  \n",
      "12    0.696072    0.673574    0.660276    0.700137    0.664220    0.686714  \n",
      "13    0.917600    0.929700    0.923500    0.930300    0.916900    0.924200  \n",
      "14    0.786283    0.810898    0.801269    0.815660    0.781521    0.806061  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_xgb_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_xgb_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_xgb_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_xgb_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_xgb_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_xgb_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_xgb_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_xgb_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>20.900000</td>\n",
       "      <td>1.969207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>152.800000</td>\n",
       "      <td>1.475730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>1.932184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1.900292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.909424</td>\n",
       "      <td>0.015423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.814649</td>\n",
       "      <td>0.066592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.625742</td>\n",
       "      <td>0.055566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.969590</td>\n",
       "      <td>0.012159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.706574</td>\n",
       "      <td>0.052507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.904556</td>\n",
       "      <td>0.016265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.826505</td>\n",
       "      <td>0.030701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.797664</td>\n",
       "      <td>0.030267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.662926</td>\n",
       "      <td>0.061208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.924470</td>\n",
       "      <td>0.010888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.797664</td>\n",
       "      <td>0.030267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP        20.900000     1.969207\n",
       "1                    TN       152.800000     1.475730\n",
       "2                    FP         4.800000     1.932184\n",
       "3                    FN        12.500000     1.900292\n",
       "4              Accuracy         0.909424     0.015423\n",
       "5             Precision         0.814649     0.066592\n",
       "6           Sensitivity         0.625742     0.055566\n",
       "7           Specificity         0.969590     0.012159\n",
       "8              F1 score         0.706574     0.052507\n",
       "9   F1 score (weighted)         0.904556     0.016265\n",
       "10     F1 score (macro)         0.826505     0.030701\n",
       "11    Balanced Accuracy         0.797664     0.030267\n",
       "12                  MCC         0.662926     0.061208\n",
       "13                  NPV         0.924470     0.010888\n",
       "14              ROC_AUC         0.797664     0.030267"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>42.100000</td>\n",
       "      <td>4.254409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>309.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>306.400000</td>\n",
       "      <td>2.836273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>2.685351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>3.717825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.921466</td>\n",
       "      <td>0.934555</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.913613</td>\n",
       "      <td>0.912304</td>\n",
       "      <td>0.012417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.839793</td>\n",
       "      <td>0.050642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.623302</td>\n",
       "      <td>0.057985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.980900</td>\n",
       "      <td>0.977700</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>0.964900</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>0.974260</td>\n",
       "      <td>0.008527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.649123</td>\n",
       "      <td>0.637168</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.739496</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.714184</td>\n",
       "      <td>0.046817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.916570</td>\n",
       "      <td>0.932239</td>\n",
       "      <td>0.887714</td>\n",
       "      <td>0.885213</td>\n",
       "      <td>0.911090</td>\n",
       "      <td>0.907449</td>\n",
       "      <td>0.901470</td>\n",
       "      <td>0.914677</td>\n",
       "      <td>0.903755</td>\n",
       "      <td>0.908686</td>\n",
       "      <td>0.906886</td>\n",
       "      <td>0.013723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.849662</td>\n",
       "      <td>0.880438</td>\n",
       "      <td>0.793792</td>\n",
       "      <td>0.787094</td>\n",
       "      <td>0.834134</td>\n",
       "      <td>0.834176</td>\n",
       "      <td>0.826714</td>\n",
       "      <td>0.845717</td>\n",
       "      <td>0.822141</td>\n",
       "      <td>0.837975</td>\n",
       "      <td>0.831184</td>\n",
       "      <td>0.026846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.813975</td>\n",
       "      <td>0.856501</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.755322</td>\n",
       "      <td>0.786283</td>\n",
       "      <td>0.810898</td>\n",
       "      <td>0.801269</td>\n",
       "      <td>0.815660</td>\n",
       "      <td>0.781521</td>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.798774</td>\n",
       "      <td>0.029592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.712181</td>\n",
       "      <td>0.765474</td>\n",
       "      <td>0.602610</td>\n",
       "      <td>0.587710</td>\n",
       "      <td>0.696072</td>\n",
       "      <td>0.673574</td>\n",
       "      <td>0.660276</td>\n",
       "      <td>0.700137</td>\n",
       "      <td>0.664220</td>\n",
       "      <td>0.686714</td>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.051614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.927700</td>\n",
       "      <td>0.944600</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>0.917600</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>0.930300</td>\n",
       "      <td>0.916900</td>\n",
       "      <td>0.924200</td>\n",
       "      <td>0.923530</td>\n",
       "      <td>0.010379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.813975</td>\n",
       "      <td>0.856501</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.755322</td>\n",
       "      <td>0.786283</td>\n",
       "      <td>0.810898</td>\n",
       "      <td>0.801269</td>\n",
       "      <td>0.815660</td>\n",
       "      <td>0.781521</td>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.798774</td>\n",
       "      <td>0.029592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   44.000000   50.000000   37.000000   36.000000   \n",
       "1                    TN  308.000000  307.000000  305.000000  305.000000   \n",
       "2                    FP    6.000000    7.000000   10.000000   11.000000   \n",
       "3                    FN   24.000000   18.000000   30.000000   30.000000   \n",
       "4              Accuracy    0.921466    0.934555    0.895288    0.892670   \n",
       "5             Precision    0.880000    0.877193    0.787234    0.765957   \n",
       "6           Sensitivity    0.647059    0.735294    0.552239    0.545455   \n",
       "7           Specificity    0.980900    0.977700    0.968300    0.965200   \n",
       "8              F1 score    0.745763    0.800000    0.649123    0.637168   \n",
       "9   F1 score (weighted)    0.916570    0.932239    0.887714    0.885213   \n",
       "10     F1 score (macro)    0.849662    0.880438    0.793792    0.787094   \n",
       "11    Balanced Accuracy    0.813975    0.856501    0.760246    0.755322   \n",
       "12                  MCC    0.712181    0.765474    0.602610    0.587710   \n",
       "13                  NPV    0.927700    0.944600    0.910400    0.910400   \n",
       "14              ROC_AUC    0.813975    0.856501    0.760246    0.755322   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    39.000000   44.000000   44.000000   44.000000   39.000000   44.000000   \n",
       "1   312.000000  304.000000  302.000000  307.000000  309.000000  305.000000   \n",
       "2     3.000000   11.000000   11.000000    8.000000    6.000000    8.000000   \n",
       "3    28.000000   23.000000   25.000000   23.000000   28.000000   25.000000   \n",
       "4     0.918848    0.910995    0.905759    0.918848    0.910995    0.913613   \n",
       "5     0.928571    0.800000    0.800000    0.846154    0.866667    0.846154   \n",
       "6     0.582090    0.656716    0.637681    0.656716    0.582090    0.637681   \n",
       "7     0.990500    0.965100    0.964900    0.974600    0.981000    0.974400   \n",
       "8     0.715596    0.721311    0.709677    0.739496    0.696429    0.727273   \n",
       "9     0.911090    0.907449    0.901470    0.914677    0.903755    0.908686   \n",
       "10    0.834134    0.834176    0.826714    0.845717    0.822141    0.837975   \n",
       "11    0.786283    0.810898    0.801269    0.815660    0.781521    0.806061   \n",
       "12    0.696072    0.673574    0.660276    0.700137    0.664220    0.686714   \n",
       "13    0.917600    0.929700    0.923500    0.930300    0.916900    0.924200   \n",
       "14    0.786283    0.810898    0.801269    0.815660    0.781521    0.806061   \n",
       "\n",
       "           ave       std  \n",
       "0    42.100000  4.254409  \n",
       "1   306.400000  2.836273  \n",
       "2     8.100000  2.685351  \n",
       "3    25.400000  3.717825  \n",
       "4     0.912304  0.012417  \n",
       "5     0.839793  0.050642  \n",
       "6     0.623302  0.057985  \n",
       "7     0.974260  0.008527  \n",
       "8     0.714184  0.046817  \n",
       "9     0.906886  0.013723  \n",
       "10    0.831184  0.026846  \n",
       "11    0.798774  0.029592  \n",
       "12    0.674897  0.051614  \n",
       "13    0.923530  0.010379  \n",
       "14    0.798774  0.029592  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.907644</td>\n",
       "      <td>0.015025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.815465</td>\n",
       "      <td>0.054811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.614922</td>\n",
       "      <td>0.087029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.969668</td>\n",
       "      <td>0.011898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.696790</td>\n",
       "      <td>0.059494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.901978</td>\n",
       "      <td>0.017317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.821127</td>\n",
       "      <td>0.033708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.792290</td>\n",
       "      <td>0.041705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.655370</td>\n",
       "      <td>0.060569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.922698</td>\n",
       "      <td>0.016032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.792290</td>\n",
       "      <td>0.041705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.907644     0.015025\n",
       "1             Precision         0.815465     0.054811\n",
       "2           Sensitivity         0.614922     0.087029\n",
       "3           Specificity         0.969668     0.011898\n",
       "4              F1 score         0.696790     0.059494\n",
       "5   F1 score (weighted)         0.901978     0.017317\n",
       "6      F1 score (macro)         0.821127     0.033708\n",
       "7     Balanced Accuracy         0.792290     0.041705\n",
       "8                   MCC         0.655370     0.060569\n",
       "9                   NPV         0.922698     0.016032\n",
       "10              ROC_AUC         0.792290     0.041705"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"logloss\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_xgb)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_xgb))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_xgb))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_xgb))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_xgb))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_xgb))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "mat_met_optimized_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:23:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[18:23:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost baseline model f1_score 0.8152 with a standard deviation of 0.0287\n",
      "XGBoost optimized model f1_score 0.8242 with a standard deviation of 0.0242\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_clf, X, Y, cv=10, scoring=\"f1_macro\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb_clf_withSemiSel.joblib']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_clf, \"OUTPUT/xgb_clf_withSemiSel.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"OUTPUT/optimized_xgb_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb_clf_withSemiSel.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        20.100000     2.806738\n",
      "1                    TN       149.700000     3.465705\n",
      "2                    FP         7.900000     3.247221\n",
      "3                    FN        13.300000     2.584140\n",
      "4              Accuracy         0.889005     0.016703\n",
      "5             Precision         0.725975     0.074897\n",
      "6           Sensitivity         0.601393     0.078644\n",
      "7           Specificity         0.949870     0.020613\n",
      "8              F1 score         0.653159     0.055457\n",
      "9   F1 score (weighted)         0.884791     0.016969\n",
      "10     F1 score (macro)         0.793489     0.031784\n",
      "11    Balanced Accuracy         0.775627     0.036139\n",
      "12                  MCC         0.594628     0.060766\n",
      "13                  NPV         0.918690     0.013762\n",
      "14              ROC_AUC         0.775627     0.036139\n",
      "CPU times: user 1.6 s, sys: 3.94 s, total: 5.54 s\n",
      "Wall time: 180 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    \n",
    "    knn_clf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:23:14,162] A new study created in memory with name: KNNClassifier\n",
      "[I 2023-12-05 18:23:14,366] Trial 0 finished with value: 0.7549543219005821 and parameters: {'n_neighbors': 16, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 0 with value: 0.7549543219005821.\n",
      "[I 2023-12-05 18:23:14,568] Trial 1 finished with value: 0.7895174787897481 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 89}. Best is trial 1 with value: 0.7895174787897481.\n",
      "[I 2023-12-05 18:23:14,770] Trial 2 finished with value: 0.7469330784926814 and parameters: {'n_neighbors': 20, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 1 with value: 0.7895174787897481.\n",
      "[I 2023-12-05 18:23:14,948] Trial 3 finished with value: 0.7561318427608248 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 94}. Best is trial 1 with value: 0.7895174787897481.\n",
      "[I 2023-12-05 18:23:15,152] Trial 4 finished with value: 0.7126597628330018 and parameters: {'n_neighbors': 26, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 1 with value: 0.7895174787897481.\n",
      "[I 2023-12-05 18:23:15,329] Trial 5 finished with value: 0.7741807734065248 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 91}. Best is trial 1 with value: 0.7895174787897481.\n",
      "[I 2023-12-05 18:23:15,507] Trial 6 finished with value: 0.7461112827696211 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 96}. Best is trial 1 with value: 0.7895174787897481.\n",
      "[I 2023-12-05 18:23:15,683] Trial 7 finished with value: 0.7741807734065248 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 1 with value: 0.7895174787897481.\n",
      "[I 2023-12-05 18:23:15,862] Trial 8 finished with value: 0.7561318427608248 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 73}. Best is trial 1 with value: 0.7895174787897481.\n",
      "[I 2023-12-05 18:23:16,131] Trial 9 finished with value: 0.7312305158218232 and parameters: {'n_neighbors': 23, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 1 with value: 0.7895174787897481.\n",
      "[I 2023-12-05 18:23:16,417] Trial 10 finished with value: 0.7913079911527402 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 10 with value: 0.7913079911527402.\n",
      "[I 2023-12-05 18:23:16,727] Trial 11 finished with value: 0.7913079911527402 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 10 with value: 0.7913079911527402.\n",
      "[I 2023-12-05 18:23:17,042] Trial 12 finished with value: 0.7913079911527402 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 10 with value: 0.7913079911527402.\n",
      "[I 2023-12-05 18:23:17,361] Trial 13 finished with value: 0.7866658354953346 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 10 with value: 0.7913079911527402.\n",
      "[I 2023-12-05 18:23:17,679] Trial 14 finished with value: 0.7890433586389165 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 10 with value: 0.7913079911527402.\n",
      "[I 2023-12-05 18:23:17,984] Trial 15 finished with value: 0.7913079911527402 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 10 with value: 0.7913079911527402.\n",
      "[I 2023-12-05 18:23:18,299] Trial 16 finished with value: 0.7830741509396957 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 10 with value: 0.7913079911527402.\n",
      "[I 2023-12-05 18:23:18,585] Trial 17 finished with value: 0.7961132320506674 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 17 with value: 0.7961132320506674.\n",
      "[I 2023-12-05 18:23:18,873] Trial 18 finished with value: 0.7963788122784062 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 18 with value: 0.7963788122784062.\n",
      "[I 2023-12-05 18:23:19,161] Trial 19 finished with value: 0.7032969493137732 and parameters: {'n_neighbors': 30, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 18 with value: 0.7963788122784062.\n",
      "[I 2023-12-05 18:23:19,447] Trial 20 finished with value: 0.7963788122784062 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 18 with value: 0.7963788122784062.\n",
      "[I 2023-12-05 18:23:19,751] Trial 21 finished with value: 0.7963788122784062 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 18 with value: 0.7963788122784062.\n",
      "[I 2023-12-05 18:23:20,071] Trial 22 finished with value: 0.7830741509396957 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 18 with value: 0.7963788122784062.\n",
      "[I 2023-12-05 18:23:20,391] Trial 23 finished with value: 0.7963788122784062 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 18 with value: 0.7963788122784062.\n",
      "[I 2023-12-05 18:23:20,706] Trial 24 finished with value: 0.7805238601275805 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 18 with value: 0.7963788122784062.\n",
      "[I 2023-12-05 18:23:20,913] Trial 25 finished with value: 0.7622369526110159 and parameters: {'n_neighbors': 19, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 80}. Best is trial 18 with value: 0.7963788122784062.\n",
      "[I 2023-12-05 18:23:21,176] Trial 26 finished with value: 0.8052232131787311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 26 with value: 0.8052232131787311.\n",
      "[I 2023-12-05 18:23:21,469] Trial 27 finished with value: 0.8052232131787311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 26 with value: 0.8052232131787311.\n",
      "[I 2023-12-05 18:23:21,755] Trial 28 finished with value: 0.8052232131787311 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 26 with value: 0.8052232131787311.\n",
      "[I 2023-12-05 18:23:21,939] Trial 29 finished with value: 0.8072474675062248 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:22,125] Trial 30 finished with value: 0.7817265305813137 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:22,311] Trial 31 finished with value: 0.8072474675062248 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:22,496] Trial 32 finished with value: 0.8072474675062248 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:22,682] Trial 33 finished with value: 0.8072474675062248 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:22,868] Trial 34 finished with value: 0.7939735113932322 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:23,054] Trial 35 finished with value: 0.8072474675062248 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:23,240] Trial 36 finished with value: 0.7865095843050125 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 35}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:23,426] Trial 37 finished with value: 0.8036855481986667 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:23,613] Trial 38 finished with value: 0.7939735113932322 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:23,799] Trial 39 finished with value: 0.7817265305813137 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 100}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:23,987] Trial 40 finished with value: 0.7628088911467539 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 29 with value: 0.8072474675062248.\n",
      "[I 2023-12-05 18:23:24,173] Trial 41 finished with value: 0.8124519528051118 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 27}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:24,359] Trial 42 finished with value: 0.8124519528051118 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:24,545] Trial 43 finished with value: 0.8124519528051118 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:24,730] Trial 44 finished with value: 0.8124519528051118 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:24,917] Trial 45 finished with value: 0.7977360480566601 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:25,104] Trial 46 finished with value: 0.8124519528051118 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:25,291] Trial 47 finished with value: 0.8124519528051118 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:25,477] Trial 48 finished with value: 0.8124519528051118 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:25,663] Trial 49 finished with value: 0.7907130433227423 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 67}. Best is trial 41 with value: 0.8124519528051118.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8125\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNClassifier\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   45.000000\n",
      "1                    TN  301.000000\n",
      "2                    FP   13.000000\n",
      "3                    FN   23.000000\n",
      "4              Accuracy    0.905759\n",
      "5             Precision    0.775862\n",
      "6           Sensitivity    0.661765\n",
      "7           Specificity    0.958600\n",
      "8              F1 score    0.714286\n",
      "9   F1 score (weighted)    0.902758\n",
      "10     F1 score (macro)    0.828930\n",
      "11    Balanced Accuracy    0.810182\n",
      "12                  MCC    0.661270\n",
      "13                  NPV    0.929000\n",
      "14              ROC_AUC    0.810182\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_knn_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_knn_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_knn_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_knn_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_knn_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_knn_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_knn_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_knn_0)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:23:25,891] Trial 50 finished with value: 0.7873120947541985 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:26,080] Trial 51 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:26,269] Trial 52 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:26,458] Trial 53 finished with value: 0.7866351197849999 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:26,647] Trial 54 finished with value: 0.7866351197849999 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:26,836] Trial 55 finished with value: 0.7920384404624442 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:27,025] Trial 56 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 47}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:27,214] Trial 57 finished with value: 0.7873120947541985 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:27,403] Trial 58 finished with value: 0.7866351197849999 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 73}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:27,592] Trial 59 finished with value: 0.7914662218533981 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:27,781] Trial 60 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 54}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:27,969] Trial 61 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:28,157] Trial 62 finished with value: 0.7873120947541985 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:28,346] Trial 63 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:28,529] Trial 64 finished with value: 0.7866351197849999 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:28,711] Trial 65 finished with value: 0.7920384404624442 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:28,891] Trial 66 finished with value: 0.7410682677496548 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:29,077] Trial 67 finished with value: 0.7275860019246796 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:29,259] Trial 68 finished with value: 0.7873120947541985 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 58}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:29,442] Trial 69 finished with value: 0.7914662218533981 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:29,625] Trial 70 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:29,809] Trial 71 finished with value: 0.7866351197849999 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:29,988] Trial 72 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 29}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:30,168] Trial 73 finished with value: 0.7837317687747384 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 63}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:30,348] Trial 74 finished with value: 0.7873120947541985 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:30,533] Trial 75 finished with value: 0.7504845735618204 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 34}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:30,718] Trial 76 finished with value: 0.7837317687747384 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:30,901] Trial 77 finished with value: 0.7914662218533981 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:31,083] Trial 78 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 38}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:31,262] Trial 79 finished with value: 0.7866351197849999 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:31,444] Trial 80 finished with value: 0.7873120947541985 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:31,623] Trial 81 finished with value: 0.7837317687747384 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 23}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:31,805] Trial 82 finished with value: 0.7837317687747384 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:31,987] Trial 83 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:32,169] Trial 84 finished with value: 0.7402495424690517 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:32,352] Trial 85 finished with value: 0.7837317687747384 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 36}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:32,534] Trial 86 finished with value: 0.7866351197849999 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:32,714] Trial 87 finished with value: 0.7873120947541985 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:32,890] Trial 88 finished with value: 0.7799227062716423 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:33,100] Trial 89 finished with value: 0.7694485933540312 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 34}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:33,283] Trial 90 finished with value: 0.7914662218533981 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:33,465] Trial 91 finished with value: 0.7837317687747384 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 22}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:33,648] Trial 92 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:33,829] Trial 93 finished with value: 0.7837317687747384 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:34,009] Trial 94 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:34,191] Trial 95 finished with value: 0.7873120947541985 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 20}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:34,374] Trial 96 finished with value: 0.7866351197849999 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:34,558] Trial 97 finished with value: 0.7873120947541985 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 90}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:34,743] Trial 98 finished with value: 0.7837317687747384 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 87}. Best is trial 41 with value: 0.8124519528051118.\n",
      "[I 2023-12-05 18:23:34,927] Trial 99 finished with value: 0.7918206369369558 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 41 with value: 0.8124519528051118.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8125\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: minkowski\n",
      "\t\tleaf_size: 27\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   45.000000   51.000000\n",
      "1                    TN  301.000000  302.000000\n",
      "2                    FP   13.000000   12.000000\n",
      "3                    FN   23.000000   17.000000\n",
      "4              Accuracy    0.905759    0.924084\n",
      "5             Precision    0.775862    0.809524\n",
      "6           Sensitivity    0.661765    0.750000\n",
      "7           Specificity    0.958600    0.961800\n",
      "8              F1 score    0.714286    0.778626\n",
      "9   F1 score (weighted)    0.902758    0.922935\n",
      "10     F1 score (macro)    0.828930    0.866406\n",
      "11    Balanced Accuracy    0.810182    0.855892\n",
      "12                  MCC    0.661270    0.733671\n",
      "13                  NPV    0.929000    0.946700\n",
      "14              ROC_AUC    0.810182    0.855892\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_knn_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_knn_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_knn_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_knn_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_knn_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_knn_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_knn_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_knn_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:23:35,158] Trial 100 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:35,342] Trial 101 finished with value: 0.8165223674727586 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:35,526] Trial 102 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:35,711] Trial 103 finished with value: 0.8105842871550065 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:35,890] Trial 104 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:36,069] Trial 105 finished with value: 0.8144999971729385 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:36,274] Trial 106 finished with value: 0.7962950748651683 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:36,454] Trial 107 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:36,638] Trial 108 finished with value: 0.8136372678960975 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:36,819] Trial 109 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:37,003] Trial 110 finished with value: 0.8136372678960975 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:37,183] Trial 111 finished with value: 0.8019490562273492 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:37,367] Trial 112 finished with value: 0.8144999971729385 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:37,550] Trial 113 finished with value: 0.8144999971729385 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:37,729] Trial 114 finished with value: 0.8136372678960975 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:37,914] Trial 115 finished with value: 0.8136372678960975 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:38,101] Trial 116 finished with value: 0.8144999971729385 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:38,284] Trial 117 finished with value: 0.8144999971729385 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:38,466] Trial 118 finished with value: 0.8144999971729385 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:38,650] Trial 119 finished with value: 0.8144999971729385 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:38,856] Trial 120 finished with value: 0.7844640367849495 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:39,041] Trial 121 finished with value: 0.8144999971729385 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:39,223] Trial 122 finished with value: 0.8054554156943654 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:39,400] Trial 123 finished with value: 0.8144999971729385 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:39,585] Trial 124 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:39,768] Trial 125 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:39,952] Trial 126 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:40,137] Trial 127 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:40,318] Trial 128 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:40,501] Trial 129 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:40,686] Trial 130 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:40,870] Trial 131 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:41,053] Trial 132 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:41,237] Trial 133 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:41,419] Trial 134 finished with value: 0.8172479701921901 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:41,602] Trial 135 finished with value: 0.8105842871550065 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:41,783] Trial 136 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:41,968] Trial 137 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:42,153] Trial 138 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:42,338] Trial 139 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:42,523] Trial 140 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:42,704] Trial 141 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:42,883] Trial 142 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:43,068] Trial 143 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:43,254] Trial 144 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:43,439] Trial 145 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:43,624] Trial 146 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:43,809] Trial 147 finished with value: 0.8184501651538458 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:44,020] Trial 148 finished with value: 0.8050272834363144 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:44,301] Trial 149 finished with value: 0.8168927360372112 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8185\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   45.000000   51.000000   37.000000\n",
      "1                    TN  301.000000  302.000000  304.000000\n",
      "2                    FP   13.000000   12.000000   11.000000\n",
      "3                    FN   23.000000   17.000000   30.000000\n",
      "4              Accuracy    0.905759    0.924084    0.892670\n",
      "5             Precision    0.775862    0.809524    0.770833\n",
      "6           Sensitivity    0.661765    0.750000    0.552239\n",
      "7           Specificity    0.958600    0.961800    0.965100\n",
      "8              F1 score    0.714286    0.778626    0.643478\n",
      "9   F1 score (weighted)    0.902758    0.922935    0.885375\n",
      "10     F1 score (macro)    0.828930    0.866406    0.790152\n",
      "11    Balanced Accuracy    0.810182    0.855892    0.758659\n",
      "12                  MCC    0.661270    0.733671    0.593549\n",
      "13                  NPV    0.929000    0.946700    0.910200\n",
      "14              ROC_AUC    0.810182    0.855892    0.758659\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "#predict\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_knn_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_knn_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_knn_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_knn_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_knn_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_knn_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_knn_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_knn_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:23:44,529] Trial 150 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:44,714] Trial 151 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:44,899] Trial 152 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 70}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:45,086] Trial 153 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:45,272] Trial 154 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:45,458] Trial 155 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:45,644] Trial 156 finished with value: 0.7855621400087999 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:45,827] Trial 157 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:46,012] Trial 158 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:46,196] Trial 159 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:46,381] Trial 160 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:46,567] Trial 161 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:46,753] Trial 162 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:46,939] Trial 163 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:47,125] Trial 164 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:47,311] Trial 165 finished with value: 0.7855621400087999 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:47,496] Trial 166 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:47,682] Trial 167 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:47,867] Trial 168 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:48,054] Trial 169 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 80}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:48,240] Trial 170 finished with value: 0.7855621400087999 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:48,426] Trial 171 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:48,612] Trial 172 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:48,798] Trial 173 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:48,984] Trial 174 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:49,274] Trial 175 finished with value: 0.8153599282690767 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:49,459] Trial 176 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:49,667] Trial 177 finished with value: 0.7779487662137816 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:49,847] Trial 178 finished with value: 0.7559158873339451 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:50,034] Trial 179 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:50,220] Trial 180 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:50,406] Trial 181 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:50,591] Trial 182 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:50,777] Trial 183 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:50,964] Trial 184 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:51,151] Trial 185 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:51,337] Trial 186 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:51,524] Trial 187 finished with value: 0.7855621400087999 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:51,711] Trial 188 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:51,898] Trial 189 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:52,085] Trial 190 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:52,270] Trial 191 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:52,450] Trial 192 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:52,638] Trial 193 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:52,825] Trial 194 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:53,010] Trial 195 finished with value: 0.7855621400087999 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:53,190] Trial 196 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:53,374] Trial 197 finished with value: 0.801643871620427 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:53,561] Trial 198 finished with value: 0.7935799445930901 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:53,749] Trial 199 finished with value: 0.7979680834512506 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8185\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   45.000000   51.000000   37.000000   42.000000\n",
      "1                    TN  301.000000  302.000000  304.000000  305.000000\n",
      "2                    FP   13.000000   12.000000   11.000000   11.000000\n",
      "3                    FN   23.000000   17.000000   30.000000   24.000000\n",
      "4              Accuracy    0.905759    0.924084    0.892670    0.908377\n",
      "5             Precision    0.775862    0.809524    0.770833    0.792453\n",
      "6           Sensitivity    0.661765    0.750000    0.552239    0.636364\n",
      "7           Specificity    0.958600    0.961800    0.965100    0.965200\n",
      "8              F1 score    0.714286    0.778626    0.643478    0.705882\n",
      "9   F1 score (weighted)    0.902758    0.922935    0.885375    0.904296\n",
      "10     F1 score (macro)    0.828930    0.866406    0.790152    0.825809\n",
      "11    Balanced Accuracy    0.810182    0.855892    0.758659    0.800777\n",
      "12                  MCC    0.661270    0.733671    0.593549    0.657891\n",
      "13                  NPV    0.929000    0.946700    0.910200    0.927100\n",
      "14              ROC_AUC    0.810182    0.855892    0.758659    0.800777\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_knn_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_knn_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_knn_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_knn_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_knn_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_knn_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_knn_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_knn_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:23:54,102] Trial 200 finished with value: 0.8168820816768889 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:54,288] Trial 201 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:54,474] Trial 202 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:54,660] Trial 203 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:54,846] Trial 204 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:55,030] Trial 205 finished with value: 0.797884281333276 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:55,215] Trial 206 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:55,422] Trial 207 finished with value: 0.7727622689123783 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:55,607] Trial 208 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:55,788] Trial 209 finished with value: 0.8080713210294759 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:55,975] Trial 210 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:56,162] Trial 211 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:56,338] Trial 212 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:56,516] Trial 213 finished with value: 0.797884281333276 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:56,698] Trial 214 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:56,874] Trial 215 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:57,057] Trial 216 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:57,242] Trial 217 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:57,428] Trial 218 finished with value: 0.8080713210294759 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:57,613] Trial 219 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:57,801] Trial 220 finished with value: 0.7585880958958369 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 61}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:57,988] Trial 221 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:58,174] Trial 222 finished with value: 0.797884281333276 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:58,360] Trial 223 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:58,547] Trial 224 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:58,727] Trial 225 finished with value: 0.7929156249874865 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:58,913] Trial 226 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:59,099] Trial 227 finished with value: 0.8080713210294759 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:59,286] Trial 228 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:59,474] Trial 229 finished with value: 0.7582898826957377 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:59,660] Trial 230 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:23:59,846] Trial 231 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:00,033] Trial 232 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:00,218] Trial 233 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:00,405] Trial 234 finished with value: 0.797884281333276 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:00,594] Trial 235 finished with value: 0.7564506603136841 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:00,876] Trial 236 finished with value: 0.8076831185811393 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:01,051] Trial 237 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:01,240] Trial 238 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:01,428] Trial 239 finished with value: 0.8080713210294759 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 53}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:01,617] Trial 240 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:01,803] Trial 241 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:01,987] Trial 242 finished with value: 0.797884281333276 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:02,174] Trial 243 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:02,388] Trial 244 finished with value: 0.7727622689123783 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:02,584] Trial 245 finished with value: 0.7998508734691654 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:02,778] Trial 246 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:02,969] Trial 247 finished with value: 0.8133552963625974 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:03,158] Trial 248 finished with value: 0.8080713210294759 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:03,347] Trial 249 finished with value: 0.797884281333276 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8185\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   51.000000   37.000000   42.000000   \n",
      "1                    TN  301.000000  302.000000  304.000000  305.000000   \n",
      "2                    FP   13.000000   12.000000   11.000000   11.000000   \n",
      "3                    FN   23.000000   17.000000   30.000000   24.000000   \n",
      "4              Accuracy    0.905759    0.924084    0.892670    0.908377   \n",
      "5             Precision    0.775862    0.809524    0.770833    0.792453   \n",
      "6           Sensitivity    0.661765    0.750000    0.552239    0.636364   \n",
      "7           Specificity    0.958600    0.961800    0.965100    0.965200   \n",
      "8              F1 score    0.714286    0.778626    0.643478    0.705882   \n",
      "9   F1 score (weighted)    0.902758    0.922935    0.885375    0.904296   \n",
      "10     F1 score (macro)    0.828930    0.866406    0.790152    0.825809   \n",
      "11    Balanced Accuracy    0.810182    0.855892    0.758659    0.800777   \n",
      "12                  MCC    0.661270    0.733671    0.593549    0.657891   \n",
      "13                  NPV    0.929000    0.946700    0.910200    0.927100   \n",
      "14              ROC_AUC    0.810182    0.855892    0.758659    0.800777   \n",
      "\n",
      "          Set4  \n",
      "0    36.000000  \n",
      "1   312.000000  \n",
      "2     3.000000  \n",
      "3    31.000000  \n",
      "4     0.910995  \n",
      "5     0.923077  \n",
      "6     0.537313  \n",
      "7     0.990500  \n",
      "8     0.679245  \n",
      "9     0.901133  \n",
      "10    0.813787  \n",
      "11    0.763895  \n",
      "12    0.662940  \n",
      "13    0.909600  \n",
      "14    0.763895  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_knn_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_knn_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_knn_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_knn_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_knn_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_knn_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_knn_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_knn_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:24:03,594] Trial 250 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:03,785] Trial 251 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:03,974] Trial 252 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:04,163] Trial 253 finished with value: 0.7997402604929345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:04,353] Trial 254 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:04,542] Trial 255 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:04,731] Trial 256 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:04,919] Trial 257 finished with value: 0.7799694002174598 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:05,107] Trial 258 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 97}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:05,295] Trial 259 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:05,484] Trial 260 finished with value: 0.7997402604929345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 49}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:05,781] Trial 261 finished with value: 0.8075971887813627 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:05,970] Trial 262 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:06,186] Trial 263 finished with value: 0.7678359072046733 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:06,371] Trial 264 finished with value: 0.7799694002174598 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:06,559] Trial 265 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:06,739] Trial 266 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:06,926] Trial 267 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:07,111] Trial 268 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:07,299] Trial 269 finished with value: 0.7997402604929345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:07,487] Trial 270 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:07,675] Trial 271 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:07,863] Trial 272 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:08,046] Trial 273 finished with value: 0.7799694002174598 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:08,231] Trial 274 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:08,418] Trial 275 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:08,598] Trial 276 finished with value: 0.7997402604929345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 82}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:08,787] Trial 277 finished with value: 0.7969665705379796 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 66}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:08,977] Trial 278 finished with value: 0.7997402604929345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:09,166] Trial 279 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:09,354] Trial 280 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:09,541] Trial 281 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:09,758] Trial 282 finished with value: 0.7633415700029659 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:10,040] Trial 283 finished with value: 0.8091860264351647 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:10,222] Trial 284 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:10,413] Trial 285 finished with value: 0.7997402604929345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:10,603] Trial 286 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:10,791] Trial 287 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:10,979] Trial 288 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:11,169] Trial 289 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:11,359] Trial 290 finished with value: 0.7997402604929345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:11,549] Trial 291 finished with value: 0.7799694002174598 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:11,738] Trial 292 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:11,929] Trial 293 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:12,118] Trial 294 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:12,308] Trial 295 finished with value: 0.7627677961579244 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:12,499] Trial 296 finished with value: 0.7997402604929345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 57}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:12,685] Trial 297 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:12,868] Trial 298 finished with value: 0.7958114437911938 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:13,058] Trial 299 finished with value: 0.7961219625966189 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8185\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   51.000000   37.000000   42.000000   \n",
      "1                    TN  301.000000  302.000000  304.000000  305.000000   \n",
      "2                    FP   13.000000   12.000000   11.000000   11.000000   \n",
      "3                    FN   23.000000   17.000000   30.000000   24.000000   \n",
      "4              Accuracy    0.905759    0.924084    0.892670    0.908377   \n",
      "5             Precision    0.775862    0.809524    0.770833    0.792453   \n",
      "6           Sensitivity    0.661765    0.750000    0.552239    0.636364   \n",
      "7           Specificity    0.958600    0.961800    0.965100    0.965200   \n",
      "8              F1 score    0.714286    0.778626    0.643478    0.705882   \n",
      "9   F1 score (weighted)    0.902758    0.922935    0.885375    0.904296   \n",
      "10     F1 score (macro)    0.828930    0.866406    0.790152    0.825809   \n",
      "11    Balanced Accuracy    0.810182    0.855892    0.758659    0.800777   \n",
      "12                  MCC    0.661270    0.733671    0.593549    0.657891   \n",
      "13                  NPV    0.929000    0.946700    0.910200    0.927100   \n",
      "14              ROC_AUC    0.810182    0.855892    0.758659    0.800777   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    36.000000   45.000000  \n",
      "1   312.000000  301.000000  \n",
      "2     3.000000   14.000000  \n",
      "3    31.000000   22.000000  \n",
      "4     0.910995    0.905759  \n",
      "5     0.923077    0.762712  \n",
      "6     0.537313    0.671642  \n",
      "7     0.990500    0.955600  \n",
      "8     0.679245    0.714286  \n",
      "9     0.901133    0.903358  \n",
      "10    0.813787    0.828930  \n",
      "11    0.763895    0.813599  \n",
      "12    0.662940    0.660039  \n",
      "13    0.909600    0.931900  \n",
      "14    0.763895    0.813599  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_knn_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_knn_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_knn_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_knn_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_knn_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_knn_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_knn_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_knn_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:24:13,309] Trial 300 finished with value: 0.7775400470214952 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:13,500] Trial 301 finished with value: 0.7869968798931384 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:13,718] Trial 302 finished with value: 0.7537198118593118 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:13,902] Trial 303 finished with value: 0.7668406958807561 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:14,198] Trial 304 finished with value: 0.7929826096091726 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:14,394] Trial 305 finished with value: 0.7860076371255657 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:14,590] Trial 306 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:14,785] Trial 307 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:14,982] Trial 308 finished with value: 0.7775400470214952 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:15,178] Trial 309 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:15,375] Trial 310 finished with value: 0.7869968798931384 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:15,573] Trial 311 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:15,769] Trial 312 finished with value: 0.7869968798931384 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:15,966] Trial 313 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:16,163] Trial 314 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:16,360] Trial 315 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:16,557] Trial 316 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 72}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:16,755] Trial 317 finished with value: 0.7775400470214952 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:16,953] Trial 318 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:17,150] Trial 319 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:17,347] Trial 320 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:17,539] Trial 321 finished with value: 0.7869968798931384 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:17,757] Trial 322 finished with value: 0.7657386071351358 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:17,949] Trial 323 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:18,139] Trial 324 finished with value: 0.7869968798931384 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:18,427] Trial 325 finished with value: 0.7929826096091726 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:18,610] Trial 326 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:18,799] Trial 327 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:18,989] Trial 328 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:19,179] Trial 329 finished with value: 0.7860076371255657 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:19,366] Trial 330 finished with value: 0.7869968798931384 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:19,556] Trial 331 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:19,747] Trial 332 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:19,937] Trial 333 finished with value: 0.7775400470214952 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:20,126] Trial 334 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:20,315] Trial 335 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:20,505] Trial 336 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:20,693] Trial 337 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:20,885] Trial 338 finished with value: 0.7869968798931384 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 77}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:21,075] Trial 339 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:21,262] Trial 340 finished with value: 0.7869968798931384 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:21,444] Trial 341 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:21,661] Trial 342 finished with value: 0.7675804344662349 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:21,851] Trial 343 finished with value: 0.7775400470214952 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:22,033] Trial 344 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:22,218] Trial 345 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:22,507] Trial 346 finished with value: 0.7917668792907545 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:22,688] Trial 347 finished with value: 0.7764002833254298 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:22,878] Trial 348 finished with value: 0.7787931175918988 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:23,070] Trial 349 finished with value: 0.7775400470214952 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8185\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   51.000000   37.000000   42.000000   \n",
      "1                    TN  301.000000  302.000000  304.000000  305.000000   \n",
      "2                    FP   13.000000   12.000000   11.000000   11.000000   \n",
      "3                    FN   23.000000   17.000000   30.000000   24.000000   \n",
      "4              Accuracy    0.905759    0.924084    0.892670    0.908377   \n",
      "5             Precision    0.775862    0.809524    0.770833    0.792453   \n",
      "6           Sensitivity    0.661765    0.750000    0.552239    0.636364   \n",
      "7           Specificity    0.958600    0.961800    0.965100    0.965200   \n",
      "8              F1 score    0.714286    0.778626    0.643478    0.705882   \n",
      "9   F1 score (weighted)    0.902758    0.922935    0.885375    0.904296   \n",
      "10     F1 score (macro)    0.828930    0.866406    0.790152    0.825809   \n",
      "11    Balanced Accuracy    0.810182    0.855892    0.758659    0.800777   \n",
      "12                  MCC    0.661270    0.733671    0.593549    0.657891   \n",
      "13                  NPV    0.929000    0.946700    0.910200    0.927100   \n",
      "14              ROC_AUC    0.810182    0.855892    0.758659    0.800777   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    36.000000   45.000000   41.000000  \n",
      "1   312.000000  301.000000  305.000000  \n",
      "2     3.000000   14.000000    8.000000  \n",
      "3    31.000000   22.000000   28.000000  \n",
      "4     0.910995    0.905759    0.905759  \n",
      "5     0.923077    0.762712    0.836735  \n",
      "6     0.537313    0.671642    0.594203  \n",
      "7     0.990500    0.955600    0.974400  \n",
      "8     0.679245    0.714286    0.694915  \n",
      "9     0.901133    0.903358    0.899231  \n",
      "10    0.813787    0.828930    0.819594  \n",
      "11    0.763895    0.813599    0.784322  \n",
      "12    0.662940    0.660039    0.654210  \n",
      "13    0.909600    0.931900    0.915900  \n",
      "14    0.763895    0.813599    0.784322  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_knn_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_knn_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_knn_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_knn_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_knn_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_knn_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_knn_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_knn_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:24:23,325] Trial 350 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:23,513] Trial 351 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:23,705] Trial 352 finished with value: 0.7994817932067428 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:23,894] Trial 353 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:24,085] Trial 354 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:24,277] Trial 355 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:24,467] Trial 356 finished with value: 0.7994817932067428 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:24,658] Trial 357 finished with value: 0.7926431090466546 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:24,850] Trial 358 finished with value: 0.7717210284871674 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:25,067] Trial 359 finished with value: 0.7828850784824612 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:25,261] Trial 360 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 40}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:25,450] Trial 361 finished with value: 0.7926431090466546 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:25,637] Trial 362 finished with value: 0.7801172718551033 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:25,828] Trial 363 finished with value: 0.77210226412275 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:26,020] Trial 364 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:26,210] Trial 365 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:26,402] Trial 366 finished with value: 0.7994817932067428 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:26,594] Trial 367 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:26,905] Trial 368 finished with value: 0.7956926706722072 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:27,088] Trial 369 finished with value: 0.7801172718551033 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:27,280] Trial 370 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:27,471] Trial 371 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:27,657] Trial 372 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:27,849] Trial 373 finished with value: 0.7994817932067428 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:28,041] Trial 374 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 88}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:28,233] Trial 375 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 93}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:28,426] Trial 376 finished with value: 0.7994817932067428 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:28,616] Trial 377 finished with value: 0.7801172718551033 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:28,835] Trial 378 finished with value: 0.7598630517052769 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:29,022] Trial 379 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:29,204] Trial 380 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:29,386] Trial 381 finished with value: 0.7904218252809769 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 55}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:29,574] Trial 382 finished with value: 0.7994817932067428 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:29,758] Trial 383 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:29,950] Trial 384 finished with value: 0.7531600972874773 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:30,141] Trial 385 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:30,331] Trial 386 finished with value: 0.7994817932067428 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:30,525] Trial 387 finished with value: 0.7801172718551033 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:30,717] Trial 388 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 51}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:31,008] Trial 389 finished with value: 0.8009773436048828 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:31,193] Trial 390 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:31,385] Trial 391 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:31,573] Trial 392 finished with value: 0.7484103141967994 and parameters: {'n_neighbors': 25, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:31,756] Trial 393 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:31,949] Trial 394 finished with value: 0.7942025025820885 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:32,141] Trial 395 finished with value: 0.7994817932067428 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:32,334] Trial 396 finished with value: 0.7962632834532807 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 85}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:32,524] Trial 397 finished with value: 0.7560089798591314 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 63}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:32,734] Trial 398 finished with value: 0.7693602447240186 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:32,927] Trial 399 finished with value: 0.7994817932067428 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8185\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   51.000000   37.000000   42.000000   \n",
      "1                    TN  301.000000  302.000000  304.000000  305.000000   \n",
      "2                    FP   13.000000   12.000000   11.000000   11.000000   \n",
      "3                    FN   23.000000   17.000000   30.000000   24.000000   \n",
      "4              Accuracy    0.905759    0.924084    0.892670    0.908377   \n",
      "5             Precision    0.775862    0.809524    0.770833    0.792453   \n",
      "6           Sensitivity    0.661765    0.750000    0.552239    0.636364   \n",
      "7           Specificity    0.958600    0.961800    0.965100    0.965200   \n",
      "8              F1 score    0.714286    0.778626    0.643478    0.705882   \n",
      "9   F1 score (weighted)    0.902758    0.922935    0.885375    0.904296   \n",
      "10     F1 score (macro)    0.828930    0.866406    0.790152    0.825809   \n",
      "11    Balanced Accuracy    0.810182    0.855892    0.758659    0.800777   \n",
      "12                  MCC    0.661270    0.733671    0.593549    0.657891   \n",
      "13                  NPV    0.929000    0.946700    0.910200    0.927100   \n",
      "14              ROC_AUC    0.810182    0.855892    0.758659    0.800777   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    36.000000   45.000000   41.000000   39.000000  \n",
      "1   312.000000  301.000000  305.000000  303.000000  \n",
      "2     3.000000   14.000000    8.000000   12.000000  \n",
      "3    31.000000   22.000000   28.000000   28.000000  \n",
      "4     0.910995    0.905759    0.905759    0.895288  \n",
      "5     0.923077    0.762712    0.836735    0.764706  \n",
      "6     0.537313    0.671642    0.594203    0.582090  \n",
      "7     0.990500    0.955600    0.974400    0.961900  \n",
      "8     0.679245    0.714286    0.694915    0.661017  \n",
      "9     0.901133    0.903358    0.899231    0.889486  \n",
      "10    0.813787    0.828930    0.819594    0.799549  \n",
      "11    0.763895    0.813599    0.784322    0.771997  \n",
      "12    0.662940    0.660039    0.654210    0.608258  \n",
      "13    0.909600    0.931900    0.915900    0.915400  \n",
      "14    0.763895    0.813599    0.784322    0.771997  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_knn_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_knn_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_knn_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_knn_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_knn_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_knn_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_knn_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_knn_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:24:33,177] Trial 400 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:33,357] Trial 401 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:33,542] Trial 402 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:33,728] Trial 403 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:33,914] Trial 404 finished with value: 0.801811109296373 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:34,100] Trial 405 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:34,280] Trial 406 finished with value: 0.7952399522136323 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:34,464] Trial 407 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:34,638] Trial 408 finished with value: 0.8006058254587742 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:34,814] Trial 409 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:34,999] Trial 410 finished with value: 0.742023431497366 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:35,280] Trial 411 finished with value: 0.8017894599890042 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:35,451] Trial 412 finished with value: 0.7796089036238112 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:35,624] Trial 413 finished with value: 0.7971374265283229 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:35,809] Trial 414 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:35,992] Trial 415 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:36,167] Trial 416 finished with value: 0.8006058254587742 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:36,379] Trial 417 finished with value: 0.791806934931589 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 21}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:36,567] Trial 418 finished with value: 0.7456198403187818 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:36,752] Trial 419 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:36,936] Trial 420 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:37,122] Trial 421 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:37,307] Trial 422 finished with value: 0.8006058254587742 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:37,483] Trial 423 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:37,668] Trial 424 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:37,842] Trial 425 finished with value: 0.801811109296373 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:38,025] Trial 426 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:38,201] Trial 427 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:38,387] Trial 428 finished with value: 0.8006058254587742 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:38,574] Trial 429 finished with value: 0.7952399522136323 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:38,760] Trial 430 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:38,946] Trial 431 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:39,124] Trial 432 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:39,309] Trial 433 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:39,593] Trial 434 finished with value: 0.8047887596903264 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:39,768] Trial 435 finished with value: 0.8006058254587742 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:39,949] Trial 436 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:40,158] Trial 437 finished with value: 0.7839031290078096 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:40,334] Trial 438 finished with value: 0.7656350890064381 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:40,517] Trial 439 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 36}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:40,701] Trial 440 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:40,888] Trial 441 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:41,074] Trial 442 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:41,259] Trial 443 finished with value: 0.7952399522136323 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:41,441] Trial 444 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:41,628] Trial 445 finished with value: 0.8006058254587742 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:41,809] Trial 446 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:41,984] Trial 447 finished with value: 0.7985628371497454 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 23}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:42,159] Trial 448 finished with value: 0.799456771506207 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:42,344] Trial 449 finished with value: 0.8006058254587742 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8185\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   51.000000   37.000000   42.000000   \n",
      "1                    TN  301.000000  302.000000  304.000000  305.000000   \n",
      "2                    FP   13.000000   12.000000   11.000000   11.000000   \n",
      "3                    FN   23.000000   17.000000   30.000000   24.000000   \n",
      "4              Accuracy    0.905759    0.924084    0.892670    0.908377   \n",
      "5             Precision    0.775862    0.809524    0.770833    0.792453   \n",
      "6           Sensitivity    0.661765    0.750000    0.552239    0.636364   \n",
      "7           Specificity    0.958600    0.961800    0.965100    0.965200   \n",
      "8              F1 score    0.714286    0.778626    0.643478    0.705882   \n",
      "9   F1 score (weighted)    0.902758    0.922935    0.885375    0.904296   \n",
      "10     F1 score (macro)    0.828930    0.866406    0.790152    0.825809   \n",
      "11    Balanced Accuracy    0.810182    0.855892    0.758659    0.800777   \n",
      "12                  MCC    0.661270    0.733671    0.593549    0.657891   \n",
      "13                  NPV    0.929000    0.946700    0.910200    0.927100   \n",
      "14              ROC_AUC    0.810182    0.855892    0.758659    0.800777   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    36.000000   45.000000   41.000000   39.000000   39.000000  \n",
      "1   312.000000  301.000000  305.000000  303.000000  305.000000  \n",
      "2     3.000000   14.000000    8.000000   12.000000   10.000000  \n",
      "3    31.000000   22.000000   28.000000   28.000000   28.000000  \n",
      "4     0.910995    0.905759    0.905759    0.895288    0.900524  \n",
      "5     0.923077    0.762712    0.836735    0.764706    0.795918  \n",
      "6     0.537313    0.671642    0.594203    0.582090    0.582090  \n",
      "7     0.990500    0.955600    0.974400    0.961900    0.968300  \n",
      "8     0.679245    0.714286    0.694915    0.661017    0.672414  \n",
      "9     0.901133    0.903358    0.899231    0.889486    0.894187  \n",
      "10    0.813787    0.828930    0.819594    0.799549    0.806886  \n",
      "11    0.763895    0.813599    0.784322    0.771997    0.775172  \n",
      "12    0.662940    0.660039    0.654210    0.608258    0.625902  \n",
      "13    0.909600    0.931900    0.915900    0.915400    0.915900  \n",
      "14    0.763895    0.813599    0.784322    0.771997    0.775172  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_knn_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_knn_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_knn_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_knn_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_knn_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_knn_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_knn_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_knn_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:24:42,603] Trial 450 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:42,795] Trial 451 finished with value: 0.7796492706406007 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:42,987] Trial 452 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:43,177] Trial 453 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:43,361] Trial 454 finished with value: 0.7825018183486828 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:43,553] Trial 455 finished with value: 0.7994655488245405 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:43,772] Trial 456 finished with value: 0.7747235589207438 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:43,965] Trial 457 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 37}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:44,261] Trial 458 finished with value: 0.8029303960175559 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:44,444] Trial 459 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:44,636] Trial 460 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:44,825] Trial 461 finished with value: 0.8040503349809771 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:45,007] Trial 462 finished with value: 0.7796492706406007 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:45,199] Trial 463 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 59}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:45,392] Trial 464 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:45,584] Trial 465 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 20}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:45,776] Trial 466 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:45,968] Trial 467 finished with value: 0.8040503349809771 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:46,160] Trial 468 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:46,353] Trial 469 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:46,547] Trial 470 finished with value: 0.7535587940503892 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:46,738] Trial 471 finished with value: 0.7683004755112167 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:46,928] Trial 472 finished with value: 0.7796492706406007 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:47,118] Trial 473 finished with value: 0.8040503349809771 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:47,335] Trial 474 finished with value: 0.7831875601421656 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 100}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:47,528] Trial 475 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 33}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:47,719] Trial 476 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:47,909] Trial 477 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:48,214] Trial 478 finished with value: 0.7987834142462388 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:48,395] Trial 479 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:48,587] Trial 480 finished with value: 0.8040503349809771 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:48,780] Trial 481 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 24}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:48,974] Trial 482 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 49}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:49,167] Trial 483 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 69}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:49,360] Trial 484 finished with value: 0.8040503349809771 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:49,554] Trial 485 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 22}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:49,747] Trial 486 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 77}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:49,938] Trial 487 finished with value: 0.7994655488245405 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 35}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:50,122] Trial 488 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:50,316] Trial 489 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:50,509] Trial 490 finished with value: 0.8040503349809771 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 30}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:50,693] Trial 491 finished with value: 0.7796492706406007 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 27}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:50,886] Trial 492 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 32}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:51,106] Trial 493 finished with value: 0.7831875601421656 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:51,299] Trial 494 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 26}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:51,493] Trial 495 finished with value: 0.7796492706406007 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:51,687] Trial 496 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 31}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:51,877] Trial 497 finished with value: 0.8040503349809771 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 28}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:52,067] Trial 498 finished with value: 0.7960779891272838 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 25}. Best is trial 100 with value: 0.8184501651538458.\n",
      "[I 2023-12-05 18:24:52,261] Trial 499 finished with value: 0.7918611689944357 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 38}. Best is trial 100 with value: 0.8184501651538458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8185\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 9\n",
      "\t\tweights: distance\n",
      "\t\tmetric: euclidean\n",
      "\t\tleaf_size: 29\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   51.000000   37.000000   42.000000   \n",
      "1                    TN  301.000000  302.000000  304.000000  305.000000   \n",
      "2                    FP   13.000000   12.000000   11.000000   11.000000   \n",
      "3                    FN   23.000000   17.000000   30.000000   24.000000   \n",
      "4              Accuracy    0.905759    0.924084    0.892670    0.908377   \n",
      "5             Precision    0.775862    0.809524    0.770833    0.792453   \n",
      "6           Sensitivity    0.661765    0.750000    0.552239    0.636364   \n",
      "7           Specificity    0.958600    0.961800    0.965100    0.965200   \n",
      "8              F1 score    0.714286    0.778626    0.643478    0.705882   \n",
      "9   F1 score (weighted)    0.902758    0.922935    0.885375    0.904296   \n",
      "10     F1 score (macro)    0.828930    0.866406    0.790152    0.825809   \n",
      "11    Balanced Accuracy    0.810182    0.855892    0.758659    0.800777   \n",
      "12                  MCC    0.661270    0.733671    0.593549    0.657891   \n",
      "13                  NPV    0.929000    0.946700    0.910200    0.927100   \n",
      "14              ROC_AUC    0.810182    0.855892    0.758659    0.800777   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    36.000000   45.000000   41.000000   39.000000   39.000000   41.000000  \n",
      "1   312.000000  301.000000  305.000000  303.000000  305.000000  308.000000  \n",
      "2     3.000000   14.000000    8.000000   12.000000   10.000000    5.000000  \n",
      "3    31.000000   22.000000   28.000000   28.000000   28.000000   28.000000  \n",
      "4     0.910995    0.905759    0.905759    0.895288    0.900524    0.913613  \n",
      "5     0.923077    0.762712    0.836735    0.764706    0.795918    0.891304  \n",
      "6     0.537313    0.671642    0.594203    0.582090    0.582090    0.594203  \n",
      "7     0.990500    0.955600    0.974400    0.961900    0.968300    0.984000  \n",
      "8     0.679245    0.714286    0.694915    0.661017    0.672414    0.713043  \n",
      "9     0.901133    0.903358    0.899231    0.889486    0.894187    0.906505  \n",
      "10    0.813787    0.828930    0.819594    0.799549    0.806886    0.831098  \n",
      "11    0.763895    0.813599    0.784322    0.771997    0.775172    0.789114  \n",
      "12    0.662940    0.660039    0.654210    0.608258    0.625902    0.683514  \n",
      "13    0.909600    0.931900    0.915900    0.915400    0.915900    0.916700  \n",
      "14    0.763895    0.813599    0.784322    0.771997    0.775172    0.789114  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_knn_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_knn_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_knn_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_knn_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_knn_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_knn_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_knn_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_knn_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2.708013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>151.200000</td>\n",
       "      <td>2.529822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>2.913570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>2.412928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.901571</td>\n",
       "      <td>0.020172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.770886</td>\n",
       "      <td>0.085266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.628242</td>\n",
       "      <td>0.073777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.959470</td>\n",
       "      <td>0.018366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.689582</td>\n",
       "      <td>0.065432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.897549</td>\n",
       "      <td>0.020561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.815526</td>\n",
       "      <td>0.038433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.793843</td>\n",
       "      <td>0.038959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.638342</td>\n",
       "      <td>0.076679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.924370</td>\n",
       "      <td>0.013731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.793843</td>\n",
       "      <td>0.038959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP        21.000000     2.708013\n",
       "1                    TN       151.200000     2.529822\n",
       "2                    FP         6.400000     2.913570\n",
       "3                    FN        12.400000     2.412928\n",
       "4              Accuracy         0.901571     0.020172\n",
       "5             Precision         0.770886     0.085266\n",
       "6           Sensitivity         0.628242     0.073777\n",
       "7           Specificity         0.959470     0.018366\n",
       "8              F1 score         0.689582     0.065432\n",
       "9   F1 score (weighted)         0.897549     0.020561\n",
       "10     F1 score (macro)         0.815526     0.038433\n",
       "11    Balanced Accuracy         0.793843     0.038959\n",
       "12                  MCC         0.638342     0.076679\n",
       "13                  NPV         0.924370     0.013731\n",
       "14              ROC_AUC         0.793843     0.038959"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>41.600000</td>\n",
       "      <td>4.452215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>312.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>305.000000</td>\n",
       "      <td>308.000000</td>\n",
       "      <td>304.600000</td>\n",
       "      <td>3.373096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>3.541814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>4.306326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.924084</td>\n",
       "      <td>0.892670</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.910995</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.895288</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.913613</td>\n",
       "      <td>0.906283</td>\n",
       "      <td>0.009052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.812312</td>\n",
       "      <td>0.055329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.537313</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.582090</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.616191</td>\n",
       "      <td>0.064177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>0.961800</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>0.990500</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.974400</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.968540</td>\n",
       "      <td>0.011227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.778626</td>\n",
       "      <td>0.643478</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.679245</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.697719</td>\n",
       "      <td>0.037528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.902758</td>\n",
       "      <td>0.922935</td>\n",
       "      <td>0.885375</td>\n",
       "      <td>0.904296</td>\n",
       "      <td>0.901133</td>\n",
       "      <td>0.903358</td>\n",
       "      <td>0.899231</td>\n",
       "      <td>0.889486</td>\n",
       "      <td>0.894187</td>\n",
       "      <td>0.906505</td>\n",
       "      <td>0.900926</td>\n",
       "      <td>0.010313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.828930</td>\n",
       "      <td>0.866406</td>\n",
       "      <td>0.790152</td>\n",
       "      <td>0.825809</td>\n",
       "      <td>0.813787</td>\n",
       "      <td>0.828930</td>\n",
       "      <td>0.819594</td>\n",
       "      <td>0.799549</td>\n",
       "      <td>0.806886</td>\n",
       "      <td>0.831098</td>\n",
       "      <td>0.821114</td>\n",
       "      <td>0.021022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.810182</td>\n",
       "      <td>0.855892</td>\n",
       "      <td>0.758659</td>\n",
       "      <td>0.800777</td>\n",
       "      <td>0.763895</td>\n",
       "      <td>0.813599</td>\n",
       "      <td>0.784322</td>\n",
       "      <td>0.771997</td>\n",
       "      <td>0.775172</td>\n",
       "      <td>0.789114</td>\n",
       "      <td>0.792361</td>\n",
       "      <td>0.029073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.661270</td>\n",
       "      <td>0.733671</td>\n",
       "      <td>0.593549</td>\n",
       "      <td>0.657891</td>\n",
       "      <td>0.662940</td>\n",
       "      <td>0.660039</td>\n",
       "      <td>0.654210</td>\n",
       "      <td>0.608258</td>\n",
       "      <td>0.625902</td>\n",
       "      <td>0.683514</td>\n",
       "      <td>0.654124</td>\n",
       "      <td>0.039355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.910200</td>\n",
       "      <td>0.927100</td>\n",
       "      <td>0.909600</td>\n",
       "      <td>0.931900</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.915400</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.916700</td>\n",
       "      <td>0.921840</td>\n",
       "      <td>0.011652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.810182</td>\n",
       "      <td>0.855892</td>\n",
       "      <td>0.758659</td>\n",
       "      <td>0.800777</td>\n",
       "      <td>0.763895</td>\n",
       "      <td>0.813599</td>\n",
       "      <td>0.784322</td>\n",
       "      <td>0.771997</td>\n",
       "      <td>0.775172</td>\n",
       "      <td>0.789114</td>\n",
       "      <td>0.792361</td>\n",
       "      <td>0.029073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   45.000000   51.000000   37.000000   42.000000   \n",
       "1                    TN  301.000000  302.000000  304.000000  305.000000   \n",
       "2                    FP   13.000000   12.000000   11.000000   11.000000   \n",
       "3                    FN   23.000000   17.000000   30.000000   24.000000   \n",
       "4              Accuracy    0.905759    0.924084    0.892670    0.908377   \n",
       "5             Precision    0.775862    0.809524    0.770833    0.792453   \n",
       "6           Sensitivity    0.661765    0.750000    0.552239    0.636364   \n",
       "7           Specificity    0.958600    0.961800    0.965100    0.965200   \n",
       "8              F1 score    0.714286    0.778626    0.643478    0.705882   \n",
       "9   F1 score (weighted)    0.902758    0.922935    0.885375    0.904296   \n",
       "10     F1 score (macro)    0.828930    0.866406    0.790152    0.825809   \n",
       "11    Balanced Accuracy    0.810182    0.855892    0.758659    0.800777   \n",
       "12                  MCC    0.661270    0.733671    0.593549    0.657891   \n",
       "13                  NPV    0.929000    0.946700    0.910200    0.927100   \n",
       "14              ROC_AUC    0.810182    0.855892    0.758659    0.800777   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    36.000000   45.000000   41.000000   39.000000   39.000000   41.000000   \n",
       "1   312.000000  301.000000  305.000000  303.000000  305.000000  308.000000   \n",
       "2     3.000000   14.000000    8.000000   12.000000   10.000000    5.000000   \n",
       "3    31.000000   22.000000   28.000000   28.000000   28.000000   28.000000   \n",
       "4     0.910995    0.905759    0.905759    0.895288    0.900524    0.913613   \n",
       "5     0.923077    0.762712    0.836735    0.764706    0.795918    0.891304   \n",
       "6     0.537313    0.671642    0.594203    0.582090    0.582090    0.594203   \n",
       "7     0.990500    0.955600    0.974400    0.961900    0.968300    0.984000   \n",
       "8     0.679245    0.714286    0.694915    0.661017    0.672414    0.713043   \n",
       "9     0.901133    0.903358    0.899231    0.889486    0.894187    0.906505   \n",
       "10    0.813787    0.828930    0.819594    0.799549    0.806886    0.831098   \n",
       "11    0.763895    0.813599    0.784322    0.771997    0.775172    0.789114   \n",
       "12    0.662940    0.660039    0.654210    0.608258    0.625902    0.683514   \n",
       "13    0.909600    0.931900    0.915900    0.915400    0.915900    0.916700   \n",
       "14    0.763895    0.813599    0.784322    0.771997    0.775172    0.789114   \n",
       "\n",
       "           ave       std  \n",
       "0    41.600000  4.452215  \n",
       "1   304.600000  3.373096  \n",
       "2     9.900000  3.541814  \n",
       "3    25.900000  4.306326  \n",
       "4     0.906283  0.009052  \n",
       "5     0.812312  0.055329  \n",
       "6     0.616191  0.064177  \n",
       "7     0.968540  0.011227  \n",
       "8     0.697719  0.037528  \n",
       "9     0.900926  0.010313  \n",
       "10    0.821114  0.021022  \n",
       "11    0.792361  0.029073  \n",
       "12    0.654124  0.039355  \n",
       "13    0.921840  0.011652  \n",
       "14    0.792361  0.029073  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.899372</td>\n",
       "      <td>0.019882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.767114</td>\n",
       "      <td>0.073523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.614031</td>\n",
       "      <td>0.081899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.959772</td>\n",
       "      <td>0.015585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.679259</td>\n",
       "      <td>0.068022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.894673</td>\n",
       "      <td>0.021137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.809764</td>\n",
       "      <td>0.039527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.042278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.627766</td>\n",
       "      <td>0.076886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.921742</td>\n",
       "      <td>0.015243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.786900</td>\n",
       "      <td>0.042278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.899372     0.019882\n",
       "1             Precision         0.767114     0.073523\n",
       "2           Sensitivity         0.614031     0.081899\n",
       "3           Specificity         0.959772     0.015585\n",
       "4              F1 score         0.679259     0.068022\n",
       "5   F1 score (weighted)         0.894673     0.021137\n",
       "6      F1 score (macro)         0.809764     0.039527\n",
       "7     Balanced Accuracy         0.786900     0.042278\n",
       "8                   MCC         0.627766     0.076886\n",
       "9                   NPV         0.921742     0.015243\n",
       "10              ROC_AUC         0.786900     0.042278"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_knn)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_knn))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_knn))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_knn))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_knn))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_knn))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "mat_met_optimized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model f1_score 0.7943 with a standard deviation of 0.0358\n",
      "KNN optimized model f1_score 0.8170 with a standard deviation of 0.0435\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"KNN baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn_clf_withSemiSel.joblib']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_clf, \"OUTPUT/knn_clf_withSemiSel.joblib\")\n",
    "#joblib.dump(optimized_knn, \"OUTPUT/optimized_knn_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn_clf_withSemiSel.joblib\")\n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        16.300000     2.263233\n",
      "1                    TN       154.500000     2.068279\n",
      "2                    FP         3.100000     2.233582\n",
      "3                    FN        17.100000     2.685351\n",
      "4              Accuracy         0.894241     0.015374\n",
      "5             Precision         0.849453     0.079902\n",
      "6           Sensitivity         0.488858     0.072050\n",
      "7           Specificity         0.980350     0.014121\n",
      "8              F1 score         0.615614     0.064893\n",
      "9   F1 score (weighted)         0.882077     0.018677\n",
      "10     F1 score (macro)         0.777127     0.036373\n",
      "11    Balanced Accuracy         0.734608     0.034946\n",
      "12                  MCC         0.591137     0.060944\n",
      "13                  NPV         0.900550     0.013813\n",
      "14              ROC_AUC         0.734608     0.034946\n",
      "CPU times: user 2.36 s, sys: 280 ms, total: 2.64 s\n",
      "Wall time: 2.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_clf = SVC()\n",
    "    \n",
    "    svm_clf.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_clf.predict(X_test) \n",
    "   \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:24:58,227] A new study created in memory with name: SVM_classifier\n",
      "[I 2023-12-05 18:24:59,420] Trial 0 finished with value: 0.7536936365030058 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:00,660] Trial 1 finished with value: 0.6802293027878075 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:03,400] Trial 2 finished with value: 0.45960630508038963 and parameters: {'C': 1.0, 'gamma': 1.0}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:06,078] Trial 3 finished with value: 0.45232669164207806 and parameters: {'C': 0.125, 'gamma': 0.25}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:08,830] Trial 4 finished with value: 0.46982342715586095 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:09,956] Trial 5 finished with value: 0.45232669164207806 and parameters: {'C': 0.0078125, 'gamma': 0.001953125}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:11,162] Trial 6 finished with value: 0.45232669164207806 and parameters: {'C': 0.03125, 'gamma': 0.001953125}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:12,385] Trial 7 finished with value: 0.45232669164207806 and parameters: {'C': 0.5, 'gamma': 0.0009765625}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:15,118] Trial 8 finished with value: 0.4669400419111348 and parameters: {'C': 1.0, 'gamma': 0.25}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:17,880] Trial 9 finished with value: 0.45232669164207806 and parameters: {'C': 0.25, 'gamma': 1.0}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:19,055] Trial 10 finished with value: 0.7536936365030058 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:20,224] Trial 11 finished with value: 0.7536936365030058 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:21,398] Trial 12 finished with value: 0.7536936365030058 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:24,079] Trial 13 finished with value: 0.7435700781442836 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:25,124] Trial 14 finished with value: 0.45232669164207806 and parameters: {'C': 0.015625, 'gamma': 0.0001220703125}. Best is trial 0 with value: 0.7536936365030058.\n",
      "[I 2023-12-05 18:25:27,632] Trial 15 finished with value: 0.7999689328082205 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 15 with value: 0.7999689328082205.\n",
      "[I 2023-12-05 18:25:30,137] Trial 16 finished with value: 0.7999689328082205 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 15 with value: 0.7999689328082205.\n",
      "[I 2023-12-05 18:25:32,652] Trial 17 finished with value: 0.7999689328082205 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 15 with value: 0.7999689328082205.\n",
      "[I 2023-12-05 18:25:35,157] Trial 18 finished with value: 0.7999689328082205 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 15 with value: 0.7999689328082205.\n",
      "[I 2023-12-05 18:25:36,204] Trial 19 finished with value: 0.45232669164207806 and parameters: {'C': 0.0625, 'gamma': 3.0517578125e-05}. Best is trial 15 with value: 0.7999689328082205.\n",
      "[I 2023-12-05 18:25:38,280] Trial 20 finished with value: 0.8213265614379403 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:40,781] Trial 21 finished with value: 0.7999689328082205 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:42,846] Trial 22 finished with value: 0.8213265614379403 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:44,452] Trial 23 finished with value: 0.7901679803522712 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:46,228] Trial 24 finished with value: 0.8175441870630458 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:48,009] Trial 25 finished with value: 0.8175441870630458 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:49,782] Trial 26 finished with value: 0.8175441870630458 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:51,037] Trial 27 finished with value: 0.7847508352207438 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:52,652] Trial 28 finished with value: 0.8189396836918709 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:54,023] Trial 29 finished with value: 0.8056047951393783 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:56,840] Trial 30 finished with value: 0.45960630508038963 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:25:58,211] Trial 31 finished with value: 0.45232669164207806 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:01,451] Trial 32 finished with value: 0.45960630508038963 and parameters: {'C': 4.0, 'gamma': 8.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:04,112] Trial 33 finished with value: 0.45232669164207806 and parameters: {'C': 0.125, 'gamma': 0.125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:05,239] Trial 34 finished with value: 0.45232669164207806 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:08,019] Trial 35 finished with value: 0.45960630508038963 and parameters: {'C': 2.0, 'gamma': 2.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:10,761] Trial 36 finished with value: 0.4667384608834667 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:12,087] Trial 37 finished with value: 0.45232669164207806 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:13,179] Trial 38 finished with value: 0.45232669164207806 and parameters: {'C': 0.03125, 'gamma': 0.000244140625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:14,389] Trial 39 finished with value: 0.45232669164207806 and parameters: {'C': 0.5, 'gamma': 0.00048828125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:15,818] Trial 40 finished with value: 0.45232669164207806 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:17,605] Trial 41 finished with value: 0.8175441870630458 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:19,385] Trial 42 finished with value: 0.8175441870630458 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:21,155] Trial 43 finished with value: 0.8175441870630458 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:22,715] Trial 44 finished with value: 0.755523541632661 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:23,906] Trial 45 finished with value: 0.7521463378231602 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:25,103] Trial 46 finished with value: 0.776648066293995 and parameters: {'C': 16.0, 'gamma': 0.001953125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:27,790] Trial 47 finished with value: 0.7435700781442836 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:30,547] Trial 48 finished with value: 0.46327389337190966 and parameters: {'C': 4.0, 'gamma': 1.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:31,989] Trial 49 finished with value: 0.5313216955784049 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8213\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_classifier\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   45.000000\n",
      "1                    TN  306.000000\n",
      "2                    FP    8.000000\n",
      "3                    FN   23.000000\n",
      "4              Accuracy    0.918848\n",
      "5             Precision    0.849057\n",
      "6           Sensitivity    0.661765\n",
      "7           Specificity    0.974500\n",
      "8              F1 score    0.743802\n",
      "9   F1 score (weighted)    0.914765\n",
      "10     F1 score (macro)    0.847795\n",
      "11    Balanced Accuracy    0.818143\n",
      "12                  MCC    0.704103\n",
      "13                  NPV    0.930100\n",
      "14              ROC_AUC    0.818143\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_svm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_svm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_svm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_svm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_svm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_svm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_svm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_svm_0)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:26:35,044] Trial 50 finished with value: 0.46597357861185273 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:37,298] Trial 51 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:39,532] Trial 52 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:41,771] Trial 53 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:42,993] Trial 54 finished with value: 0.7418607888704202 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:44,039] Trial 55 finished with value: 0.45232669164207806 and parameters: {'C': 0.03125, 'gamma': 3.0517578125e-05}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:45,284] Trial 56 finished with value: 0.45232669164207806 and parameters: {'C': 0.0078125, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:46,585] Trial 57 finished with value: 0.629854716059052 and parameters: {'C': 0.5, 'gamma': 0.00390625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:49,366] Trial 58 finished with value: 0.5106578759225083 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:52,234] Trial 59 finished with value: 0.4519321532908262 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:55,448] Trial 60 finished with value: 0.45232669164207806 and parameters: {'C': 0.25, 'gamma': 8.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:57,690] Trial 61 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:26:59,926] Trial 62 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:02,158] Trial 63 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:04,836] Trial 64 finished with value: 0.45232669164207806 and parameters: {'C': 0.015625, 'gamma': 2.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:07,630] Trial 65 finished with value: 0.45173476607424573 and parameters: {'C': 4.0, 'gamma': 0.5}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:08,856] Trial 66 finished with value: 0.7118658991781233 and parameters: {'C': 16.0, 'gamma': 0.000244140625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:10,500] Trial 67 finished with value: 0.756386009232748 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:12,725] Trial 68 finished with value: 0.7911088408373376 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:13,993] Trial 69 finished with value: 0.6511506617411887 and parameters: {'C': 2.0, 'gamma': 0.0009765625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:15,236] Trial 70 finished with value: 0.7126661417999272 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:17,470] Trial 71 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:19,702] Trial 72 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:21,194] Trial 73 finished with value: 0.45232669164207806 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:22,468] Trial 74 finished with value: 0.7626613800944086 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:23,711] Trial 75 finished with value: 0.5594237423341895 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:25,743] Trial 76 finished with value: 0.7916463734978716 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:28,545] Trial 77 finished with value: 0.4519321532908262 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:31,165] Trial 78 finished with value: 0.45232669164207806 and parameters: {'C': 0.125, 'gamma': 0.0625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:33,408] Trial 79 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:35,612] Trial 80 finished with value: 0.7911088408373376 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:37,845] Trial 81 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:40,079] Trial 82 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:42,849] Trial 83 finished with value: 0.46597357861185273 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:45,081] Trial 84 finished with value: 0.7923789477954051 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:46,124] Trial 85 finished with value: 0.45232669164207806 and parameters: {'C': 0.0078125, 'gamma': 3.0517578125e-05}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:47,375] Trial 86 finished with value: 0.45232669164207806 and parameters: {'C': 0.03125, 'gamma': 0.00390625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:48,589] Trial 87 finished with value: 0.7418607888704202 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:49,934] Trial 88 finished with value: 0.6783769520757698 and parameters: {'C': 0.5, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:52,807] Trial 89 finished with value: 0.4519321532908262 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:54,849] Trial 90 finished with value: 0.7916463734978716 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:56,303] Trial 91 finished with value: 0.7786152805328708 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:27:57,761] Trial 92 finished with value: 0.7786152805328708 and parameters: {'C': 4.0, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:00,459] Trial 93 finished with value: 0.45232669164207806 and parameters: {'C': 0.25, 'gamma': 0.125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:03,598] Trial 94 finished with value: 0.45232669164207806 and parameters: {'C': 0.015625, 'gamma': 8.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:06,294] Trial 95 finished with value: 0.4519321532908262 and parameters: {'C': 1.0, 'gamma': 0.5}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:09,110] Trial 96 finished with value: 0.4519321532908262 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:11,143] Trial 97 finished with value: 0.7916463734978716 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:12,399] Trial 98 finished with value: 0.45232669164207806 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:14,625] Trial 99 finished with value: 0.7911088408373376 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8213\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   45.000000   52.000000\n",
      "1                    TN  306.000000  303.000000\n",
      "2                    FP    8.000000   11.000000\n",
      "3                    FN   23.000000   16.000000\n",
      "4              Accuracy    0.918848    0.929319\n",
      "5             Precision    0.849057    0.825397\n",
      "6           Sensitivity    0.661765    0.764706\n",
      "7           Specificity    0.974500    0.965000\n",
      "8              F1 score    0.743802    0.793893\n",
      "9   F1 score (weighted)    0.914765    0.928250\n",
      "10     F1 score (macro)    0.847795    0.875620\n",
      "11    Balanced Accuracy    0.818143    0.864837\n",
      "12                  MCC    0.704103    0.752112\n",
      "13                  NPV    0.930100    0.949800\n",
      "14              ROC_AUC    0.818143    0.864837\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_svm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_svm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_svm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_svm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_svm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_svm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_svm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_svm_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:28:16,069] Trial 100 finished with value: 0.7213846521050717 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:18,523] Trial 101 finished with value: 0.8135924480639289 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:20,980] Trial 102 finished with value: 0.8135924480639289 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:23,434] Trial 103 finished with value: 0.8135924480639289 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:25,888] Trial 104 finished with value: 0.8135924480639289 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:28,343] Trial 105 finished with value: 0.8135924480639289 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:30,800] Trial 106 finished with value: 0.8135924480639289 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:33,257] Trial 107 finished with value: 0.8135924480639289 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:34,570] Trial 108 finished with value: 0.45212762274139184 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:35,630] Trial 109 finished with value: 0.7853686217766117 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 20 with value: 0.8213265614379403.\n",
      "[I 2023-12-05 18:28:37,483] Trial 110 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:39,344] Trial 111 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:41,204] Trial 112 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:43,064] Trial 113 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:44,924] Trial 114 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:46,772] Trial 115 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:48,619] Trial 116 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:50,461] Trial 117 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:52,301] Trial 118 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:54,144] Trial 119 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:55,989] Trial 120 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:57,836] Trial 121 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:28:59,685] Trial 122 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:01,529] Trial 123 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:03,377] Trial 124 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:05,233] Trial 125 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:07,089] Trial 126 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:08,931] Trial 127 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:10,770] Trial 128 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:11,898] Trial 129 finished with value: 0.5853804994536006 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:13,002] Trial 130 finished with value: 0.7897314481894625 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:14,839] Trial 131 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:16,688] Trial 132 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:18,529] Trial 133 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:20,375] Trial 134 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:23,197] Trial 135 finished with value: 0.4700617371578186 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:25,937] Trial 136 finished with value: 0.7730025940941277 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:27,799] Trial 137 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:30,601] Trial 138 finished with value: 0.47708672796086143 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:32,456] Trial 139 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:33,572] Trial 140 finished with value: 0.45212762274139184 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:35,430] Trial 141 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:37,285] Trial 142 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:39,142] Trial 143 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:40,461] Trial 144 finished with value: 0.5306927635768504 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:42,319] Trial 145 finished with value: 0.8266256708928432 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:43,443] Trial 146 finished with value: 0.7995496680627202 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:45,394] Trial 147 finished with value: 0.8250459831200148 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:46,521] Trial 148 finished with value: 0.45212762274139184 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:47,770] Trial 149 finished with value: 0.45212762274139184 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 110 with value: 0.8266256708928432.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8266\n",
      "\tBest params:\n",
      "\t\tC: 8.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   45.000000   52.000000   44.000000\n",
      "1                    TN  306.000000  303.000000  300.000000\n",
      "2                    FP    8.000000   11.000000   15.000000\n",
      "3                    FN   23.000000   16.000000   23.000000\n",
      "4              Accuracy    0.918848    0.929319    0.900524\n",
      "5             Precision    0.849057    0.825397    0.745763\n",
      "6           Sensitivity    0.661765    0.764706    0.656716\n",
      "7           Specificity    0.974500    0.965000    0.952400\n",
      "8              F1 score    0.743802    0.793893    0.698413\n",
      "9   F1 score (weighted)    0.914765    0.928250    0.897989\n",
      "10     F1 score (macro)    0.847795    0.875620    0.819426\n",
      "11    Balanced Accuracy    0.818143    0.864837    0.804549\n",
      "12                  MCC    0.704103    0.752112    0.640991\n",
      "13                  NPV    0.930100    0.949800    0.928800\n",
      "14              ROC_AUC    0.818143    0.864837    0.804549\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_svm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_svm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_svm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_svm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_svm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_svm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_svm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_svm_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:29:50,656] Trial 150 finished with value: 0.4519335623018662 and parameters: {'C': 0.03125, 'gamma': 4.0}. Best is trial 110 with value: 0.8266256708928432.\n",
      "[I 2023-12-05 18:29:52,702] Trial 151 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:29:54,750] Trial 152 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:29:56,807] Trial 153 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:29:59,409] Trial 154 finished with value: 0.5568892039225171 and parameters: {'C': 8.0, 'gamma': 0.125}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:00,870] Trial 155 finished with value: 0.7472689415495396 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:03,935] Trial 156 finished with value: 0.4519335623018662 and parameters: {'C': 0.25, 'gamma': 8.0}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:06,577] Trial 157 finished with value: 0.4628330339346829 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:09,164] Trial 158 finished with value: 0.4519335623018662 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:11,223] Trial 159 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:12,368] Trial 160 finished with value: 0.6820792254025085 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:14,429] Trial 161 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:16,491] Trial 162 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:18,559] Trial 163 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:20,068] Trial 164 finished with value: 0.773056388006843 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:21,208] Trial 165 finished with value: 0.7395706729521987 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:22,752] Trial 166 finished with value: 0.8160838302317135 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:24,806] Trial 167 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:26,865] Trial 168 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:28,921] Trial 169 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:30,038] Trial 170 finished with value: 0.7775936664539793 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:32,110] Trial 171 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:34,196] Trial 172 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:35,560] Trial 173 finished with value: 0.4519335623018662 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:37,627] Trial 174 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:39,692] Trial 175 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:41,755] Trial 176 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:43,830] Trial 177 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:45,899] Trial 178 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:47,061] Trial 179 finished with value: 0.58695420549312 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:49,627] Trial 180 finished with value: 0.7692387183563404 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:51,690] Trial 181 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:53,750] Trial 182 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:55,826] Trial 183 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:57,884] Trial 184 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:30:59,946] Trial 185 finished with value: 0.8356471891528088 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:31:01,096] Trial 186 finished with value: 0.7764517823152316 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:31:02,485] Trial 187 finished with value: 0.539929643240997 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:31:05,136] Trial 188 finished with value: 0.4625124940087427 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:31:07,137] Trial 189 finished with value: 0.8330783245107309 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 151 with value: 0.8356471891528088.\n",
      "[I 2023-12-05 18:31:09,200] Trial 190 finished with value: 0.8372608474133578 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:11,265] Trial 191 finished with value: 0.8372608474133578 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:13,895] Trial 192 finished with value: 0.46231523246314055 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:15,979] Trial 193 finished with value: 0.8372608474133578 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:17,141] Trial 194 finished with value: 0.58695420549312 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:19,236] Trial 195 finished with value: 0.8372608474133578 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:21,319] Trial 196 finished with value: 0.8372608474133578 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:23,402] Trial 197 finished with value: 0.8372608474133578 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:25,479] Trial 198 finished with value: 0.8372608474133578 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:28,185] Trial 199 finished with value: 0.4628330339346829 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 190 with value: 0.8372608474133578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8373\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   45.000000   52.000000   44.000000   39.000000\n",
      "1                    TN  306.000000  303.000000  300.000000  304.000000\n",
      "2                    FP    8.000000   11.000000   15.000000   12.000000\n",
      "3                    FN   23.000000   16.000000   23.000000   27.000000\n",
      "4              Accuracy    0.918848    0.929319    0.900524    0.897906\n",
      "5             Precision    0.849057    0.825397    0.745763    0.764706\n",
      "6           Sensitivity    0.661765    0.764706    0.656716    0.590909\n",
      "7           Specificity    0.974500    0.965000    0.952400    0.962000\n",
      "8              F1 score    0.743802    0.793893    0.698413    0.666667\n",
      "9   F1 score (weighted)    0.914765    0.928250    0.897989    0.892545\n",
      "10     F1 score (macro)    0.847795    0.875620    0.819426    0.803194\n",
      "11    Balanced Accuracy    0.818143    0.864837    0.804549    0.776467\n",
      "12                  MCC    0.704103    0.752112    0.640991    0.614596\n",
      "13                  NPV    0.930100    0.949800    0.928800    0.918400\n",
      "14              ROC_AUC    0.818143    0.864837    0.804549    0.776467\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_svm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_svm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_svm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_svm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_svm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_svm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_svm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_svm_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:31:30,473] Trial 200 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:32,425] Trial 201 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:33,558] Trial 202 finished with value: 0.8102840261122901 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:35,515] Trial 203 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:36,619] Trial 204 finished with value: 0.6636182199410358 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:38,584] Trial 205 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:40,544] Trial 206 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:42,502] Trial 207 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:44,448] Trial 208 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:47,060] Trial 209 finished with value: 0.5571009408543879 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:50,259] Trial 210 finished with value: 0.4703948345458272 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:52,223] Trial 211 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:54,190] Trial 212 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:56,165] Trial 213 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:31:58,896] Trial 214 finished with value: 0.4703948345458272 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:01,486] Trial 215 finished with value: 0.4521307134365375 and parameters: {'C': 0.03125, 'gamma': 0.5}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:02,805] Trial 216 finished with value: 0.7455658799545313 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:04,013] Trial 217 finished with value: 0.8106576193119095 and parameters: {'C': 32.0, 'gamma': 0.0078125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:04,944] Trial 218 finished with value: 0.4521307134365375 and parameters: {'C': 0.015625, 'gamma': 0.000244140625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:06,106] Trial 219 finished with value: 0.4521307134365375 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:07,522] Trial 220 finished with value: 0.7777447448049336 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:09,460] Trial 221 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:10,921] Trial 222 finished with value: 0.8176779158197032 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:12,847] Trial 223 finished with value: 0.8169022655726822 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:14,101] Trial 224 finished with value: 0.4521307134365375 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:15,396] Trial 225 finished with value: 0.6541199813816257 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:16,445] Trial 226 finished with value: 0.7694751296220025 and parameters: {'C': 32.0, 'gamma': 0.00048828125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:18,251] Trial 227 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:20,059] Trial 228 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:21,989] Trial 229 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:23,045] Trial 230 finished with value: 0.755974497638175 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:24,850] Trial 231 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:26,661] Trial 232 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:28,468] Trial 233 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:30,276] Trial 234 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:32,087] Trial 235 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:33,178] Trial 236 finished with value: 0.6122602668860198 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:35,109] Trial 237 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:36,918] Trial 238 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:38,002] Trial 239 finished with value: 0.7773779541273971 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:39,927] Trial 240 finished with value: 0.8159091808979018 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:41,726] Trial 241 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:43,533] Trial 242 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:45,334] Trial 243 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:48,014] Trial 244 finished with value: 0.4703948345458272 and parameters: {'C': 64.0, 'gamma': 1.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:50,506] Trial 245 finished with value: 0.4521307134365375 and parameters: {'C': 0.125, 'gamma': 0.0625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:52,313] Trial 246 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:54,130] Trial 247 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:55,941] Trial 248 finished with value: 0.8203083983572637 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:32:58,602] Trial 249 finished with value: 0.4772138057452418 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 190 with value: 0.8372608474133578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8373\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   52.000000   44.000000   39.000000   \n",
      "1                    TN  306.000000  303.000000  300.000000  304.000000   \n",
      "2                    FP    8.000000   11.000000   15.000000   12.000000   \n",
      "3                    FN   23.000000   16.000000   23.000000   27.000000   \n",
      "4              Accuracy    0.918848    0.929319    0.900524    0.897906   \n",
      "5             Precision    0.849057    0.825397    0.745763    0.764706   \n",
      "6           Sensitivity    0.661765    0.764706    0.656716    0.590909   \n",
      "7           Specificity    0.974500    0.965000    0.952400    0.962000   \n",
      "8              F1 score    0.743802    0.793893    0.698413    0.666667   \n",
      "9   F1 score (weighted)    0.914765    0.928250    0.897989    0.892545   \n",
      "10     F1 score (macro)    0.847795    0.875620    0.819426    0.803194   \n",
      "11    Balanced Accuracy    0.818143    0.864837    0.804549    0.776467   \n",
      "12                  MCC    0.704103    0.752112    0.640991    0.614596   \n",
      "13                  NPV    0.930100    0.949800    0.928800    0.918400   \n",
      "14              ROC_AUC    0.818143    0.864837    0.804549    0.776467   \n",
      "\n",
      "          Set4  \n",
      "0    40.000000  \n",
      "1   304.000000  \n",
      "2    11.000000  \n",
      "3    27.000000  \n",
      "4     0.900524  \n",
      "5     0.784314  \n",
      "6     0.597015  \n",
      "7     0.965100  \n",
      "8     0.677966  \n",
      "9     0.895011  \n",
      "10    0.809571  \n",
      "11    0.781047  \n",
      "12    0.628496  \n",
      "13    0.918400  \n",
      "14    0.781047  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_svm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_svm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_svm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_svm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_svm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_svm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_svm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_svm_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:33:00,941] Trial 250 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:02,218] Trial 251 finished with value: 0.4521322481185558 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:03,365] Trial 252 finished with value: 0.4521322481185558 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:04,705] Trial 253 finished with value: 0.4521322481185558 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:06,153] Trial 254 finished with value: 0.7105773158033146 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:07,344] Trial 255 finished with value: 0.7963098761649879 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:10,245] Trial 256 finished with value: 0.4665508885974403 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:11,578] Trial 257 finished with value: 0.4521322481185558 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:12,643] Trial 258 finished with value: 0.4521322481185558 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:14,645] Trial 259 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:16,159] Trial 260 finished with value: 0.7685725248228675 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:18,955] Trial 261 finished with value: 0.5484744203890378 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:22,297] Trial 262 finished with value: 0.4665508885974403 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:24,302] Trial 263 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:25,836] Trial 264 finished with value: 0.8062545444090562 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:28,687] Trial 265 finished with value: 0.4665508885974403 and parameters: {'C': 16.0, 'gamma': 2.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:31,507] Trial 266 finished with value: 0.46603293737196383 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:33,538] Trial 267 finished with value: 0.8136457902520178 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:34,678] Trial 268 finished with value: 0.6631329462264324 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:35,971] Trial 269 finished with value: 0.8167178763550169 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:37,326] Trial 270 finished with value: 0.4521322481185558 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:39,366] Trial 271 finished with value: 0.8136457902520178 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:40,545] Trial 272 finished with value: 0.7083363002806177 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:42,603] Trial 273 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:44,683] Trial 274 finished with value: 0.8136457902520178 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:46,729] Trial 275 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:47,897] Trial 276 finished with value: 0.7503520769438494 and parameters: {'C': 8.0, 'gamma': 0.0009765625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:49,552] Trial 277 finished with value: 0.8182441957670603 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:51,602] Trial 278 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:53,678] Trial 279 finished with value: 0.8136457902520178 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:55,722] Trial 280 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:57,130] Trial 281 finished with value: 0.502766763117945 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:58,320] Trial 282 finished with value: 0.783535144139306 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:33:59,509] Trial 283 finished with value: 0.5667948930503703 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:01,583] Trial 284 finished with value: 0.8136457902520178 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:03,629] Trial 285 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:06,432] Trial 286 finished with value: 0.7624764182855241 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:09,328] Trial 287 finished with value: 0.4665508885974403 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:10,620] Trial 288 finished with value: 0.4521322481185558 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:12,660] Trial 289 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:15,396] Trial 290 finished with value: 0.4521322481185558 and parameters: {'C': 0.03125, 'gamma': 0.25}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:16,855] Trial 291 finished with value: 0.7105773158033146 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:18,891] Trial 292 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:20,066] Trial 293 finished with value: 0.4521322481185558 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:22,138] Trial 294 finished with value: 0.8136457902520178 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:23,551] Trial 295 finished with value: 0.6469321080306224 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:25,599] Trial 296 finished with value: 0.8173119215654264 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:27,142] Trial 297 finished with value: 0.7685725248228675 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:28,292] Trial 298 finished with value: 0.4521322481185558 and parameters: {'C': 0.015625, 'gamma': 0.00390625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:29,452] Trial 299 finished with value: 0.4521322481185558 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 190 with value: 0.8372608474133578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8373\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   52.000000   44.000000   39.000000   \n",
      "1                    TN  306.000000  303.000000  300.000000  304.000000   \n",
      "2                    FP    8.000000   11.000000   15.000000   12.000000   \n",
      "3                    FN   23.000000   16.000000   23.000000   27.000000   \n",
      "4              Accuracy    0.918848    0.929319    0.900524    0.897906   \n",
      "5             Precision    0.849057    0.825397    0.745763    0.764706   \n",
      "6           Sensitivity    0.661765    0.764706    0.656716    0.590909   \n",
      "7           Specificity    0.974500    0.965000    0.952400    0.962000   \n",
      "8              F1 score    0.743802    0.793893    0.698413    0.666667   \n",
      "9   F1 score (weighted)    0.914765    0.928250    0.897989    0.892545   \n",
      "10     F1 score (macro)    0.847795    0.875620    0.819426    0.803194   \n",
      "11    Balanced Accuracy    0.818143    0.864837    0.804549    0.776467   \n",
      "12                  MCC    0.704103    0.752112    0.640991    0.614596   \n",
      "13                  NPV    0.930100    0.949800    0.928800    0.918400   \n",
      "14              ROC_AUC    0.818143    0.864837    0.804549    0.776467   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    40.000000   45.000000  \n",
      "1   304.000000  298.000000  \n",
      "2    11.000000   17.000000  \n",
      "3    27.000000   22.000000  \n",
      "4     0.900524    0.897906  \n",
      "5     0.784314    0.725806  \n",
      "6     0.597015    0.671642  \n",
      "7     0.965100    0.946000  \n",
      "8     0.677966    0.697674  \n",
      "9     0.895011    0.896329  \n",
      "10    0.809571    0.818129  \n",
      "11    0.781047    0.808837  \n",
      "12    0.628496    0.637061  \n",
      "13    0.918400    0.931200  \n",
      "14    0.781047    0.808837  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_svm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_svm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_svm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_svm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_svm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_svm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_svm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_svm_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:34:31,864] Trial 300 finished with value: 0.7983930471439498 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:34,555] Trial 301 finished with value: 0.4597938403984022 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:37,182] Trial 302 finished with value: 0.529563650224963 and parameters: {'C': 16.0, 'gamma': 0.125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:39,257] Trial 303 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:40,735] Trial 304 finished with value: 0.8066316318817695 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:42,009] Trial 305 finished with value: 0.45252241850566194 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:44,622] Trial 306 finished with value: 0.4597938403984022 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:46,692] Trial 307 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:49,791] Trial 308 finished with value: 0.4597938403984022 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:51,080] Trial 309 finished with value: 0.802367977040413 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:52,172] Trial 310 finished with value: 0.7364150497428874 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:54,806] Trial 311 finished with value: 0.4597938403984022 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:56,872] Trial 312 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:34:58,934] Trial 313 finished with value: 0.7983930471439498 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:01,404] Trial 314 finished with value: 0.7932360374652855 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:02,514] Trial 315 finished with value: 0.6609080961886207 and parameters: {'C': 4.0, 'gamma': 0.00048828125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:04,590] Trial 316 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:05,874] Trial 317 finished with value: 0.5285980668016711 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:07,942] Trial 318 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:09,061] Trial 319 finished with value: 0.7938823088966316 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:11,145] Trial 320 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:13,158] Trial 321 finished with value: 0.7975189994282286 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:15,235] Trial 322 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:16,324] Trial 323 finished with value: 0.7187649960046056 and parameters: {'C': 32.0, 'gamma': 0.0001220703125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:18,421] Trial 324 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:20,501] Trial 325 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:21,557] Trial 326 finished with value: 0.45252241850566194 and parameters: {'C': 0.03125, 'gamma': 0.001953125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:22,952] Trial 327 finished with value: 0.7217996190790272 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:25,016] Trial 328 finished with value: 0.7983930471439498 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:27,634] Trial 329 finished with value: 0.4597938403984022 and parameters: {'C': 8.0, 'gamma': 1.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:29,794] Trial 330 finished with value: 0.45252241850566194 and parameters: {'C': 0.0078125, 'gamma': 0.0625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:31,874] Trial 331 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:33,181] Trial 332 finished with value: 0.6581383623409514 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:35,782] Trial 333 finished with value: 0.46700333929076604 and parameters: {'C': 8.0, 'gamma': 0.25}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:37,860] Trial 334 finished with value: 0.7983930471439498 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:39,101] Trial 335 finished with value: 0.45252241850566194 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:40,182] Trial 336 finished with value: 0.45252241850566194 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:41,612] Trial 337 finished with value: 0.743521117451851 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:43,678] Trial 338 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:44,801] Trial 339 finished with value: 0.7324994916294465 and parameters: {'C': 2.0, 'gamma': 0.00390625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:46,814] Trial 340 finished with value: 0.8021807295402386 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:48,888] Trial 341 finished with value: 0.7983930471439498 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:50,968] Trial 342 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:53,542] Trial 343 finished with value: 0.45252241850566194 and parameters: {'C': 0.0625, 'gamma': 4.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:55,621] Trial 344 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:35:58,261] Trial 345 finished with value: 0.529563650224963 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:01,359] Trial 346 finished with value: 0.4597938403984022 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:03,428] Trial 347 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:05,498] Trial 348 finished with value: 0.8055899659863094 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:08,138] Trial 349 finished with value: 0.4597938403984022 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 190 with value: 0.8372608474133578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8373\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   52.000000   44.000000   39.000000   \n",
      "1                    TN  306.000000  303.000000  300.000000  304.000000   \n",
      "2                    FP    8.000000   11.000000   15.000000   12.000000   \n",
      "3                    FN   23.000000   16.000000   23.000000   27.000000   \n",
      "4              Accuracy    0.918848    0.929319    0.900524    0.897906   \n",
      "5             Precision    0.849057    0.825397    0.745763    0.764706   \n",
      "6           Sensitivity    0.661765    0.764706    0.656716    0.590909   \n",
      "7           Specificity    0.974500    0.965000    0.952400    0.962000   \n",
      "8              F1 score    0.743802    0.793893    0.698413    0.666667   \n",
      "9   F1 score (weighted)    0.914765    0.928250    0.897989    0.892545   \n",
      "10     F1 score (macro)    0.847795    0.875620    0.819426    0.803194   \n",
      "11    Balanced Accuracy    0.818143    0.864837    0.804549    0.776467   \n",
      "12                  MCC    0.704103    0.752112    0.640991    0.614596   \n",
      "13                  NPV    0.930100    0.949800    0.928800    0.918400   \n",
      "14              ROC_AUC    0.818143    0.864837    0.804549    0.776467   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    40.000000   45.000000   48.000000  \n",
      "1   304.000000  298.000000  298.000000  \n",
      "2    11.000000   17.000000   15.000000  \n",
      "3    27.000000   22.000000   21.000000  \n",
      "4     0.900524    0.897906    0.905759  \n",
      "5     0.784314    0.725806    0.761905  \n",
      "6     0.597015    0.671642    0.695652  \n",
      "7     0.965100    0.946000    0.952100  \n",
      "8     0.677966    0.697674    0.727273  \n",
      "9     0.895011    0.896329    0.904065  \n",
      "10    0.809571    0.818129    0.835155  \n",
      "11    0.781047    0.808837    0.823864  \n",
      "12    0.628496    0.637061    0.671466  \n",
      "13    0.918400    0.931200    0.934200  \n",
      "14    0.781047    0.808837    0.823864  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_svm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_svm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_svm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_svm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_svm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_svm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_svm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_svm_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:36:10,649] Trial 350 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:13,568] Trial 351 finished with value: 0.4743472529100578 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:14,774] Trial 352 finished with value: 0.6831539696236497 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:16,930] Trial 353 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:18,328] Trial 354 finished with value: 0.5379970735018361 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:19,649] Trial 355 finished with value: 0.8299816226952412 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:21,832] Trial 356 finished with value: 0.8300719201345708 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:23,024] Trial 357 finished with value: 0.6831539696236497 and parameters: {'C': 8.0, 'gamma': 0.000244140625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:24,142] Trial 358 finished with value: 0.7298119344764741 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:26,140] Trial 359 finished with value: 0.8333633893645812 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:28,296] Trial 360 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:30,838] Trial 361 finished with value: 0.8228820547406295 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:32,953] Trial 362 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:34,348] Trial 363 finished with value: 0.7330570346786384 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:35,422] Trial 364 finished with value: 0.4521293044254975 and parameters: {'C': 0.03125, 'gamma': 0.0009765625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:37,529] Trial 365 finished with value: 0.8291108991735496 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:38,654] Trial 366 finished with value: 0.5955351256094368 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:39,924] Trial 367 finished with value: 0.4521293044254975 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:42,037] Trial 368 finished with value: 0.8300719201345708 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:43,188] Trial 369 finished with value: 0.7823429051718545 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:45,294] Trial 370 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:46,649] Trial 371 finished with value: 0.6634534180901539 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:49,493] Trial 372 finished with value: 0.4743472529100578 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:52,144] Trial 373 finished with value: 0.7240030073619284 and parameters: {'C': 1.0, 'gamma': 0.0625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:53,408] Trial 374 finished with value: 0.4521293044254975 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:55,504] Trial 375 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:57,628] Trial 376 finished with value: 0.8300719201345708 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:36:59,728] Trial 377 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:00,829] Trial 378 finished with value: 0.4521293044254975 and parameters: {'C': 2.0, 'gamma': 3.0517578125e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:03,649] Trial 379 finished with value: 0.4809466317920454 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:05,745] Trial 380 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:07,851] Trial 381 finished with value: 0.8300719201345708 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:09,199] Trial 382 finished with value: 0.4521293044254975 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:11,290] Trial 383 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:14,185] Trial 384 finished with value: 0.4743472529100578 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:16,293] Trial 385 finished with value: 0.8300719201345708 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:17,501] Trial 386 finished with value: 0.7988408533393033 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:18,637] Trial 387 finished with value: 0.4521293044254975 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:20,740] Trial 388 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:23,579] Trial 389 finished with value: 0.5466782645238875 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:26,896] Trial 390 finished with value: 0.4743472529100578 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:28,836] Trial 391 finished with value: 0.8333633893645812 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:30,934] Trial 392 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:33,780] Trial 393 finished with value: 0.4743472529100578 and parameters: {'C': 32.0, 'gamma': 2.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:35,144] Trial 394 finished with value: 0.5379970735018361 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:37,241] Trial 395 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:39,348] Trial 396 finished with value: 0.8343778257162452 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:42,162] Trial 397 finished with value: 0.4743472529100578 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:43,265] Trial 398 finished with value: 0.7588229813917262 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:44,555] Trial 399 finished with value: 0.8299816226952412 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 190 with value: 0.8372608474133578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8373\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   52.000000   44.000000   39.000000   \n",
      "1                    TN  306.000000  303.000000  300.000000  304.000000   \n",
      "2                    FP    8.000000   11.000000   15.000000   12.000000   \n",
      "3                    FN   23.000000   16.000000   23.000000   27.000000   \n",
      "4              Accuracy    0.918848    0.929319    0.900524    0.897906   \n",
      "5             Precision    0.849057    0.825397    0.745763    0.764706   \n",
      "6           Sensitivity    0.661765    0.764706    0.656716    0.590909   \n",
      "7           Specificity    0.974500    0.965000    0.952400    0.962000   \n",
      "8              F1 score    0.743802    0.793893    0.698413    0.666667   \n",
      "9   F1 score (weighted)    0.914765    0.928250    0.897989    0.892545   \n",
      "10     F1 score (macro)    0.847795    0.875620    0.819426    0.803194   \n",
      "11    Balanced Accuracy    0.818143    0.864837    0.804549    0.776467   \n",
      "12                  MCC    0.704103    0.752112    0.640991    0.614596   \n",
      "13                  NPV    0.930100    0.949800    0.928800    0.918400   \n",
      "14              ROC_AUC    0.818143    0.864837    0.804549    0.776467   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    40.000000   45.000000   48.000000   42.000000  \n",
      "1   304.000000  298.000000  298.000000  303.000000  \n",
      "2    11.000000   17.000000   15.000000   12.000000  \n",
      "3    27.000000   22.000000   21.000000   25.000000  \n",
      "4     0.900524    0.897906    0.905759    0.903141  \n",
      "5     0.784314    0.725806    0.761905    0.777778  \n",
      "6     0.597015    0.671642    0.695652    0.626866  \n",
      "7     0.965100    0.946000    0.952100    0.961900  \n",
      "8     0.677966    0.697674    0.727273    0.694215  \n",
      "9     0.895011    0.896329    0.904065    0.898917  \n",
      "10    0.809571    0.818129    0.835155    0.818336  \n",
      "11    0.781047    0.808837    0.823864    0.794385  \n",
      "12    0.628496    0.637061    0.671466    0.642695  \n",
      "13    0.918400    0.931200    0.934200    0.923800  \n",
      "14    0.781047    0.808837    0.823864    0.794385  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_svm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_svm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_svm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_svm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_svm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_svm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_svm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_svm_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:37:45,969] Trial 400 finished with value: 0.7169809356580864 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:47,241] Trial 401 finished with value: 0.45213083910751584 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:48,783] Trial 402 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:49,968] Trial 403 finished with value: 0.45213083910751584 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:50,990] Trial 404 finished with value: 0.8028230392094317 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:52,286] Trial 405 finished with value: 0.7348887456156243 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:54,600] Trial 406 finished with value: 0.8194065181997885 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:56,150] Trial 407 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:57,668] Trial 408 finished with value: 0.8270772975968246 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:37:58,713] Trial 409 finished with value: 0.45213083910751584 and parameters: {'C': 0.25, 'gamma': 0.0001220703125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:00,103] Trial 410 finished with value: 0.8302519622207619 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:01,350] Trial 411 finished with value: 0.45213083910751584 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:02,391] Trial 412 finished with value: 0.7915168180694213 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:03,753] Trial 413 finished with value: 0.7785131458854436 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:05,304] Trial 414 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:07,848] Trial 415 finished with value: 0.7755394547588947 and parameters: {'C': 8.0, 'gamma': 0.0625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:10,535] Trial 416 finished with value: 0.4626331844978238 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:12,089] Trial 417 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:13,638] Trial 418 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:16,304] Trial 419 finished with value: 0.46894856885189473 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:17,729] Trial 420 finished with value: 0.8203056383242165 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:18,802] Trial 421 finished with value: 0.45213083910751584 and parameters: {'C': 16.0, 'gamma': 3.0517578125e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:20,089] Trial 422 finished with value: 0.45213083910751584 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:21,644] Trial 423 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:23,200] Trial 424 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:24,715] Trial 425 finished with value: 0.8270772975968246 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:26,269] Trial 426 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:27,343] Trial 427 finished with value: 0.45213083910751584 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:28,893] Trial 428 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:31,535] Trial 429 finished with value: 0.5465958581272284 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:33,088] Trial 430 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:34,226] Trial 431 finished with value: 0.8102252200706843 and parameters: {'C': 8.0, 'gamma': 0.00390625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:36,973] Trial 432 finished with value: 0.4628305717144043 and parameters: {'C': 32.0, 'gamma': 4.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:38,436] Trial 433 finished with value: 0.8271640963431157 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:41,485] Trial 434 finished with value: 0.45213083910751584 and parameters: {'C': 0.125, 'gamma': 8.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:43,036] Trial 435 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:45,705] Trial 436 finished with value: 0.46934477362720484 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:47,220] Trial 437 finished with value: 0.8270772975968246 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:49,918] Trial 438 finished with value: 0.4628305717144043 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:50,997] Trial 439 finished with value: 0.7923847871078538 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:52,202] Trial 440 finished with value: 0.8177962291203661 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:53,393] Trial 441 finished with value: 0.45213083910751584 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:54,943] Trial 442 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:56,464] Trial 443 finished with value: 0.8270772975968246 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:57,538] Trial 444 finished with value: 0.7169809356580864 and parameters: {'C': 8.0, 'gamma': 0.00048828125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:38:59,406] Trial 445 finished with value: 0.7374379646439957 and parameters: {'C': 0.5, 'gamma': 0.03125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:00,958] Trial 446 finished with value: 0.8245743913720034 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:02,344] Trial 447 finished with value: 0.8302519622207619 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:03,455] Trial 448 finished with value: 0.45213083910751584 and parameters: {'C': 0.25, 'gamma': 0.0009765625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:04,721] Trial 449 finished with value: 0.45213083910751584 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8373\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   52.000000   44.000000   39.000000   \n",
      "1                    TN  306.000000  303.000000  300.000000  304.000000   \n",
      "2                    FP    8.000000   11.000000   15.000000   12.000000   \n",
      "3                    FN   23.000000   16.000000   23.000000   27.000000   \n",
      "4              Accuracy    0.918848    0.929319    0.900524    0.897906   \n",
      "5             Precision    0.849057    0.825397    0.745763    0.764706   \n",
      "6           Sensitivity    0.661765    0.764706    0.656716    0.590909   \n",
      "7           Specificity    0.974500    0.965000    0.952400    0.962000   \n",
      "8              F1 score    0.743802    0.793893    0.698413    0.666667   \n",
      "9   F1 score (weighted)    0.914765    0.928250    0.897989    0.892545   \n",
      "10     F1 score (macro)    0.847795    0.875620    0.819426    0.803194   \n",
      "11    Balanced Accuracy    0.818143    0.864837    0.804549    0.776467   \n",
      "12                  MCC    0.704103    0.752112    0.640991    0.614596   \n",
      "13                  NPV    0.930100    0.949800    0.928800    0.918400   \n",
      "14              ROC_AUC    0.818143    0.864837    0.804549    0.776467   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    40.000000   45.000000   48.000000   42.000000   40.000000  \n",
      "1   304.000000  298.000000  298.000000  303.000000  307.000000  \n",
      "2    11.000000   17.000000   15.000000   12.000000    8.000000  \n",
      "3    27.000000   22.000000   21.000000   25.000000   27.000000  \n",
      "4     0.900524    0.897906    0.905759    0.903141    0.908377  \n",
      "5     0.784314    0.725806    0.761905    0.777778    0.833333  \n",
      "6     0.597015    0.671642    0.695652    0.626866    0.597015  \n",
      "7     0.965100    0.946000    0.952100    0.961900    0.974600  \n",
      "8     0.677966    0.697674    0.727273    0.694215    0.695652  \n",
      "9     0.895011    0.896329    0.904065    0.898917    0.902149  \n",
      "10    0.809571    0.818129    0.835155    0.818336    0.820862  \n",
      "11    0.781047    0.808837    0.823864    0.794385    0.785809  \n",
      "12    0.628496    0.637061    0.671466    0.642695    0.655850  \n",
      "13    0.918400    0.931200    0.934200    0.923800    0.919200  \n",
      "14    0.781047    0.808837    0.823864    0.794385    0.785809  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_svm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_svm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_svm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_svm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_svm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_svm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_svm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_svm_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 18:39:07,026] Trial 450 finished with value: 0.8064392916425058 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:08,502] Trial 451 finished with value: 0.7647479005696948 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:09,836] Trial 452 finished with value: 0.45252395318768024 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:11,846] Trial 453 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:13,857] Trial 454 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:14,991] Trial 455 finished with value: 0.7098362001590155 and parameters: {'C': 2.0, 'gamma': 0.001953125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:16,157] Trial 456 finished with value: 0.6405027192353112 and parameters: {'C': 16.0, 'gamma': 0.0001220703125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:18,875] Trial 457 finished with value: 0.7541549976844449 and parameters: {'C': 32.0, 'gamma': 0.0625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:20,877] Trial 458 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:23,577] Trial 459 finished with value: 0.45252395318768024 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:25,585] Trial 460 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:27,583] Trial 461 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:30,346] Trial 462 finished with value: 0.481206563376391 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:32,346] Trial 463 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:34,341] Trial 464 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:36,382] Trial 465 finished with value: 0.8064392916425058 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:37,509] Trial 466 finished with value: 0.45252395318768024 and parameters: {'C': 8.0, 'gamma': 3.0517578125e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:39,514] Trial 467 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:41,512] Trial 468 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:42,728] Trial 469 finished with value: 0.791895645766616 and parameters: {'C': 32.0, 'gamma': 0.00390625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:44,413] Trial 470 finished with value: 0.8139925783660844 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:45,426] Trial 471 finished with value: 0.45252395318768024 and parameters: {'C': 0.125, 'gamma': 6.103515625e-05}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:47,432] Trial 472 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:50,272] Trial 473 finished with value: 0.47113747357530755 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:52,318] Trial 474 finished with value: 0.8064392916425058 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:55,568] Trial 475 finished with value: 0.47113747357530755 and parameters: {'C': 8.0, 'gamma': 8.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:39:58,308] Trial 476 finished with value: 0.5348690216321709 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:00,314] Trial 477 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:02,362] Trial 478 finished with value: 0.8064392916425058 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:05,147] Trial 479 finished with value: 0.47113747357530755 and parameters: {'C': 8.0, 'gamma': 2.0}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:07,834] Trial 480 finished with value: 0.45252395318768024 and parameters: {'C': 0.03125, 'gamma': 0.5}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:09,120] Trial 481 finished with value: 0.45252395318768024 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:10,564] Trial 482 finished with value: 0.7174569459828956 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:11,914] Trial 483 finished with value: 0.8016349295912069 and parameters: {'C': 8.0, 'gamma': 0.0078125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:13,917] Trial 484 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:15,070] Trial 485 finished with value: 0.7500946880570961 and parameters: {'C': 32.0, 'gamma': 0.000244140625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:16,112] Trial 486 finished with value: 0.45252395318768024 and parameters: {'C': 0.015625, 'gamma': 0.00048828125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:18,149] Trial 487 finished with value: 0.8079090701879217 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:20,614] Trial 488 finished with value: 0.7970832394638472 and parameters: {'C': 8.0, 'gamma': 0.03125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:22,008] Trial 489 finished with value: 0.6122291771869387 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:24,014] Trial 490 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:25,489] Trial 491 finished with value: 0.7647479005696948 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:26,682] Trial 492 finished with value: 0.7957696551429833 and parameters: {'C': 32.0, 'gamma': 0.0009765625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:28,678] Trial 493 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:30,675] Trial 494 finished with value: 0.8067738602718325 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:32,252] Trial 495 finished with value: 0.8105130671751308 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:34,295] Trial 496 finished with value: 0.8064392916425058 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:35,457] Trial 497 finished with value: 0.7832043425305992 and parameters: {'C': 8.0, 'gamma': 0.001953125}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:36,816] Trial 498 finished with value: 0.45252395318768024 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 190 with value: 0.8372608474133578.\n",
      "[I 2023-12-05 18:40:37,998] Trial 499 finished with value: 0.5466700967096176 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 190 with value: 0.8372608474133578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8373\n",
      "\tBest params:\n",
      "\t\tC: 32.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   45.000000   52.000000   44.000000   39.000000   \n",
      "1                    TN  306.000000  303.000000  300.000000  304.000000   \n",
      "2                    FP    8.000000   11.000000   15.000000   12.000000   \n",
      "3                    FN   23.000000   16.000000   23.000000   27.000000   \n",
      "4              Accuracy    0.918848    0.929319    0.900524    0.897906   \n",
      "5             Precision    0.849057    0.825397    0.745763    0.764706   \n",
      "6           Sensitivity    0.661765    0.764706    0.656716    0.590909   \n",
      "7           Specificity    0.974500    0.965000    0.952400    0.962000   \n",
      "8              F1 score    0.743802    0.793893    0.698413    0.666667   \n",
      "9   F1 score (weighted)    0.914765    0.928250    0.897989    0.892545   \n",
      "10     F1 score (macro)    0.847795    0.875620    0.819426    0.803194   \n",
      "11    Balanced Accuracy    0.818143    0.864837    0.804549    0.776467   \n",
      "12                  MCC    0.704103    0.752112    0.640991    0.614596   \n",
      "13                  NPV    0.930100    0.949800    0.928800    0.918400   \n",
      "14              ROC_AUC    0.818143    0.864837    0.804549    0.776467   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    40.000000   45.000000   48.000000   42.000000   40.000000   46.000000  \n",
      "1   304.000000  298.000000  298.000000  303.000000  307.000000  301.000000  \n",
      "2    11.000000   17.000000   15.000000   12.000000    8.000000   12.000000  \n",
      "3    27.000000   22.000000   21.000000   25.000000   27.000000   23.000000  \n",
      "4     0.900524    0.897906    0.905759    0.903141    0.908377    0.908377  \n",
      "5     0.784314    0.725806    0.761905    0.777778    0.833333    0.793103  \n",
      "6     0.597015    0.671642    0.695652    0.626866    0.597015    0.666667  \n",
      "7     0.965100    0.946000    0.952100    0.961900    0.974600    0.961700  \n",
      "8     0.677966    0.697674    0.727273    0.694215    0.695652    0.724409  \n",
      "9     0.895011    0.896329    0.904065    0.898917    0.902149    0.905200  \n",
      "10    0.809571    0.818129    0.835155    0.818336    0.820862    0.834732  \n",
      "11    0.781047    0.808837    0.823864    0.794385    0.785809    0.814164  \n",
      "12    0.628496    0.637061    0.671466    0.642695    0.655850    0.673592  \n",
      "13    0.918400    0.931200    0.934200    0.923800    0.919200    0.929000  \n",
      "14    0.781047    0.808837    0.823864    0.794385    0.785809    0.814164  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_svm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_svm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_svm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_svm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_svm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_svm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_svm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_svm_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>2.635231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>150.300000</td>\n",
       "      <td>2.057507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>2.263233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>2.685351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.904712</td>\n",
       "      <td>0.017241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.756792</td>\n",
       "      <td>0.061802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.078604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.953730</td>\n",
       "      <td>0.014283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.710608</td>\n",
       "      <td>0.059349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.902327</td>\n",
       "      <td>0.018480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.826768</td>\n",
       "      <td>0.034517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.813813</td>\n",
       "      <td>0.039429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.657078</td>\n",
       "      <td>0.067347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.932610</td>\n",
       "      <td>0.015380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.813813</td>\n",
       "      <td>0.039429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP        22.500000     2.635231\n",
       "1                    TN       150.300000     2.057507\n",
       "2                    FP         7.300000     2.263233\n",
       "3                    FN        10.900000     2.685351\n",
       "4              Accuracy         0.904712     0.017241\n",
       "5             Precision         0.756792     0.061802\n",
       "6           Sensitivity         0.673913     0.078604\n",
       "7           Specificity         0.953730     0.014283\n",
       "8              F1 score         0.710608     0.059349\n",
       "9   F1 score (weighted)         0.902327     0.018480\n",
       "10     F1 score (macro)         0.826768     0.034517\n",
       "11    Balanced Accuracy         0.813813     0.039429\n",
       "12                  MCC         0.657078     0.067347\n",
       "13                  NPV         0.932610     0.015380\n",
       "14              ROC_AUC         0.813813     0.039429"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>44.100000</td>\n",
       "      <td>4.040077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>304.000000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>298.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>307.000000</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>302.400000</td>\n",
       "      <td>3.098387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>2.923088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.400000</td>\n",
       "      <td>3.405877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.918848</td>\n",
       "      <td>0.929319</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.900524</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.905759</td>\n",
       "      <td>0.903141</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.908377</td>\n",
       "      <td>0.907068</td>\n",
       "      <td>0.010044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.786116</td>\n",
       "      <td>0.039673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.661765</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.626866</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.652895</td>\n",
       "      <td>0.053468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.974500</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.952400</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.965100</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.952100</td>\n",
       "      <td>0.961900</td>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.961700</td>\n",
       "      <td>0.961530</td>\n",
       "      <td>0.009296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.793893</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.694215</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>0.711996</td>\n",
       "      <td>0.036956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.914765</td>\n",
       "      <td>0.928250</td>\n",
       "      <td>0.897989</td>\n",
       "      <td>0.892545</td>\n",
       "      <td>0.895011</td>\n",
       "      <td>0.896329</td>\n",
       "      <td>0.904065</td>\n",
       "      <td>0.898917</td>\n",
       "      <td>0.902149</td>\n",
       "      <td>0.905200</td>\n",
       "      <td>0.903522</td>\n",
       "      <td>0.010746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.847795</td>\n",
       "      <td>0.875620</td>\n",
       "      <td>0.819426</td>\n",
       "      <td>0.803194</td>\n",
       "      <td>0.809571</td>\n",
       "      <td>0.818129</td>\n",
       "      <td>0.835155</td>\n",
       "      <td>0.818336</td>\n",
       "      <td>0.820862</td>\n",
       "      <td>0.834732</td>\n",
       "      <td>0.828282</td>\n",
       "      <td>0.021149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.818143</td>\n",
       "      <td>0.864837</td>\n",
       "      <td>0.804549</td>\n",
       "      <td>0.776467</td>\n",
       "      <td>0.781047</td>\n",
       "      <td>0.808837</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.794385</td>\n",
       "      <td>0.785809</td>\n",
       "      <td>0.814164</td>\n",
       "      <td>0.807210</td>\n",
       "      <td>0.025880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.704103</td>\n",
       "      <td>0.752112</td>\n",
       "      <td>0.640991</td>\n",
       "      <td>0.614596</td>\n",
       "      <td>0.628496</td>\n",
       "      <td>0.637061</td>\n",
       "      <td>0.671466</td>\n",
       "      <td>0.642695</td>\n",
       "      <td>0.655850</td>\n",
       "      <td>0.673592</td>\n",
       "      <td>0.662096</td>\n",
       "      <td>0.040771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.930100</td>\n",
       "      <td>0.949800</td>\n",
       "      <td>0.928800</td>\n",
       "      <td>0.918400</td>\n",
       "      <td>0.918400</td>\n",
       "      <td>0.931200</td>\n",
       "      <td>0.934200</td>\n",
       "      <td>0.923800</td>\n",
       "      <td>0.919200</td>\n",
       "      <td>0.929000</td>\n",
       "      <td>0.928290</td>\n",
       "      <td>0.009478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.818143</td>\n",
       "      <td>0.864837</td>\n",
       "      <td>0.804549</td>\n",
       "      <td>0.776467</td>\n",
       "      <td>0.781047</td>\n",
       "      <td>0.808837</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>0.794385</td>\n",
       "      <td>0.785809</td>\n",
       "      <td>0.814164</td>\n",
       "      <td>0.807210</td>\n",
       "      <td>0.025880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   45.000000   52.000000   44.000000   39.000000   \n",
       "1                    TN  306.000000  303.000000  300.000000  304.000000   \n",
       "2                    FP    8.000000   11.000000   15.000000   12.000000   \n",
       "3                    FN   23.000000   16.000000   23.000000   27.000000   \n",
       "4              Accuracy    0.918848    0.929319    0.900524    0.897906   \n",
       "5             Precision    0.849057    0.825397    0.745763    0.764706   \n",
       "6           Sensitivity    0.661765    0.764706    0.656716    0.590909   \n",
       "7           Specificity    0.974500    0.965000    0.952400    0.962000   \n",
       "8              F1 score    0.743802    0.793893    0.698413    0.666667   \n",
       "9   F1 score (weighted)    0.914765    0.928250    0.897989    0.892545   \n",
       "10     F1 score (macro)    0.847795    0.875620    0.819426    0.803194   \n",
       "11    Balanced Accuracy    0.818143    0.864837    0.804549    0.776467   \n",
       "12                  MCC    0.704103    0.752112    0.640991    0.614596   \n",
       "13                  NPV    0.930100    0.949800    0.928800    0.918400   \n",
       "14              ROC_AUC    0.818143    0.864837    0.804549    0.776467   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    40.000000   45.000000   48.000000   42.000000   40.000000   46.000000   \n",
       "1   304.000000  298.000000  298.000000  303.000000  307.000000  301.000000   \n",
       "2    11.000000   17.000000   15.000000   12.000000    8.000000   12.000000   \n",
       "3    27.000000   22.000000   21.000000   25.000000   27.000000   23.000000   \n",
       "4     0.900524    0.897906    0.905759    0.903141    0.908377    0.908377   \n",
       "5     0.784314    0.725806    0.761905    0.777778    0.833333    0.793103   \n",
       "6     0.597015    0.671642    0.695652    0.626866    0.597015    0.666667   \n",
       "7     0.965100    0.946000    0.952100    0.961900    0.974600    0.961700   \n",
       "8     0.677966    0.697674    0.727273    0.694215    0.695652    0.724409   \n",
       "9     0.895011    0.896329    0.904065    0.898917    0.902149    0.905200   \n",
       "10    0.809571    0.818129    0.835155    0.818336    0.820862    0.834732   \n",
       "11    0.781047    0.808837    0.823864    0.794385    0.785809    0.814164   \n",
       "12    0.628496    0.637061    0.671466    0.642695    0.655850    0.673592   \n",
       "13    0.918400    0.931200    0.934200    0.923800    0.919200    0.929000   \n",
       "14    0.781047    0.808837    0.823864    0.794385    0.785809    0.814164   \n",
       "\n",
       "           ave       std  \n",
       "0    44.100000  4.040077  \n",
       "1   302.400000  3.098387  \n",
       "2    12.100000  2.923088  \n",
       "3    23.400000  3.405877  \n",
       "4     0.907068  0.010044  \n",
       "5     0.786116  0.039673  \n",
       "6     0.652895  0.053468  \n",
       "7     0.961530  0.009296  \n",
       "8     0.711996  0.036956  \n",
       "9     0.903522  0.010746  \n",
       "10    0.828282  0.021149  \n",
       "11    0.807210  0.025880  \n",
       "12    0.662096  0.040771  \n",
       "13    0.928290  0.009478  \n",
       "14    0.807210  0.025880  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.904188</td>\n",
       "      <td>0.017927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.756170</td>\n",
       "      <td>0.060850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.671580</td>\n",
       "      <td>0.081848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.953414</td>\n",
       "      <td>0.014938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.708736</td>\n",
       "      <td>0.059681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.901731</td>\n",
       "      <td>0.018894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.825671</td>\n",
       "      <td>0.034831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.812496</td>\n",
       "      <td>0.040825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.655157</td>\n",
       "      <td>0.068486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.932290</td>\n",
       "      <td>0.015726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.812496</td>\n",
       "      <td>0.040825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.904188     0.017927\n",
       "1             Precision         0.756170     0.060850\n",
       "2           Sensitivity         0.671580     0.081848\n",
       "3           Specificity         0.953414     0.014938\n",
       "4              F1 score         0.708736     0.059681\n",
       "5   F1 score (weighted)         0.901731     0.018894\n",
       "6      F1 score (macro)         0.825671     0.034831\n",
       "7     Balanced Accuracy         0.812496     0.040825\n",
       "8                   MCC         0.655157     0.068486\n",
       "9                   NPV         0.932290     0.015726\n",
       "10              ROC_AUC         0.812496     0.040825"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_svm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_svm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_svm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_svm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_svm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_svm))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model f1_score 0.7803 with a standard deviation of 0.0352\n",
      "SVM optimized model f1_score 0.8277 with a standard deviation of 0.0267\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVC \n",
    "svm_baseline_CVscore = cross_val_score(svm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"SVM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm_clf_withSemiSel.joblib']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_clf, \"OUTPUT/svm_clf_withSemiSel.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm_clf_withSemiSel.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "12c9e7da-0ac4-498b-99fa-c8eff6e462b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults_withSemiSel.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d0ae45b3-e999-4133-b075-775055c49863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults_withSemiSel.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
