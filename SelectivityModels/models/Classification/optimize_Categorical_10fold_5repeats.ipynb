{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec6ac7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arma/miniforge3/envs/teachopencadd/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.experimental import enable_hist_gradient_boosting\n",
    "#from sklearn.ensemble import HistGradientBoostingclfressor\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import optuna\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b2ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to this notebook\n",
    "HERE = Path(_dh[-1])\n",
    "HDAC1and6 = Path(HERE).resolve().parents[1]/'input'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3db03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4159460</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[3213041, 3821889, 42902, 17455853, 2562256, 1...</td>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4250057</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1546832, 783386, 646233, 476717, 607926, 1860...</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL137875</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[5547132, 4188685, 8095127, 3724449, 7332227, ...</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL2442795</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5483437, 3005158, 245334, 933718, 2729629, 48...</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3692676</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[2532187, 22954272, 8033062, 20297601, 1461017...</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4159460  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4250057  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2       CHEMBL137875  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "3      CHEMBL2442795  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      CHEMBL3692676  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "4  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \n",
       "0  [3213041, 3821889, 42902, 17455853, 2562256, 1...              -0.12  \n",
       "1  [1546832, 783386, 646233, 476717, 607926, 1860...               0.63  \n",
       "2  [5547132, 4188685, 8095127, 3724449, 7332227, ...               0.27  \n",
       "3  [5483437, 3005158, 245334, 933718, 2729629, 48...               0.35  \n",
       "4  [2532187, 22954272, 8033062, 20297601, 1461017...               0.24  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(HDAC1and6/\"HDAC1and6_1024B.csv\")\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3d2d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>smiles</th>\n",
       "      <th>type_HDAC1</th>\n",
       "      <th>Standard_Value_HDAC1</th>\n",
       "      <th>pChEMBL_HDAC1</th>\n",
       "      <th>type_HDAC6</th>\n",
       "      <th>Standard_Value_HDAC6</th>\n",
       "      <th>pChEMBL_HDAC6</th>\n",
       "      <th>SelectivityRatio</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4098975</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>109.647820</td>\n",
       "      <td>6.96</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.141254</td>\n",
       "      <td>9.85</td>\n",
       "      <td>776.247117</td>\n",
       "      <td>2.89</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL3912061</td>\n",
       "      <td>CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12</td>\n",
       "      <td>IC50</td>\n",
       "      <td>616.595002</td>\n",
       "      <td>6.21</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.169824</td>\n",
       "      <td>9.77</td>\n",
       "      <td>3630.780548</td>\n",
       "      <td>3.56</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4243347</td>\n",
       "      <td>O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1.995262</td>\n",
       "      <td>8.70</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.199526</td>\n",
       "      <td>9.70</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL4247128</td>\n",
       "      <td>C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>83.176377</td>\n",
       "      <td>7.08</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.251189</td>\n",
       "      <td>9.60</td>\n",
       "      <td>331.131122</td>\n",
       "      <td>2.52</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL4126811</td>\n",
       "      <td>CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>436.515832</td>\n",
       "      <td>6.36</td>\n",
       "      <td>IC50</td>\n",
       "      <td>0.331131</td>\n",
       "      <td>9.48</td>\n",
       "      <td>1318.256739</td>\n",
       "      <td>3.12</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>CHEMBL4278591</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3cccc(OCCCCCCCC(=O)NO)c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1047.128548</td>\n",
       "      <td>5.98</td>\n",
       "      <td>IC50</td>\n",
       "      <td>6.760830</td>\n",
       "      <td>8.17</td>\n",
       "      <td>154.881662</td>\n",
       "      <td>2.19</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>CHEMBL4649511</td>\n",
       "      <td>O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO</td>\n",
       "      <td>IC50</td>\n",
       "      <td>3.311311</td>\n",
       "      <td>8.48</td>\n",
       "      <td>IC50</td>\n",
       "      <td>12.882496</td>\n",
       "      <td>7.89</td>\n",
       "      <td>0.257040</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>CHEMBL4291781</td>\n",
       "      <td>CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(OCCCCCCCC(=O)NO)cc...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>1698.243652</td>\n",
       "      <td>5.77</td>\n",
       "      <td>IC50</td>\n",
       "      <td>14.125375</td>\n",
       "      <td>7.85</td>\n",
       "      <td>120.226444</td>\n",
       "      <td>2.08</td>\n",
       "      <td>HDAC6-selective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>CHEMBL3215861</td>\n",
       "      <td>CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC</td>\n",
       "      <td>Ki</td>\n",
       "      <td>28.183829</td>\n",
       "      <td>7.55</td>\n",
       "      <td>Ki</td>\n",
       "      <td>245.470892</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.114815</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>Dual-binder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>CHEMBL3233708</td>\n",
       "      <td>O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...</td>\n",
       "      <td>IC50</td>\n",
       "      <td>354.813389</td>\n",
       "      <td>6.45</td>\n",
       "      <td>IC50</td>\n",
       "      <td>295.120923</td>\n",
       "      <td>6.53</td>\n",
       "      <td>1.202264</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Non-binder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1339 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     molecule_chembl_id                                             smiles  \\\n",
       "0         CHEMBL4098975  O=C(CCCCCCC(=O)Nc1ccc(NCCCn2cc(-c3ncnc4[nH]ccc...   \n",
       "1         CHEMBL3912061     CS(=O)(=O)NCCc1cn(Cc2ccc(C(=O)NO)cc2)c2ccccc12   \n",
       "2         CHEMBL4243347  O=C(CCCCCCC(=O)Nc1ccc(Nc2nc(-c3cn[nH]c3)c3cc[n...   \n",
       "3         CHEMBL4247128  C=CCCn1cc(-c2nc(Nc3ccc(NC(=O)CCCCCCC(=O)NO)cc3...   \n",
       "4         CHEMBL4126811  CC(C)(C)OC(=O)Nc1ccc(-c2cc(C(=O)NCc3ccc(C(=O)N...   \n",
       "...                 ...                                                ...   \n",
       "1334      CHEMBL4278591  CC(=O)Nc1ccc(-c2ccnc(Nc3cccc(OCCCCCCCC(=O)NO)c...   \n",
       "1335      CHEMBL4649511        O=C(CCCCCCCNc1nc2cc(C(=O)O)ccc2c2cnccc12)NO   \n",
       "1336      CHEMBL4291781  CC(=O)Nc1ccc(-c2ccnc(Nc3ccc(OCCCCCCCC(=O)NO)cc...   \n",
       "1337      CHEMBL3215861           CCCCc1nc2cc(/C=C/C(=O)NO)ccc2n1CCN(CC)CC   \n",
       "1338      CHEMBL3233708  O=C(/C=C/c1ccc(OC[C@H](Cc2c[nH]c3ccccc23)NCc2c...   \n",
       "\n",
       "     type_HDAC1  Standard_Value_HDAC1  pChEMBL_HDAC1 type_HDAC6  \\\n",
       "0          IC50            109.647820           6.96       IC50   \n",
       "1          IC50            616.595002           6.21       IC50   \n",
       "2          IC50              1.995262           8.70       IC50   \n",
       "3          IC50             83.176377           7.08       IC50   \n",
       "4          IC50            436.515832           6.36       IC50   \n",
       "...         ...                   ...            ...        ...   \n",
       "1334       IC50           1047.128548           5.98       IC50   \n",
       "1335       IC50              3.311311           8.48       IC50   \n",
       "1336       IC50           1698.243652           5.77       IC50   \n",
       "1337         Ki             28.183829           7.55         Ki   \n",
       "1338       IC50            354.813389           6.45       IC50   \n",
       "\n",
       "      Standard_Value_HDAC6  pChEMBL_HDAC6  SelectivityRatio  \\\n",
       "0                 0.141254           9.85        776.247117   \n",
       "1                 0.169824           9.77       3630.780548   \n",
       "2                 0.199526           9.70         10.000000   \n",
       "3                 0.251189           9.60        331.131122   \n",
       "4                 0.331131           9.48       1318.256739   \n",
       "...                    ...            ...               ...   \n",
       "1334              6.760830           8.17        154.881662   \n",
       "1335             12.882496           7.89          0.257040   \n",
       "1336             14.125375           7.85        120.226444   \n",
       "1337            245.470892           6.61          0.114815   \n",
       "1338            295.120923           6.53          1.202264   \n",
       "\n",
       "      SelectivityWindow            label  \n",
       "0                  2.89  HDAC6-selective  \n",
       "1                  3.56  HDAC6-selective  \n",
       "2                  1.00      Dual-binder  \n",
       "3                  2.52  HDAC6-selective  \n",
       "4                  3.12  HDAC6-selective  \n",
       "...                 ...              ...  \n",
       "1334               2.19  HDAC6-selective  \n",
       "1335              -0.59      Dual-binder  \n",
       "1336               2.08  HDAC6-selective  \n",
       "1337              -0.94      Dual-binder  \n",
       "1338               0.08       Non-binder  \n",
       "\n",
       "[1339 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = pd.read_csv(HDAC1and6/\"HDAC1and6_dataset.csv\", )\n",
    "df_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b33ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_labeled[['molecule_chembl_id',  'label']], on='molecule_chembl_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca3dbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 'Dual-binder']['SelectivityWindow'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63178d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_chembl_id</th>\n",
       "      <th>fp_MACCS</th>\n",
       "      <th>fp_Morgan3</th>\n",
       "      <th>fp_MorganF</th>\n",
       "      <th>fp_MAP4</th>\n",
       "      <th>SelectivityWindow</th>\n",
       "      <th>label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL4159460</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[3213041, 3821889, 42902, 17455853, 2562256, 1...</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL4250057</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1546832, 783386, 646233, 476717, 607926, 1860...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL137875</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[5547132, 4188685, 8095127, 3724449, 7332227, ...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>Non-binder</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL2442795</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[5483437, 3005158, 245334, 933718, 2729629, 48...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>Dual-binder</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  molecule_chembl_id                                           fp_MACCS  \\\n",
       "0      CHEMBL4159460  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1      CHEMBL4250057  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2       CHEMBL137875  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "3      CHEMBL2442795  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                          fp_Morgan3  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "\n",
       "                                          fp_MorganF  \\\n",
       "0  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "3  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                             fp_MAP4  SelectivityWindow  \\\n",
       "0  [3213041, 3821889, 42902, 17455853, 2562256, 1...              -0.12   \n",
       "1  [1546832, 783386, 646233, 476717, 607926, 1860...               0.63   \n",
       "2  [5547132, 4188685, 8095127, 3724449, 7332227, ...               0.27   \n",
       "3  [5483437, 3005158, 245334, 933718, 2729629, 48...               0.35   \n",
       "\n",
       "         label  Class  \n",
       "0  Dual-binder    3.0  \n",
       "1   Non-binder    4.0  \n",
       "2   Non-binder    4.0  \n",
       "3  Dual-binder    3.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['Classes'] = np.where(df['label']== 'hDAC1-selective', 2)\n",
    "df['Class'] = np.zeros(len(df))\n",
    "\n",
    "df.loc[df[df.label == 'hDAC1-selective'].index, \"Class\"] = 1.0\n",
    "df.loc[df[df.label == 'hDAC6-selective'].index, \"Class\"] = 2.0\n",
    "df.loc[df[df.label == 'Dual-binder'].index, \"Class\"] = 3.0\n",
    "df.loc[df[df.label == 'Non-binder'].index, \"Class\"] = 4.0\n",
    "df.loc[df[df.label == 'Semi-selective'].index, \"Class\"] = 5.0\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0957d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for selectivity\n",
    "df[\"selectivity\"] = np.zeros(len(df))\n",
    "\n",
    "# Mark every molecule as selective if SelectivityWindow is >=2 or >=-2, 0 otherwise\n",
    "df.loc[df[df.SelectivityWindow >= 2.0].index, \"selectivity\"] = 1.0\n",
    "df.loc[df[df.SelectivityWindow <= -2.0].index, \"selectivity\"] = 1.0\n",
    "#By using Morgan fingerprints with radius of 3 and 1024 bits\n",
    "indices =  np.array(df.index)\n",
    "X = np.array(list((df['fp_Morgan3']))).astype(float)\n",
    "#X.shape\n",
    "Y =  df[\"selectivity\"].values\n",
    "Y_class = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9534e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMS = 10\n",
    "random_state= [146736, 1367, 209056, 1847464, 89563, 967034, 3689, 689547, 578929, 7458910]\n",
    "X_tr_all = []\n",
    "Y_tr_all = []\n",
    "X_te_all = []\n",
    "Y_te_all = []\n",
    "Y_tr_class_all = []\n",
    "Y_te_class_all = []\n",
    "index_tr_all= []\n",
    "index_te_all = []\n",
    "\n",
    "for i in range(NUMS):\n",
    "    X_tr, X_te, Y_tr, Y_te, Y_tr_class, Y_te_class, index_tr, index_te = train_test_split(X, Y, Y_class,indices, test_size=0.2, random_state=random_state[i], stratify=Y_class)\n",
    "    X_tr_all.append(X_tr)\n",
    "    Y_tr_all.append(Y_tr)\n",
    "    X_te_all.append(X_te)\n",
    "    Y_te_all.append(Y_te)\n",
    "    Y_tr_class_all.append(Y_tr_class)\n",
    "    Y_te_class_all.append(Y_te_class)\n",
    "    index_tr_all.append(index_tr)\n",
    "    index_te_all.append(index_te)\n",
    "globals_dict = globals()\n",
    "    \n",
    "for i in range(0, len(index_te_all)):\n",
    "    globals_dict[f\"trainSet{i}\"] = df.iloc[index_tr_all[i]]\n",
    "    globals_dict[f\"testSet{i}\"] = df.iloc[index_te_all[i]]\n",
    "    globals_dict[f\"trainindex{i}\"] = df.index[index_tr_all[i]]\n",
    "    globals_dict[f\"testindex{i}\"] = df.index[index_te_all[i]]  \n",
    "    globals_dict[f\"X_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_trainSet{i}\"] = np.array(list(df.iloc[index_tr_all[i]]['selectivity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_trainSet{i}_class\"] = np.array(list(df.iloc[index_tr_all[i]]['Class'])).astype(float)\n",
    "    globals_dict[f\"X_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['fp_Morgan3'])).astype(float)\n",
    "    globals_dict[f\"Y_testSet{i}\"] = np.array(list(df.iloc[index_te_all[i]]['selectivity'])).astype(float)\n",
    "    \n",
    "    globals_dict[f\"Y_testSet{i}_class\"] = np.array(list(df.iloc[index_te_all[i]]['Class'])).astype(float)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7463b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import math\n",
    "\n",
    "def matrix_metrix(real_values,pred_values,beta):\n",
    "\n",
    "    CM = confusion_matrix(real_values,pred_values)\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0] \n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    Population = TN+FN+TP+FP\n",
    "    Prevalence = round( (TP+FP) / Population,2)\n",
    "    Accuracy   = round( (TP+TN) / Population,4)\n",
    "    Precision  = round( TP / (TP+FP),4 )\n",
    "    NPV        = round( TN / (TN+FN),4 )\n",
    "    FDR        = round( FP / (TP+FP),4 )\n",
    "    FOR        = round( FN / (TN+FN),4 ) \n",
    "    check_Pos  = Precision + FDR\n",
    "    check_Neg  = NPV + FOR\n",
    "    Recall     = round( TP / (TP+FN),4 )\n",
    "    FPR        = round( FP / (TN+FP),4 )\n",
    "    FNR        = round( FN / (TP+FN),4 )\n",
    "    TNR        = round( TN / (TN+FP),4 ) \n",
    "    check_Pos2 = Recall + FNR\n",
    "    check_Neg2 = FPR + TNR\n",
    "    LRPos      = round( Recall/FPR,4 ) \n",
    "    LRNeg      = round( FNR / TNR ,4 )\n",
    "    DOR        = round( LRPos/LRNeg)\n",
    "    BalancedAccuracy = round( 0.5*(Recall+TNR),4)\n",
    "    F1         = round ( 2 * ((Precision*Recall)/(Precision+Recall)),4)   \n",
    "    F1_weighted = round(f1_score(real_values, pred_values, average=\"weighted\"), 4)\n",
    "    F1_micro = round(f1_score(real_values, pred_values, average=\"micro\"), 4)\n",
    "    F1_macro = round(f1_score(real_values, pred_values, average=\"macro\"), 4)\n",
    "    FBeta      = round ( (1+beta**2)*((Precision*Recall)/((beta**2 * Precision)+ Recall)) ,4)\n",
    "    MCC        = round ( ((TP*TN)-(FP*FN))/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))  ,4)\n",
    "    BM         = Recall+TNR-1\n",
    "    MK         = Precision+NPV-1\n",
    "\n",
    "    mat_met = pd.DataFrame({\n",
    "    'Metric':['TP','TN','FP','FN','Prevalence','Accuracy','Precision','NPV','FDR','FOR','check_Pos',\n",
    "              'check_Neg','Recall','FPR','FNR','TNR','check_Pos2','check_Neg2','LR+','LR-','DOR','BalancedAccuracy',\n",
    "              'F1','F1_weighted','F1_micro', 'F1_macro', 'FBeta','MCC','BM','MK'],     \n",
    "    'Value':[TP,TN,FP,FN,Prevalence,Accuracy,Precision,NPV,FDR,FOR,check_Pos,check_Neg,Recall,FPR,FNR,TNR,check_Pos2,check_Neg2,LRPos,LRNeg,DOR,BalancedAccuracy,F1,F1_weighted,F1_micro, F1_macro, FBeta,MCC,BM,MK]})  \n",
    "    return (mat_met)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faaebf",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ce7c3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        25.500000     2.677063\n",
      "1                    TN        96.400000     2.716207\n",
      "2                    FP         4.100000     2.558211\n",
      "3                    FN         7.900000     2.685351\n",
      "4              Accuracy         0.910380     0.031269\n",
      "5             Precision         0.864992     0.082081\n",
      "6           Sensitivity         0.763601     0.080124\n",
      "7           Specificity         0.959190     0.025511\n",
      "8              F1 score         0.809035     0.068394\n",
      "9   F1 score (weighted)         0.908372     0.032170\n",
      "10     F1 score (macro)         0.875213     0.044224\n",
      "11    Balanced Accuracy         0.861394     0.045325\n",
      "12                  MCC         0.754946     0.088374\n",
      "13                  NPV         0.924620     0.024289\n",
      "14              ROC_AUC         0.861394     0.045325\n",
      "CPU times: user 43.6 s, sys: 108 ms, total: 43.7 s\n",
      "Wall time: 6.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        rf_clf =  RandomForestClassifier(random_state=1121218, max_features = None, n_jobs=8,oob_score=True,\n",
    "                                           max_samples=0.8, )\n",
    "        rf_clf.fit(x_train, y_train)\n",
    "        y_pred = rf_clf.predict(x_test)  \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "mat_met_rf = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "                    \n",
    "print(mat_met_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b453df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "\n",
    "def objective_rf_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    cv_scores = np.empty(10)\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        x_train, x_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=8, random_state=1121218, max_features = None, \n",
    "                                   oob_score=True,\n",
    "                                   max_samples=0.8,) \n",
    "        \n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred,  average=\"macro\")\n",
    "      \n",
    "    \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab658a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_rf_CV(trial,X, Y, Y_class):\n",
    "    param_grid = {\n",
    "    #min_samples_split : trial.suggest_int('min_samples_split', 2, 50)\n",
    "    #min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 25)\n",
    "    #max_depth = trial.suggest_int('max_depth', 1, 10000)\n",
    "    #\"max_features\" : trial.suggestegorical(\"max_features\", [None]),\n",
    "    #oob_score = trial.suggestegorical('oob_score', ['True','False']),\n",
    "    #max_leaf_nodes = trial.suggest_int('max_leaf_nodes', 1, 10000)\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        rf = RandomForestClassifier(**param_grid, n_jobs=8, random_state=1121218, max_features = None, oob_score=True,\n",
    "                                           max_samples=0.8,)\n",
    "   \n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f39a44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 07:52:41,070] A new study created in memory with name: RFclassifier\n",
      "[I 2023-12-05 07:53:04,968] Trial 0 finished with value: 0.865084916877071 and parameters: {'n_estimators': 508}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:53:32,125] Trial 1 finished with value: 0.8624387485044653 and parameters: {'n_estimators': 566}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:53:42,085] Trial 2 finished with value: 0.8590594560214253 and parameters: {'n_estimators': 185}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:54:23,535] Trial 3 finished with value: 0.861378427408618 and parameters: {'n_estimators': 802}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:55:01,200] Trial 4 finished with value: 0.865084916877071 and parameters: {'n_estimators': 733}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:55:45,949] Trial 5 finished with value: 0.8640245957812238 and parameters: {'n_estimators': 873}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:56:09,675] Trial 6 finished with value: 0.8636687096022364 and parameters: {'n_estimators': 457}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:56:42,354] Trial 7 finished with value: 0.8624387485044653 and parameters: {'n_estimators': 630}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:57:21,045] Trial 8 finished with value: 0.8640245957812238 and parameters: {'n_estimators': 748}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:57:42,924] Trial 9 finished with value: 0.865084916877071 and parameters: {'n_estimators': 418}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:57:52,771] Trial 10 finished with value: 0.857670554280665 and parameters: {'n_estimators': 180}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:58:12,174] Trial 11 finished with value: 0.8626419072287017 and parameters: {'n_estimators': 368}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:59:02,832] Trial 12 finished with value: 0.8629977934076891 and parameters: {'n_estimators': 981}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 07:59:35,629] Trial 13 finished with value: 0.8624387485044653 and parameters: {'n_estimators': 631}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:00:12,149] Trial 14 finished with value: 0.863465550878 and parameters: {'n_estimators': 713}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:00:27,665] Trial 15 finished with value: 0.8597339660686769 and parameters: {'n_estimators': 296}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:00:53,765] Trial 16 finished with value: 0.865084916877071 and parameters: {'n_estimators': 504}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:01:40,703] Trial 17 finished with value: 0.8640245957812238 and parameters: {'n_estimators': 917}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:02:15,035] Trial 18 finished with value: 0.8624387485044653 and parameters: {'n_estimators': 669}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:02:30,227] Trial 19 finished with value: 0.857583402742312 and parameters: {'n_estimators': 291}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:02:59,147] Trial 20 finished with value: 0.8624387485044653 and parameters: {'n_estimators': 559}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:03:24,100] Trial 21 finished with value: 0.865084916877071 and parameters: {'n_estimators': 481}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:03:45,742] Trial 22 finished with value: 0.8636687096022364 and parameters: {'n_estimators': 415}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:04:03,785] Trial 23 finished with value: 0.8597339660686769 and parameters: {'n_estimators': 344}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:04:44,027] Trial 24 finished with value: 0.861378427408618 and parameters: {'n_estimators': 783}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:05:13,751] Trial 25 finished with value: 0.861378427408618 and parameters: {'n_estimators': 576}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:05:20,292] Trial 26 finished with value: 0.8606536832933885 and parameters: {'n_estimators': 118}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:05:41,495] Trial 27 finished with value: 0.8636687096022364 and parameters: {'n_estimators': 409}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:06:07,882] Trial 28 finished with value: 0.865084916877071 and parameters: {'n_estimators': 512}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:06:40,212] Trial 29 finished with value: 0.8624387485044653 and parameters: {'n_estimators': 629}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:07:22,761] Trial 30 finished with value: 0.861378427408618 and parameters: {'n_estimators': 826}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:07:48,895] Trial 31 finished with value: 0.865084916877071 and parameters: {'n_estimators': 505}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:08:18,794] Trial 32 finished with value: 0.861378427408618 and parameters: {'n_estimators': 579}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:08:42,656] Trial 33 finished with value: 0.8625679961461223 and parameters: {'n_estimators': 460}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:08:56,499] Trial 34 finished with value: 0.8584888727188096 and parameters: {'n_estimators': 263}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:09:16,968] Trial 35 finished with value: 0.8636687096022364 and parameters: {'n_estimators': 392}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:09:44,658] Trial 36 finished with value: 0.8640581145035362 and parameters: {'n_estimators': 534}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:10:07,093] Trial 37 finished with value: 0.8636687096022364 and parameters: {'n_estimators': 432}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:10:42,799] Trial 38 finished with value: 0.863465550878 and parameters: {'n_estimators': 695}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:11:00,844] Trial 39 finished with value: 0.8597339660686769 and parameters: {'n_estimators': 344}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:11:31,526] Trial 40 finished with value: 0.8624387485044653 and parameters: {'n_estimators': 594}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:11:56,058] Trial 41 finished with value: 0.8636687096022364 and parameters: {'n_estimators': 472}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:12:22,542] Trial 42 finished with value: 0.865084916877071 and parameters: {'n_estimators': 512}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:12:47,286] Trial 43 finished with value: 0.8636687096022364 and parameters: {'n_estimators': 477}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:13:19,521] Trial 44 finished with value: 0.8624387485044653 and parameters: {'n_estimators': 623}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:13:57,487] Trial 45 finished with value: 0.8640245957812238 and parameters: {'n_estimators': 737}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:14:09,998] Trial 46 finished with value: 0.8572468363607889 and parameters: {'n_estimators': 235}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:14:29,386] Trial 47 finished with value: 0.8615411937725875 and parameters: {'n_estimators': 371}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:15:13,632] Trial 48 finished with value: 0.8624052297821528 and parameters: {'n_estimators': 863}. Best is trial 0 with value: 0.865084916877071.\n",
      "[I 2023-12-05 08:15:36,852] Trial 49 finished with value: 0.865084916877071 and parameters: {'n_estimators': 448}. Best is trial 0 with value: 0.865084916877071.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8651\n",
      "\tBest params:\n",
      "\t\tn_estimators: 508\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_rf = optuna.create_study(direction='maximize', study_name=\"RFclassifier\")\n",
    "func_rf_0 = lambda trial: objective_rf_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_rf.optimize(func_rf_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a10ec04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   44.000000\n",
      "1                    TN  195.000000\n",
      "2                    FP    4.000000\n",
      "3                    FN   25.000000\n",
      "4              Accuracy    0.891791\n",
      "5             Precision    0.916667\n",
      "6           Sensitivity    0.637681\n",
      "7           Specificity    0.979900\n",
      "8              F1 score    0.752137\n",
      "9   F1 score (weighted)    0.884792\n",
      "10     F1 score (macro)    0.841462\n",
      "11    Balanced Accuracy    0.808790\n",
      "12                  MCC    0.704227\n",
      "13                  NPV    0.886400\n",
      "14              ROC_AUC    0.808790\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_0 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    " \n",
    "data_testing = pd.DataFrame()    \n",
    "    \n",
    "optimized_rf_0.fit(X_trainSet0, Y_trainSet0,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_0 = optimized_rf_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_rf_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_rf_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_rf_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_rf_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_rf_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_rf_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_rf_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_rf_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_rf_0)\n",
    "data_testing['y_test_idx0'] = testindex0\n",
    "data_testing['y_test_Set0'] = Y_testSet0\n",
    "data_testing['y_pred_Set0'] = y_pred_rf_0\n",
    "\n",
    "\n",
    "mat_met_rf_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP), np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                           np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116b62f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 08:16:15,235] Trial 50 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 670}. Best is trial 50 with value: 0.8754500915084371.\n",
      "[I 2023-12-05 08:16:50,824] Trial 51 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 673}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:17:26,507] Trial 52 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 674}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:18:01,782] Trial 53 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 665}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:18:37,482] Trial 54 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 673}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:19:12,987] Trial 55 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 671}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:19:48,436] Trial 56 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 669}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:20:28,662] Trial 57 finished with value: 0.8732717244287181 and parameters: {'n_estimators': 758}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:21:03,707] Trial 58 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 662}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:21:41,078] Trial 59 finished with value: 0.8728793813976591 and parameters: {'n_estimators': 706}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:22:22,881] Trial 60 finished with value: 0.8744054883736585 and parameters: {'n_estimators': 793}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:22:57,195] Trial 61 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 648}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:23:31,538] Trial 62 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 650}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:24:04,126] Trial 63 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 614}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:24:42,210] Trial 64 finished with value: 0.8744054883736585 and parameters: {'n_estimators': 721}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:25:22,753] Trial 65 finished with value: 0.8732717244287181 and parameters: {'n_estimators': 768}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:25:57,195] Trial 66 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 652}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:26:40,748] Trial 67 finished with value: 0.8744054883736585 and parameters: {'n_estimators': 823}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:27:17,289] Trial 68 finished with value: 0.8728793813976591 and parameters: {'n_estimators': 689}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:27:46,336] Trial 69 finished with value: 0.8743163275634969 and parameters: {'n_estimators': 548}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:28:24,739] Trial 70 finished with value: 0.8744054883736585 and parameters: {'n_estimators': 724}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:28:59,031] Trial 71 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 648}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:29:33,502] Trial 72 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 650}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:30:05,108] Trial 73 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 597}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:30:44,478] Trial 74 finished with value: 0.8732717244287181 and parameters: {'n_estimators': 745}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:31:16,508] Trial 75 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 604}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:31:53,112] Trial 76 finished with value: 0.8744054883736585 and parameters: {'n_estimators': 691}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:32:23,496] Trial 77 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 573}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:32:57,338] Trial 78 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 641}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:33:32,752] Trial 79 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 670}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:34:10,615] Trial 80 finished with value: 0.8744054883736585 and parameters: {'n_estimators': 719}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:34:45,419] Trial 81 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 657}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:35:21,941] Trial 82 finished with value: 0.8728793813976591 and parameters: {'n_estimators': 690}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:35:55,293] Trial 83 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 631}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:36:27,575] Trial 84 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 611}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:37:08,533] Trial 85 finished with value: 0.8744054883736585 and parameters: {'n_estimators': 774}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:37:36,972] Trial 86 finished with value: 0.8756634586363251 and parameters: {'n_estimators': 536}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:38:07,824] Trial 87 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 582}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:38:42,756] Trial 88 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 661}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:39:22,189] Trial 89 finished with value: 0.8732717244287181 and parameters: {'n_estimators': 746}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:39:55,666] Trial 90 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 632}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:40:29,941] Trial 91 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 650}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:41:06,832] Trial 92 finished with value: 0.8728793813976591 and parameters: {'n_estimators': 696}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:41:42,548] Trial 93 finished with value: 0.8739239845324377 and parameters: {'n_estimators': 676}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:42:12,195] Trial 94 finished with value: 0.8743163275634969 and parameters: {'n_estimators': 559}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:42:45,111] Trial 95 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 622}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:43:19,119] Trial 96 finished with value: 0.8770455922498069 and parameters: {'n_estimators': 644}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:43:58,153] Trial 97 finished with value: 0.8732717244287181 and parameters: {'n_estimators': 736}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:44:35,144] Trial 98 finished with value: 0.8728793813976591 and parameters: {'n_estimators': 702}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:45:06,231] Trial 99 finished with value: 0.8754500915084371 and parameters: {'n_estimators': 589}. Best is trial 51 with value: 0.8770455922498069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_1 = lambda trial: objective_rf_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_rf.optimize(func_rf_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "048b4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   44.000000   44.000000\n",
      "1                    TN  195.000000  191.000000\n",
      "2                    FP    4.000000    9.000000\n",
      "3                    FN   25.000000   24.000000\n",
      "4              Accuracy    0.891791    0.876866\n",
      "5             Precision    0.916667    0.830189\n",
      "6           Sensitivity    0.637681    0.647059\n",
      "7           Specificity    0.979900    0.955000\n",
      "8              F1 score    0.752137    0.727273\n",
      "9   F1 score (weighted)    0.884792    0.871459\n",
      "10     F1 score (macro)    0.841462    0.823877\n",
      "11    Balanced Accuracy    0.808790    0.801029\n",
      "12                  MCC    0.704227    0.657735\n",
      "13                  NPV    0.886400    0.888400\n",
      "14              ROC_AUC    0.808790    0.801029\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_1 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_1.fit(X_trainSet1, Y_trainSet1,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_1 = optimized_rf_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_rf_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_rf_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_rf_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_rf_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_rf_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_rf_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_rf_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_rf_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_rf_1)\n",
    "data_testing['y_test_idx1'] = testindex1\n",
    "data_testing['y_test_Set1'] = Y_testSet1\n",
    "data_testing['y_pred_Set1'] = y_pred_rf_1\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set1'] =set1\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6fb31da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 08:45:47,802] Trial 100 finished with value: 0.864127300024353 and parameters: {'n_estimators': 714}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:46:21,613] Trial 101 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 642}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:46:56,064] Trial 102 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 655}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:47:31,406] Trial 103 finished with value: 0.864127300024353 and parameters: {'n_estimators': 673}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:48:23,474] Trial 104 finished with value: 0.864127300024353 and parameters: {'n_estimators': 991}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:48:55,666] Trial 105 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 610}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:49:31,635] Trial 106 finished with value: 0.864127300024353 and parameters: {'n_estimators': 682}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:50:09,005] Trial 107 finished with value: 0.864127300024353 and parameters: {'n_estimators': 709}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:50:51,219] Trial 108 finished with value: 0.8626366371659528 and parameters: {'n_estimators': 804}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:51:25,621] Trial 109 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 656}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:52:05,246] Trial 110 finished with value: 0.8626366371659528 and parameters: {'n_estimators': 756}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:52:38,350] Trial 111 finished with value: 0.864127300024353 and parameters: {'n_estimators': 630}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:53:12,190] Trial 112 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 644}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:53:43,740] Trial 113 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 600}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:54:22,102] Trial 114 finished with value: 0.864127300024353 and parameters: {'n_estimators': 730}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:54:57,814] Trial 115 finished with value: 0.864127300024353 and parameters: {'n_estimators': 680}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:55:27,677] Trial 116 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 567}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:56:00,214] Trial 117 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 618}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:56:33,834] Trial 118 finished with value: 0.864127300024353 and parameters: {'n_estimators': 641}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:57:10,661] Trial 119 finished with value: 0.864127300024353 and parameters: {'n_estimators': 703}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:57:45,416] Trial 120 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 660}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:58:20,036] Trial 121 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 660}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:58:51,084] Trial 122 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 589}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 08:59:27,096] Trial 123 finished with value: 0.864127300024353 and parameters: {'n_estimators': 685}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:00:00,503] Trial 124 finished with value: 0.864127300024353 and parameters: {'n_estimators': 633}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:00:35,622] Trial 125 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 664}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:01:07,574] Trial 126 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 607}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:01:44,827] Trial 127 finished with value: 0.864127300024353 and parameters: {'n_estimators': 708}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:02:18,970] Trial 128 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 648}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:02:51,589] Trial 129 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 620}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:03:29,922] Trial 130 finished with value: 0.864127300024353 and parameters: {'n_estimators': 729}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:04:03,400] Trial 131 finished with value: 0.864127300024353 and parameters: {'n_estimators': 633}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:04:39,636] Trial 132 finished with value: 0.864127300024353 and parameters: {'n_estimators': 688}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:05:14,406] Trial 133 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 661}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:05:49,494] Trial 134 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 668}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:06:22,642] Trial 135 finished with value: 0.864127300024353 and parameters: {'n_estimators': 631}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:06:51,582] Trial 136 finished with value: 0.8610885522441469 and parameters: {'n_estimators': 548}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:07:40,748] Trial 137 finished with value: 0.864127300024353 and parameters: {'n_estimators': 936}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:08:12,015] Trial 138 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 594}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:08:48,479] Trial 139 finished with value: 0.864127300024353 and parameters: {'n_estimators': 693}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:09:22,614] Trial 140 finished with value: 0.864127300024353 and parameters: {'n_estimators': 649}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:09:58,195] Trial 141 finished with value: 0.864127300024353 and parameters: {'n_estimators': 675}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:10:30,565] Trial 142 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 616}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:11:05,348] Trial 143 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 658}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:11:42,752] Trial 144 finished with value: 0.864127300024353 and parameters: {'n_estimators': 711}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:12:16,768] Trial 145 finished with value: 0.8625604740505635 and parameters: {'n_estimators': 645}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:12:52,558] Trial 146 finished with value: 0.864127300024353 and parameters: {'n_estimators': 680}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:13:23,180] Trial 147 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 581}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:13:56,058] Trial 148 finished with value: 0.8626553782179365 and parameters: {'n_estimators': 624}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:14:02,720] Trial 149 finished with value: 0.8645724325800807 and parameters: {'n_estimators': 119}. Best is trial 51 with value: 0.8770455922498069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_2 = lambda trial: objective_rf_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_rf.optimize(func_rf_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74530207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   44.000000   44.000000   48.000000\n",
      "1                    TN  195.000000  191.000000  195.000000\n",
      "2                    FP    4.000000    9.000000    6.000000\n",
      "3                    FN   25.000000   24.000000   19.000000\n",
      "4              Accuracy    0.891791    0.876866    0.906716\n",
      "5             Precision    0.916667    0.830189    0.888889\n",
      "6           Sensitivity    0.637681    0.647059    0.716418\n",
      "7           Specificity    0.979900    0.955000    0.970100\n",
      "8              F1 score    0.752137    0.727273    0.793388\n",
      "9   F1 score (weighted)    0.884792    0.871459    0.903166\n",
      "10     F1 score (macro)    0.841462    0.823877    0.866574\n",
      "11    Balanced Accuracy    0.808790    0.801029    0.843284\n",
      "12                  MCC    0.704227    0.657735    0.741165\n",
      "13                  NPV    0.886400    0.888400    0.911200\n",
      "14              ROC_AUC    0.808790    0.801029    0.843284\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimized_rf_2 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_2.fit(X_trainSet2, Y_trainSet2,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_2 = optimized_rf_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_rf_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_rf_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_rf_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_rf_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_rf_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_rf_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_rf_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_rf_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_rf_2)\n",
    "data_testing['y_test_idx2'] = testindex2\n",
    "data_testing['y_test_Set2'] = Y_testSet2\n",
    "data_testing['y_pred_Set2'] = y_pred_rf_2\n",
    "\n",
    "set2 = pd.DataFrame({'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_rf_test['Set2'] =set2\n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53b2d0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 09:14:41,672] Trial 150 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 691}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:15:14,523] Trial 151 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 650}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:15:48,362] Trial 152 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 669}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:16:20,741] Trial 153 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 640}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:16:51,080] Trial 154 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 599}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:17:26,454] Trial 155 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 700}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:17:59,919] Trial 156 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 663}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:18:30,920] Trial 157 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 615}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:19:03,000] Trial 158 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 637}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:19:39,518] Trial 159 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 725}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:20:14,106] Trial 160 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 684}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:20:47,348] Trial 161 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 656}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:21:20,086] Trial 162 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 646}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:21:54,227] Trial 163 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 674}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:22:25,997] Trial 164 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 629}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:22:56,671] Trial 165 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 610}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:23:32,398] Trial 166 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 708}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:24:05,461] Trial 167 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 652}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:24:39,522] Trial 168 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 674}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:25:11,772] Trial 169 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 638}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:25:40,832] Trial 170 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 574}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:26:08,174] Trial 171 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 542}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:26:41,692] Trial 172 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 663}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:27:16,651] Trial 173 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 692}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:27:43,616] Trial 174 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 532}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:28:14,885] Trial 175 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 619}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:28:41,069] Trial 176 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 515}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:29:11,111] Trial 177 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 595}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:29:43,734] Trial 178 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 647}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:30:18,045] Trial 179 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 678}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:30:49,961] Trial 180 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 632}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:31:23,498] Trial 181 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 663}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:31:58,689] Trial 182 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 696}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:32:32,630] Trial 183 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 672}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:33:08,686] Trial 184 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 714}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:33:41,279] Trial 185 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 646}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:34:13,155] Trial 186 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 629}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:34:46,490] Trial 187 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 660}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:35:17,230] Trial 188 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 609}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:35:51,887] Trial 189 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 684}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:36:29,530] Trial 190 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 745}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:37:03,529] Trial 191 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 671}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:37:36,492] Trial 192 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 651}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:38:11,688] Trial 193 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 696}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:38:36,525] Trial 194 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 488}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:39:08,553] Trial 195 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 631}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:39:43,036] Trial 196 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 682}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:40:16,165] Trial 197 finished with value: 0.8703346614990766 and parameters: {'n_estimators': 653}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:40:52,557] Trial 198 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 719}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:41:26,328] Trial 199 finished with value: 0.8688256188576297 and parameters: {'n_estimators': 666}. Best is trial 51 with value: 0.8770455922498069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_3 = lambda trial: objective_rf_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_rf.optimize(func_rf_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c0700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   44.000000   44.000000   48.000000   48.000000\n",
      "1                    TN  195.000000  191.000000  195.000000  191.000000\n",
      "2                    FP    4.000000    9.000000    6.000000    8.000000\n",
      "3                    FN   25.000000   24.000000   19.000000   21.000000\n",
      "4              Accuracy    0.891791    0.876866    0.906716    0.891791\n",
      "5             Precision    0.916667    0.830189    0.888889    0.857143\n",
      "6           Sensitivity    0.637681    0.647059    0.716418    0.695652\n",
      "7           Specificity    0.979900    0.955000    0.970100    0.959800\n",
      "8              F1 score    0.752137    0.727273    0.793388    0.768000\n",
      "9   F1 score (weighted)    0.884792    0.871459    0.903166    0.887876\n",
      "10     F1 score (macro)    0.841462    0.823877    0.866574    0.848720\n",
      "11    Balanced Accuracy    0.808790    0.801029    0.843284    0.827726\n",
      "12                  MCC    0.704227    0.657735    0.741165    0.704903\n",
      "13                  NPV    0.886400    0.888400    0.911200    0.900900\n",
      "14              ROC_AUC    0.808790    0.801029    0.843284    0.827726\n"
     ]
    }
   ],
   "source": [
    "optimized_rf_3 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_3.fit(X_trainSet3, Y_trainSet3,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_3 = optimized_rf_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_rf_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_rf_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_rf_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_rf_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_rf_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_rf_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_rf_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_rf_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_rf_3)\n",
    "data_testing['y_test_idx3'] = testindex3\n",
    "data_testing['y_test_Set3'] = Y_testSet3\n",
    "data_testing['y_pred_Set3'] = y_pred_rf_3\n",
    "\n",
    "\n",
    "set3 = pd.DataFrame({'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set3'] =set3   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5ca425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 09:42:03,553] Trial 200 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 617}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:42:38,249] Trial 201 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 642}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:43:12,104] Trial 202 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 625}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:43:48,158] Trial 203 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 664}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:44:21,040] Trial 204 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 606}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:44:55,812] Trial 205 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 644}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:45:32,727] Trial 206 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 681}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:46:03,127] Trial 207 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 560}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:46:40,944] Trial 208 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 699}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:47:12,730] Trial 209 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 585}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:47:48,303] Trial 210 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 656}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:48:20,685] Trial 211 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 599}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:48:54,947] Trial 212 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 632}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:49:28,320] Trial 213 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 618}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:50:03,339] Trial 214 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 646}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:50:39,757] Trial 215 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 672}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:51:14,359] Trial 216 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 638}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:51:50,008] Trial 217 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 658}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:52:22,712] Trial 218 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 604}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:52:59,684] Trial 219 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 684}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:53:33,607] Trial 220 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 625}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:54:06,041] Trial 221 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 596}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:54:34,639] Trial 222 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 527}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:55:05,689] Trial 223 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 572}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:55:41,825] Trial 224 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 667}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:56:16,697] Trial 225 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 644}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:56:50,579] Trial 226 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 625}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:57:26,473] Trial 227 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 662}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:57:59,739] Trial 228 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 612}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:58:37,452] Trial 229 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 695}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:59:12,081] Trial 230 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 639}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 09:59:42,606] Trial 231 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 561}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:00:14,236] Trial 232 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 585}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:00:45,452] Trial 233 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 575}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:01:22,121] Trial 234 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 677}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:01:57,439] Trial 235 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 652}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:02:30,916] Trial 236 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 617}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:03:00,458] Trial 237 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 544}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:03:32,760] Trial 238 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 595}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:04:08,235] Trial 239 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 655}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:04:44,960] Trial 240 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 677}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:05:21,142] Trial 241 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 669}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:05:55,591] Trial 242 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 637}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:06:33,034] Trial 243 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 693}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:07:08,508] Trial 244 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 654}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:07:42,613] Trial 245 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 630}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:08:18,832] Trial 246 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 669}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:08:53,770] Trial 247 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 644}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:09:32,004] Trial 248 finished with value: 0.8641892870449087 and parameters: {'n_estimators': 707}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:10:09,014] Trial 249 finished with value: 0.8656183270733099 and parameters: {'n_estimators': 684}. Best is trial 51 with value: 0.8770455922498069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_4 = lambda trial: objective_rf_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_rf.optimize(func_rf_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77894dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   44.000000   48.000000   48.000000   \n",
      "1                    TN  195.000000  191.000000  195.000000  191.000000   \n",
      "2                    FP    4.000000    9.000000    6.000000    8.000000   \n",
      "3                    FN   25.000000   24.000000   19.000000   21.000000   \n",
      "4              Accuracy    0.891791    0.876866    0.906716    0.891791   \n",
      "5             Precision    0.916667    0.830189    0.888889    0.857143   \n",
      "6           Sensitivity    0.637681    0.647059    0.716418    0.695652   \n",
      "7           Specificity    0.979900    0.955000    0.970100    0.959800   \n",
      "8              F1 score    0.752137    0.727273    0.793388    0.768000   \n",
      "9   F1 score (weighted)    0.884792    0.871459    0.903166    0.887876   \n",
      "10     F1 score (macro)    0.841462    0.823877    0.866574    0.848720   \n",
      "11    Balanced Accuracy    0.808790    0.801029    0.843284    0.827726   \n",
      "12                  MCC    0.704227    0.657735    0.741165    0.704903   \n",
      "13                  NPV    0.886400    0.888400    0.911200    0.900900   \n",
      "14              ROC_AUC    0.808790    0.801029    0.843284    0.827726   \n",
      "\n",
      "          Set4  \n",
      "0    41.000000  \n",
      "1   188.000000  \n",
      "2    12.000000  \n",
      "3    27.000000  \n",
      "4     0.854478  \n",
      "5     0.773585  \n",
      "6     0.602941  \n",
      "7     0.940000  \n",
      "8     0.677686  \n",
      "9     0.848088  \n",
      "10    0.791855  \n",
      "11    0.771471  \n",
      "12    0.593151  \n",
      "13    0.874400  \n",
      "14    0.771471  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_4 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_4.fit(X_trainSet4, Y_trainSet4,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_4 = optimized_rf_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_rf_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_rf_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_rf_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_rf_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_rf_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_rf_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_rf_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_rf_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_rf_4)\n",
    "data_testing['y_test_idx4'] = testindex4\n",
    "data_testing['y_test_Set4'] = Y_testSet4\n",
    "data_testing['y_pred_Set4'] = y_pred_rf_4\n",
    "\n",
    "set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set4'] =set4   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37431445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 10:10:46,136] Trial 250 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 612}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:11:22,414] Trial 251 finished with value: 0.8613377130864599 and parameters: {'n_estimators': 671}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:11:57,511] Trial 252 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 651}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:12:31,494] Trial 253 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 629}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:13:04,301] Trial 254 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 606}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:13:39,838] Trial 255 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 656}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:14:16,633] Trial 256 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 680}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:14:54,482] Trial 257 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 703}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:15:28,691] Trial 258 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 634}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:16:00,174] Trial 259 finished with value: 0.8637727145456807 and parameters: {'n_estimators': 584}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:16:36,094] Trial 260 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 665}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:17:10,977] Trial 261 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 644}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:17:44,523] Trial 262 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 620}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:18:21,633] Trial 263 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 688}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:18:52,047] Trial 264 finished with value: 0.8598470502280599 and parameters: {'n_estimators': 562}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:19:27,653] Trial 265 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 660}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:20:00,240] Trial 266 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 601}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:20:34,777] Trial 267 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 639}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:21:11,278] Trial 268 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 678}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:21:45,069] Trial 269 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 623}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:22:20,674] Trial 270 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 657}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:22:59,366] Trial 271 finished with value: 0.8637530968908056 and parameters: {'n_estimators': 718}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:23:36,503] Trial 272 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 688}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:24:04,949] Trial 273 finished with value: 0.8613377130864599 and parameters: {'n_estimators': 525}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:24:39,743] Trial 274 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 643}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:25:15,552] Trial 275 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 663}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:25:47,063] Trial 276 finished with value: 0.8627213383544596 and parameters: {'n_estimators': 585}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:26:20,311] Trial 277 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 616}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:26:49,984] Trial 278 finished with value: 0.8598470502280599 and parameters: {'n_estimators': 549}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:27:26,277] Trial 279 finished with value: 0.8613377130864599 and parameters: {'n_estimators': 672}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:28:00,506] Trial 280 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 634}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:28:38,222] Trial 281 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 699}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:29:13,305] Trial 282 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 652}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:29:46,408] Trial 283 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 612}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:30:22,785] Trial 284 finished with value: 0.8613377130864599 and parameters: {'n_estimators': 675}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:30:57,010] Trial 285 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 635}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:31:24,065] Trial 286 finished with value: 0.8598470502280599 and parameters: {'n_estimators': 501}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:31:59,951] Trial 287 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 664}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:32:32,553] Trial 288 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 602}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:33:03,498] Trial 289 finished with value: 0.8597820178127833 and parameters: {'n_estimators': 571}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:33:41,528] Trial 290 finished with value: 0.8637530968908056 and parameters: {'n_estimators': 705}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:34:18,878] Trial 291 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 687}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:34:54,087] Trial 292 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 653}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:35:28,262] Trial 293 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 632}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:36:03,259] Trial 294 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 648}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:36:39,584] Trial 295 finished with value: 0.8613377130864599 and parameters: {'n_estimators': 674}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:36:50,017] Trial 296 finished with value: 0.8641602358453102 and parameters: {'n_estimators': 186}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:37:23,670] Trial 297 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 622}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:37:55,395] Trial 298 finished with value: 0.8612726806711833 and parameters: {'n_estimators': 588}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:38:32,672] Trial 299 finished with value: 0.8623240568624047 and parameters: {'n_estimators': 693}. Best is trial 51 with value: 0.8770455922498069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_5 = lambda trial: objective_rf_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_rf.optimize(func_rf_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd17f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   44.000000   48.000000   48.000000   \n",
      "1                    TN  195.000000  191.000000  195.000000  191.000000   \n",
      "2                    FP    4.000000    9.000000    6.000000    8.000000   \n",
      "3                    FN   25.000000   24.000000   19.000000   21.000000   \n",
      "4              Accuracy    0.891791    0.876866    0.906716    0.891791   \n",
      "5             Precision    0.916667    0.830189    0.888889    0.857143   \n",
      "6           Sensitivity    0.637681    0.647059    0.716418    0.695652   \n",
      "7           Specificity    0.979900    0.955000    0.970100    0.959800   \n",
      "8              F1 score    0.752137    0.727273    0.793388    0.768000   \n",
      "9   F1 score (weighted)    0.884792    0.871459    0.903166    0.887876   \n",
      "10     F1 score (macro)    0.841462    0.823877    0.866574    0.848720   \n",
      "11    Balanced Accuracy    0.808790    0.801029    0.843284    0.827726   \n",
      "12                  MCC    0.704227    0.657735    0.741165    0.704903   \n",
      "13                  NPV    0.886400    0.888400    0.911200    0.900900   \n",
      "14              ROC_AUC    0.808790    0.801029    0.843284    0.827726   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    41.000000   49.000000  \n",
      "1   188.000000  190.000000  \n",
      "2    12.000000   10.000000  \n",
      "3    27.000000   19.000000  \n",
      "4     0.854478    0.891791  \n",
      "5     0.773585    0.830508  \n",
      "6     0.602941    0.720588  \n",
      "7     0.940000    0.950000  \n",
      "8     0.677686    0.771654  \n",
      "9     0.848088    0.889147  \n",
      "10    0.791855    0.850374  \n",
      "11    0.771471    0.835294  \n",
      "12    0.593151    0.704249  \n",
      "13    0.874400    0.909100  \n",
      "14    0.771471    0.835294  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_5 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_5.fit(X_trainSet5, Y_trainSet5,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_5 = optimized_rf_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_rf_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_rf_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_rf_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_rf_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_rf_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_rf_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_rf_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_rf_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_rf_5)\n",
    "data_testing['y_test_idx5'] = testindex5\n",
    "data_testing['y_test_Set5'] = Y_testSet5\n",
    "data_testing['y_pred_Set5'] = y_pred_rf_5\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set5'] =Set5   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f360eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 10:39:14,743] Trial 300 finished with value: 0.8528502424999364 and parameters: {'n_estimators': 719}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:39:50,183] Trial 301 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 666}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:40:24,561] Trial 302 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 646}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:40:56,545] Trial 303 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 601}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:41:29,920] Trial 304 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 626}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:42:06,369] Trial 305 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 681}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:42:41,231] Trial 306 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 657}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:43:15,314] Trial 307 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 638}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:43:48,164] Trial 308 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 617}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:44:25,444] Trial 309 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 700}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:45:01,029] Trial 310 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 668}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:45:31,386] Trial 311 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 567}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:46:05,855] Trial 312 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 647}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:46:42,039] Trial 313 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 680}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:47:21,040] Trial 314 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 732}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:47:38,326] Trial 315 finished with value: 0.8550248237098858 and parameters: {'n_estimators': 316}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:48:10,673] Trial 316 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 605}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:48:45,622] Trial 317 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 655}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:49:19,320] Trial 318 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 630}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:49:54,812] Trial 319 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 665}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:50:23,694] Trial 320 finished with value: 0.8550216062599871 and parameters: {'n_estimators': 541}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:51:00,720] Trial 321 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 693}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:51:32,062] Trial 322 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 588}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:52:06,220] Trial 323 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 640}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:52:39,420] Trial 324 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 622}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:53:15,551] Trial 325 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 675}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:53:50,332] Trial 326 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 651}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:54:27,762] Trial 327 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 704}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:54:59,926] Trial 328 finished with value: 0.8539412053220451 and parameters: {'n_estimators': 604}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:55:35,277] Trial 329 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 663}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:56:09,298] Trial 330 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 638}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:56:45,927] Trial 331 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 686}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:57:31,447] Trial 332 finished with value: 0.8528502424999364 and parameters: {'n_estimators': 858}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:58:04,710] Trial 333 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 621}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:58:40,186] Trial 334 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 663}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:59:10,972] Trial 335 finished with value: 0.8517666241120956 and parameters: {'n_estimators': 576}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 10:59:45,217] Trial 336 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 643}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:00:21,699] Trial 337 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 685}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:00:51,255] Trial 338 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 553}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:01:28,937] Trial 339 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 708}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:02:03,924] Trial 340 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 655}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:02:36,709] Trial 341 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 613}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:03:12,890] Trial 342 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 675}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:03:46,675] Trial 343 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 633}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:04:18,406] Trial 344 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 597}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:04:53,118] Trial 345 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 645}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:05:28,860] Trial 346 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 668}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:06:05,881] Trial 347 finished with value: 0.8539437021803525 and parameters: {'n_estimators': 691}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:06:39,344] Trial 348 finished with value: 0.8528600837925119 and parameters: {'n_estimators': 627}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:07:06,580] Trial 349 finished with value: 0.8528477456416288 and parameters: {'n_estimators': 506}. Best is trial 51 with value: 0.8770455922498069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_6 = lambda trial: objective_rf_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_rf.optimize(func_rf_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd421234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   44.000000   48.000000   48.000000   \n",
      "1                    TN  195.000000  191.000000  195.000000  191.000000   \n",
      "2                    FP    4.000000    9.000000    6.000000    8.000000   \n",
      "3                    FN   25.000000   24.000000   19.000000   21.000000   \n",
      "4              Accuracy    0.891791    0.876866    0.906716    0.891791   \n",
      "5             Precision    0.916667    0.830189    0.888889    0.857143   \n",
      "6           Sensitivity    0.637681    0.647059    0.716418    0.695652   \n",
      "7           Specificity    0.979900    0.955000    0.970100    0.959800   \n",
      "8              F1 score    0.752137    0.727273    0.793388    0.768000   \n",
      "9   F1 score (weighted)    0.884792    0.871459    0.903166    0.887876   \n",
      "10     F1 score (macro)    0.841462    0.823877    0.866574    0.848720   \n",
      "11    Balanced Accuracy    0.808790    0.801029    0.843284    0.827726   \n",
      "12                  MCC    0.704227    0.657735    0.741165    0.704903   \n",
      "13                  NPV    0.886400    0.888400    0.911200    0.900900   \n",
      "14              ROC_AUC    0.808790    0.801029    0.843284    0.827726   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    41.000000   49.000000   52.000000  \n",
      "1   188.000000  190.000000  191.000000  \n",
      "2    12.000000   10.000000    9.000000  \n",
      "3    27.000000   19.000000   16.000000  \n",
      "4     0.854478    0.891791    0.906716  \n",
      "5     0.773585    0.830508    0.852459  \n",
      "6     0.602941    0.720588    0.764706  \n",
      "7     0.940000    0.950000    0.955000  \n",
      "8     0.677686    0.771654    0.806202  \n",
      "9     0.848088    0.889147    0.904988  \n",
      "10    0.791855    0.850374    0.872388  \n",
      "11    0.771471    0.835294    0.859853  \n",
      "12    0.593151    0.704249    0.746921  \n",
      "13    0.874400    0.909100    0.922700  \n",
      "14    0.771471    0.835294    0.859853  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_6 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_6.fit(X_trainSet6, Y_trainSet6,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_6 = optimized_rf_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_rf_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_rf_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_rf_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_rf_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_rf_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_rf_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_rf_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_rf_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_rf_6)\n",
    "data_testing['y_test_idx6'] = testindex6\n",
    "data_testing['y_test_Set6'] = Y_testSet6\n",
    "data_testing['y_pred_Set6'] = y_pred_rf_6\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set6'] =Set6   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26e94d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 11:07:44,413] Trial 350 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 654}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:08:21,984] Trial 351 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 726}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:08:56,925] Trial 352 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 674}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:09:46,774] Trial 353 finished with value: 0.8543858555403538 and parameters: {'n_estimators': 965}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:10:18,752] Trial 354 finished with value: 0.8558882608273295 and parameters: {'n_estimators': 614}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:10:48,897] Trial 355 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 581}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:11:22,168] Trial 356 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 642}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:11:58,888] Trial 357 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 710}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:12:32,762] Trial 358 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 655}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:13:08,121] Trial 359 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 684}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:13:40,769] Trial 360 finished with value: 0.8558882608273295 and parameters: {'n_estimators': 628}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:14:11,714] Trial 361 finished with value: 0.8574958057989089 and parameters: {'n_estimators': 595}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:14:46,226] Trial 362 finished with value: 0.8558882608273295 and parameters: {'n_estimators': 665}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:15:19,441] Trial 363 finished with value: 0.8542125693690293 and parameters: {'n_estimators': 639}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:15:47,074] Trial 364 finished with value: 0.8572071720158752 and parameters: {'n_estimators': 532}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:16:23,033] Trial 365 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 693}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:16:54,542] Trial 366 finished with value: 0.8574958057989089 and parameters: {'n_estimators': 608}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:17:23,624] Trial 367 finished with value: 0.8560146977071911 and parameters: {'n_estimators': 562}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:17:57,558] Trial 368 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 655}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:18:32,758] Trial 369 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 673}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:19:05,053] Trial 370 finished with value: 0.8527314612773115 and parameters: {'n_estimators': 620}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:19:38,433] Trial 371 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 640}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:20:12,888] Trial 372 finished with value: 0.8558882608273295 and parameters: {'n_estimators': 665}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:20:49,143] Trial 373 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 699}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:21:24,785] Trial 374 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 683}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:21:58,674] Trial 375 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 652}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:22:31,139] Trial 376 finished with value: 0.8558882608273295 and parameters: {'n_estimators': 626}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:23:01,887] Trial 377 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 590}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:23:39,224] Trial 378 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 717}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:24:13,727] Trial 379 finished with value: 0.8575076268264004 and parameters: {'n_estimators': 662}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:24:46,940] Trial 380 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 638}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:25:22,362] Trial 381 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 680}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:25:54,087] Trial 382 finished with value: 0.8574958057989089 and parameters: {'n_estimators': 610}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:26:27,942] Trial 383 finished with value: 0.8558882608273295 and parameters: {'n_estimators': 649}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:27:00,562] Trial 384 finished with value: 0.8558882608273295 and parameters: {'n_estimators': 627}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:27:35,567] Trial 385 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 673}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:28:05,040] Trial 386 finished with value: 0.8560146977071911 and parameters: {'n_estimators': 567}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:28:41,843] Trial 387 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 706}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:29:15,596] Trial 388 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 650}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:29:47,002] Trial 389 finished with value: 0.8558882608273295 and parameters: {'n_estimators': 603}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:30:23,325] Trial 390 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 693}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:30:56,455] Trial 391 finished with value: 0.8544071527356116 and parameters: {'n_estimators': 634}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:31:20,913] Trial 392 finished with value: 0.8570807351360136 and parameters: {'n_estimators': 466}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:31:55,849] Trial 393 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 668}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:32:28,187] Trial 394 finished with value: 0.8542125693690293 and parameters: {'n_estimators': 619}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:32:51,378] Trial 395 finished with value: 0.8570807351360136 and parameters: {'n_estimators': 439}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:33:30,392] Trial 396 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 747}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:33:57,580] Trial 397 finished with value: 0.8572071720158752 and parameters: {'n_estimators': 522}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:34:31,919] Trial 398 finished with value: 0.8558882608273295 and parameters: {'n_estimators': 657}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:35:07,899] Trial 399 finished with value: 0.8560265187346825 and parameters: {'n_estimators': 686}. Best is trial 51 with value: 0.8770455922498069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_7 = lambda trial: objective_rf_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_rf.optimize(func_rf_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61c60073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   44.000000   48.000000   48.000000   \n",
      "1                    TN  195.000000  191.000000  195.000000  191.000000   \n",
      "2                    FP    4.000000    9.000000    6.000000    8.000000   \n",
      "3                    FN   25.000000   24.000000   19.000000   21.000000   \n",
      "4              Accuracy    0.891791    0.876866    0.906716    0.891791   \n",
      "5             Precision    0.916667    0.830189    0.888889    0.857143   \n",
      "6           Sensitivity    0.637681    0.647059    0.716418    0.695652   \n",
      "7           Specificity    0.979900    0.955000    0.970100    0.959800   \n",
      "8              F1 score    0.752137    0.727273    0.793388    0.768000   \n",
      "9   F1 score (weighted)    0.884792    0.871459    0.903166    0.887876   \n",
      "10     F1 score (macro)    0.841462    0.823877    0.866574    0.848720   \n",
      "11    Balanced Accuracy    0.808790    0.801029    0.843284    0.827726   \n",
      "12                  MCC    0.704227    0.657735    0.741165    0.704903   \n",
      "13                  NPV    0.886400    0.888400    0.911200    0.900900   \n",
      "14              ROC_AUC    0.808790    0.801029    0.843284    0.827726   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    41.000000   49.000000   52.000000   47.000000  \n",
      "1   188.000000  190.000000  191.000000  190.000000  \n",
      "2    12.000000   10.000000    9.000000   11.000000  \n",
      "3    27.000000   19.000000   16.000000   20.000000  \n",
      "4     0.854478    0.891791    0.906716    0.884328  \n",
      "5     0.773585    0.830508    0.852459    0.810345  \n",
      "6     0.602941    0.720588    0.764706    0.701493  \n",
      "7     0.940000    0.950000    0.955000    0.945300  \n",
      "8     0.677686    0.771654    0.806202    0.752000  \n",
      "9     0.848088    0.889147    0.904988    0.881431  \n",
      "10    0.791855    0.850374    0.872388    0.838287  \n",
      "11    0.771471    0.835294    0.859853    0.823383  \n",
      "12    0.593151    0.704249    0.746921    0.680079  \n",
      "13    0.874400    0.909100    0.922700    0.904800  \n",
      "14    0.771471    0.835294    0.859853    0.823383  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_7 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_7.fit(X_trainSet7, Y_trainSet7,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_7 = optimized_rf_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_rf_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_rf_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_rf_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_rf_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_rf_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_rf_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_rf_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_rf_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_rf_7)\n",
    "data_testing['y_test_idx7'] = testindex7\n",
    "data_testing['y_test_Set7'] = Y_testSet7\n",
    "data_testing['y_pred_Set7'] = y_pred_rf_7\n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set7'] =Set7   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c09790c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 11:35:42,583] Trial 400 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 582}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:36:16,214] Trial 401 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 640}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:36:51,186] Trial 402 finished with value: 0.853426425245065 and parameters: {'n_estimators': 668}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:37:20,163] Trial 403 finished with value: 0.8524139164011089 and parameters: {'n_estimators': 544}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:37:54,509] Trial 404 finished with value: 0.853426425245065 and parameters: {'n_estimators': 655}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:38:26,627] Trial 405 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 612}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:39:02,564] Trial 406 finished with value: 0.8524665778256392 and parameters: {'n_estimators': 685}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:39:35,940] Trial 407 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 634}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:40:11,285] Trial 408 finished with value: 0.8549510084760117 and parameters: {'n_estimators': 673}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:40:42,772] Trial 409 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 598}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:41:16,956] Trial 410 finished with value: 0.8524139164011089 and parameters: {'n_estimators': 649}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:41:53,935] Trial 411 finished with value: 0.8534790866695952 and parameters: {'n_estimators': 704}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:42:26,727] Trial 412 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 624}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:43:04,719] Trial 413 finished with value: 0.8519545034386484 and parameters: {'n_estimators': 726}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:43:39,617] Trial 414 finished with value: 0.853426425245065 and parameters: {'n_estimators': 665}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:44:13,830] Trial 415 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 642}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:44:50,004] Trial 416 finished with value: 0.8534790866695952 and parameters: {'n_estimators': 692}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:45:20,583] Trial 417 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 581}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:45:53,106] Trial 418 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 617}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:46:28,627] Trial 419 finished with value: 0.853426425245065 and parameters: {'n_estimators': 677}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:47:03,333] Trial 420 finished with value: 0.853426425245065 and parameters: {'n_estimators': 660}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:47:36,898] Trial 421 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 638}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:48:08,499] Trial 422 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 600}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:48:42,807] Trial 423 finished with value: 0.8544857710236332 and parameters: {'n_estimators': 653}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:49:12,514] Trial 424 finished with value: 0.8524139164011089 and parameters: {'n_estimators': 562}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:49:45,705] Trial 425 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 629}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:50:22,863] Trial 426 finished with value: 0.8534790866695952 and parameters: {'n_estimators': 707}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:50:58,672] Trial 427 finished with value: 0.8519545034386484 and parameters: {'n_estimators': 680}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:51:33,401] Trial 428 finished with value: 0.853426425245065 and parameters: {'n_estimators': 660}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:52:07,172] Trial 429 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 643}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:52:39,245] Trial 430 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 610}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:53:15,581] Trial 431 finished with value: 0.8534790866695952 and parameters: {'n_estimators': 692}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:53:50,846] Trial 432 finished with value: 0.853426425245065 and parameters: {'n_estimators': 670}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:54:12,503] Trial 433 finished with value: 0.8505637657551912 and parameters: {'n_estimators': 408}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:54:38,788] Trial 434 finished with value: 0.8544857710236332 and parameters: {'n_estimators': 497}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:55:12,039] Trial 435 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 626}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:55:43,283] Trial 436 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 591}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:56:17,390] Trial 437 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 647}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:56:52,746] Trial 438 finished with value: 0.853426425245065 and parameters: {'n_estimators': 674}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:57:27,416] Trial 439 finished with value: 0.853426425245065 and parameters: {'n_estimators': 656}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:58:00,328] Trial 440 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 624}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:58:37,683] Trial 441 finished with value: 0.8534790866695952 and parameters: {'n_estimators': 712}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:59:13,851] Trial 442 finished with value: 0.8534790866695952 and parameters: {'n_estimators': 693}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 11:59:42,508] Trial 443 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 545}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:00:15,846] Trial 444 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 638}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:00:50,893] Trial 445 finished with value: 0.853426425245065 and parameters: {'n_estimators': 666}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:01:20,966] Trial 446 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 571}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:01:53,218] Trial 447 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 611}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:02:29,043] Trial 448 finished with value: 0.8534790866695952 and parameters: {'n_estimators': 681}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:03:03,021] Trial 449 finished with value: 0.8534732621796772 and parameters: {'n_estimators': 648}. Best is trial 51 with value: 0.8770455922498069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_8 = lambda trial: objective_rf_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_rf.optimize(func_rf_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b28fc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   44.000000   48.000000   48.000000   \n",
      "1                    TN  195.000000  191.000000  195.000000  191.000000   \n",
      "2                    FP    4.000000    9.000000    6.000000    8.000000   \n",
      "3                    FN   25.000000   24.000000   19.000000   21.000000   \n",
      "4              Accuracy    0.891791    0.876866    0.906716    0.891791   \n",
      "5             Precision    0.916667    0.830189    0.888889    0.857143   \n",
      "6           Sensitivity    0.637681    0.647059    0.716418    0.695652   \n",
      "7           Specificity    0.979900    0.955000    0.970100    0.959800   \n",
      "8              F1 score    0.752137    0.727273    0.793388    0.768000   \n",
      "9   F1 score (weighted)    0.884792    0.871459    0.903166    0.887876   \n",
      "10     F1 score (macro)    0.841462    0.823877    0.866574    0.848720   \n",
      "11    Balanced Accuracy    0.808790    0.801029    0.843284    0.827726   \n",
      "12                  MCC    0.704227    0.657735    0.741165    0.704903   \n",
      "13                  NPV    0.886400    0.888400    0.911200    0.900900   \n",
      "14              ROC_AUC    0.808790    0.801029    0.843284    0.827726   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    41.000000   49.000000   52.000000   47.000000   51.000000  \n",
      "1   188.000000  190.000000  191.000000  190.000000  198.000000  \n",
      "2    12.000000   10.000000    9.000000   11.000000    4.000000  \n",
      "3    27.000000   19.000000   16.000000   20.000000   15.000000  \n",
      "4     0.854478    0.891791    0.906716    0.884328    0.929104  \n",
      "5     0.773585    0.830508    0.852459    0.810345    0.927273  \n",
      "6     0.602941    0.720588    0.764706    0.701493    0.772727  \n",
      "7     0.940000    0.950000    0.955000    0.945300    0.980200  \n",
      "8     0.677686    0.771654    0.806202    0.752000    0.842975  \n",
      "9     0.848088    0.889147    0.904988    0.881431    0.926822  \n",
      "10    0.791855    0.850374    0.872388    0.838287    0.898596  \n",
      "11    0.771471    0.835294    0.859853    0.823383    0.876463  \n",
      "12    0.593151    0.704249    0.746921    0.680079    0.803209  \n",
      "13    0.874400    0.909100    0.922700    0.904800    0.929600  \n",
      "14    0.771471    0.835294    0.859853    0.823383    0.876463  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_8 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_8.fit(X_trainSet8, Y_trainSet8,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_8 = optimized_rf_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_rf_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_rf_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_rf_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_rf_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_rf_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_rf_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_rf_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_rf_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_rf_8)\n",
    "data_testing['y_test_idx8'] = testindex8\n",
    "data_testing['y_test_Set8'] = Y_testSet8\n",
    "data_testing['y_pred_Set8'] = y_pred_rf_8\n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "mat_met_rf_test['Set8'] =Set8   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "282487d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:03:39,942] Trial 450 finished with value: 0.8654894821747658 and parameters: {'n_estimators': 629}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:04:18,235] Trial 451 finished with value: 0.8655801594176495 and parameters: {'n_estimators': 732}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:04:49,076] Trial 452 finished with value: 0.8650318157245405 and parameters: {'n_estimators': 592}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:05:23,654] Trial 453 finished with value: 0.865554997823496 and parameters: {'n_estimators': 664}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:06:00,016] Trial 454 finished with value: 0.864123416931796 and parameters: {'n_estimators': 695}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:06:34,138] Trial 455 finished with value: 0.8640459551820492 and parameters: {'n_estimators': 652}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:07:09,563] Trial 456 finished with value: 0.8626056482690189 and parameters: {'n_estimators': 681}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:07:21,908] Trial 457 finished with value: 0.8626485199161964 and parameters: {'n_estimators': 231}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:07:54,003] Trial 458 finished with value: 0.8650318157245405 and parameters: {'n_estimators': 618}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:08:27,423] Trial 459 finished with value: 0.8639717135119886 and parameters: {'n_estimators': 638}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:09:02,059] Trial 460 finished with value: 0.865554997823496 and parameters: {'n_estimators': 664}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:09:33,784] Trial 461 finished with value: 0.8650318157245405 and parameters: {'n_estimators': 606}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:10:20,343] Trial 462 finished with value: 0.8652475705306557 and parameters: {'n_estimators': 896}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:10:47,522] Trial 463 finished with value: 0.8677427440434394 and parameters: {'n_estimators': 517}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:11:21,445] Trial 464 finished with value: 0.8655637238448264 and parameters: {'n_estimators': 650}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:11:54,434] Trial 465 finished with value: 0.8639717135119886 and parameters: {'n_estimators': 632}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:12:32,058] Trial 466 finished with value: 0.8655801594176495 and parameters: {'n_estimators': 717}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:13:07,316] Trial 467 finished with value: 0.8640459551820492 and parameters: {'n_estimators': 675}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:13:37,406] Trial 468 finished with value: 0.8651005514529568 and parameters: {'n_estimators': 573}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:14:13,467] Trial 469 finished with value: 0.8626056482690189 and parameters: {'n_estimators': 691}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:14:47,664] Trial 470 finished with value: 0.865554997823496 and parameters: {'n_estimators': 655}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:15:16,638] Trial 471 finished with value: 0.8650230897032098 and parameters: {'n_estimators': 553}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:15:47,048] Trial 472 finished with value: 0.8654894821747658 and parameters: {'n_estimators': 597}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:16:19,513] Trial 473 finished with value: 0.8650318157245405 and parameters: {'n_estimators': 620}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:16:52,904] Trial 474 finished with value: 0.865554997823496 and parameters: {'n_estimators': 639}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:17:27,217] Trial 475 finished with value: 0.8640459551820492 and parameters: {'n_estimators': 674}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:18:02,862] Trial 476 finished with value: 0.8655801594176495 and parameters: {'n_estimators': 699}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:18:36,608] Trial 477 finished with value: 0.865554997823496 and parameters: {'n_estimators': 661}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:19:10,316] Trial 478 finished with value: 0.8654894821747658 and parameters: {'n_estimators': 645}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:19:41,675] Trial 479 finished with value: 0.8650318157245405 and parameters: {'n_estimators': 614}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:20:16,150] Trial 480 finished with value: 0.8626056482690189 and parameters: {'n_estimators': 683}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:20:46,683] Trial 481 finished with value: 0.8650318157245405 and parameters: {'n_estimators': 584}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:21:26,621] Trial 482 finished with value: 0.8655801594176495 and parameters: {'n_estimators': 765}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:21:59,521] Trial 483 finished with value: 0.8654894821747658 and parameters: {'n_estimators': 631}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:22:18,655] Trial 484 finished with value: 0.8670356049518159 and parameters: {'n_estimators': 368}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:22:53,390] Trial 485 finished with value: 0.865554997823496 and parameters: {'n_estimators': 664}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:23:29,718] Trial 486 finished with value: 0.864123416931796 and parameters: {'n_estimators': 707}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:24:03,618] Trial 487 finished with value: 0.8655637238448264 and parameters: {'n_estimators': 650}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:24:38,967] Trial 488 finished with value: 0.8640459551820492 and parameters: {'n_estimators': 677}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:25:10,613] Trial 489 finished with value: 0.8650318157245405 and parameters: {'n_estimators': 605}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:25:43,514] Trial 490 finished with value: 0.8654894821747658 and parameters: {'n_estimators': 630}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:26:16,841] Trial 491 finished with value: 0.865554997823496 and parameters: {'n_estimators': 653}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:26:52,047] Trial 492 finished with value: 0.8626056482690189 and parameters: {'n_estimators': 691}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:27:25,401] Trial 493 finished with value: 0.8639717135119886 and parameters: {'n_estimators': 638}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:27:59,700] Trial 494 finished with value: 0.8640459551820492 and parameters: {'n_estimators': 674}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:28:31,643] Trial 495 finished with value: 0.8654894821747658 and parameters: {'n_estimators': 625}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:29:05,294] Trial 496 finished with value: 0.865554997823496 and parameters: {'n_estimators': 662}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:29:35,842] Trial 497 finished with value: 0.8665408583659872 and parameters: {'n_estimators': 601}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:30:12,247] Trial 498 finished with value: 0.8655801594176495 and parameters: {'n_estimators': 717}. Best is trial 51 with value: 0.8770455922498069.\n",
      "[I 2023-12-05 12:30:45,017] Trial 499 finished with value: 0.8670727664862733 and parameters: {'n_estimators': 641}. Best is trial 51 with value: 0.8770455922498069.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_rf_1 = optuna.create_study(direction='maximize', study_name=\"RFclfressor_1\")\n",
    "func_rf_9 = lambda trial: objective_rf_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_rf.optimize(func_rf_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d6f415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   44.000000   48.000000   48.000000   \n",
      "1                    TN  195.000000  191.000000  195.000000  191.000000   \n",
      "2                    FP    4.000000    9.000000    6.000000    8.000000   \n",
      "3                    FN   25.000000   24.000000   19.000000   21.000000   \n",
      "4              Accuracy    0.891791    0.876866    0.906716    0.891791   \n",
      "5             Precision    0.916667    0.830189    0.888889    0.857143   \n",
      "6           Sensitivity    0.637681    0.647059    0.716418    0.695652   \n",
      "7           Specificity    0.979900    0.955000    0.970100    0.959800   \n",
      "8              F1 score    0.752137    0.727273    0.793388    0.768000   \n",
      "9   F1 score (weighted)    0.884792    0.871459    0.903166    0.887876   \n",
      "10     F1 score (macro)    0.841462    0.823877    0.866574    0.848720   \n",
      "11    Balanced Accuracy    0.808790    0.801029    0.843284    0.827726   \n",
      "12                  MCC    0.704227    0.657735    0.741165    0.704903   \n",
      "13                  NPV    0.886400    0.888400    0.911200    0.900900   \n",
      "14              ROC_AUC    0.808790    0.801029    0.843284    0.827726   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    41.000000   49.000000   52.000000   47.000000   51.000000   49.000000  \n",
      "1   188.000000  190.000000  191.000000  190.000000  198.000000  197.000000  \n",
      "2    12.000000   10.000000    9.000000   11.000000    4.000000    4.000000  \n",
      "3    27.000000   19.000000   16.000000   20.000000   15.000000   18.000000  \n",
      "4     0.854478    0.891791    0.906716    0.884328    0.929104    0.917910  \n",
      "5     0.773585    0.830508    0.852459    0.810345    0.927273    0.924528  \n",
      "6     0.602941    0.720588    0.764706    0.701493    0.772727    0.731343  \n",
      "7     0.940000    0.950000    0.955000    0.945300    0.980200    0.980100  \n",
      "8     0.677686    0.771654    0.806202    0.752000    0.842975    0.816667  \n",
      "9     0.848088    0.889147    0.904988    0.881431    0.926822    0.914503  \n",
      "10    0.791855    0.850374    0.872388    0.838287    0.898596    0.881891  \n",
      "11    0.771471    0.835294    0.859853    0.823383    0.876463    0.855721  \n",
      "12    0.593151    0.704249    0.746921    0.680079    0.803209    0.773425  \n",
      "13    0.874400    0.909100    0.922700    0.904800    0.929600    0.916300  \n",
      "14    0.771471    0.835294    0.859853    0.823383    0.876463    0.855721  \n"
     ]
    }
   ],
   "source": [
    "optimized_rf_9 = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "\n",
    "\n",
    "#learn\n",
    "        \n",
    "optimized_rf_9.fit(X_trainSet9, Y_trainSet9,)\n",
    "\n",
    "#predict        \n",
    "y_pred_rf_9 = optimized_rf_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_rf_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_rf_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_rf_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_rf_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_rf_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_rf_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_rf_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_rf_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_rf_9)\n",
    "data_testing['y_test_idx9'] = testindex9\n",
    "data_testing['y_test_Set9'] = Y_testSet9\n",
    "data_testing['y_pred_Set9'] = y_pred_rf_9\n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })  \n",
    "\n",
    "mat_met_rf_test['Set9'] =Set9   \n",
    "print(mat_met_rf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56f46996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8770\n",
      "\tBest params:\n",
      "\t\tn_estimators: 673\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\tNumber of trials: {len(study_rf.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_rf.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_rf.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11f01be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwgAAAHJCAYAAAAhLh4vAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACb9UlEQVR4nOzde1xUZf4H8M85c+EOgqjgBRQvVOZtwxJFUSut1k1NU3M1rZ9h983cLm5X2622y2Ztl02sVbuYqXjLzUumGSRm1iapq6YoooCC3BGYyzm/P8aZ5s7MMDMw8Hm/Xhac85xznnmAmfN9zvM8X0GWZRlEREREREQAxJauABERERERtR4MEIiIiIiIyIQBAhERERERmTBAICIiIiIiEwYIRERERERkwgCBiIiIiIhMGCAQEREREZEJAwQiIiIiIjJhgEBERERERCYMEIgC3OjRoyEIgk+vMXfuXAiCgNOnT/v0Oq5asWIFBEHAihUrWroqXtHWXo8v+eP3nYiovWOAQOShAwcO4K677kJSUhJCQkIQGRmJAQMG4LHHHsO5c+e8dp3WdnPuD9988w0EQcDzzz/f0lVxmfEmf+7cuQ7LGF/X6NGjvXrt559/HoIg4JtvvvHqef3B+Ptt/i8sLAwDBgzAX/7yF1RWVvrkur74ORARtRXKlq4AUaCRZRlPPvkkXn31VSiVStx44424/fbbodFosHfvXrz++ut47733sHLlSkydOtXn9fnoo49w6dIln17j5ZdfxpNPPolu3br59Dqumjx5MoYNG4b4+PiWropXtLXX44mJEydi8ODBAICSkhJ88cUXePnll7Fu3Trs378fHTp0aNH6ERG1JwwQiNz0wgsv4NVXX0XPnj2xZcsW9O/f32J/VlYWZs2ahRkzZmDHjh0YO3asT+uTkJDg0/MDQHx8fKu6eY2KikJUVFRLV8Nr2trr8cSkSZMsnr68/vrruO6663DkyBG8/fbbeOaZZ1quckRE7QyHGBG54dSpU/jb3/4GlUqFzZs32wQHADBlyhQsWbIEer0e9913HyRJMu0zH2u+ZcsWDB8+HGFhYYiOjsbUqVPx66+/WpxLEASsXLkSANCrVy/TEIyePXuaytgbk20+ROfAgQO46aab0KFDB3To0AFTpkxBYWEhAODXX3/FtGnT0KlTJ4SEhGDMmDHIy8uzeU32hjn17NnTZmiI+T/zm73jx4/jySefREpKCjp16oSgoCAkJibinnvuwZkzZ2yuNWbMGADA4sWLLc5pHELjbMz+gQMHcNttt6Fz586m69x3330oKipy+rqWLl2KAQMGIDg4GF26dME999zjs+Et1hy9nv/+97+YPn06EhMTERQUhI4dO2LgwIH405/+BK1WC8Dwc1i8eDEAYMyYMRbtZa6oqAj3338/evbsCbVajU6dOmHy5Mn44YcfnNbnP//5D0aNGoXIyEgIgoCKigqEhoaid+/ekGXZ7uuZMGECBEHAjz/+6HGbhIeHY86cOQCA77//vsnykiThvffew9ChQxEeHo6wsDCkpKTgvffes/s3CAB79uyxaK9AGtJGRORLfIJA5Ibly5dDp9Ph9ttvx4ABAxyWmzdvHl544QUcP34ce/bsMd3wGq1fvx5bt27F5MmTMXr0aPz888/IysrC7t27sXfvXiQnJwMAnnvuOWzcuBEHDx7En/70J9MwC1eHW/zwww945ZVXkJ6ejnnz5uGXX37B+vXrcejQIWzYsAFpaWm46qqrcOedd+LMmTPIysrCDTfcgPz8fISHhzs99yOPPGL3BvqLL77ATz/9hNDQUIvX+/7772PMmDEYPnw41Go1Dh06hA8//BCbN2/Gjz/+iO7duwMw9CQDwMqVK5Genm4xTtw8MLJn06ZNuP322yEIAqZOnYqEhAQcOHAA77//PjZt2oScnBwkJSXZHPf4449j+/bt+MMf/oBx48Zh9+7d+OCDD0w/v5bw888/IzU1FaIo4tZbb0WvXr1QXV2NEydO4F//+hdefPFFqFQqPPLII9i4cSP27NmDOXPm2G2j/Px8pKWlobi4GNdffz3uuOMOFBYWYu3atfjPf/6DtWvXYuLEiTbHrV27Ftu2bcMtt9yCe++9F6dOnUJ0dDRmzJiB5cuXY+fOnbjxxhstjiksLMTWrVtxzTXX4JprrmlWGzgKQOyZOXMmPv/8cyQkJGDevHkQBAEbNmzAAw88gG+//RarV68GAAwePBjPPfccFi9ejMTERItAlnMSiIguk4nIZWPGjJEByJmZmU2WveOOO2QA8l//+lfTtuXLl8sAZADyF198YVH+zTfflAHIY8eOtdg+Z84cGYB86tQpu9dJT0+Xrf+Ud+/ebbrOJ598YrHv7rvvlgHIUVFR8t/+9jeLfS+++KIMQH7zzTfdqoPRjh07ZKVSKffp00cuLS01bT979qzc0NBgU/7LL7+URVGU58+fb7f+zz33nN3rGNtx+fLlpm01NTVyTEyMrFAo5O+++86i/EsvvSQDkG+44Qa7ryshIUEuKCgwbddqtfLIkSNlAPK+ffucvmbrOg0aNEh+7rnn7P4zXi89Pb3J17NgwQIZgLxhwwaba5WXl8t6vd70/XPPPScDkHfv3m23bjfeeKMMQP773/9usT07O1sWRVGOjo6Wq6urbeojCIK8detWm/MdOHBABiBPmTLFZt8zzzzj8t+ILP/2MzB/7bIsy3V1dXL//v1lAPLixYtN2+39vn/66acyADklJUWura01ba+trZV/97vf2f07sPdzICIiAz5BIHJDSUkJAKBHjx5NljWWsTe0ZezYsZgwYYLFtgcffBBvv/02du3ahYKCAiQmJja7viNHjsQf//hHi21z5szBv//9b0RHR+PJJ5+02Ddr1iw89dRT+Pnnn92+1qFDhzB16lRERUXhyy+/RGxsrGmfo8nNN998M6666irs2LHD7etZ27hxI8rLy/HHP/4Rw4cPt9j35z//GUuXLsXOnTvttu2zzz5rMZdDqVTirrvuQnZ2Nn744Qdcd911Ltfj4MGDOHjwYPNeDGAaBmP+JMYoOjra5fOcPXsWX331FRITE7Fw4UKLfWlpaZgxYwZWrVqFDRs24M4777TYf+utt+Kmm26yOec111yDoUOHYvPmzTh//jy6dOkCANDr9fjwww8RERGBmTNnulxHwPDzMw5hO3/+PL744gucO3cOvXv3xkMPPeT02H//+98ADJPpw8LCTNvDwsLw97//HePGjcOHH35o87dARET2cQ4CkRvky0MeXFmH3VjGXtn09HSbbQqFAmlpaQAMY8+9wd4Qj65duwIwDLVQKBR29509e9at6xQXF+P3v/89GhsbsWHDBvTt29divyzL+OSTT3DDDTegU6dOUCqVpnHfhw4d8sqysMY2sx7OBQAqlcrU5vbaNiUlxWabMcCrqKhwqx5z5syBLMt2/+3evdvl88yYMQMKhQKTJk3CnDlz8NFHH+HkyZNu1QX47fWOHDkSSqVtn9ANN9wAAPjpp59s9jkLjO6//35otVrTzTlgGF5WVFSEWbNmWdyou2LTpk1YvHgxFi9ejJUrVyIyMhKPPfYY9u/f32RA9N///heiKNr9uxozZgwUCoXd10dERPYxQCByg3ElH+MkX2eMN9n2Vv8x9rhai4uLAwBUVVV5WkUL9lbGMd4kOttnnADrirq6OkyYMAGFhYVYvnw5Ro4caVPm0UcfxezZs3HkyBGMHz8eCxcuxHPPPYfnnnsOiYmJ0Gg0Ll/PEWObGdvQmvHnYK9tnbWFXq9vdt08MXToUGRnZ2Ps2LFYu3Yt5syZgz59+uDKK6/E559/7vJ5mtMujo4BgOnTpyMmJgYffPCBKXBeunQpAODee+91uX5Gy5cvNwVSly5dwpEjR/Dqq68iJiamyWOrqqoQExMDlUpls0+pVCI2NhbV1dVu14mIqL3iECMiN6SlpWH37t3YuXMn5s2b57CcXq839RaPGDHCZv/58+ftHmccwhQoS15KkoQ77rgDP/30E1588UXccccdNmUuXLiAf/7zn7j66quxd+9eREREWOz/7LPPvFIXY5sZ29BacXGxRblAkJqaii1btqCxsRE//vgjtm3bhrfffht33HEHOnXq5NISus1pF2dPykJCQjB37ly88cYb+Oqrr9CvXz/s2LEDw4YNw8CBA115eV4TFRWF8vJyaLVamyBBp9OhrKwMkZGRfq0TEVEg4xMEIjfMnTsXCoUC69evx5EjRxyW+/e//42ioiIkJyfbHfZgb2UcvV6PnJwcAMCQIUNM243DgFqqJ9uZRx55BF988QXuvvtu/OUvf7FbJj8/H5IkYdy4cTbBwdmzZ5Gfn29zjCev2dhm9rIJ63Q6U9v+7ne/c/mcrUVQUBCGDx+OF154Af/85z8hyzI2btxo2u+svYztkpOTA51OZ7PfGMh60i733XcfBEHA0qVLsWzZMkiShPnz57t9nuYaMmQIJEnCt99+a7Pv22+/hV6vt3l9oii2yr8pIqLWgAECkRuSkpLwl7/8BVqtFn/4wx/sBgkbN27En/70JygUCrz33nsQRds/s127dmHLli0W29555x2cPHkSY8aMsZhE27FjRwCuDWvypzfffBNvv/02rr/+erz//vsOyxmX3czJybG4IautrcU999xj96bVk9c8adIkxMTE4LPPPsO+ffts6pqfn48bbrjBL4nlvCE7O9vusB/j06fg4GDTNmft1b17d9x44404ffo03nzzTYt933//PVatWoXo6GhMnjzZ7Tr26dMHN954IzZv3ozMzEx06NAB06dPd/s8zXX33XcDABYtWmSRVfzSpUumifj/93//Z3FMx44dW93fFBFRa8EhRkRuev7551FXV4c33ngDgwYNwvjx49G/f39otVrs3bsX33//PUJCQvDZZ585HAJy6623YvLkyZg8eTL69OmDgwcP4ssvv0RMTAzee+89i7LXX389XnvtNdxzzz2YMmUKwsPD0aFDBzz44IP+eLl2lZSUYOHChRAEAQMGDMCLL75oU2bw4MGYNGkS4uLiMGPGDKxevRqDBw/GuHHjUFVVha+++grBwcEYPHiwzapJycnJ6NatG1avXg2VSoWEhAQIgoDZs2c7XN0pPDwc//73v3H77bcjPT0dt99+OxISEvDjjz9ix44diIuLM42RDwT/+Mc/sGPHDowePRpJSUkIDw/H4cOHsXXrVnTo0AEZGRmmsmPGjIEoili0aBF++eUX06Tep59+GgDw/vvvY8SIEXjsscewY8cOpKSkmPIgiKKI5cuX2zzdcdV9992HHTt2oKysDA8//DBCQkKa/+LdNHPmTGzatAlr1qxB//79MWnSJAiCgI0bN+LUqVOYNm2azQpG119/PVavXo2JEydiyJAhUCqVGDVqFEaNGuX3+hMRtTots7oqUeD7/vvv5TvvvFPu2bOnHBwcLIeFhcn9+/eXFy5cKBcWFto9xny9+y1btsjDhg2TQ0ND5aioKPm2226Tjx07Zve4f/zjH/IVV1whq9VqGYCcmJho2ucsD4K9PAKnTp2SAchz5syxey3YWR/eOg+C8RzO/pmfv66uTv7LX/4i9+7dWw4KCpK7d+8u33///XJZWZnd+suyLO/fv18eO3asHBkZKQuCYLHOv728AebHTZo0SY6NjZVVKpXco0cP+d5775XPnTtnU9ZZfoemcjFYM9bJUbuan9OVPAjbt2+X586dK1955ZVyZGSkHBoaKvfr109+6KGH5NOnT9uc++OPP5YHDRokBwcHm34G5s6ePSvfe++9ckJCgqxSqeSOHTvKEydOlPfv3+/wtdhrX2s6nU6OjY2VAciHDx9usrw1R3kQHHH0+6LX6+V3331Xvuaaa+SQkBA5JCRE/t3vfie/8847FjkjjM6fPy/fcccdcufOnWVRFN36WRMRtXWCLLuRqpKImmXFihW46667sHz5cosMrkSB6uTJk+jbty/S0tLszgEgIqLAwzkIRETksddeew2yLLfokDciIvIuzkEgIiK3FBQU4OOPP8avv/6Kjz/+GEOGDMHUqVNbulpEROQlDBCIiMgtp06dwjPPPIOwsDCMHz8e//rXv+yu1kVERIGJcxCIiIiIiMiEXT5ERERERGTCAIGIiIiIiEwYIBARERERkQkDBCIiIiIiMuEqRl5QUVEBnU7n9fN26tQJpaWlXj8vWWI7+wfb2X/Y1v7BdvYfb7e1UqlEdHS0185H1NYwQPACnU4HrVbr1XMKgmA6Nxea8h22s3+wnf2Hbe0fbGf/YVsT+R+HGBERERERkQkDBCIiIiIiMmGAQEREREREJgwQiIiIiIjIhJOUiYiIiFpAfX09zp8/D1mWOQGbfC40NBRxcXEulWWAQERERORn9fX1OHfuHCIiIiCKHNBBvldXV4fKykp06NChybL8jSQiIiLys/PnzzM4IL8KDQ1FRUWFS2X5W0lERETkZ7IsMzggvxIEweWhbPzNJCIiIvIzzjmg1owBAnmd8U1PkiSLiVfmb4bWE7LsvVE6mrRl7zjrcs7OzTdlIiIiIsc4SZm8ok6jR2ZuEXafqMTFOh2sb8EFAGqF4f9aCZAByDIgCkCQUkSIWoRKFDEsMRxaPbDrRCUadRIAIFgpYkyfDlApBOwrqIFGr0e9VgZkGRIArV6GWiEgIkiBqGAlahr10EoS6rUyBAAhahEKQUBkkAI1jXroZRlKUcTIpEjMH97Nr+1ERETUHlxzzTXIyMjA/Pnzm1WmuVavXo2nn34aJ06c8Nk1vKG11ZMBAjVbnUaPjDXHcaq8wWEZGUCj3na7XgYuaSVc0hqCgY2Hym3KXNJK+M//bLeba9DJaNDpUFqns3s8AFyo1Vpsz8orw4HCWnzxJ9eW/CIiImrvzp07h9deew1ff/01ysvL0aVLF9x8881YuHAhYmJi3DrX9u3bERoa6rW62Qs4Jk6ciOuvv95r17D2xRdf4J577sGBAwfQvXt3m/3Dhw/H6NGj8dJLL/msDr7AAIGaLTO3CAWXgwOlpEOQTtvEEa1H2fl6/HPjT5jzu2jDIw3yCVkQoK+uhlRby3b2Mba1f7Cd/UjJW5WmyLIMQRB8fp3Tp0/jlltuQe/evbF06VIkJCTg2LFjWLx4Mb7++mts3boV0dHRLp8vNjbWh7U1CAkJQUhIiM/Of9NNNyEmJgaff/45Fi5caLHv+++/x4kTJ5CZmemz6/sK/+qo2bLzqyEBCNU24A/530Ep2fbit2aKcyo05sfCZlwUeY8AlIdHoLG2hu3sa2xr/2A7+40iKQlISmrparQ6dRo9/pVzFt+erIBOkqEUBYzqHY370rojTK3wyTWffPJJqNVqrFmzxnTT3b17d1x99dW47rrr8NJLL+G1114zla+trcW9996Lbdu2ISIiAn/6058wb948037rHv/q6mosXrwYW7duRUNDAwYPHowXXngBV199temYbdu24R//+AeOHj2KsLAwDBs2DCtWrMCkSZNQWFiIZ555Bs888wwA4MKFCxZDd06cOIHhw4fju+++Q9++fU3n/Ne//oUPPvgABw4cgCAIOHbsGJ5//nnk5uYiNDQUo0ePxl//+ld07NjRpk1UKhWmTp2K1atX49FHH7UI1D777DMMGjQIV199Nf71r39h9erVKCgoQIcOHTBu3Dg8++yzCA8Pt9vWDz30EKqqqvDRRx+Ztj399NM4dOgQNm7cCMAQGL7zzjtYuXIlLly4gKSkJCxcuBB/+MMfXP6ZOsIAgZpFlmXoJMMQng6NNabgQC/65s3JF3QQgQCqbyASBEBQKiAoFOxs9TG2tX+wnf1I9H3PeKCp0+hx96rDOH2xAZLZ9rU/n8cPZ6rw75n9vR4kVFRUYPfu3fjLX/5i0yPfpUsXTJkyBZs2bcKrr75qukl+99138cgjj+Cxxx7D7t278cwzz6BPnz4YPXq0zfllWcbMmTMRHR2NVatWITIyEitXrsTUqVORm5uL6OhofPXVV7jrrrvwyCOP4N1334VGo8HOnTsBAMuXL8eYMWMwe/ZszJo1y+5r6NOnDwYNGoSsrCw8+eSTpu3r16/HbbfdBkEQcP78eUyaNAmzZs3CCy+8gIaGBrzwwgu45557sH79ervn/eMf/4j3338fe/fuxYgRIwAYkpJt2rQJzz77LABAFEW8+OKL6NGjB86cOYMnnngCL7zwAl599VX3fhBmXn75ZfznP//Bq6++iqSkJOzbtw/3338/OnbsiOHDh3t8XoABAjWTIAhQXl7HWSkZJhlcCI3GV4nXtmS13NI9OgR333klVzfyIUEQEBsfD21xMdvZx9jW/sF29h9/DJ0JNP/KOWsTHACAJAOnyxvwr5yz+PPYRK9eMz8/H7IsW/S8m+vbty8qKytRVlaGTp06AQCuvfZaPPzwwwCA3r17Y//+/Vi6dKndACEnJwf/+9//cOTIEQQFBQGA6WnCF198gTvvvBNLlizBpEmT8MQTT5iOMz5diI6OhkKhQHh4OLp06eLwdUyZMgUffvihKUA4efIkDh48iHfeeQeAIdAYMGAAnnrqKdMxb731FgYPHoyTJ0+id+/eNudMTk7GNddcg88++8wUIGzevBmSJOG2224DAIt5EYmJiXjyySfx+OOPexwg1NXV4f3330dWVhaGDh0KAOjZsye+//57fPTRR80OELjMKTXbyKRIiAKguhwg6MTAiTtFAbjxSsdvJERERK3NtycrbIIDI0kGsk+6li3Xm4yBsnlAl5KSYlEmJSUFv/76q93jDx48iLq6OiQnJ6Nnz56mf2fOnMHp06cBAIcPH8aoUaOaVc/Jkyfj7NmzOHDgAABg3bp1uPrqq5GcnAwAyMvLw3fffWdRB+PNtrEe9sycORNbtmxBbW0tAGDVqlW45ZZbEBUVBcAQAE2dOhUDBw5Er1698OCDD6K8vBx1dXUevY7jx4+joaEBt99+u0Vd16xZ47SergqcOzlqtTJSu+JAYS0U5cYAITDiTlEAesYEY+H4ZNSUl7Z0dYiIiJpkGNrr/KmVVpK9PnG5V69eEAQBx48fxy233GKz/8SJE+jQoYPdcfqukCQJXbp0wYYNG2z2GW+yg4ODPTq3uS5dumDEiBFYv349UlJSsGHDBtx5550W9Rg3bpxpHoP1sY5MnjwZzzzzDDZu3Ijhw4fj+++/Nz3pKCwsxMyZMzFnzhw8+eSTiI6Oxvfff49HHnkEOp39eZv2smxrtb8tAiNdHt69atUqxMVZrsZofALTHAwQqNnC1ApkTuuHTZ8XQVMuQi/Y/lqZ8iAIgFZvmwch9HIehOsu50HYfaISDXbyIHxfUAONXkK99nISNgAavQy1QkREsIioICVqNHro9DLqLy9vGno5D0JEkAI1Gj0kCVCKAtIu50EID1Kixn/NRURE5DHD0F7nN/5KUfD60KyYmBikp6dj+fLlmD9/vsU8hPPnzyMrKwu33367xXV//PFHi3P8+OOPDocoDRw4EBcuXIBSqURCQoLdMldddRW+/fZb3HHHHXb3q1Qq6PV21lS3MnXqVLzwwguYPHkyTp8+jcmTJ1vUY8uWLUhISIDSjRW0wsPDceutt+Kzzz5DQUEBEhMTTcONfv75Z+h0OixevNh0479p0yan5+vYsSOOHj1qse3QoUNQqVQADMOagoKCcPbs2WYPJ7KHAQJ5RZhagWn9Y6DTd4F4RV+ohw2BJEmmNwpBECx6M6wfRVr3dDx1Y6Ldx5XWZY1fWx9vr4y9fRzbSkREgWZU72is/fk87D1IEAXDfl/4+9//jt///veYPn06Fi1aZLHMaVxcHP7yl79YlN+/fz/efvtt3HLLLfjmm2+wefNmfPrpp3bPnZ6ejpSUFMyZM8c0mbmkpARff/01br75ZgwePBh//vOfMWXKFPTs2ROTJ0+GTqfD119/jYceeggA0KNHD+zbtw+TJ0+GWq12+DTj97//PR5//HE8/vjjGDFiBOLj40377r77bnzyySeYP38+HnjgAcTExODUqVPYuHEj3njjDSgUjid/z5w5E7feeiuOHz+O+++/33SP0bNnT+h0OnzwwQcYN24c9u/fj5UrVzpt67S0NLz77rv4/PPPMXToUKxduxZHjx7FgAEDABgCkvvvvx/PPvssJEnCddddh9raWuzfvx9hYWGYMWOG0/M3JTDGglBAkPWGx2Ti5ehWFEUIgmD3Ztx8u/U+R2XslXV0o+/s3AwKiIgokN2X1h09Y4JtFngyDJ0NwX1ptgm7vCEpKQk7duxAz549cc899+Daa6/FwoULMWLECHz55Zc2ORDuu+8+5OXl4frrr8cbb7yBxYsXY+zYsXbPLQgCPvvsM6SmpuKRRx5Bamoq5s+fjzNnzpgmPY8YMQIffPABtm/fjrFjx2LKlCn46aefTOd44okncObMGVx77bW48sorHb6OiIgIjBs3DocPH8bUqVMt9sXFxWHLli3Q6/WYPn060tPT8fTTTyMyMtLusB9zw4YNQ58+fVBTU4Pp06ebtg8YMAAvvPAC3n77baSnpyMrK8tiErQ9Y8eOxaOPPooXXngB48aNQ21tLaZNm2ZR5sknn8TChQvxz3/+E2lpaZg+fTp27NiBxMTmT1AXZC6/0GylpaUW48K8QRAExMfHoziAVsjQ7t0L/fFfoRwyGMpBg1q6Oi4JxHYORGxn/2Fb+wfb2X980dYqlcp009lS8vPzERER4fHxxjwI2ScroJVkqEQBI32cB8Hbrr76ajz55JMOlyUl76upqUGSC3lFOMSIvMc40YZZL4mIiHwqTK3An8cm4s9jE/2WSdlbLl26hP3796O0tNS0ehC1LhxiRF4jaw0BgnB5iBERERH5XiAFBwDw8ccfY/78+cjIyDCt4U+tC7t6yXsuz0GAkwk8RERE1L7Nnz/fInEYtT58gkDeY5yHwScIRERERAGLAQJ5jXx5DoLQgnMQOFmQiIiIqHk4xIi8xzhJ2c9PEOo0emTmFiE7vxo6SYJSFDEyKRIZqV0DZiUHIiIiotaCAQJ5j9b/cxDqNHpkrDmOgvIGSGbbs/LKcKCwFpnT+jFIICIiInIDhxiR18g6HWTIEFQql4f6uFJOlmVTOev/Z+YW2QQHACDJQEFFA5buPef0mhySRERERGSJTxCo2eo0emTuPYfoH4rQqJOwpuIw5KBgRAUrMap3lM1QH1eGBNVp9Hg35yy2H6tEg1aC8TbemDVSrRAQFaxETaPeJjgwkmRg/S8XkXOqBsMSwwEI2FdQA41ej3qtDAFAqFpEsPooUhPCkZEaz6cNRERE1O4xQKBmMQ7xOVtWi2kaPQBAKyig08loqNXaDPVxZUgQAMz7/BgKKhptriddjhQaLp+/KZIMlNRosPFQud39l7QSUKdDVmU9DhTWcEgSERFRO/HQQw+hqqoKH330UUtXpdXhECNqFuMQH4VkCA5kQYBO+O0G2zjUJzO3yKK8oyFBmblFhjJ2ggNfsq4nERERWXrooYfQuXNn07/k5GRMnz4dhw8f9to1Xn31VYwZM8ZpmUWLFuG6666zu6+4uBhxcXHYsmWL1+rUHjFAoGbJzq+GBEB5OUDQCwrAKqOjJAM5+dUW5e0xlsu+XNbfzOtJREREtsaOHYtffvkFv/zyC9atWwelUolZs2b5tQ4zZ87EqVOnsG/fPpt9q1evRkxMDMaPH+/XOrU1DBDIY7IsQycZbvcVsuH/etH+r5ROkiFJkqm8I1q9BK1e792KukEnyZy4TERE5IBarUaXLl3QpUsXDBgwAA899BDOnTuHsrIyU5ni4mLcc8896Nu3L5KTk3HnnXfizJkzpv3fffcdxo8fj549e6JPnz74/e9/j8LCQqxevRqvv/46Dh8+bHpKsXr1aps6DBgwAAMHDsSqVats9q1evRq33347RFHEI488gpSUFCQkJCA1NRWZmZlOX9s111yDpUuXWmwbM2YMXn31VdP31dXVWLhwIa666iokJSXhtttuw6FDh1xuv0DBAIE8JggClKaAQDb7ry2FKEAURbPy9ikVIlR+XCbVmkIUIFg9ASEiIvIlWZYha7Ut868ZnWK1tbVYt24devXqhZiYGADApUuXMHnyZISFhWHTpk344osvEBoaihkzZkCj0UCn02HOnDlITU3F7t278eWXX2L27NkQBAETJ07EfffdhyuuuML0lGLixIl2rz1z5kxs3rwZtbW1pm179+7FqVOnMHPmTEiShPj4eCxbtgzZ2dlYuHAhXnrpJWzatMnj1yvLMmbOnIkLFy5g1apV2LlzJwYMGICpU6eioqLC4/O2Rq1ikvL27duxefNmVFZWonv37pg7dy6uvPJKh+Wzs7OxefNmFBcXIzQ0FIMHD8bs2bMREREBAHj++edx5MgRm+OGDBmCRYsWAQDWrFmDdevWWeyPiorCsmXLvPjK2r6RSZHIyiuDYHp/sb25FgVDOfPykp33I/Nyaw+W2RbwMfPrExER+Y1Oh0sff9wilw6dPdutBKdfffUVevbsCcAQDHTp0gWffvopxMsdgBs3boQoiliyZImpw+2f//wn+vbti++++w6DBw9GdXU1xo0bh169egEA+vXrZzp/WFgYFAoFunTp4rQeU6ZMwfPPP48vvvgCd9xxBwBg1apVSElJQXJyMgDgiSeeMJVPTEzEDz/8gE2bNjkMOpqSk5OD//3vfzhy5AiCgoIAAIsXL8bWrVvxxRdf4M477/TovK1RiwcIe/fuxYoVKzBv3jwkJydj586deOmll7BkyRLExsbalD969CjeeecdzJkzBykpKSgvL8eyZcvw/vvv47HHHgMA/PnPf4bOmNUXQE1NDR577DGkpqZanKtHjx545plnTN+LTfRuk62M1K44UFiLqnrD2H3r+35RAHpGByMjtatF+YKKBosgwbrc/jM1fp2obH19IiIisjVixAjTkJvKykosX74cM2bMwPbt29GjRw8cPHgQp06dMt38GzU0NOD06dMYM2YMZsyYgenTpyM9PR2jRo3CxIkTmwwIrEVFReGWW27BqlWrcMcdd6C2thZbtmzB3/72N1OZFStW4NNPP8XZs2dRX18PrVaLq6++2uPXfvDgQdTV1ZkCEOvX1pa0eICwZcsWjB07Ftdffz0AYO7cuTh48CB27NiBmTNn2pQ/fvw4OnfujFtuuQUA0LlzZ9xwww3YvHmzqUx4eLjFMd999x2CgoIwbNgwi+2iKKJDhw5efkXtS5hagcxp/fDxDh1CzimgFUSIAqBWiIgKUWBUkmUeBGP5zNwi5ORXQyfJUIoC0qzyIHwwPRnv5pzFjmOVqLebB8Fw/tTECAACck9Xo6pBB41ehkoUIIqAAAGhahEqUcR1l/MgfF9QA41eQr3WMBfCkAdBheEJ4biHeRCIiKglKJWGnvwWurY7QkNDkZSUZPp+0KBB6N27Nz755BMsWrQIkiRh0KBBeO+992yONXb8/vOf/8Q999yDXbt2YePGjXj55Zexdu1apKSkuFWXP/7xj5gyZQry8/Oxd+9eAMCkSZMAAJs2bcKzzz6L559/HkOHDkVYWBjeffdd/PTTTw7PJwiCzZAr8w5nSZLQpUsXbNiwwebYqKgot+re2rVogKDT6ZCfn2/6YRoNHDgQx44ds3tMcnIyVq9ejZ9++glDhgxBVVUV9u3bhyFDhji8zq5duzB8+HAEBwdbbC8pKcH8+fOhVCrRt29f3HHHHU4jWK1WC632t7X3BUFASEiI6WtvMp4vEMbDhwcpMT+1KxovdoYQGoYF034HWZYd1j08SIlHRyfg0dFwWC48SIknru+JJ67/Ldux8Q/X/P/m7O1zVs4oPj4eJSUlnJzsQ4H0+xzo2Nb+wXb2n/bQ1oIguDXMpzURBMMcw/r6egCGe7hNmzahU6dOpqHf9gwYMAADBgzAn/70J9x8881Yv349UlJSoFarITWxoIlRWloaEhMTsXr1auTk5GDixImmTuJ9+/Zh6NChuPvuu03lm+rlj42Nxfnz503f19TUWEyuHjhwIC5cuAClUomEhASX6hioWjRAqK6uhiRJNlFXVFQUKisr7R6TnJyMhx9+GG+++Sa0Wi30ej1SUlIsfgHMnThxAoWFhbjvvvsstvft2xcPPPAAunbtisrKSqxfvx5PP/003njjDYe/0Bs2bLCYt9CrVy+88sor6NSpkxuv2j1xcXE+O7c3aUURleERUERGICY+vqWr47ZAaedAx3b2H7a1f7Cd/Ydt3TpoNBrTTXRVVRU+/PBD1NXVmZYVnTJlCt59913ceeedeOKJJxAfH49z587hP//5Dx544AFotVp8/PHHGD9+POLi4nDixAnk5+dj2rRpAAzDvwsKCvDLL7+ga9euCA8PN433tyYIAu644w68//77qKysxHPPPWfa16tXL6xZswa7du1CYmIi1q5di59//tnpjX1aWhpWr16N8ePHIyoqCn//+98thp+np6cjJSUFc+bMwTPPPIM+ffqgpKQEX3/9NW6++WYMHjy4uc3barT4ECPAfq+Ao56Cs2fPYvny5Zg6dSoGDRqEiooKfPLJJ1i2bJlNEAAYnh706NEDffr0sdhu/sQhISEB/fr1w0MPPYQ9e/ZgwoQJdq89efJki33GOpaWllo8gvIGQRAQFxcXMD3bUmkpGmtrIEBGY3FxS1fHZYHWzoGK7ew/bGv/YDv7jy/aWqlU+rRzry3btWsXBgwYAMAwpLtv37744IMPMGLECACGIUibNm3CX//6V9x1112ora1FXFwcRo0ahYiICNTX1+PXX3/F559/joqKCnTp0gV333035syZAwCYMGEC/vOf/+C2225DVVUV/vnPf2LGjBkO6zNjxgy8+uqr6NOnj0XytDlz5uDQoUPIyMiAIAiYPHky7rrrLnz99dcOz/WnP/0JBQUF+OMf/4jIyEg88cQTFk8QBEHAZ599hpdeegmPPPIILl68iM6dO2PYsGFt7vdJkFvwnU2n02HWrFl49NFHce2115q2L1++HKdPn8bixYttjnn77beh1Wrx6KOPmrYdPXoUzz77LJYuXYro6GjT9sbGRmRkZGD69OmmOQvO/PWvf0VcXBzuuecet15HaWmpxdAjbxAEAfHx8SguLg6IDx+ptBSa/3wJISIcQVOmOC1rPcTH06FBrpRpSqC1c6BiO/sP29o/2M7+44u2VqlULX5Dl5+f73QIDpEv1NTUWMwhcaRFnyAolUokJSUhLy/PIkDIy8vD0KFD7R7T2NgIhdU6+cbHP9ZvHLm5udDpdBg5cmSTddFqtTh37pzT5VXJCWPbO7g5r9PokZlbhOz8augkCaIgIDJIgaoGHWoa9ZcnFwOiYMhDEHJ5cvGwy5OL9xXUWBxX06iHXpahtFNGKYoYaTXpmYiIiIhc0+JDjCZMmIC3334bSUlJ6NevH3bu3ImysjLceOONAAxr2paXl+PBBx8EAKSkpGDp0qXYsWOHaYjRypUr0adPH1OSDqNdu3Zh6NChdiP0jz76CCkpKYiNjUVVVRWysrJQX1+P9PR037/otsgUnNkGCHUaPTLWHEdBeQPMpx1dqLV86tKoBwwLpcq4dHmVoY2Hym3OZ32cvTJZeWU4UFiLzGn9LIIEd58wEBEREbU3LR4gDB8+HDU1NcjKykJFRQV69OiBRYsWmR79VVRUWKTvHj16NOrr67Ft2zZ89NFHCAsLQ//+/TFr1iyL8xYVFeHo0aN4+umn7V63vLwcb731FqqrqxEZGYm+ffvixRdfbPFHjgHLFB/Y3nxn5hbZBAe+JslAQUUDMnOLkJHa1eLphfkThvCgFv8TICIiImpVWnQOQlvBOQiAVFICzbbtEDpEIchq2drblh9GSY2mRerVJVyFULXCJkARBSAxOhjLpiejT2L3gGnnQBVov8+BjG3tH2xn/+EcBCLvcXUOAlMHk3c4mIMgyzJ0Lq5n7AtVDTq7Ty9MTxj2FrVIvYiIqH3jcFdqzRggkHc4mIMgCAKUYsv9mmn0ssOhTZIMZJ+q8mt9iIiIAMPno6sJwYi8wZ15mAwQyCucPfUdmRQJsQU6SgQAaoXzC+v0MocHEBGR33Xp0gU1NTUMEshvLl26ZLOgjyOcoUneZScyzUjtigOFtSioaIDkp3txUQB6RgejVqNHQ63j+SFKhcDHvERE5HchISHo1q0bzp8/D1lmZxX5XmhoKKKiolwqywCBvMQ4B8F2T5hagcxp/ZCZW4Sc/GroJBmiAEQEKVDVqMPFOp3TwCEpJgj1WtniuBqNHpIEKEUB113Og/B9QQ10kgylKCAtKRKzrumCWZ/+z+F5RQEY2cu1PxQiIiJvCwkJQc+ePVu6GkQ2GCCQd1zu+XDUGx+mVmBBeg8sSLcdA1fbqMP8tb/aPGEwPgVYejmXgbuZlJfsKURto+NHt+FqBTKGd3X7pRIRERG1ZQwQyDucJEqzZn1THx6ktHnCYHwKYJ4N2fo4e8GI+bbs/Go4e2AbohKZaZmIiIjICgME8g7X4wO7nD1h8Kg6LiyvKsngmE8iIiIiK1zFiLzEfh4ET3hj0rAry6sqRE5QJiIiIrLGAIG8w0GitJbkbHlVUTDsJyIiIiJLDBDIO5qYg+BsKI8rw3w8GQp0z7B4JEYH2wQJogAkdghCRionKBMRERFZ4xwE8g479+91Gj0yc4uQnV8NnSRBKYoYeXniMQCH+4wTh50d72hysfUxoiAgKSYYNRo9dHoZ9VrDvIRqjR6zPz2KkUlReO62Tr5pEyIiIqIAxACBvOtyb32dRo+MNcdRUN4A86nCWXll2H+mBgBQWNFos+9AYS0yp/UDAIfHG8tYBwmOrllWp0WPDkGAEqi4pIME4NLlQCErrxQHS77De7f1RqiKD9SIiIiIeEdEXmI5ByEzt8jmRh0wrBxUUNGIAqvg4Ld9DcjMLWrieEMZa55e88SFWmTutT0fERERUXvEAIG8w2qScnZ+tc3NuCskGcjJr3Z6vLGMteZcM/tUlQdHEhEREbU9HGJE3mE2SdmVHATOaPVSk/kUdJJsypdgnMDcnGvq9LJX8i8QERERBToGCORdgms5CJwpr9chWOn8eEEA3vz2rMUE5ksazwMEUfRO/gUiIiKiQMchRuQdVkOMnOUgaIok/zaJ2B4BQINWQtbBMpTUaFBWp0NJjcbpMU2p10io0+g9Pp6IiIiorWCAQN5hFSBkpHZFYnSw1y8jCkBEkIiaRr1H8w0cqb28PCoRERFRe8cAgbzDKpFZmFqBzGn9mr10aKhKRHyEGp3CVIiPUGPKwFiEqBT20i5YHOPu0wtHE5+JiIiI2hvOQSAv++3OPFQlIlQtOh36IwJOnwSEqRVYN/cqw5kvT0jefaLSaQ2M1y2r07lRb8uJz0RERETtFQME8i6ze2uXJisLsJuF2UghChY37K6cU6nw7KmF9bWIiIiI2iMOMSKvkK3mIBg5m6wsCkBSTLDT/SOTIm22N3XOkUmRbk+SdnQtIiIiovaGAQJ5h4MAwThZ2fpmXRSAntHB+MfE3k73Z6R2tblUU+fMSO3qsIw9ogD0jLF/LSIiIqL2hkOMyDuM8YFVhjPjZOXM3CLk5FdDJ8lQigLSkiKRkdrVpf3WXD3GXpnrEsMBCPi+oMawTSHgpqu74o+Dopo9oZqIiIioLRBkWXa2IAy5oLS0FFqt1qvnFAQB8fHxKC4uRiD8iHT/+x903++HoldPqNLTHZZrahKwJ5OEXTnGXhlZliGKYkC1c6AKtN/nQMa29g+2s//4oq1VKhU6derklXMRtUXsMiXvcDDEyFpTN/KeTBJ25Rh7ZTghmYiIiMgWAwTyDvagEREREbUJDBDIO0zxAXvliYiIiAIZAwTyLg7bISIiIgpoDBDIS0zLGBERERFRAGOAQN7h4iRlIiIiImrdGCCQd5gmKTNAICIiIgpkDBDIOxgfEBEREbUJDBDISzjEiIiIiKgtYIBA3sE5CERERERtAgME8g4mSiMiIiJqExggkJfxCQIRERFRIGOAQN7BScpEREREbQIDBPISzkEgIiIiagsYIJB3cJIyERERUZvAAIG8QmaiNCIiIqI2gQECeQfjAyIiIqI2gQECeYkhQhA4xIiIiIgooDFAIO9gHgQiIiKiNoEBAnkXnyAQERERBTQGCOQdnKRMRERE1CYwQCDvYHzgd8aVo2RZNltFCpAkyaaMdTnrY6zPa122qes7q5Oj44mIiKh1UrZ0BaitYB4Ef6jT6JGZW4Q9J6tQVa9Fo95yv4DfYjWlCEQEKVCvlaDVy5Dk3/YBgCgAwUoR45Kjcde1cVi+vxjbjlagQSdbnC9IKSAqWIlRvaMw65ou+OTH89hzsgrVDTpo9DJUIiBe/rnrZRkaszqZjg9R4uYB5Zg1KAqhKvZLEBERtWYMEMg7mCjN5+o0emSsOY7T5Q1w1Cdvvl0nARX1egclAUkGLmklbDx0EVsOX4TOzkllAA06GQ21Wqw7WIaNv1yEVrIsaAhSHDxlMB5fo8VHuaex52gwMqf1Q5ha4eSVEhERUUtiVx55B+cg+FxmbhEKnAQHzWEvOLAmAzbBgTskGSioaEBmbpHH5yAiIiLfY4BA3sH4wOey86shNV2sVZNkICe/uqWrQURERE60iiFG27dvx+bNm1FZWYnu3btj7ty5uPLKKx2Wz87OxubNm1FcXIzQ0FAMHjwYs2fPRkREBADg+eefx5EjR2yOGzJkCBYtWuTxdckZDjHyJVmWoZMCPTww0EmGCcxMqkdERNQ6tXiAsHfvXqxYsQLz5s1DcnIydu7ciZdeeglLlixBbGysTfmjR4/inXfewZw5c5CSkoLy8nIsW7YM77//Ph577DEAwJ///GfodDrTMTU1NXjssceQmprq8XWpCVypxqcEQYBSbBsP/BSiwOCAiIioFWvxO44tW7Zg7NixuP766029+LGxsdixY4fd8sePH0fnzp1xyy23oHPnzrjiiitwww03ID8/31QmPDwcHTp0MP3Ly8tDUFAQhg0b5vF1yUW88fOZkUmREAO8eUXB8DqIiIio9WrRJwg6nQ75+fmYNGmSxfaBAwfi2LFjdo9JTk7G6tWr8dNPP2HIkCGoqqrCvn37MGTIEIfX2bVrF4YPH47g4GCPrwsAWq0WWq3W9L0gCAgJCTF97U3G8wVUT6tgqG8g1TmQ2nn+8G44UFjrdBUjTymFpicqCwCUomAYIuTBNUQB6BkTjPnDuwVEeweqQPqdDmRsZ/9hWxP5X4sGCNXV1ZAkCVFRURbbo6KiUFlZafeY5ORkPPzww3jzzTeh1Wqh1+uRkpKCu+++2275EydOoLCwEPfdd1+zrgsAGzZswLp160zf9+rVC6+88go6derUxCv1XFxcnM/O7U3VHTqgMTwCYbGxCI2Pb+nquC1Q2vmLP8XhH9uPYfuREpTXNqJBJ5tyH5h/dMoAVKKAyBAV6jWGfAV6sxt7AYaHPaFqJSYO7oqHr++Lt7/+Fet/OotL2t/mOggAglUKxISpMe6qLrh3dG+8/81JbD9Sgoo6DTR6GWqF8FseBEmyyaNgfvzC8ckID2rxkY3tQqD8Tgc6trP/sK2J/KdVfFLb6xVw1FNw9uxZLF++HFOnTsWgQYNQUVGBTz75BMuWLbMIAox27dqFHj16oE+fPs26LgBMnjwZEyZMsClbWlpqMefBGwRBQFxcHEpKSgIiE62mvAL62ho0lJejqri4pavjskBrZwDIGBqDjKExpom+xnobfx8lSYJ4eb6C+WRg83I2x9RV4IFhsXhgWKxFOxjLmpezd33za1ifX5ZliKJoaueaAGnnQBWIv9OBiO3sP75oa6VS6dPOPaJA16IBQmRkJERRtOm1r6qqsundN9qwYQOSk5Nx6623AgASExMRHByMZ599FjNmzEB0dLSpbGNjI7777jtMnz692dcFAJVKBZVKZXefrz4gZFkOjA8fWQIuZ+oNiPpaCZh2tmJeZ3sBgHUZR8c0dX5H5eztt/e1+f8DsZ0DEdvaP9jO/sO2JvKfFp2krFQqkZSUhLy8PIvteXl5SE5OtntMY2OjTS+/eW+pudzcXOh0OowcObLZ16UmmJqeY0SJiIiIAlmLr2I0YcIEfP3119i1axfOnj2LFStWoKysDDfeeCMAYNWqVXjnnXdM5VNSUrB//37s2LED58+fx9GjR7F8+XL06dMHMTExFufetWsXhg4dasqP4M51yT2m0e2MD4iIiIgCWovPQRg+fDhqamqQlZWFiooK9OjRA4sWLTKNDayoqEBZWZmp/OjRo1FfX49t27bho48+QlhYGPr3749Zs2ZZnLeoqAhHjx7F008/7dF1yU187EtERETUJggyB/Q1W2lpqcXyp94gCALi4+NRXFwcEGMuNbt3Qyo4A2XqMCgDaJhWoLVzoGI7+w/b2j/Yzv7ji7ZWqVTsECRyosWHGFEbYbUqDhEREREFJgYI5B3sQCMiIiJqExggkJcYJynzCQIRERFRIGvxScrURsgMEPzJUZIywDYRmnVSNOtkZtbMh4kZk67ZK2vvHM4Sp3GcNhERUWBggEDewQDB5+o0emTmFmHPySpUN+ig0ctQiYaVZbUSIMmOR3oJAEQBUCsE6GUZGr3j6wQrALVSRE2j5PbIMQGAWgGIl38PzK8lCv9FsFLEuORoPJDWDWFqhZtnJyIiIn9ggEDewc5hn6rT6JGx5jhOlzdYNHWjkxt9czIAvQzU65r+QTXogQa95FE9ZVOdbK8jycAlrYSNhy7iv+dq8cH0ZAYJRERErRDnIJCX8AmCL2XmFqHAKjgIZAUVjcjMLWrpahAREZEdDBDIOzi+3Key86vhWZ9+65WTX93SVSAiIiI7PB5idO7cORw5cgQ1NTUYO3YsOnTogPLycoSHh0OtVnuzjhRI+ATB62RZhk5qa+EBoJMkiwnNRERE1Dq4HSBIkoSlS5fim2++MW0bPHgwOnTogMzMTPTq1QvTp0/3Zh0pEJieIPBmz9sEQYBSbHsP+xSiyOCAiIioFXL7rmP9+vXIycnB7Nmz8Y9//MNi35AhQ/Dzzz97q24USBgf+NTIpEiIbaxtRyZFtnQViIiIyA63A4RvvvkGU6ZMwYQJE9C1a1eLfZ07d8aFCxe8VjkKJJyk7EsZqV2RGB3cZuKvntFByEjt2nRBIiIi8ju3A4Ty8nL069fP7j6VSoWGhoZmV4oCEPMg+FSYWoHMaf0wdVAsuoSrEKwUIApAkEJAsBJQCM4f3ggwlAlRCghqYmXRYAUQGSR6FIwIAIIUQIhKsLiWMQ9DmErEpKs7YhmXOCUiImq13J6DEBUV5fApQVFREWJiYppdKQpAnIPgc2FqBRak98CC9B4Bl0kZAOLj41FSUsKMykRERK2c208QhgwZgvXr16O8vNy0TRAEXLp0CVu3bsU111zj1QpSgGB84FfGm27zm3lBECy22/va+lh7/8yJlydHOyrnaJujaxEREVHr5/YThGnTpuG///0vFixYgP79+wMAPvvsMxQWFkKhUGDq1KleryQFAvYKExEREbUFbj9B6NChA15++WWMGDECp06dgiiKKCgowODBg/G3v/0N4eHhvqgnBQr2EhMREREFNI8SpXXo0AEZGRnergsFMJmTlImI2hwmMyRqnzzOpExkwTTCiB8kRESBrE6jR2ZuEbLzq6GTJChFESOTIpGR2pWrjxG1E24HCO+9957T/YIg4L777vO4QhSgTCvltHA9iIjIY3UaPTLWHEdBeQMks+1ZeWU4UFiLzGn9GCQQtQNuBwiHDx+22VZbW4uGhgaEhoYiLCzMKxWjQMMhRkREgS4zt8gmOAAASQYKKhqQmVuEBek9WqRuROQ/bgcI7777rt3thw4dwgcffIBHH3202ZWiAMQ5CEREAS87v9omODCSZCAnvxoL0v1aJSJqAW6vYuTI1VdfjZtuugnLly/31ikpkHAOAhFRQJNlGTrJUXhgoJNkJjskage8FiAAQPfu3XHixAlvnpIChekJQstWg4iIPCMIApSi89sChcikh0TtgVcDhCNHjiAyMtKbp6SAwR4lIqJANzIpEqKD+39RMOwnorbP7TkI69ats9mm1WpRUFCAn3/+GbfeeqtXKkYBij1LREQBKyO1Kw4U1qKgogGSWb+PKAA9o4ORkdrV73XikCYi/3M7QFi7dq3tSZRKdO7cGdOmTWOA0F5xkjIRUcALUyuQOa0fMnOLkJNfDZ0kQykKSPNCHgR3kq6Z52LQSzKC1EeRmhCOjNR4LrNK5AduBwiff/65L+pBgY4dPEREbUKYWoEF6T2wIL35mZQ9SbpmNxdDnRZZlfU4UFjDXAxEfuDVOQjUjvEJAhFRm9Pc4CBjzXFkHSxDSY0GZXU6lNRokJVXhow1x1Gn0ds9zpVcDETkWwwQyEsYIBAR0W88vdF3JRcDEfmWS0OMpk+f7vIJBUHA6tWrPa4QBSg+QSAiIjOeJF1zJxcDl1sl8h2XAoQpU6bwD5Gc4xwEIiK6zNMbfeZiIGodXAoQpk2b5ut6UKDjMnRERHRZc270RyZFIiuvzGKZVSPmYiDyD85BIO9irw4REcGzpGt1Gj20esnucS2Zi4GovXF7mVOjM2fO4Ny5c9BoNDb70tPT7RxBbRvnIBAR0W/cTbpmd3nTy5SigD/0j8H9I7pxiVMiP3A7QGhsbMSrr76KQ4cOOSzDAKEd4iRlIiIy427SNUerHgGAJMtQiSKDAyI/cTtAyMrKwoULF/D888/j+eefx8KFCxESEoKvvvoKZ86cwSOPPOKDalJrxykIRERkzZ2ka02tepR9qgqPpHf3TUWJyILbcxB++OEHTJw4EcnJyQCA2NhYDBgwAI8++ih69eqFHTt2eL2SFAj4BIGIiBxzFhy4tOqR3rDqERH5ntsBQmlpKbp16wbx8uoE5nMQRo4ciR9++MF7taPAcflNm0vPERGRu1xZ9Uip4PKmRP7idoAQFhaGxsZGAEBUVBSKi4tN+3Q6nWkftTPGTh2+eRMRkQeaXPWoV5R/K0TUjrkdICQkJKCoyJAevX///tiwYQOOHj2KEydOICsrC4mJiV6vJAUAPvYlIqJmyEjtisToYJsgQRSAPp3DkTGcy5sS+YvbAcKYMWPQ0NAAALjjjjvQ2NiI5557Dk899RRKS0tx5513er2SRERE1LYZVz2aMjAW8RFqdApTIT5CjakDO2H9/SO4ghGRH7m0itGKFSswduxYJCQkYPjw4abtnTt3xltvvYVDhw5BEAQkJycjPDzcZ5Wl1oyTlImIqHnsrXokCALCg5SoaenKEbUjLgUIW7duxdatW5GUlISxY8dixIgRCA0NBQAEBwcjJSXFp5WkAMA8CERE5EWckEzUclwaYvTWW29h4sSJqKysxAcffID58+fjnXfewZEjR3xdPwoUnKRMTZBl2yUKzb+3t3yhcZu9cs6OdXQtR+dxto+orfH099qV45z9HfvqmkTkfS49QYiLi8PMmTMxY8YMHDx4ELt370Zubi6ys7PRuXNnjB07Funp6YiJifF1fam14ps42VGn0ePdnLPYcawSDTrDGudBCgHxkWrUaSRoJQn1WhkCgBC1CJUoYlhiOAABe09Xo7pBB41ehkoExMvBpwRAa7ZNEASEqEUoBAHhahHF1Ro06mXTtbpEqHC+RotGvQxZNkx4VCsE6GUZGr1lfRWX9xmvoVYIiApWYlTvKLuZX4kCRZ1Gj8zcImTnV0MnSVCKItJ6RWD+8G42v9fmCc3qNHos3VuEnFO/HTcyKRL3DItHeJDSbhmFICC1ZwQAAfsKauweZ34N6wRq1udTKUSMv7ocswZFIVTl9tRJIvKAIHsYntfW1iI7OxvffPMNTp8+DVEUMXDgQIwdOxbXXXedt+vZqpWWlkKr1Xr1nIIgID4+HsXFxa2+B0WWZTSu/AgAEDRjOoTg4BaukesCqZ0DTZ1Gj3mfH0NBReAvfSwKQGJ0MDKn9Wv1QQJ/p/0jkNq5TqNHxprjKChvsMlUrBCBP1zVEXddG4dPfjxvCiDEywF3QUUj9HZenkIAYkKViAgSUVChsVvGEVGATYBv7BzQ6oHtx8qhk2yP8ebfoEqlQqdOnZp9HqK2yqUnCPaEh4fj5ptvxs0334yCggJs374dX3/9NQ4ePIjVq1d7s47U2pl/OHKIEV2WmVvUJoIDAJBkoKCiAZm5RViQ3qOlq0PklndzzuFUeYPdfXoJ2HjoIv5zpBw6SYb5ff4FJ+fUy0BpnQ6lde7XR5KBRj1gGJsq45LWEA1sPFTu9Bj+DRL5T7Of1eXn52Pnzp3Yt28fACAyMrLZlaIAwwCB7MjOr27pKniVJAM5bew1UdtXp9HjiyMXmyyntQoOWiP+DRL5j0dPEGpqapCdnY3du3fjzJkzEEURgwYNwtixY3HNNdd4u45EFGBkWYZWr2+6YIDRSbLNeGmi1mzp3nPQW48rCmD8GyTyD5cDBFmW8d///hfffPMNfvzxR+h0OnTp0gUzZszA6NGjER0d7ct6ElEAEQQBKoUCQNsKEhSiwBsTCig5p9pW9gD+DRL5h0sBwqpVq/Dtt9+ioqICarUaqampGDt2LK666ipf169ds1520fpN0VEviqdlXe2VsSlnXBISMocYkcnIpEisPVjW0tXwGlEwvCaiQCHLMnRS23l8wL9BIv9xKUDYtGkTkpKScNtttyEtLc2UJM1btm/fjs2bN6OyshLdu3fH3LlzceWVVzosn52djc2bN6O4uBihoaEYPHgwZs+ejYiICFOZuro6fPbZZ9i/fz/q6urQuXNnzJ49G7/73e8AAGvWrMG6desszhsVFYVly5Z59bW5y7gUXc6paujkw6hrMKyOZL0EpPXScbOu6WKxAoXSxbIavd5mmcmRSZE2SzraWyLPeK5V+8+hw88XIMkyvtT/D8P7RHNJSEJGalfsP1PTJiYqiwLQMzoYGaldW7oqRC4TBAFKse0sC9ozhn+DRP7i0jKnBQUFSExM9EkF9u7di7fffhvz5s1DcnIydu7cia+//hpLlixBbGysTfmjR4/iueeew5w5c5CSkoLy8nIsW7YMcXFxeOyxxwAAOp0OzzzzDCIjIzF58mR07NgRFy9eRHBwMHr27AnAECB8//33eOaZZ0znFkXRo0nW3lrm1NlSdM4IAJSiYLMChadlrZeTc1Qv47mg1WLa8a8BAJ8l3wAoFFwSkgA0kQdBK0Gnl1F/eQWT0MsB6nWXA9vc09WoMuVBECCKAGTDuifm2wQICL2cByFMLaK4RoNGnVUehFotGnWWeRAkWb68kspvjHkQjNdQK0REhSgwKilw8iDwd9o/AqWdl+wpRFZeGaTWW8UmCQBmXpeAu34X7bU8CFzmlMg5l54g+Co4AIAtW7Zg7NixuP766wEAc+fOxcGDB7Fjxw7MnDnTpvzx48fRuXNn3HLLLQCAzp0744YbbsDmzZtNZXbt2oXa2lr89a9/hVJpeIn23ghEUUSHDh188Ko8k5lbhILyBoRo6tG5vqJF6yJWAes2lGNWShzWHSiBeKoSjn4LlJLZXZYgcDk6MglTK/D42EQ8cX1PxMXFobi42GJ/U0PcjNvslXN2rPGGzdk28/MYObsGUSDKSO2KA4W1Dpc5be0i1AI+nd0fA/smtvpgjKgt8TgPgjfodDrk5+dj0qRJFtsHDhyIY8eO2T0mOTkZq1evxk8//YQhQ4agqqoK+/btw5AhQ0xlfvzxR/Tt2xcffvghDhw4gMjISIwYMQKTJk2CaPa4taSkBPPnz4dSqUTfvn1xxx13oEuXLj55ra7Izq+GBCC2oRLDi35psXoYCWUKaOs7Az9fwDDrrlY7ZEEwPZUwLke3IN23daTAIVxOimTvhtz6a+tt9so5O9bZuZydx5V9RIEkTK1A5rR+mPjhIVO+gZYkAE6fdIsC0DFUBYUAjLycwdyYsZmI/KdF/+qqq6shSRKioqIstkdFRaGystLuMcnJyXj44Yfx5ptvQqvVQq/XIyUlBXfffbepzPnz51FaWoq0tDQsWrQIxcXF+PDDDyFJEqZOnQoA6Nu3Lx544AF07doVlZWVWL9+PZ5++mm88cYbFnMZzGm1WouhRIIgICQkxPR1c8iyDP3lZ8D1iiAUhdsOr/K3yCAFhK5dUXJCQrWq6QChOCwWsvBbAKaTbHtxWxveCPoH29l/2Nb+EUjtHKZWIEytCIgAoWOoChvv7m/RmRdIbU3UVrSKsNyV3j6js2fPYvny5Zg6dSoGDRqEiooKfPLJJ1i2bBnuu+8+AIab7cjISMyfPx+iKCIpKQkVFRXYvHmzKUAwf+KQkJCAfv364aGHHsKePXswYcIEu9fesGGDxcTmXr164ZVXXvHaOMYg9VGgTosLYTG4EBbjlXM2R/foEPSaOxY/nt+FsxX1bh8fpFaia9fAmFAWFxfX0lVwWSAPewmkdg50bGv/CJR2Nn6+tDRBFOBsQkSQWolu3brZ3RcobU3UFrRogBAZGQlRFG2eFlRVVdk8VTDasGEDkpOTceuttwIwzI8IDg7Gs88+ixkzZiA6OhodOnSAUqm06IHo1q0bKisrodPpTPMSzAUHByMhIcFmjLS5yZMnWwQPxpu00tJS6HQ6l1+3I6kJ4ciqrG8Vk8lEARieEI7i4mKP6mV+fGsmCALi4uJQUlLSqse21mn0WLq3CDmnqqDTy1AqBKT1isL84YEzcTYQ2rktYFv7R6C1c3M/XyKCRNQ0Nv8JRK/oIOSXN9ith6PPDV+0tVKp5CRlIic8DhAuXbqE48ePo6amBkOGDEF4eLj7F1cqkZSUhLy8PFx77bWm7Xl5eRg6dKjdYxobG6FQWN4QGQMB4xtHcnIyvvvuO0iSZNpXXFyM6Ohou8EBYBg+dO7cOafLq6pUKqhUKrv7vPGmlZEajwOFNSiosP/m6YhxNSG9LDd5nCtljUs63pMaD1mWHdbL0bmsjw8Esiy32ro6WkUqK68UBwprAmK1KKPW3M5tDdvaPwKlnZt6H3e0sp3x/XzJpN5Yvr8YW46UQ+cgThBgWAlM56A5ekYH4R8Te+ORjSdt6uHK50agtDVRW+BRgLBu3Tps2rQJGo0GAPDyyy8jPDwcL7zwAgYOHGgz6diZCRMm4O2330ZSUhL69euHnTt3oqysDDfeeCMAQ5K28vJyPPjggwCAlJQULF26FDt27DANMVq5ciX69OmDmBjDsJxx48Zh27ZtWLFiBW666SaUlJRgw4YNuPnmm03X/eijj5CSkoLY2FhUVVUhKysL9fX1SE9vuVm1xslkxjwIellA7eU8CNZLQH5fUAOdJEMpCkgzy22Qk19t2u5KWY1esllmMs0qD4JFvczO7+i61sdT8xhXt7L+TOZqUUTkKlfex789WWVaWtjeEr+Pj03EA2ndkbm3CN/m2y8765ouWL6/2GJp42CliHHJ0XggrZvTevBzg6j1cCkPgrnt27dj+fLlGDduHIYMGYK///3vePnll5GUlIQtW7Zg//79eOGFF9yqhDFRWkVFBXr06IE5c+aYsjS/++67KC0txfPPP28qv3XrVnz11Ve4cOECwsLC0L9/f8yaNcsUIACG5VBXrlyJ06dPIyYmBmPGjLFYxejNN9/E//73P1RXVyMyMhJ9+/bFjBkz0L17d7fqDngvD4I560eqrTqTchPbW7NAWMv8tuWHUVKjcbg/PkKNrLv6+7FG7guEdm4r2Nb+Eejt3NT7uCvv502VtbfcsKv1MOeLtmYeBCLn3H6CsG3bNkyYMAGzZs2CZJXC3fgH7K7x48dj/Pjxdvc98MADNttuvvlmi6cB9vTr1w8vvviiw/2PPPKIW3VsCa4uAenKdlfKunpz7851qXlkWYZOcj7uVyfZDyBbK+tlTs1vIiRJsvmdtJevwN6xjs5rvs38+sbrmc9Vsj5fILVpoNSVWoem3sdd+X1qqqw75yCi1sXtAOHChQsYNGiQ3X0hISG4dOlSsytF5E/mN5KObiqN++zdZDaVlMveceblnd3kCoIApeg8c6hCFFr9h2ydRo+nNvyC9T8Wot7RAGUHmloW0VUiAEEA9FYnE2BIxqSVgAadYRy2ACBIKSAqWIlRvVtnFuU6jR6ZuUXIzq+GTpKgFEWMTIrE/OH2V4AhsqclgksGtEStn9sBQmhoKKqqquzuu3DhAiIjI5tdKbLPneFA3ngD9sc1Worx5irnVDV08mHU1mvQqJNtxvmHKAXER6pRp5GglSRc0kjQ6g2Tss3vM0UBCFII6BKhwvkarcWNplph+L9GD4vzG29Yrc9lpBAAtUJAo/UdrdV1Rya17r+5Oo0e8z4/hoKKRo+O99bgDcnByWQA1RrZZluDTkZDrRZZeWU4UFjbqiaDO564bqjrF3/icpDkmHVwqRAElwNhZ+/7zvbZC2jTekVg/vBuFtcM5M8VorbE7QDh6quvxqZNm5CSkgK1Wg3A0NOp1+vx1VdfOXy6QJ4xLG95Dtn51dDo9ajXyhAAhFyeUDzSbGKXox5Fd3o/mzqHN67R0hzdXNlTr5ORX970ja0kG8qerrCcKyADcJSE2tENq5H+8jkdMa76kZHaunNNZOYWeRwctAatcTJ4UxPX/7H9GDKGtnwuFWp9jO9/p8sbLN5+1h4sw7aj5fh45hXoHBFkc4xhmWXb930ATX4mOHrPXZd3ERsOXcRNyTFQKQTsK6gJ2M8VorbG7UnKJSUlWLRoEUJCQnDttddi69atGD16NE6fPo2ysjK88soriI1t+SzA/uSrScoRMZ3wh7f2OL2RFQUgMToYb066vHScVVnjfld6Px29iXvzGq3Bkj2FyDpY1mRw0Nr16RiMf93e+tu8qUnWgaI1TQZvqk27R4dg7Z1XBuTk2UARqJOUl+wpxLqDZQ77JgQAncNVGNX7txWJ7C1tKgpAjw6GQKKwotHpZ4In77nm5wgPUnKSMpGfOR/cbEdcXBz++te/olu3bti+fTsA4Ntvv0VERAQWL17c7oIDX3p9+7Eme7mNPYYLN9neuJvvz8wtavJ6TfVKeuMarUF2fnXABwcAUKeRWn1wIMsytHoHj1ACjHEyuDV/3xy6NHFdz/Xiyb7s/Gqnw/ZkAOdrtVh3sAxTVxzBxkP28x4Y3vcbUWAVHPy277fPBE/ecwPtc4WorfEoD0L37t3x1FNPQavVoqamBuHh4abhRuQ9O/933qU3VUmGITOlk/05+dVY0ESKB2dv4t66Rktz5eYqUATC6kWCIEClUAAI/CDBfDK4p0PtvPXzamriulIh2F3Jido3d97/ZABaT9Mu47fPhEdGef6eazzHo6M9rgYRecjtAOHHH3/EkCFDIIoiVCqVRe4B8h5Dz6sbb85NFNVJst0lHc0nPTf5Ju7CNQLhhrWpm6tAEQirFwGGSdRrD5a1dDWaxXwyeFMThK2H2vliblCdxvHfqigAN17ZxaPXSW2bIAhQ+PE9Q3c5wGjOe66jJ3dE5FtuBwivvvoqoqKiMGrUKIwePdqjxGLUNEPPqxtv5E2sBXnxkhaTlh+GKAiIDFKgplEPvSxb3Kw09cHRVB9QIN2wZuWVoRmdYy0uEFYvMspI7Yr9Z2oCdqKy9WRwdzJbuxtM2OPOpHpRAHrGBGPh+GTUlJe690KpXRjVO8pvAbvxM6E577mB8rlC1Na4HdY/+eSTuPLKK7F161YsXLgQTz31FHbu3In6+npf1K9du+HKLnDlbVEUgKSYYIhOCksyUFanw4VaLU5cbMD5Wi3K6nQoqdEgK68M8z4/hjqN58NAAu2GNTHaeXu1ZoGyepFRmFqBD2dcgVnXJSBE6X6je+vHJMKwdKy980eqBYQoBdO1BADBSsOytVMGxmKp2Y18U0PxcvKrTd+7Ekw0xdE5jEJVIjqFqRAfocaUgbHInJaM8CCPRo9SO5CR2hURQb5/imr+mWB8z/VEoHyuELU1bq9iZFRXV4ecnBzs2bMHJ0+ehFqtxrXXXosxY8bg6quv9nY9WzVfrmKU+tJXqGl03m8YGSQiSCmi/JLOJgmUv0QGKZB1V/9WP2nWyDwPgl4WUNNUHgStBJ1exiWNHpqm8iDUatGgtcqDIAAanWd5EGQAGr0MtUJEVIgCo5JaZ+IuZ8xXfDHPwB5omZRlWcbEfx9CWZ3O4WvtFKbCxrv7QxCEJlcbcmVlJFfOsW7uVRZZbQNxdZ1AE8jtXFqrwexPj6La0RrMLuoZHQQZQGFlo8XTAWMnhnlgXafR492cs3ZXRHLE+LnCVYyI/M/jbqawsDCMHz8e48ePx9mzZ/HNN99gz549+O6777B69Wpv1rHdCg9SIlStcBogCABqGiVUW5VRCIabTn8NowlRiQF1wxqmVmBBeg88OlpAXFwcSkpKIMu2Y11bOpOy+XVb+/wOV1m/BvPvRTtjlc1vfJ2dy9l5Xb2es/MZt7ma2dql1YaamLfj6jmI3NEp3BCYZuYW4duTVSir09rNMK4UBehl2eZzRCkCE67qiAfSDFm7M3OLkJNfDZ0kQykKSLMzxyZMrcDjYxPxQFp3ZF7OqaDRS6hq0NkNGCKDRHz8xysC6nOFqC1p9nNoWZZx8eJFlJWV4dKlSwHXk9KaybIMT+cN62XDEIkGJ4m2vEmSAzcDZlM3hU2Vc+fG1d2bXPNtgdi2bZGz8dTmwyrcCSYc8cY5iOwxdpIsSO+B2kYdlu0rtrnJn3VNF3zy43mL7SPsZD82nMe1z4AwtQILRvfAgtGG8pe0kkWAoRCAkS5mdSYi3/E4QCgpKTE9NSgvL0dMTAwmTJiAMWPGeLN+7ZogCFC6M1HZisaP4414k0LtRUZqVxworEVBRYPdYRXmc0NcDSac8cY5iJwJD1I6vMl35+bf3c8AQRDMApXA7WQiaovcDhB2796Nb775BkePHoVSqURKSgrGjBmDgQMH2h0eQM2T1isKWXmlHg0VUisE03h5X+JNCrUnYWoFMqf1c2lYhTvBhCPeOAeRq1wZmufvaxOR/7kdILz//vvo2bMn7rrrLqSlpSE8PNwX9aLL5g/vigOFNXZvDkQBTid7RQYrEaZW2BxrTRSAhA6GyWZnKhqbSndgcyxvUqi9cbXX051gwtm1mnsOIiIid7i9ilFBQQESExN9VZ+A5KtVjIyrNtQ26uzeHNRrJWw5Um73eFEApgyMRUZqV4tjRQGICFKgRqOHJMHiRgMwTDbbcvgi6p3MXRAABCkDd0Udc4G8EkkgYTsbeGMIRVPnYFv7B9vZf3zR1lzFiMg5j5c5pd/4OkAw/xEZbw7qNHrM+/yYw+RTPaODsGx6ssWNu/WNhaMbDVNiJjtPLRI7BCFzejJCVWKbeBzMD3n/YDv7D9vaP9jO/sMAgcj/XBpitG7dOowdOxYxMTFYt25dk+WnTp3a7IqRfcab8szcIhQ6yUw7uFuYTa++q8tAckgDERERUfvlUoCwdu1aDB48GDExMVi7dm2T5Rkg+J6zbK4A8H1BbbPOz5UliHyPf1tERNQauRQgfP7553a/ppbhjQRM7miPNzDWycmsk50Zv7bOxGu93965mrqmo+8dXY8CizGLd3Z+NXSSBKUoYiSfzlELs5fw0VlZb5ZztywR+V6zE6WR/wmCAEUTb6TNyUvQHt+oZVk2JQvac7IK1Q06NOpkGJtBJQKiYGjTICVQ3SCZ5mcIgmHOx5VdQvHj2TroJAmiICAySIGqBh1qGvXQ6GWoFQKigpUYdTkJUKhKRJ1Gj2X7ik03i8bjahr10MsyFIKAlB7hOHL+EgoqGiHLhuslxQTjHxN7o1O4uuUajdxmmt9T3mDxBDArrwwHCmuROa0fgwTymzqNHu/mnMX2Y5VovLwkXrBSxLjkaDyQZpkMzdXA1p0AmMEyUevl9iTl6dOn48UXX0SfPn1s9uXn52PRokXt7imDLycpFxUV2eyrbdRhyorDqGm0/xTBuILRI6O6u9xzbcxm6eyN2lEveqAyfDiew1fHK3FJo3dreVdvEOA4E7YrVKKAdXOvCogggRM6DZbsKUTWwTK7wwONf7cL0ns06xpsa/8I9HZuaqGLxOggfHB5oQtHga0oAInRwabA1tVyxuu7WpaTlIn8z6tPECRJCvibxtbA0KtSjNwz/0OjRgeFKGBYYjgAAfsKalBZr0WDk2VIFQLwzYlKfHH4ot2ea+MbuTEg0Oj1qG7Q2+RUyMorw/4zNRjSLRz7Cmqg0etRr5UhAAhRi1CZBRGBtqpRUx+O/tDcjzmtJGPhppP46I9XeqU+5HvO5g5JMpCTX40F6X6tErVTmblFTt//CioakZlbhAXpPQxlrW7kAcPvbEFFg9vlTNd3sSwR+Z9XA4T8/HyEhoZ685TtjqNelY2H7Oc7sEcrAaV1OtP3DToZDbVa0zCGNyf1xiMbT9p9czZneKNutPshcklrOHLtwTKs/6UMHUKUFgFDa3883NSHY6DIL29o6Sq0C+bjs617MK23Wc9dMW6TJKnJuUNavWTqaHE0B8ZYH+M268A8EHuzyf+y86ubLGMMWF0NbN0JgBksE7VuLgUIX375Jb788kvT96+99hpUKpVFGY1Gg6qqKgwbNsy7NWxnHPWqeIOxZ2bhpqaDA3foJeDi5YAkUMZSu/LhGBBkTlz2FfPx2Q1aya0nPp4OHyu7pEPaOwdN34uX7/3VCgERQQqEqUWcqdBY/O0qReCGvtEIVonYV1ADvSQjSH0UqQnhyEiNb9V/h9QyZFmGVq9vspxOklwKbHWS7HI5YwDrz4U2iMh9LgUIkZGR6N69OwDDePsuXbrYPClQqVRISEjALbfc4v1atiNNLV/aXJJs6HX21TUC4fGwqx+OAUEAgwMfaO4QNG/14RsnwjfoZDTodCitsy2jk4BtxyosN9ZpkVVZjwOFNa0+WCf/EwQBKoUCgPP3QYUoQhRFKJt4j1GIgsvljDf87pQlIv9zKUBIS0tDWloaAGDx4sWYN28eunXr5tOKtUeuLF/qnQv59vSt/fGwqx+OgSApJrilq9AmtYUhaIEQrFPLGZkUibUHy5osY/x/Vl6ZKWA1Jwrul3O3LBH5n9tdj8899xyDAx8RBKHJXhXvXMj3lzB/lNwatYUPH5Uo4B8Te7d0NdqktjIEzRisE1nLSO2KxOggh/t7RgchI7WrWdlg05A3I1EAekYHu13O3bJE5H9u343u3r0ba9assbtvzZo12LNnT7Mr1Z6NTIq0ecP0JvHyGvq+vAbQ+h8PN/Xh2BIUAtAxTAGVCz+cpJiggFniNNC0qSFoaP3BOrWMMLUCH0xPxqSrYxCqEiEKhs+HUJWISVd3xLLLS5way2ZO64cpA2MRH6FGpzAV4iPUmDIwFkvNhrC5Ws7dskTkf26vYrR161aMHj3a7r7IyEhs3boV6emtdGxJAMhI7YoDhbUoqGiw++i1OYw9M/+YeHkVIzvXUIowrUh0XWI4/nuuDoWVjW7VJRAeDxs/HN/LOYcdxytRrzXcEBqTBM0d2gWf/nQBOfnV0OglXNLoIQgCQtUilIKAkZeXjJUlCe/tLcKOY5VouLxOrFoEukYFoUajR02DMUmaiMhgEcN7RkKrB3af+K288Zr3j+iK8CClaQnab09WoapBZzo+KkSBtJ4RuHdEd354+lBbGoIGtP5gnVpOmFqBx8cm4vGxiU1mUg5TK7AgvQcWpDvPg+NqOXfLEpF/uZ0o7c4778Rjjz2GAQMG2Ow7dOgQXnvtNaxcudJrFQwE3k6UVqfRY1luMfaeqUWjRgelKOC6y3kQvi+ogU6Sm9yWe7ra5uZyVJJtHoSc/GrTsWl2chqYl9PoJdRfXt40WCXYzZ1gDEICpQfIPCGdow8oVxPE2fuAdbRMpaPyjq4d6B+egZZUasmewibHZwcCbyVfI1uB9jsdyJgojcj/PMqDcOnSJYfbJX9Msm3jwtQKLBjdA686yKTs6GbT0TZ7+5rbG2SefdlekBEIwYE5Zzff5vtcLWe9zdk+V+oVyMFBIMpI7Yr9Z2oCeqIyx3ITEZGn3A4QEhIS8N133+G6666z2ZeTk4OEhASvVIwMHCVmslfO0bambi5dvfm0vlHm42Fqq4xD0N7NOYsdxypR74M8CKJgGF42pk8HAMCuXytQb5Uh/bc8CCIigkWEqZznQTA+TQxSKzE8IRz3MA8CERF5wO0A4aabbsLbb7+Nd955B+PHj0fHjh1x8eJF7NixA99//z0efPBBX9STWjEGB9QWORqf7WkmZUdfGz11Y6LT87iaSRkAunbtyqEvRETkMbcDhLS0NJw7dw4bN25Edna2absoipgyZQpGjhzp1QpS62Scm5CdXw2dJEEpihgZoMOLiJrS1DAz6232nt65MlTNlfMYv3f1HERERO7yaA7C9OnTMWbMGOTl5aG6uhqRkZEYNGgQJ/z4iDd76B1NjHVnMmydRo+MNcdRYJWROSuvDAcKa5m5ldocb/8NNjUZ3p1r8gkeERF5m0cBAgB07twZN9xwgzfrQmbqNHo8v/kwth8qglbfvB76Oo0e7+acxfZjlWg0W1pzTJ8OUCkE7D1djWrTikcCooKVGNU7yuG1MnOLbIIDgJlbqW2x95QsrVcE5g/vZvfvwtkNfm2jDsv2Fds8cZt1TRd8fOA8ck4ZtouCgMggBWoa9dDLssXfvb3VxazrlpHaFRHBKv80EBERtVluL3MKAFqtFt988w0OHz6M2tpa/N///R/i4+Pxww8/ICEhAV26dPFFXVstXyxzmrHmuE2eAlEAEqOD3eqhr9PoMe/zY26vxuLsWrctP4ySGo3DY+Mj1Mi6q79b12spXKrQPwKtnUtrNZj96VFUN9rmQlCIwB+u6ogH0gwZ5c1v1M1v8LWShHqtDMgyGvVys/KaKMzykwwz5iepaLQJ0gFDoqvJv+uOu34XjVCVHzKzt1OB9jsdyLjMKZH/uf0Eobq6GosXL8bZs2fRoUMHVFZWor6+HgDwww8/4ODBg5g3b57XK9qeeLOHPjO3yKOlGh1dS5Zl6JpYytaYuZXDHlzH9mo96jR6zPr0f6hptP97rpeAjYcu4r/nagHA5kb9Qq33OgvMr3mxTgcA2Hio3GnZS1oJn35/Bt8dP2+RDZeIiMhVbncvffLJJ7h06RJefvllvPfeexb7+vfvjyNHjnitcu1Vdn613Z5BwHDjnpNf7da5PGXvWoIgQCk6/7Vh5lbX1Gn0WLKnELctP4yJ/z6E25YfxpI9hajTtI0MvoEqM7fIYXBgrqCiEQUOevFbg9MVjcjMtc2jQkRE1BS3A4SffvoJ06ZNQ1JSks1NoHHJU/KcOz30rpxLq2/ezaa9a41MijStz25NFAz7yTnjMLKsg2UoqdGgrE6HkhoNsvLKkLHmOIOEFvTtyaqWroLXuNOZQEREZOR2gFBfX+9w3J5Op2Mm5WbyZg+9IAhQKZo3vMDetTJSuyIxOtgmSGDmVte5MoyM/E+WZejb0HhynSRxfDwREbnN7QChc+fOOH78uN19J06cQNeuvDlsLm/20DenN9/RtcLUCmRO64cpA2MRH6FGpzAV4iPUmDIwFku5xKlLvDmMjLzHlQA9kChEkcP9iIjIbR4lStu0aRN69OiB3/3udwAMH6onTpzA1q1bMXnyZK9Xsr3JSO2KA4W1dlcxcreHPiO1K/afqfFoFSNn1wpTK7AgvQcWpHOCrbs40bt1G5kUiXUHy9AW+t053I+IiDzhdoAwceJEHDt2DK+//jrCwsIAAC+++CJqamowePBg3HLLLV6vZHsTplZg2fRkfHqwCtsOFUGnl6EUBaR5kAchTK3AB9OT8W7OWew4VomGy3kQghQCxvaNhkohIPd0NapMeRBERIUoMCrJcR4Ea7yJdQ8nerduxgD9dHmDwyBBFICEDkGQARRWNrq1hKkIQBANKxN5wnjt/nFh2Ha0HHoH1+4ZHcThfkRE5BGP8iDIsoy9e/fip59+QlVVFSIiInDNNddg+PDhENvQ43lXeTsPAmC57rMkSV65WazT6LF0b5EpKZO9JEztrde6pdYyX7KnEFl5ZXZvLEUBmDIwtk0lmwu0NeONichy8quh0Uu4pNFDEASEqkWoRNEUrAMwldNJMkQBiAhSoEajh04vo15riALMj7tnWDwEQbA4TikKuDYhHIIg4PuCGugkGQoBGNYzAsBv26w7Cuo0erybfRY7jv8W/IcoRUxiHgSfC7Tf6UDGPAhE/udRgECWfB0geONHZEq+ZjUx1pPka21JS33IO0uG1zM6uM3N5QjkmynzoNlZAO0sk7I7x7mzzXo/AIiiGLBtHUgC+Xc60DBAIPI/di+1E1w1p3XhRO/AYX5T7uwG3Xqfp8e5s816f3t6+kdERL7j0hyExYsXY968eejWrRsWL17stKwgCAgPD0dycjLGjRsHlUrllYpS87iyas6CdL9Wqd3jRG8iIiJqjdyepOzKY+7z58/jhx9+QGFhIe69995mVZCaj6vmtH5sdyIiImotXAoQnnvuOdPXzz//vEsn3rVrF1atWuVRpci7uGoOEREREbnKZ3MQrrzySlOeBGp53ky+RkRERERtl9tDjABAkiTs3bsXhw8fRk1NDSIiItC/f3+kpqZCoTBMroyPj8f999/v1cqS57yZfI2IiIiI2i63A4Tq6mq89NJLOHXqFERRREREBGpqarBr1y588cUXeOqppxAZ6V5v9Pbt27F582ZUVlaie/fumDt3Lq688kqH5bOzs7F582YUFxcjNDQUgwcPxuzZsxEREWEqU1dXh88++wz79+9HXV0dOnfujNmzZ1s81XD3uoHMuGqO9drrniRfIyIiIqK2y+0AYeXKlSgqKsJDDz1kSoxmfKKwbNkyrFy5Eg899JDL59u7dy9WrFiBefPmITk5GTt37sRLL72EJUuWIDY21qb80aNH8c4772DOnDlISUlBeXk5li1bhvfffx+PPfYYAECn0+Fvf/sbIiMj8eijj6Jjx464ePEigoODPb5uW8BVc4h8z5O/LXfzJBAREfmS2wHCjz/+iBkzZiAtLc20TRRFpKWloaqqCmvXrnXrfFu2bMHYsWNx/fXXAwDmzp2LgwcPYseOHZg5c6ZN+ePHj6Nz58645ZZbAACdO3fGDTfcgM2bN5vK7Nq1C7W1tfjrX/8KpdLwEq0Torh73baGNxxE3mPMvJydb5ul3NHTOWfHALC7755h8QgP8mhkKBERkcs8Wua0e/fudvf16NHDrSyHOp0O+fn5mDRpksX2gQMH4tixY3aPSU5OxurVq/HTTz9hyJAhqKqqwr59+zBkyBBTmR9//BF9+/bFhx9+iAMHDiAyMhIjRozApEmTIIqiR9cFAK1Wa5ExWRAEhISEmL72JuP5WktPZFvtxfS0nck9bbmdHWUpz8orw4HCWiybnmwTJDg7Zv+ZGggAzlQ0Wuxbe7AM6/PKEBuuwqikDpg/3H7w0ZbbujVhO/sP25rI/9wOEAYMGIBffvkFAwcOtNmXl5eH/v37u3yu6upqSJKEqKgoi+1RUVGorKy0e0xycjIefvhhvPnmm9BqtdDr9UhJScHdd99tKnP+/HmUlpYiLS0NixYtQnFxMT788ENIkoSpU6d6dF0A2LBhA9atW2f6vlevXnjllVd8mq49Li6uyTK1jTq8vv0Ydv7vPLR6GSqFgBuu7II/j08GAIf7muqJdHbettaL6Uo7U/O1xXZ+fvNhw+R/q+3GLOWfHqzCc7f2d+OYRofX0svA+RotsvJKcbCkHuvvH+Hwb7EttnVrxHb2H7Y1kf+4dJdXW1tr+nrq1Kl4/fXXIUkS0tLS0KFDB1RWViI7Oxv79+/Hn//8Z7crYa9XwFFPwdmzZ7F8+XJMnToVgwYNQkVFBT755BMsW7YM9913HwBDb3dkZCTmz58PURSRlJSEiooKbN68GVOnTvXougAwefJkTJgwwaZsaWkpdDqday/WRYIgIC4uDiUlJU6fytRp9Ljn82M2PZEf5Z7G7v8V2+2J/Cj3NPYcLbHbs+nKeZs6NpC42s7UPG25nbcfKrJYGcycJAPbDhUhY2iMy8e4QpKBExdq8cL6n7BgdA+LfW25rVsTtrP/+KKtlUqlTzv3iAKdSwHC//3f/9ls27JlC7Zs2WKz/YknnsDnn3/u0sUjIyMhiqJNr31VVZVN777Rhg0bkJycjFtvvRUAkJiYiODgYDz77LOYMWMGoqOj0aFDByiVSohmycG6deuGyspK6HQ6j64LACqVCiqVyu4+X31AyLLs9NxL956zuYkHnPdEGns2l+49hwXpPeyWcX5e58cGoqbambyjrbWzLMvQ6pvIUq6XIUmSqUPBlWNcIclAdn4VHkm3P+SzrbV1a8V29h+2NZH/uBQgTJkyxSdj/5RKJZKSkpCXl4drr73WtD0vLw9Dhw61e0xjY6Mp14KRMRAwvnEkJyfju+++gyRJpn3FxcWIjo42TVp297qtVXZ+tc1NvCskGcjJr8aCdPfP29SxRO2FJ1nKXTnGVTpJbrPzg4iIqOW4FCBMmzbNZxWYMGEC3n77bSQlJaFfv37YuXMnysrKcOONNwIAVq1ahfLycjz44IMAgJSUFCxduhQ7duwwDTFauXIl+vTpg5gYw2P8cePGYdu2bVixYgVuuukmlJSUYMOGDbj55ptdvm4gkGUZOsnznkhHNxeunJc3JkQGI5MikZVXZnfIkKMs5c6OcYd18EFEROQNHs00lWUZNTU1EAQB4eHhzfqAGj58OGpqapCVlYWKigr06NEDixYtMo0NrKioQFlZman86NGjUV9fj23btuGjjz5CWFgY+vfvj1mzZpnKxMbG4umnn8bKlSvx2GOPISYmBjfffLPFqkVNXTcQNLcn0tHNhSe9okTtlSdZyp0dk9AhCDKAwspGpwGEo+CDiIiouQTZjQF9x48fx8aNG3Ho0CE0NhrGtwcFBeHqq6/G5MmT0bdvX59VtDUrLS21WP7UGwRBQHx8PIqLi52OuVyyp9CjnkhRAKYMjHU4j8DZeZs6NpC42s7UPG29nY05DdzJUu7sGMCQB+Hbk1Uoq9NCb9VkxuBj6bR+Nudv623dWrCd/ccXba1SqQKqQ5DI31wOELZv344VK1YAMIzfN/5hlZaWIj8/H4Ah2dj48eN9U9NWzNcBgvkER2um9dTd6Il0dnPhynmbOjaQ8EPeP9pTO3s7k3Jtow7L9hW7HHy0p7ZuSWxn/2GAQOR/Lg0xOn78OJYvX44hQ4Zg3rx56Nixo8X+ixcvYtmyZVixYgV69+6NPn36+KSy7UmdRo/nNx/G9kNF0OodZ2YNUyuQOa1fkz2R7vRsunLethAcEPmCJ0PvnB0THqTEgvQeWJDedhMWEhFR6+LSE4Q33ngDFRUVWLx4scXSoeYkScJzzz2H6OhoPProo16vaGvm7ScIznrvE6ODkemk956ZlN3DXkD/YDv7D9vaP9jO/sMnCET+59IM16NHj2L8+PEOgwPAsNTouHHjcPToUa9Vrr3KzC0y5CCweh805iDIzC1yeKyzm/jm3OC3xeCAiIiIiGy5FCDU1tYiNja2yXKdOnWyyLpMnnElBwERERERkS+4FCBERESgtLS0yXJlZWWIiIhodqXaM3dyEBAREREReZtLAUJycjJ27NgBycmNqyRJ2LZtG6644gqvVa49Yg4CIiIiImpJLgUIEyZMwK+//orXX38dFRUVNvvLy8vx+uuv4+TJk/jDH/7g9Uq2NyOTIiE6uP/3R3IkPp0gIiIiar9cWua0X79+mDNnDlauXIn7778fvXv3RufOnQEAFy5cwMmTJyHLMubOncslTr3Ak8yszWVM2pSdXw2d5HhZVSIiIiJq21wKEADg5ptvRq9evbBx40YcPnwYv/76KwBArVZj0KBBmDx5MpKTk31W0fYkTK3AsunJ+PRgFbYdKoJO79scBKZlVcsbLCZHZ+WV4UBhrdNlVYmIiIiobXE5QACAK664Ak8++SQkSUJNTQ0AwwRmZ8ufkmfC1Ao8d2t/ZAyNcZpJ2RtMy6pabTdfVnVBeg+fXZ+IiIiIWg+P7uxFUURUVBSioqIYHPiBryckc1lVIiIiIjLi3X07x2VViYiIiMgcA4R2jsuqEhEREZE5BgjU4suqEhEREVHrwQCBkJHaFYnRwTZBgi+XVSUiIiKi1smtVYyobQpTK5A5rR8yc4uQk18NneTbZVWJiIiIqPVigEAADEHCgvQeWJBumLjMOQdERERE7ROHGAU4X6wuxOCAiIiIqP3iE4QAVKfRIzO3CNn51dBJEpSiiJEOhgPxaQARERERuYMBQoCp0+iRsea4TebjrLwyHCisRea0fgDgcgBBRBRIrDs93O0EMS/PDhQiIvsYIASYzNwim+AAMGQ8LqhowLs5Z3Gw6JLTAIJBAhEFEuunpqIgIDJIgZpGPfSy3GQniPnxGr0e9VoZAoAQtQgVO1CIiGwwQAgw2fnVNsGBkSQDO45VokErOQwgMnOLsCC9h6+rSUTkFY6eml6o1VqUc9QJ4uh4ALiklZweS0TUXnGScgCRZRk6yVF4YNCgsw0OjCQZyMmv9nqdiIh8xdFTU2vmnSDuHu/oWCKi9opPEAKIIAhQis2L6XSS3Oxxt+5MkiYiag5nT02tGTtBFqS7f7y9Y+3hvAUiag8YIASYkUmRyMorg2Sn414UgGClaHpsbo9CFJodHDQ1SZpBAhF5gytPTa2Zd4K4e7y9DhRZlnFJK9npFInCc7d1cqtuRESBggFCgMlI7YoDhbUoqGiwCBJEAegZHYyBXcOw+fBFhwHEyKTIZl2/qUnSnONARN7iyVNT804Qd483Hmv+lLRRp0NlvQTrt9SsvFIcLPkO793WG6EqjtYloraF72oBJkytQOa0fpgyMBbxEWp0ClMhPkKNKQNjsXRaPzyQ1g2J0cEQrR4SGAOIjNSuzbp+U5OkvT3HgYjat5FJkTbvZ02V9+R4YweK8Slp1sEylNRoUGEnOAAM73cnLtQicy/nLRBR28MnCAEoTK3AgvQeWJBufzxs5rR+yMwtQk5+NXSSDKUoIM0LcwRceVzvjTkORERGjp6a2qMUYdMJ4srx5h0ork6KBgxBQvapKjyS3t21F0NEFCAYIAQ4ezfiTQUQzblWU4/rmzvHgYjInPGp6dK957D+F/vDJ42igpU2w32Mxxs7TTR6CfWX52mFXs6DYN6B4s6kaADQ6dkpQkRtDwOENs7bH1pNTZJu7hwHIiJrYWoFHh2dgJxTNSip0Tgsp1KIbnWa2JuQ7O6kaHaKEFFbxDkI5JaM1K4+neNAROSIs/kErnZQmN/MW9/YezIpelRSlFvliYgCAQMEcktTk6S5xCkR+Yo/OijcmRTdIUSFjOHsFCGitodDjMhtvprjQETkjPV8Am8uwmDk6qToyCAR2xeMglRXwYzyRNTmMECgZmFwQET+5OsOiqYmNSsFASN7R2H+8G7oEhmM4jqvXp6IqFVggEBERAHJVx0UrkxqZucIEbVlnINARETkgLNJzUREbRUDBCIiIiIiMmGAQEREREREJgwQiIioxRlXArK3IpAsy05XCjLud3Sss+sZSU4SpJmXdVZPIqK2gpOUiYioRdRp9MjMLcKek1WobtBBo5ehVgiIClZiWGIEdBKw60QlGnWGm/cghYDxV8Tg/hFdIQgC3s05i21HK9Cg++1mPVQlYkyfDlAqBHxfUAOdJEEhCBjVOwqzrumCT348j+z8alPGZK1OQo1GgiwDggAkxQTj9VuTEBakRGZuEbLzq6HR61GvlQFZhgRAq5ehVoiICT+KEYnhyEiNZw4YImpTBJndIM1WWloKrVbr1XMKgoD4+HgUFxezp8qH2M7+wXb2n0Bp6zqNHhlrjuN0eQM8qaUIwHGfv4NjBECW4dL1FAKgd6GgCCAxJhiZTBTpM774nVapVOjUqZNXzkXUFnGIERER+V1mbhEKPAwOAPeDAwCQXAwOANeCA2M9CioakJlb5EGNiIhaJwYIRETkd9n51R7d5LdGkgzk5Fe3dDWIiLyGAQIREfmVLMumOQBthU5yPpGaiCiQMEAgIiK/EgQBSrFtffwoRIGJ1IiozWhb79BERBQQRiZFQmxD99MjkyJbugpERF7DAIGIiPwuI7UrEqOD0RZihMggBTJSu7Z0NYiIvIYBAhER+V2YWoHMaf0wdVAsuoSrEOSHFUIFABFqwa0nF0oRiA4RHQYykUEiPv7jFVzilIjaFCZKIyKiFhGmVmBBeg8sSO8BWZZxSSshc28Rck5VQyfJUIoCrksMByAg93Q1qkzJ1EREhSgwKikKGaldEaoy9HXVafRYtq8YOfnV0Ogl1GsNE6FD1SJUooi0pEhTT/+72Wex43glGi4nYVMJQLcOQajTSpAkQCkKpvKhKtFQt9wi5OQb6qYQgZsHdMMfB0WZrk9E1FYwUZoXMFFa4GI7+wfb2X/aSlvLsmwz6de4zd4+Z8c7K29sI/P9TZ1flmWIotgm2jkQMFEakf+x24OIiOzeeLXkja+9G3TjNldWCzIv46y8INiuPtTU+blaERG1da1iiNH27duxefNmVFZWonv37pg7dy6uvPJKh+Wzs7OxefNmFBcXIzQ0FIMHD8bs2bMREREBAPjmm2/w3nvv2Rz3ySefQK1WAwDWrFmDdevWWeyPiorCsmXLvPjKiIharzqNHpm5RcjOr4ZOkqAURQy7PKRnX0GNadvIy0NtOM7eda485SAiaq1aPEDYu3cvVqxYgXnz5iE5ORk7d+7ESy+9hCVLliA2Ntam/NGjR/HOO+9gzpw5SElJQXl5OZYtW4b3338fjz32mKlcSEgI3nrrLYtjjcGBUY8ePfDMM8+Yvhfb2LrcRESO1Gn0yFhzHAXlDRYZjTceKrcpm5VXhgOFtcic1o9BghP2Ai4GV0QUiFr8jnjLli0YO3Ysrr/+etPTg9jYWOzYscNu+ePHj6Nz58645ZZb0LlzZ1xxxRW44YYbkJ+fb1FOEAR06NDB4p81URQt9kdGch1rImofMnOLbIIDRyQZKKhoQGZukc/rFaiMAVfWwTKU1GhQVqdDSY0GWXllyFhzHHUafUtXkYjIZS0aIOh0OuTn52PQoEEW2wcOHIhjx47ZPSY5ORkXL17ETz/9BFmWUVlZiX379mHIkCEW5RoaGnD//ffj3nvvxd///necOnXK5lwlJSWYP38+HnjgAbz55ps4f/68914cEVErlp1f7VJwYCTJQE5+tc/qE+gcBVwMrogoELXoEKPq6mpIkoSoqCiL7VFRUaisrLR7THJyMh5++GG8+eab0Gq10Ov1SElJwd13320q07VrV9x///1ISEhAfX09vvzySzzzzDN47bXXEB8fDwDo27cvHnjgAXTt2hWVlZVYv349nn76abzxxhumuQzWtFqtxWpFgiAgJCTE9LU3uTMZjzzHdvYPtrP/uNLWsixDL7k/AVkn2a74015Zt3POKccBlyQb9j86mu3mCb5/EPlfi89BAJyvVmHt7NmzWL58OaZOnYpBgwahoqICn3zyCZYtW4b77rsPANCvXz/069fPdExycjKeeOIJbN261RRImD9xSEhIQL9+/fDQQw9hz549mDBhgt1rb9iwwWJic69evfDKK6/4dKm0uLg4n52bfsN29g+2s/801dZB6qNAnXvLMweplejalRmDzcXFxUGWZUg44rScDBFxcXG8yW0Gvn8Q+U+LBgiRkZEQRdHmaUFVVZXNUwWjDRs2IDk5GbfeeisAIDExEcHBwXj22WcxY8YMREdH2xwjiiJ69+6NkpISh3UJDg5GQkICiouLHZaZPHmyRfBgfKMvLS2FTqdzeJwnBEFAXFwcSkpKuMa2D7Gd/YPt7D+utnVqQjiyKuvh6oMEUQCGJ4Q7fY9sT6zbWWxiwJYAyelnkJGrqx+1p1WSfPH+oVQqmQeByIkWDRCUSiWSkpKQl5eHa6+91rQ9Ly8PQ4cOtXtMY2MjFArL1SCMqw85euOQZRkFBQXo0aOHw7potVqcO3fO6fKqKpUKKpXK4TV8QZZl3lD5AdvZP9jO/tNUW2ekxuNAYQ0KKhqaDBJEAegZHYx7UuN9+vMzT4RmrqnhUk0lVDM/n7NEa+ZJ0+wlUHN0fVmWkdYrEll5ZXbbUhSAtF6RDtvO1dWP2vsqSXz/IPKfFh9iNGHCBLz99ttISkpCv379sHPnTpSVleHGG28EAKxatQrl5eV48MEHAQApKSlYunQpduzYYRpitHLlSvTp0wcxMTEAgLVr16Jv376Ij483zUE4ffo0/u///s903Y8++ggpKSmIjY1FVVUVsrKyUF9fj/T0dP83AhGRn4WpFcic1g+ZuUXIya+GTpKhFAVcdzkPwvcFNaZtaT68CTXe9O45WYWqei0a7Sz2E6oSMS45Gg+kdUOYWuE0f8Pe09WobtChUWe4kbS+nRQABCkFRAUrMap3FGZd0wXL9xdj+7FKNGgli/KiAAQrLa/tSEZqVxworLUJuIzBVUaq/aFZjpabtV5a1tVyRETe0OIBwvDhw1FTU4OsrCxUVFSgR48eWLRokenRX0VFBcrKykzlR48ejfr6emzbtg0fffQRwsLC0L9/f8yaNctUpq6uDpmZmaisrERoaCh69eqFxYsXo0+fPqYy5eXleOutt1BdXY3IyEj07dsXL774Ih85ElG7EaZWYEF6DyxId94T7yvGm97T5Q02N/LmLmklbDx0Ef89V4t/Tu6DRzaedCl/gz0ygAadjIZaLdYdLMOGvDLoHFxcki2v/cH0ZIc34Y4CrqaCK1dWP1qQ3sPlckRE3iDIfF7XbKWlpRarG3mDIAiIj49HcXExH6n6ENvZP9jO/hNIbb1kTyGyDpa5tdxqn47ByL/oWv4Gb7t9UKzpJrypdnY1uLpt+WGU1Ggc7o+PUCPrrv4ul2uLfPE7rVKp2CFI5ESLP0EgIqL2yd1cDABw8qLzpw2+lJVneJqdkdoV4UHOPz5dnWisk5y3gE6SIUmSS+Xa08RlIvKtFs+kTERE7Y8rN8d2j/NBXVwlyfBqZmRBEKAUnX8MK0QBoii6VI7BARF5CwMEIqI2yHzFF+PX1t8bv5bMbtTtlbW3z/qfeRnr/zuqR1M3va2Racz/Xu9kRh6ZFAnRwX29KBj2u1OOiMgbOMSIiKiNqNPo8W7OOXx1/CAuafTN7m0XAKgVhv9r9HA6HEghAGqFAAmAxsEKQm2FJAPZp6q8ci5XVz/ydJUkIiJPMEAgImoD6jR6zPv8GAoqGr12Thmwu+yoPXoZqHe0HFArJcAQ2HhSbZ3eO2vyu7r6kaerJBEReYIBAhFRG5CZW+TV4KAlCLD/1CFUZRiKdEnr+BlGsFLATVfEIPd0NapcyYMQosSopN/yIOw4Vol6qzwIzigV3hvz39Rys+6WIyJqLgYIRERtQHZ+dUtXodm6hKuw/u6r7WZSXrKn0Gmm4j/072hagtTdTMqPj03E42MTLbInv/FNIdb/4vh6I3tFeeEV23L1pp/BARH5UuDNECMiIguyLEOrb/6qOi1NL/+2VKf5P8AwBj8xOthmoq69MfjGY8z/b34uRzfX5mXmD2/iesM55p+I2i4+QSAiCnCCIEClUAAI7CBBIYpOh9f4cwy+K9ezXsEJgMVTCOsnGMbXZr6qk3h5JSdHZew9AbG+lr1y9rabX4+IyBkGCEREbcDIpEisPVjW0tVolqaW6vT3GHx71zOsFHUWO45VokFnmBOhFoG4SDXO12jRoJNN8xjM51QIZt/bm+cgCoAsXy4nGFZKslcuWGG4Vkm1Bg0exIMCgF4xQVgyqQ86havdPwERtQvsSiAiagMMQ3CCWroaHusZHeTWUp3+HoNvDA7mfX4MGw+V45JWgiQbbuQb9MDpCg3qzYIDADZfS3C89KsxIJBgWBHKUTnjtTwJDoz1yC9vxNQVR1Baq/HsJETU5jFAICJqA8LUCnwwPRmTr+6IMLUC3rh9FgAEKYBgZdMfFgoBCFEKCFYKpt5yV4SqREy6uiOWTU9u9Ut1toWVooy0koyFm062dDWIqJXiECMiojYiTK3A49cnYsmsYSgqKnK6io/51/bGwpvvtx4Xb495GUfj7x2NkQ8UbWGlKHP55Q0tXQUiaqUYIBARtUHmN97WN+HW+9wp6+p17a0YFEjBgLW2slKUBRmQJIkTl4nIBt8ViIiImvDbSlFtiAAGB0RkF98ZiIiIXNDUKkuBJikmuKWrQEStFAMEIqI2xnzde0fr9Ft/be978+2SJDV5TfPrOruW9dfO5ja0JoG+UpQ5lSjgHxN7t3Q1iKiV4hwEIqI2oE6jR2ZuEb7Nr0JV/X/RoDPcdIsCEKQQEB+pRp1GglaSUK+VIQAIUYtQCAIigxSoadRDL8tQiiJGJkVi1jVdkJlbhG1HK6C/fP9uvYa+MSfA9mOVaNBKpqU5jdmHVSIgXp7jEKwSTNcNVgm4pJGg1f+2LGiwUsS45Gg8kNatWasZ+TI/gnGlqDf3FOLL/1U4XIrUyDwPQmvBPAhE5ApBDpSum1astLQUWq3Wq+cUBAHx8fEoLi4OmN61QMR29g+2s2/VafTIWHMcp8sbvHJDKsDweNnRlFyVKOCjPybjyS2nvL7sZ2J0ED5wc8lTY3CUnV8NnSSZghxfZFg2uqSV8MnBSmzNK0JVgw4avQy1QkRksIhRSVGYP7wbQlWGzNC1jTosyy1GzilDRmYBMsKDRBRXa9F4OfoKUggYlxyNB0d2BwBk7i2yKB8ZrECNRoIkAUpRwHWJ4dDqgd0nfkvYZjzHXdfG4dMfL5iOVwhAWlIk7hkWj4hglU/aw5d88f6hUqnQqVMnr5yLqC1igOAFDBACF9vZP9jOvrVkTyGyDpbB+SAg74oKElHV6Jsr3j4oFgvSe7hU1hgcFZQ3WLx+UQASo4OROa2fT4IE899pSZJsloR1xLqM+TKwrpa3LuvsHP7IOO1rDBCI/I9zEIiIAlx2frVfgwMAPgsOACDHjXwDmblFNsEBYMhMXFDRgMzcIu9Wzg57S7o2Vdb8e2fHOVt21pVzBHpwQEQtgwECEVEAk2UZuiYmEAcanSS53FPsLDiSZPeCjeZwZUK2J+ciImoJnKRMRBTABEGAso2tZa8QRZd6vl0JjnSS7LNhNrWNOrzxTSGy86ug0estJmGbTwRXuTAnoiXmURAROcIAgYgowI1MikRWXhkkP3Y8+3IOgqv5BlwJjhSi8yE8nqrT6DHnve9w4nytzROMS1rzrw17s/LKcKCw1u6cCEfzKJwdQ0TkS22r24mIqB0yrM8fDG/dBgsAnN2OqkQB70/r55OcAD2jg5CR2tXl8iOTIk3LqloTBd8lN1u6twgnLtgGB444mxPRGuZREBGZY4BARBTgwtQKZE7rh6mDYtElQoVg5W93zKIAhCgFJMUEoUuECh1DlQhViQhViYgNU6JLuAp9OgajS4QKncJUiI9QY+qgWKy/uz9+f2U0FGY33wKApJggrJt7FRKjQ/DB9GRMujrGsJynWX1E4bf8CyEqAaEqETGhCtN1Y0IVCFEKUAi/lQ1ViZh0dUcsc3OJU2NwZB0kiALQMzrYrWDDHTmnqtx+YuNoTkRrmUdBRGTEIUZERG1AmFqBBek98OjoBMTFxaGkpMSU/djRMplNLaH51I098dSNPU3ZjkWr4TxhagUeH5uIx8cmWiy1aTyPo2tZf21dR3dfd+a0fsjMLUJOvmHdf6UoIM2H4/dlWYZO79l4Lus5ES09j4KIyB4GCEREbYyzZTfNt7myhKZxe1M3p/bO6+hazurgCWNwtCDdP+v+C4IApcKza1jPiWjJeRRERI5wiBEREbUZ/rqRTusV5XDugzPVDTos2VOIOs1veaqdzaNwdAwRkS8xQCAiInLT/OFd0adzuNtBwiWthKy8MmSsOW664Xc0j8LZMUREvsQAgYiIvKa9JPkKUyuw/v4RmDqwE+Ij1BaTv40Tsh2NQrJencg4j2LKwFiEqux/LHNFIyLyJ85BICKiZmmvSb7Cg5RYMLoHHknvbncS9m3LD6OkRmP3WOPqRAvSDd8b51Fk51fjkta1Y4iIfIUBAhEReYxJvgysJx57sjoRVzQiotaCQ4yIiMhjTPJlnyerE3FFIyJqLRggEBGRx5jkyzFPsjy3VGZoIiJzDBCIiMgj7gyJaY88yfLcUpmhiYjMcQ4CERF5hENinPMky3NLZIYmIrLGAIGIiDw2MikSWXllkOw8JOCQGM+yPPs7MzQRkTUOMSIiIo9xSIzrPLnRZ3BARC2BTxCIiMhjHBJDRNT2MEAgIqJm4ZAYIqK2hUOMiIjIaxgcEBEFPgYIRERERERkwgCBiIiIiIhMGCAQEREREZEJAwQiIqIA0l4zUxOR/3AVIyIiolauTqNHZm4RsvOroZMkKEURI7mULBH5CAMEIiKiVqxOo0fGmuMoKG+AZLY9K68MBwprkTmtH4MEIvIqDjEiIiJqxTJzi2yCAwCQZKCgogGZuUUtUi8iarsYIBAREbVi2fnVNsGBkSQDOfnVfq0PEbV9DBCIiIhaKVmWoZMchQcGOknmxGUi8ioGCERERK2UIAhQis4/qhWiwAzWRORVDBCIiIhasZFJkRAd3P+LgmE/EZE3tYpVjLZv347NmzejsrIS3bt3x9y5c3HllVc6LJ+dnY3NmzejuLgYoaGhGDx4MGbPno2IiAgAwDfffIP33nvP5rhPPvkEarXa4+sSERH5W0ZqVxworEVBRQMks5FEogD0jA5GRmrXlqscEbVJLR4g7N27FytWrMC8efOQnJyMnTt34qWXXsKSJUsQGxtrU/7o0aN45513MGfOHKSkpKC8vBzLli3D+++/j8cee8xULiQkBG+99ZbFsebBgbvXJSIiaglhagUyp/VDZm4RcvKroZNkKEUBacyDQEQ+0uIBwpYtWzB27Fhcf/31AIC5c+fi4MGD2LFjB2bOnGlT/vjx4+jcuTNuueUWAEDnzp1xww03YPPmzRblBEFAhw4dvHZdIiKilhKmVmBBeg8sSDdMXOacAyLypRYNEHQ6HfLz8zFp0iSL7QMHDsSxY8fsHpOcnIzVq1fjp59+wpAhQ1BVVYV9+/ZhyJAhFuUaGhpw//33Q5Ik9OzZE9OnT0evXr08vi4AaLVaaLVa0/eCICAkJMT0tTcZz8cPAd9iO/sH29l/2Nb+0ZLt3N5+tvydJvK/Fg0QqqurIUkSoqKiLLZHRUWhsrLS7jHJycl4+OGH8eabb0Kr1UKv1yMlJQV33323qUzXrl1x//33IyEhAfX19fjyyy/xzDPP4LXXXkN8fLxH1wWADRs2YN26dabve/XqhVdeeQWdOnVy/8W7KC4uzmfnpt+wnf2D7ew/bGv/YDv7D9uayH9afIgRYL9XwFFPwdmzZ7F8+XJMnToVgwYNQkVFBT755BMsW7YM9913HwCgX79+6Nevn+mY5ORkPPHEE9i6datFIOHOdQFg8uTJmDBhgk3Z0tJS6HS6Jl6lewRBQFxcHEpKSri+tQ+xnf2D7ew/bGv/YDv7jy/aWqlU+rRzjyjQtWiAEBkZCVEUbXrtq6qqbHr3jTZs2IDk5GTceuutAIDExEQEBwfj2WefxYwZMxAdHW1zjCiK6N27N0pKSjy+LgCoVCqoVCq7+3z1ASHLTIDjD2xn/2A7+w/b2j/Yzv7DtibynxbNg6BUKpGUlIS8vDyL7Xl5eUhOTrZ7TGNjo00vv3g5iYyjNw5ZllFQUGCatOzJdYmIiIiI2oMWT5Q2YcIEfP3119i1axfOnj2LFStWoKysDDfeeCMAYNWqVXjnnXdM5VNSUrB///7/b+/+Y6qq/ziOv678UkRAYIYMkBBuKWSjudaam9YsN9fGVujIucnE5pJVrrHM4QQdRdKPuVxzo1CSsh+Ed9kPi+EfNW8bumY5oV8MGM0hQt3Lry6/uuf7F6fvFevLN+899wrPx8bgfs4HeN8XZ+y87/mce9TU1KTe3l79+OOPOn78uLKyspSQkCBJamho0Hfffafe3l51dXXp6NGj6urq0sMPPzzj3wsAAADMRUG/BuH+++/X0NCQGhsb5XK5lJaWpr1795prA10ul/r7+83569atk8fj0RdffKETJ05o4cKFysnJ0datW805IyMjqqmpkdvtVnR0tG6//XYdOHBAWVlZM/69AAAAwFxkM1jQd9P6+vp83v7UH2w2m5YuXaqenh7WXAYQOVuDnK1D1tYgZ+sEIuuIiAheEAT+QdCXGAEAAAAIHTQIAAAAAExBvwZhNggPD1yMgfzZ+As5W4OcrUPW1iBn6/gza/5uwD/jGgQAAAAAJpYYhSiPx6M9e/bI4/EEu5RZjZytQc7WIWtrkLN1yBqwHg1CiDIMQ52dnbw7RoCRszXI2TpkbQ1ytg5ZA9ajQQAAAABgokEAAAAAYKJBCFEREREqKChQREREsEuZ1cjZGuRsHbK2Bjlbh6wB6/EuRgAAAABMnEEAAAAAYKJBAAAAAGCiQQAAAABgokEAAAAAYAoPdgGY7ssvv9Tp06fldruVmpqqoqIirVixIthl3TLa2tp0+vRpdXZ2yuVyqbS0VPfee6+53TAMNTQ06OzZsxoeHlZ2draKi4uVlpZmzpmYmFB9fb2cTqfGx8eVm5urHTt2KDExMRhPKSQ5HA6dP39eV65cUWRkpOx2u7Zu3aqUlBRzDln7R1NTk5qamtTX1ydJSk1NVUFBgfLy8iSRc6A4HA6999572rhxo4qKiiSRtT98+OGH+uijj3zG4uLi9Oabb0oiYyAUcAYhxHzzzTeqq6vTo48+qkOHDmnFihV68cUX1d/fH+zSbhljY2PKyMjQ9u3bb7j9448/1meffabt27erqqpK8fHxqqyslMfjMefU1dXp/PnzeuaZZ3Tw4EGNjo7qpZdektfrtepphLy2tjZt2LBBL7zwgvbt2yev16vKykqNjo6ac8jaPxISErRlyxZVVVWpqqpKubm5qq6u1q+//iqJnAOhvb1dzc3NWrZsmc84WftHWlqaampqzI9XX33V3EbGQAgwEFL27t1r1NTU+Izt3r3bePfdd4NU0a1t06ZNRktLi/nY6/UaTzzxhOFwOMyx8fFxY9u2bUZTU5NhGIYxMjJiFBYWGk6n05zz22+/GZs3bzYuXrxoVem3nIGBAWPTpk1Ga2urYRhkHWhFRUXG2bNnyTkAPB6P8fTTTxvff/+9UV5ebhw/ftwwDPZpf/nggw+M0tLSG24jYyA0cAYhhExOTqqjo0N33323z/iqVav0008/Bamq2eXatWtyu90+GUdERGjlypVmxh0dHfrzzz+1atUqc05CQoLS09P1888/W17zreKPP/6QJMXExEgi60Dxer1yOp0aGxuT3W4n5wB46623lJeX55OXxD7tT1evXtXOnTtVUlKiw4cPq7e3VxIZA6GCaxBCyODgoLxer+Li4nzG4+Li5Ha7g1PULDOV440ynlrG5Xa7FR4ebh7o/vcc/g43ZhiG3n77bd15551KT0+XRNb+1t3drbKyMk1MTGj+/PkqLS1VamqqedBEzv7hdDrV2dmpqqqqadvYp/0jOztbJSUlSklJkdvt1qlTp7Rv3z699tprZAyECBqEEGSz2WY0hn/v+jyNGdxQfCZz5qra2lp1d3fr4MGD07aRtX+kpKTo5Zdf1sjIiFpaWvTGG2/owIED5nZyvnn9/f2qq6tTWVmZIiMj/3YeWd+cqYvrJSk9PV12u11PPfWUvvrqK2VnZ0siYyDYWGIUQmJjYzVv3rxpr4AMDAxMezUF/058fLwkTct4cHDQzDg+Pl6Tk5MaHh6eNmfq+/GXY8eO6dtvv1V5ebnPO4iQtX+Fh4crOTlZy5cv15YtW5SRkaHPP/+cnP2oo6NDAwMDev7551VYWKjCwkK1tbXpzJkzKiwsNPMka/+aP3++0tPT1dPTw/4MhAgahBASHh6uzMxMXbp0yWf80qVLuuOOO4JU1eyyZMkSxcfH+2Q8OTmptrY2M+PMzEyFhYX5zHG5XOru7pbdbre85lBlGIZqa2vV0tKi/fv3a8mSJT7byTqwDMPQxMQEOfvRXXfdpVdeeUXV1dXmx/Lly7VmzRpVV1frtttuI+sAmJiY0JUrV7R48WL2ZyBEsMQoxDzyyCM6cuSIMjMzZbfb1dzcrP7+fj300EPBLu2WMTo6qqtXr5qPr127pq6uLsXExCgpKUkbN26Uw+HQ0qVLlZycLIfDoaioKK1Zs0aSFB0drQcffFD19fVatGiRYmJiVF9fr/T09GkXLc5ltbW1OnfunJ577jktWLDAfMUvOjpakZGRstlsZO0nJ0+eVF5enhITEzU6Oiqn06nW1laVlZWRsx8tWLDAvIZmSlRUlBYtWmSOk/XNO3HihFavXq2kpCQNDAyosbFRHo9Ha9euZX8GQoTNYNFeyJm6UZrL5VJaWpq2bdumlStXBrusW0Zra6vP2uwpa9euVUlJiXkTnubmZo2MjCgrK0vFxcU+Bwbj4+N65513dO7cOZ+b8CQlJVn5VELa5s2bbzi+a9curVu3TpLI2k+OHj2qy5cvy+VyKTo6WsuWLVN+fr55METOgVNRUaGMjIxpN0oj63/v8OHD+uGHHzQ4OKjY2FhlZ2ersLBQqampksgYCAU0CAAAAABMXIMAAAAAwESDAAAAAMBEgwAAAADARIMAAAAAwESDAAAAAMBEgwAAAADARIMAAAAAwMSdlAHMSn93I7frlZeXKycnZ9p4RUWFz+f/x818LwAAwUaDAGBWqqys9Hnc2Nio1tZW7d+/32d86u6t19uxY0fAagMAIJTRIACYlex2u8/j2NhY2Wy2aePXGxsbU1RU1N82DgAAzHY0CADmrIqKCg0NDam4uFgnT55UV1eXVq9erd27d99wmVBDQ4MuXryonp4eeb1eJScna8OGDXrggQdks9mC8yQAAPAzGgQAc5rL5dKRI0eUn5+vxx9//B8P9Pv6+rR+/XolJSVJkn755RcdO3ZMv//+uwoKCqwqGQCAgKJBADCnDQ8P69lnn1Vubu7/nLtr1y7za6/Xq5ycHBmGoTNnzuixxx7jLAIAYFagQQAwpy1cuHBGzYEkXb58WQ6HQ+3t7fJ4PD7bBgYGFB8fH4AKAQCwFg0CgDlt8eLFM5rX3t6uyspK5eTkaOfOnUpMTFR4eLguXLigU6dOaXx8PMCVAgBgDRoEAHPaTJcFOZ1OhYWFac+ePYqMjDTHL1y4EKjSAAAICu6kDAAzYLPZFBYWpnnz/vq3OT4+rq+//jqIVQEA4H+cQQCAGbjnnnv06aef6vXXX9f69es1NDSkTz75RBEREcEuDQAAv+IMAgDMQG5urp588kl1d3fr0KFDev/993XfffcpPz8/2KUBAOBXNsMwjGAXAQAAACA0cAYBAAAAgIkGAQAAAICJBgEAAACAiQYBAAAAgIkGAQAAAICJBgEAAACAiQYBAAAAgIkGAQAAAICJBgEAAACAiQYBAAAAgIkGAQAAAICJBgEAAACA6T/q7kcbL8b4WgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fdae427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>25.700000</td>\n",
       "      <td>3.020302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>96.300000</td>\n",
       "      <td>2.750757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>2.616189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>3.056868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.911138</td>\n",
       "      <td>0.034703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.862303</td>\n",
       "      <td>0.086345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.769661</td>\n",
       "      <td>0.091315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.958200</td>\n",
       "      <td>0.026063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.811146</td>\n",
       "      <td>0.077441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.909225</td>\n",
       "      <td>0.036002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.876490</td>\n",
       "      <td>0.049803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.863929</td>\n",
       "      <td>0.051602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.757156</td>\n",
       "      <td>0.099160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.926430</td>\n",
       "      <td>0.027668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.863929</td>\n",
       "      <td>0.051602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP        25.700000     3.020302\n",
       "1                    TN        96.300000     2.750757\n",
       "2                    FP         4.200000     2.616189\n",
       "3                    FN         7.700000     3.056868\n",
       "4              Accuracy         0.911138     0.034703\n",
       "5             Precision         0.862303     0.086345\n",
       "6           Sensitivity         0.769661     0.091315\n",
       "7           Specificity         0.958200     0.026063\n",
       "8              F1 score         0.811146     0.077441\n",
       "9   F1 score (weighted)         0.909225     0.036002\n",
       "10     F1 score (macro)         0.876490     0.049803\n",
       "11    Balanced Accuracy         0.863929     0.051602\n",
       "12                  MCC         0.757156     0.099160\n",
       "13                  NPV         0.926430     0.027668\n",
       "14              ROC_AUC         0.863929     0.051602"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_rf_CV(study_rf.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c0d030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>47.300000</td>\n",
       "      <td>3.400980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>192.600000</td>\n",
       "      <td>3.373096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>3.020302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>3.893014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.906716</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.854478</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.906716</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.929104</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>0.895149</td>\n",
       "      <td>0.021286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.773585</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.927273</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.861159</td>\n",
       "      <td>0.052152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.701493</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.699061</td>\n",
       "      <td>0.055047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.979900</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.970100</td>\n",
       "      <td>0.959800</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.945300</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.961540</td>\n",
       "      <td>0.015101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.752137</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.793388</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.677686</td>\n",
       "      <td>0.771654</td>\n",
       "      <td>0.806202</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>0.842975</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.770798</td>\n",
       "      <td>0.047553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.884792</td>\n",
       "      <td>0.871459</td>\n",
       "      <td>0.903166</td>\n",
       "      <td>0.887876</td>\n",
       "      <td>0.848088</td>\n",
       "      <td>0.889147</td>\n",
       "      <td>0.904988</td>\n",
       "      <td>0.881431</td>\n",
       "      <td>0.926822</td>\n",
       "      <td>0.914503</td>\n",
       "      <td>0.891227</td>\n",
       "      <td>0.022473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.841462</td>\n",
       "      <td>0.823877</td>\n",
       "      <td>0.866574</td>\n",
       "      <td>0.848720</td>\n",
       "      <td>0.791855</td>\n",
       "      <td>0.850374</td>\n",
       "      <td>0.872388</td>\n",
       "      <td>0.838287</td>\n",
       "      <td>0.898596</td>\n",
       "      <td>0.881891</td>\n",
       "      <td>0.851403</td>\n",
       "      <td>0.030569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.808790</td>\n",
       "      <td>0.801029</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.827726</td>\n",
       "      <td>0.771471</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.859853</td>\n",
       "      <td>0.823383</td>\n",
       "      <td>0.876463</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.830301</td>\n",
       "      <td>0.031054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.704227</td>\n",
       "      <td>0.657735</td>\n",
       "      <td>0.741165</td>\n",
       "      <td>0.704903</td>\n",
       "      <td>0.593151</td>\n",
       "      <td>0.704249</td>\n",
       "      <td>0.746921</td>\n",
       "      <td>0.680079</td>\n",
       "      <td>0.803209</td>\n",
       "      <td>0.773425</td>\n",
       "      <td>0.710906</td>\n",
       "      <td>0.060009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.886400</td>\n",
       "      <td>0.888400</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>0.900900</td>\n",
       "      <td>0.874400</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.922700</td>\n",
       "      <td>0.904800</td>\n",
       "      <td>0.929600</td>\n",
       "      <td>0.916300</td>\n",
       "      <td>0.904380</td>\n",
       "      <td>0.017231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.808790</td>\n",
       "      <td>0.801029</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.827726</td>\n",
       "      <td>0.771471</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.859853</td>\n",
       "      <td>0.823383</td>\n",
       "      <td>0.876463</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.830301</td>\n",
       "      <td>0.031054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   44.000000   44.000000   48.000000   48.000000   \n",
       "1                    TN  195.000000  191.000000  195.000000  191.000000   \n",
       "2                    FP    4.000000    9.000000    6.000000    8.000000   \n",
       "3                    FN   25.000000   24.000000   19.000000   21.000000   \n",
       "4              Accuracy    0.891791    0.876866    0.906716    0.891791   \n",
       "5             Precision    0.916667    0.830189    0.888889    0.857143   \n",
       "6           Sensitivity    0.637681    0.647059    0.716418    0.695652   \n",
       "7           Specificity    0.979900    0.955000    0.970100    0.959800   \n",
       "8              F1 score    0.752137    0.727273    0.793388    0.768000   \n",
       "9   F1 score (weighted)    0.884792    0.871459    0.903166    0.887876   \n",
       "10     F1 score (macro)    0.841462    0.823877    0.866574    0.848720   \n",
       "11    Balanced Accuracy    0.808790    0.801029    0.843284    0.827726   \n",
       "12                  MCC    0.704227    0.657735    0.741165    0.704903   \n",
       "13                  NPV    0.886400    0.888400    0.911200    0.900900   \n",
       "14              ROC_AUC    0.808790    0.801029    0.843284    0.827726   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    41.000000   49.000000   52.000000   47.000000   51.000000   49.000000   \n",
       "1   188.000000  190.000000  191.000000  190.000000  198.000000  197.000000   \n",
       "2    12.000000   10.000000    9.000000   11.000000    4.000000    4.000000   \n",
       "3    27.000000   19.000000   16.000000   20.000000   15.000000   18.000000   \n",
       "4     0.854478    0.891791    0.906716    0.884328    0.929104    0.917910   \n",
       "5     0.773585    0.830508    0.852459    0.810345    0.927273    0.924528   \n",
       "6     0.602941    0.720588    0.764706    0.701493    0.772727    0.731343   \n",
       "7     0.940000    0.950000    0.955000    0.945300    0.980200    0.980100   \n",
       "8     0.677686    0.771654    0.806202    0.752000    0.842975    0.816667   \n",
       "9     0.848088    0.889147    0.904988    0.881431    0.926822    0.914503   \n",
       "10    0.791855    0.850374    0.872388    0.838287    0.898596    0.881891   \n",
       "11    0.771471    0.835294    0.859853    0.823383    0.876463    0.855721   \n",
       "12    0.593151    0.704249    0.746921    0.680079    0.803209    0.773425   \n",
       "13    0.874400    0.909100    0.922700    0.904800    0.929600    0.916300   \n",
       "14    0.771471    0.835294    0.859853    0.823383    0.876463    0.855721   \n",
       "\n",
       "           ave       std  \n",
       "0    47.300000  3.400980  \n",
       "1   192.600000  3.373096  \n",
       "2     7.700000  3.020302  \n",
       "3    20.400000  3.893014  \n",
       "4     0.895149  0.021286  \n",
       "5     0.861159  0.052152  \n",
       "6     0.699061  0.055047  \n",
       "7     0.961540  0.015101  \n",
       "8     0.770798  0.047553  \n",
       "9     0.891227  0.022473  \n",
       "10    0.851403  0.030569  \n",
       "11    0.830301  0.031054  \n",
       "12    0.710906  0.060009  \n",
       "13    0.904380  0.017231  \n",
       "14    0.830301  0.031054  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_rf_test['ave'] = mat_met_rf_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test['std'] = mat_met_rf_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_rf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36fe8bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.908582</td>\n",
       "      <td>0.020198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.866825</td>\n",
       "      <td>0.042572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.749770</td>\n",
       "      <td>0.074188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.961406</td>\n",
       "      <td>0.013415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.801971</td>\n",
       "      <td>0.051255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.905962</td>\n",
       "      <td>0.022054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.871239</td>\n",
       "      <td>0.031761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.855586</td>\n",
       "      <td>0.037081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.059407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.920848</td>\n",
       "      <td>0.021389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.855586</td>\n",
       "      <td>0.037081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.908582     0.020198\n",
       "1             Precision         0.866825     0.042572\n",
       "2           Sensitivity         0.749770     0.074188\n",
       "3           Specificity         0.961406     0.013415\n",
       "4              F1 score         0.801971     0.051255\n",
       "5   F1 score (weighted)         0.905962     0.022054\n",
       "6      F1 score (macro)         0.871239     0.031761\n",
       "7     Balanced Accuracy         0.855586     0.037081\n",
       "8                   MCC         0.747917     0.059407\n",
       "9                   NPV         0.920848     0.021389\n",
       "10              ROC_AUC         0.855586     0.037081"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "data_rf=pd.DataFrame()\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_rf = RandomForestClassifier(n_estimators = study_rf.best_params['n_estimators'],\n",
    "                                            n_jobs=8, \n",
    "                                            random_state=5, \n",
    "                                            max_features = None,\n",
    "                                            oob_score=True,\n",
    "                                            max_samples=0.8,\n",
    "                                          )\n",
    "        optimizedCV_rf.fit(X_train,\n",
    "                          y_train, \n",
    "                          \n",
    "                  )\n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_rf = optimizedCV_rf.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_rf': y_pred_optimized_rf } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "   \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_rf)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "    \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_rf))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_rf))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_rf))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_rf, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_rf))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_rf))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_rf))\n",
    "    data_rf['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_rf['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_rf['y_pred_rf' + str(i)] = data_inner['y_pred_rf']\n",
    "   # data_rf['correct' + str(i)] = correct_value\n",
    "   # data_rf['pred' + str(i)] = y_pred_optimized_rf\n",
    "\n",
    "mat_met_optimized_rf = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5e07fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF baseline model f1_score 0.8682 with a standard deviation of 0.0257\n",
      "RF optimized model f1_score 0.8700 with a standard deviation of 0.0257\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized RF \n",
    "rf_baseline_CVscore = cross_val_score(rf_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#rf_opt_testSet_score = cross_val_score(optimized_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "rf_opt_CVscore = cross_val_score(optimizedCV_rf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"RF baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_baseline_CVscore), np.std(rf_baseline_CVscore, ddof=1)))\n",
    "#print(\"RF optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (rf_opt_testSet_score.mean(), rf_opt_testSet_score.std()))\n",
    "print(\"RF optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(rf_opt_CVscore), np.std(rf_opt_CVscore, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ebe6aad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_rf_clf.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf_clf, \"OUTPUT/rf_clf.joblib\")\n",
    "#joblib.dump(optimized_rf, \"OUTPUT/optimized_rf.joblib\") # fitted to whole training set with last random_state selected\n",
    "joblib.dump(optimizedCV_rf, \"OUTPUT/optimizedCV_rf_clf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c21965b",
   "metadata": {},
   "source": [
    "## LGBMclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3717154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        25.200000     2.780887\n",
      "1                    TN        95.300000     2.830391\n",
      "2                    FP         5.200000     2.859681\n",
      "3                    FN         8.200000     2.616189\n",
      "4              Accuracy         0.899944     0.030046\n",
      "5             Precision         0.833770     0.076743\n",
      "6           Sensitivity         0.754237     0.079918\n",
      "7           Specificity         0.948290     0.028335\n",
      "8              F1 score         0.789182     0.065060\n",
      "9   F1 score (weighted)         0.898149     0.030496\n",
      "10     F1 score (macro)         0.861735     0.042038\n",
      "11    Balanced Accuracy         0.851262     0.043481\n",
      "12                  MCC         0.727583     0.081736\n",
      "13                  NPV         0.921210     0.022855\n",
      "14              ROC_AUC         0.851262     0.043481\n",
      "CPU times: user 9.16 s, sys: 72.1 ms, total: 9.23 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TP=np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP= np.empty(10)\n",
    "FN= np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W=np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "        \n",
    "        lgbm_clf = lgbm.LGBMClassifier(\n",
    "        objective=\"binary\",\n",
    "        random_state=1121218,\n",
    "        #n_estimators=150,\n",
    "        boosting_type =\"gbdt\",  # default histogram binning of LGBM,\n",
    "        n_jobs=8,\n",
    "        #min_child_samples = 15,\n",
    "        subsample=0.8, # also called bagging_fraction\n",
    "        subsample_freq=10,\n",
    "     \n",
    "           )\n",
    "\n",
    "\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_clf.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=eval_set,\n",
    "                    eval_metric=\"logloss\",\n",
    "                    #early_stopping_rounds=150,\n",
    "                    verbose=False,\n",
    "                    )\n",
    "\n",
    "        y_pred = lgbm_clf.predict(X_test) \n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met_lgbm = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "print(mat_met_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfeeaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna  \n",
    "\n",
    "def objective_lgbm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        }\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0709063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is basically inner set parameters\n",
    "def detailed_objective_lgbm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 0.2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        #\"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 150, 300),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0, 0.001),\n",
    "        #\"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1.0,100),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 30, 750),\n",
    "        #\"min_child_samples\": trial.suggest_int(\"min_child_samples\", 15, 100),\n",
    "        #\"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n",
    "        #\"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6,1),\n",
    "        #\"bagging_freq\": trial.suggestegorical(\"bagging_freq\", [1]),\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "  \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M =np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        lgbm_model = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                            random_state=1121218, \n",
    "                                            boosting_type =\"gbdt\", \n",
    "                                            **param_grid, n_jobs=8,\n",
    "                                            subsample=0.8, # also called bagging_fraction\n",
    "                                            subsample_freq=10,\n",
    "                                         )\n",
    "    \n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        lgbm_model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "        y_pred = lgbm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    print(mat_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b1d2b480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:35:47,807] A new study created in memory with name: LGBMClassifier\n",
      "[I 2023-12-05 12:35:48,475] Trial 0 finished with value: 0.8580370824614223 and parameters: {'n_estimators': 849, 'learning_rate': 0.1169998110615008, 'max_depth': 6, 'max_bin': 237, 'num_leaves': 523}. Best is trial 0 with value: 0.8580370824614223.\n",
      "[I 2023-12-05 12:35:49,057] Trial 1 finished with value: 0.8622076528594148 and parameters: {'n_estimators': 737, 'learning_rate': 0.16915393210796775, 'max_depth': 5, 'max_bin': 292, 'num_leaves': 217}. Best is trial 1 with value: 0.8622076528594148.\n",
      "[I 2023-12-05 12:35:50,017] Trial 2 finished with value: 0.8802999198522674 and parameters: {'n_estimators': 886, 'learning_rate': 0.1190381174320308, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 626}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:35:50,773] Trial 3 finished with value: 0.8639618136570167 and parameters: {'n_estimators': 153, 'learning_rate': 0.0922692518252608, 'max_depth': 7, 'max_bin': 235, 'num_leaves': 331}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:35:51,694] Trial 4 finished with value: 0.8647804084576857 and parameters: {'n_estimators': 865, 'learning_rate': 0.11418749679655933, 'max_depth': 11, 'max_bin': 251, 'num_leaves': 607}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:35:53,457] Trial 5 finished with value: 0.859275215129224 and parameters: {'n_estimators': 481, 'learning_rate': 0.022358529178471948, 'max_depth': 7, 'max_bin': 258, 'num_leaves': 741}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:35:54,164] Trial 6 finished with value: 0.8617163545661812 and parameters: {'n_estimators': 571, 'learning_rate': 0.11887762593183962, 'max_depth': 7, 'max_bin': 264, 'num_leaves': 291}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:35:56,673] Trial 7 finished with value: 0.8147354923854285 and parameters: {'n_estimators': 400, 'learning_rate': 0.0047052864097514805, 'max_depth': 10, 'max_bin': 232, 'num_leaves': 253}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:35:57,602] Trial 8 finished with value: 0.8567653519200817 and parameters: {'n_estimators': 262, 'learning_rate': 0.04487227236710037, 'max_depth': 5, 'max_bin': 164, 'num_leaves': 607}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:35:57,912] Trial 9 finished with value: 0.8357957130640801 and parameters: {'n_estimators': 70, 'learning_rate': 0.1868329134331789, 'max_depth': 3, 'max_bin': 255, 'num_leaves': 607}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:35:58,654] Trial 10 finished with value: 0.8711278586019822 and parameters: {'n_estimators': 656, 'learning_rate': 0.14950623856989742, 'max_depth': 12, 'max_bin': 193, 'num_leaves': 111}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:35:59,413] Trial 11 finished with value: 0.859074001896083 and parameters: {'n_estimators': 658, 'learning_rate': 0.15298910236580907, 'max_depth': 12, 'max_bin': 193, 'num_leaves': 34}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:00,085] Trial 12 finished with value: 0.8669710564647704 and parameters: {'n_estimators': 736, 'learning_rate': 0.14945381522259596, 'max_depth': 9, 'max_bin': 195, 'num_leaves': 98}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:00,814] Trial 13 finished with value: 0.8591194465086135 and parameters: {'n_estimators': 900, 'learning_rate': 0.19991543074448168, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 423}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:01,792] Trial 14 finished with value: 0.8622131708216919 and parameters: {'n_estimators': 695, 'learning_rate': 0.08509745134975373, 'max_depth': 9, 'max_bin': 298, 'num_leaves': 454}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:02,657] Trial 15 finished with value: 0.8677238629493592 and parameters: {'n_estimators': 575, 'learning_rate': 0.13959326790421586, 'max_depth': 10, 'max_bin': 160, 'num_leaves': 750}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:03,638] Trial 16 finished with value: 0.8597133864272349 and parameters: {'n_estimators': 798, 'learning_rate': 0.07771171662164766, 'max_depth': 12, 'max_bin': 212, 'num_leaves': 158}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:04,420] Trial 17 finished with value: 0.8576520897330692 and parameters: {'n_estimators': 404, 'learning_rate': 0.13660812959960764, 'max_depth': 10, 'max_bin': 278, 'num_leaves': 358}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:05,198] Trial 18 finished with value: 0.8633601940077694 and parameters: {'n_estimators': 609, 'learning_rate': 0.16692022727198066, 'max_depth': 11, 'max_bin': 179, 'num_leaves': 488}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:06,302] Trial 19 finished with value: 0.8601734634887744 and parameters: {'n_estimators': 780, 'learning_rate': 0.0656716189684462, 'max_depth': 9, 'max_bin': 216, 'num_leaves': 672}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:07,138] Trial 20 finished with value: 0.8699979006825374 and parameters: {'n_estimators': 460, 'learning_rate': 0.10650608303563097, 'max_depth': 11, 'max_bin': 281, 'num_leaves': 168}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:07,977] Trial 21 finished with value: 0.8795731193497602 and parameters: {'n_estimators': 507, 'learning_rate': 0.10349788637127166, 'max_depth': 11, 'max_bin': 278, 'num_leaves': 171}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:08,796] Trial 22 finished with value: 0.8615566822480097 and parameters: {'n_estimators': 316, 'learning_rate': 0.09886222478642404, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 56}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:09,599] Trial 23 finished with value: 0.8700727420791032 and parameters: {'n_estimators': 520, 'learning_rate': 0.12816053299757904, 'max_depth': 11, 'max_bin': 287, 'num_leaves': 130}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:10,510] Trial 24 finished with value: 0.8741066927937091 and parameters: {'n_estimators': 654, 'learning_rate': 0.10167215592119073, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 202}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:11,324] Trial 25 finished with value: 0.8669600216684424 and parameters: {'n_estimators': 277, 'learning_rate': 0.10153721116321138, 'max_depth': 10, 'max_bin': 241, 'num_leaves': 218}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:12,256] Trial 26 finished with value: 0.8720087219682414 and parameters: {'n_estimators': 526, 'learning_rate': 0.07867602885123406, 'max_depth': 8, 'max_bin': 268, 'num_leaves': 291}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:13,088] Trial 27 finished with value: 0.8660222307117251 and parameters: {'n_estimators': 385, 'learning_rate': 0.12382176315869164, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 387}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:13,978] Trial 28 finished with value: 0.8757707308612416 and parameters: {'n_estimators': 804, 'learning_rate': 0.10601341081828076, 'max_depth': 12, 'max_bin': 221, 'num_leaves': 197}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:14,744] Trial 29 finished with value: 0.862650512541637 and parameters: {'n_estimators': 802, 'learning_rate': 0.11510670802290457, 'max_depth': 8, 'max_bin': 247, 'num_leaves': 572}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:15,859] Trial 30 finished with value: 0.8645516508831145 and parameters: {'n_estimators': 841, 'learning_rate': 0.07013696433894234, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 523}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:16,719] Trial 31 finished with value: 0.8719566111895094 and parameters: {'n_estimators': 725, 'learning_rate': 0.1034977786276089, 'max_depth': 12, 'max_bin': 225, 'num_leaves': 211}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:17,697] Trial 32 finished with value: 0.8760348205025743 and parameters: {'n_estimators': 899, 'learning_rate': 0.09293496301122987, 'max_depth': 12, 'max_bin': 299, 'num_leaves': 197}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:18,662] Trial 33 finished with value: 0.8611892614668506 and parameters: {'n_estimators': 888, 'learning_rate': 0.08994346902560146, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 280}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:19,458] Trial 34 finished with value: 0.8662396005142323 and parameters: {'n_estimators': 831, 'learning_rate': 0.11718948613112994, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 347}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:20,380] Trial 35 finished with value: 0.8667823271470769 and parameters: {'n_estimators': 773, 'learning_rate': 0.09174054625711756, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 249}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:21,037] Trial 36 finished with value: 0.8600762678005213 and parameters: {'n_estimators': 861, 'learning_rate': 0.10947520407539407, 'max_depth': 3, 'max_bin': 279, 'num_leaves': 163}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:21,693] Trial 37 finished with value: 0.864974778917124 and parameters: {'n_estimators': 194, 'learning_rate': 0.12806078367904186, 'max_depth': 6, 'max_bin': 262, 'num_leaves': 684}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:22,817] Trial 38 finished with value: 0.8594532977788912 and parameters: {'n_estimators': 900, 'learning_rate': 0.05718248605831514, 'max_depth': 11, 'max_bin': 227, 'num_leaves': 79}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:23,707] Trial 39 finished with value: 0.8649032225941073 and parameters: {'n_estimators': 834, 'learning_rate': 0.0931578765589897, 'max_depth': 10, 'max_bin': 250, 'num_leaves': 321}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:24,542] Trial 40 finished with value: 0.8637744808548071 and parameters: {'n_estimators': 734, 'learning_rate': 0.11587027144674257, 'max_depth': 11, 'max_bin': 274, 'num_leaves': 248}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:25,447] Trial 41 finished with value: 0.8657502383680764 and parameters: {'n_estimators': 669, 'learning_rate': 0.09908280775472567, 'max_depth': 12, 'max_bin': 287, 'num_leaves': 207}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:26,291] Trial 42 finished with value: 0.8658915356681248 and parameters: {'n_estimators': 609, 'learning_rate': 0.10905247964788593, 'max_depth': 12, 'max_bin': 259, 'num_leaves': 185}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:27,279] Trial 43 finished with value: 0.8669247643493708 and parameters: {'n_estimators': 778, 'learning_rate': 0.08170522496748894, 'max_depth': 12, 'max_bin': 177, 'num_leaves': 132}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:28,173] Trial 44 finished with value: 0.863729930729576 and parameters: {'n_estimators': 823, 'learning_rate': 0.09345460099702536, 'max_depth': 12, 'max_bin': 241, 'num_leaves': 295}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:29,029] Trial 45 finished with value: 0.8659709157499599 and parameters: {'n_estimators': 687, 'learning_rate': 0.10750763139787387, 'max_depth': 12, 'max_bin': 185, 'num_leaves': 233}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:29,830] Trial 46 finished with value: 0.8700988250080879 and parameters: {'n_estimators': 760, 'learning_rate': 0.12006655512877307, 'max_depth': 11, 'max_bin': 206, 'num_leaves': 124}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:30,668] Trial 47 finished with value: 0.860865276825335 and parameters: {'n_estimators': 628, 'learning_rate': 0.08541423734866571, 'max_depth': 5, 'max_bin': 266, 'num_leaves': 185}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:31,442] Trial 48 finished with value: 0.8745626988161523 and parameters: {'n_estimators': 558, 'learning_rate': 0.13545081044372007, 'max_depth': 11, 'max_bin': 291, 'num_leaves': 71}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:32,118] Trial 49 finished with value: 0.8690437507986942 and parameters: {'n_estimators': 497, 'learning_rate': 0.13405886135694642, 'max_depth': 9, 'max_bin': 292, 'num_leaves': 38}. Best is trial 2 with value: 0.8802999198522674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8803\n",
      "\tBest params:\n",
      "\t\tn_estimators: 886\n",
      "\t\tlearning_rate: 0.1190381174320308\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 270\n",
      "\t\tnum_leaves: 626\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_lgbm = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier\")\n",
    "func_lgbm_0 = lambda trial: objective_lgbm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_lgbm.optimize(func_lgbm_0, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f9cdad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   37.000000\n",
      "1                    TN  190.000000\n",
      "2                    FP    9.000000\n",
      "3                    FN   32.000000\n",
      "4              Accuracy    0.847015\n",
      "5             Precision    0.804348\n",
      "6           Sensitivity    0.536232\n",
      "7           Specificity    0.954800\n",
      "8              F1 score    0.643478\n",
      "9   F1 score (weighted)    0.835895\n",
      "10     F1 score (macro)    0.773046\n",
      "11    Balanced Accuracy    0.745503\n",
      "12                  MCC    0.569354\n",
      "13                  NPV    0.855900\n",
      "14              ROC_AUC    0.745503\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_0 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "                                         \n",
    "    \n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "optimized_lgbm_0.fit(X_trainSet0,\n",
    "                Y_trainSet0,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_0 = optimized_lgbm_0.predict(X_testSet0)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_lgbm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_lgbm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_lgbm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_lgbm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_lgbm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_lgbm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_lgbm_0)\n",
    "\n",
    "\n",
    "mat_met_lgbm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "    \n",
    "print(mat_met_lgbm_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44ae2113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:36:32,915] Trial 50 finished with value: 0.8705196085923369 and parameters: {'n_estimators': 446, 'learning_rate': 0.14330341998609344, 'max_depth': 10, 'max_bin': 300, 'num_leaves': 90}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:33,683] Trial 51 finished with value: 0.8714860288910234 and parameters: {'n_estimators': 556, 'learning_rate': 0.12468152788159684, 'max_depth': 11, 'max_bin': 275, 'num_leaves': 70}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:34,607] Trial 52 finished with value: 0.8781527402245966 and parameters: {'n_estimators': 567, 'learning_rate': 0.11259448450930827, 'max_depth': 12, 'max_bin': 282, 'num_leaves': 147}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:35,445] Trial 53 finished with value: 0.8666380406330771 and parameters: {'n_estimators': 415, 'learning_rate': 0.13390250132605397, 'max_depth': 12, 'max_bin': 285, 'num_leaves': 147}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:36,292] Trial 54 finished with value: 0.8725843381674624 and parameters: {'n_estimators': 565, 'learning_rate': 0.11092937875122076, 'max_depth': 11, 'max_bin': 294, 'num_leaves': 109}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:36,912] Trial 55 finished with value: 0.8702851338647442 and parameters: {'n_estimators': 364, 'learning_rate': 0.12073050406463487, 'max_depth': 4, 'max_bin': 281, 'num_leaves': 96}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:37,816] Trial 56 finished with value: 0.8644767263181612 and parameters: {'n_estimators': 864, 'learning_rate': 0.1428278324695144, 'max_depth': 10, 'max_bin': 256, 'num_leaves': 685}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:38,266] Trial 57 finished with value: 0.8618881256961028 and parameters: {'n_estimators': 74, 'learning_rate': 0.13127349161520308, 'max_depth': 6, 'max_bin': 291, 'num_leaves': 48}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:38,998] Trial 58 finished with value: 0.8600278137298278 and parameters: {'n_estimators': 500, 'learning_rate': 0.1566106329095535, 'max_depth': 12, 'max_bin': 271, 'num_leaves': 150}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:39,873] Trial 59 finished with value: 0.8695617101103021 and parameters: {'n_estimators': 441, 'learning_rate': 0.12426675502028024, 'max_depth': 11, 'max_bin': 285, 'num_leaves': 416}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:40,785] Trial 60 finished with value: 0.8747422555258512 and parameters: {'n_estimators': 540, 'learning_rate': 0.1138335007555685, 'max_depth': 12, 'max_bin': 205, 'num_leaves': 270}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:41,685] Trial 61 finished with value: 0.8687277692014748 and parameters: {'n_estimators': 528, 'learning_rate': 0.11439481531786991, 'max_depth': 12, 'max_bin': 200, 'num_leaves': 262}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:42,579] Trial 62 finished with value: 0.8778610109987509 and parameters: {'n_estimators': 595, 'learning_rate': 0.09821496935731926, 'max_depth': 12, 'max_bin': 208, 'num_leaves': 184}. Best is trial 2 with value: 0.8802999198522674.\n",
      "[I 2023-12-05 12:36:43,533] Trial 63 finished with value: 0.8871009773800717 and parameters: {'n_estimators': 616, 'learning_rate': 0.10205639024483008, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 234}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:44,441] Trial 64 finished with value: 0.8718729588120766 and parameters: {'n_estimators': 625, 'learning_rate': 0.09649189928206436, 'max_depth': 12, 'max_bin': 227, 'num_leaves': 175}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:45,365] Trial 65 finished with value: 0.8791956889382153 and parameters: {'n_estimators': 593, 'learning_rate': 0.10367486378426881, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 229}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:46,322] Trial 66 finished with value: 0.8697263003930547 and parameters: {'n_estimators': 596, 'learning_rate': 0.09881477726545863, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 229}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:47,182] Trial 67 finished with value: 0.8663336446336579 and parameters: {'n_estimators': 478, 'learning_rate': 0.10370960585942672, 'max_depth': 11, 'max_bin': 213, 'num_leaves': 308}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:48,345] Trial 68 finished with value: 0.8800940077156681 and parameters: {'n_estimators': 589, 'learning_rate': 0.07539639668272985, 'max_depth': 12, 'max_bin': 188, 'num_leaves': 462}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:49,358] Trial 69 finished with value: 0.8782767589347065 and parameters: {'n_estimators': 705, 'learning_rate': 0.08819374759108942, 'max_depth': 11, 'max_bin': 170, 'num_leaves': 559}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:50,515] Trial 70 finished with value: 0.8770643370825624 and parameters: {'n_estimators': 698, 'learning_rate': 0.07333069342092009, 'max_depth': 10, 'max_bin': 159, 'num_leaves': 583}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:51,535] Trial 71 finished with value: 0.871521321519357 and parameters: {'n_estimators': 588, 'learning_rate': 0.08655931081036701, 'max_depth': 11, 'max_bin': 171, 'num_leaves': 638}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:52,556] Trial 72 finished with value: 0.8714200688159577 and parameters: {'n_estimators': 649, 'learning_rate': 0.08708410082583365, 'max_depth': 12, 'max_bin': 180, 'num_leaves': 546}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:53,613] Trial 73 finished with value: 0.8737318574883222 and parameters: {'n_estimators': 633, 'learning_rate': 0.0804464623296506, 'max_depth': 11, 'max_bin': 189, 'num_leaves': 490}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:54,591] Trial 74 finished with value: 0.8698807581388494 and parameters: {'n_estimators': 581, 'learning_rate': 0.0978347116603585, 'max_depth': 12, 'max_bin': 218, 'num_leaves': 465}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:55,534] Trial 75 finished with value: 0.8734430579836202 and parameters: {'n_estimators': 711, 'learning_rate': 0.10435163539398731, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 628}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:56,506] Trial 76 finished with value: 0.8732350793132925 and parameters: {'n_estimators': 667, 'learning_rate': 0.09011984464237552, 'max_depth': 12, 'max_bin': 204, 'num_leaves': 372}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:57,315] Trial 77 finished with value: 0.8743169240043931 and parameters: {'n_estimators': 510, 'learning_rate': 0.10985159818831255, 'max_depth': 7, 'max_bin': 189, 'num_leaves': 717}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:58,389] Trial 78 finished with value: 0.8655055947802153 and parameters: {'n_estimators': 606, 'learning_rate': 0.08264973816796338, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 543}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:36:59,593] Trial 79 finished with value: 0.8800113955316219 and parameters: {'n_estimators': 546, 'learning_rate': 0.0750011343892114, 'max_depth': 11, 'max_bin': 173, 'num_leaves': 496}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:00,683] Trial 80 finished with value: 0.8685628656972563 and parameters: {'n_estimators': 461, 'learning_rate': 0.07563322213895705, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 478}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:01,853] Trial 81 finished with value: 0.8739140783843748 and parameters: {'n_estimators': 554, 'learning_rate': 0.06497640291681543, 'max_depth': 12, 'max_bin': 158, 'num_leaves': 414}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:02,645] Trial 82 finished with value: 0.8781975306497666 and parameters: {'n_estimators': 541, 'learning_rate': 0.09653129277713156, 'max_depth': 8, 'max_bin': 172, 'num_leaves': 511}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:03,547] Trial 83 finished with value: 0.878791018064131 and parameters: {'n_estimators': 533, 'learning_rate': 0.08892952813173251, 'max_depth': 8, 'max_bin': 174, 'num_leaves': 447}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:04,453] Trial 84 finished with value: 0.878027726126788 and parameters: {'n_estimators': 534, 'learning_rate': 0.07855222219494504, 'max_depth': 8, 'max_bin': 174, 'num_leaves': 440}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:05,331] Trial 85 finished with value: 0.8718950811048934 and parameters: {'n_estimators': 480, 'learning_rate': 0.08985390178268686, 'max_depth': 8, 'max_bin': 183, 'num_leaves': 507}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:06,285] Trial 86 finished with value: 0.8773147833354488 and parameters: {'n_estimators': 509, 'learning_rate': 0.08527591303743837, 'max_depth': 9, 'max_bin': 174, 'num_leaves': 525}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:07,083] Trial 87 finished with value: 0.8688098194949578 and parameters: {'n_estimators': 640, 'learning_rate': 0.09353844143501822, 'max_depth': 7, 'max_bin': 162, 'num_leaves': 569}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:07,997] Trial 88 finished with value: 0.8709499616465394 and parameters: {'n_estimators': 538, 'learning_rate': 0.10262959270203145, 'max_depth': 9, 'max_bin': 157, 'num_leaves': 499}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:08,814] Trial 89 finished with value: 0.8714435809279918 and parameters: {'n_estimators': 425, 'learning_rate': 0.1066535081114171, 'max_depth': 8, 'max_bin': 171, 'num_leaves': 460}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:09,860] Trial 90 finished with value: 0.8735840038307954 and parameters: {'n_estimators': 686, 'learning_rate': 0.07117864396025332, 'max_depth': 10, 'max_bin': 183, 'num_leaves': 433}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:10,823] Trial 91 finished with value: 0.8789383685572532 and parameters: {'n_estimators': 568, 'learning_rate': 0.09579427103220509, 'max_depth': 9, 'max_bin': 153, 'num_leaves': 398}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:11,738] Trial 92 finished with value: 0.8838848473874483 and parameters: {'n_estimators': 576, 'learning_rate': 0.0960850012423025, 'max_depth': 9, 'max_bin': 154, 'num_leaves': 401}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:12,693] Trial 93 finished with value: 0.8702360068524616 and parameters: {'n_estimators': 577, 'learning_rate': 0.08275377437766115, 'max_depth': 9, 'max_bin': 150, 'num_leaves': 405}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:13,575] Trial 94 finished with value: 0.8680942028198961 and parameters: {'n_estimators': 614, 'learning_rate': 0.08902593545491615, 'max_depth': 10, 'max_bin': 153, 'num_leaves': 372}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:14,518] Trial 95 finished with value: 0.8738921335214125 and parameters: {'n_estimators': 496, 'learning_rate': 0.09430158234496676, 'max_depth': 9, 'max_bin': 165, 'num_leaves': 336}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:15,415] Trial 96 finished with value: 0.8732944226717881 and parameters: {'n_estimators': 519, 'learning_rate': 0.07814111437093858, 'max_depth': 7, 'max_bin': 156, 'num_leaves': 393}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:16,368] Trial 97 finished with value: 0.8814936664160229 and parameters: {'n_estimators': 619, 'learning_rate': 0.10235226512432333, 'max_depth': 9, 'max_bin': 162, 'num_leaves': 434}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:17,366] Trial 98 finished with value: 0.8849211196085254 and parameters: {'n_estimators': 337, 'learning_rate': 0.10233882348946577, 'max_depth': 9, 'max_bin': 163, 'num_leaves': 441}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:18,155] Trial 99 finished with value: 0.878591239344221 and parameters: {'n_estimators': 255, 'learning_rate': 0.11912182895444799, 'max_depth': 9, 'max_bin': 154, 'num_leaves': 429}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8871\n",
      "\tBest params:\n",
      "\t\tn_estimators: 616\n",
      "\t\tlearning_rate: 0.10205639024483008\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 234\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_lgbm_1 = lambda trial: objective_lgbm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_lgbm.optimize(func_lgbm_1, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dafbda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   37.000000   46.000000\n",
      "1                    TN  190.000000  188.000000\n",
      "2                    FP    9.000000   12.000000\n",
      "3                    FN   32.000000   22.000000\n",
      "4              Accuracy    0.847015    0.873134\n",
      "5             Precision    0.804348    0.793103\n",
      "6           Sensitivity    0.536232    0.676471\n",
      "7           Specificity    0.954800    0.940000\n",
      "8              F1 score    0.643478    0.730159\n",
      "9   F1 score (weighted)    0.835895    0.869647\n",
      "10     F1 score (macro)    0.773046    0.823616\n",
      "11    Balanced Accuracy    0.745503    0.808235\n",
      "12                  MCC    0.569354    0.651416\n",
      "13                  NPV    0.855900    0.895200\n",
      "14              ROC_AUC    0.745503    0.808235\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_1 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_lgbm_1.fit(X_trainSet1,\n",
    "                Y_trainSet1,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_1 = optimized_lgbm_1.predict(X_testSet1)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_lgbm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_lgbm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_lgbm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_lgbm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_lgbm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_lgbm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_lgbm_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set1'] =set1\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f6ed3dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:37:19,108] Trial 100 finished with value: 0.8669435459604063 and parameters: {'n_estimators': 333, 'learning_rate': 0.10606631261200805, 'max_depth': 9, 'max_bin': 162, 'num_leaves': 361}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:19,977] Trial 101 finished with value: 0.8649251951501657 and parameters: {'n_estimators': 386, 'learning_rate': 0.10076205437083206, 'max_depth': 8, 'max_bin': 162, 'num_leaves': 441}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:20,816] Trial 102 finished with value: 0.8620873406591354 and parameters: {'n_estimators': 239, 'learning_rate': 0.11237516935907746, 'max_depth': 9, 'max_bin': 167, 'num_leaves': 400}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:21,721] Trial 103 finished with value: 0.8699548134928362 and parameters: {'n_estimators': 572, 'learning_rate': 0.10149604308515113, 'max_depth': 10, 'max_bin': 178, 'num_leaves': 452}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:22,540] Trial 104 finished with value: 0.8670260437685945 and parameters: {'n_estimators': 554, 'learning_rate': 0.09407082765528138, 'max_depth': 8, 'max_bin': 241, 'num_leaves': 474}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:23,388] Trial 105 finished with value: 0.8589092303203791 and parameters: {'n_estimators': 455, 'learning_rate': 0.11017961133962155, 'max_depth': 10, 'max_bin': 153, 'num_leaves': 378}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:24,199] Trial 106 finished with value: 0.8641831736432394 and parameters: {'n_estimators': 154, 'learning_rate': 0.11627261329461167, 'max_depth': 9, 'max_bin': 163, 'num_leaves': 451}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:25,033] Trial 107 finished with value: 0.855203502639531 and parameters: {'n_estimators': 618, 'learning_rate': 0.10476824526036067, 'max_depth': 9, 'max_bin': 175, 'num_leaves': 411}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:25,916] Trial 108 finished with value: 0.8576550064003131 and parameters: {'n_estimators': 599, 'learning_rate': 0.10055181269908137, 'max_depth': 8, 'max_bin': 187, 'num_leaves': 479}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:26,832] Trial 109 finished with value: 0.8603660893466051 and parameters: {'n_estimators': 492, 'learning_rate': 0.09549274903638279, 'max_depth': 9, 'max_bin': 192, 'num_leaves': 348}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:27,714] Trial 110 finished with value: 0.8544894043733583 and parameters: {'n_estimators': 294, 'learning_rate': 0.10783453368798569, 'max_depth': 10, 'max_bin': 248, 'num_leaves': 419}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:28,529] Trial 111 finished with value: 0.8604986829791572 and parameters: {'n_estimators': 586, 'learning_rate': 0.1171636849255275, 'max_depth': 9, 'max_bin': 154, 'num_leaves': 426}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:29,414] Trial 112 finished with value: 0.8655056433135837 and parameters: {'n_estimators': 256, 'learning_rate': 0.11950204409743732, 'max_depth': 9, 'max_bin': 154, 'num_leaves': 434}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:30,245] Trial 113 finished with value: 0.8613148003786337 and parameters: {'n_estimators': 217, 'learning_rate': 0.12250703509478628, 'max_depth': 9, 'max_bin': 160, 'num_leaves': 382}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:31,145] Trial 114 finished with value: 0.8597431674506728 and parameters: {'n_estimators': 751, 'learning_rate': 0.09059804960245758, 'max_depth': 8, 'max_bin': 155, 'num_leaves': 318}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:32,035] Trial 115 finished with value: 0.8668621551964437 and parameters: {'n_estimators': 352, 'learning_rate': 0.11217026473502922, 'max_depth': 10, 'max_bin': 151, 'num_leaves': 397}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:32,822] Trial 116 finished with value: 0.8628510809417269 and parameters: {'n_estimators': 168, 'learning_rate': 0.12760491027998716, 'max_depth': 10, 'max_bin': 159, 'num_leaves': 233}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:33,702] Trial 117 finished with value: 0.8652972634049126 and parameters: {'n_estimators': 311, 'learning_rate': 0.09888677534056353, 'max_depth': 9, 'max_bin': 262, 'num_leaves': 447}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:34,800] Trial 118 finished with value: 0.8684214601037652 and parameters: {'n_estimators': 678, 'learning_rate': 0.0843630705516915, 'max_depth': 12, 'max_bin': 230, 'num_leaves': 473}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:35,458] Trial 119 finished with value: 0.8505400535330212 and parameters: {'n_estimators': 518, 'learning_rate': 0.10716091799015297, 'max_depth': 4, 'max_bin': 168, 'num_leaves': 428}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:36,299] Trial 120 finished with value: 0.856475794749495 and parameters: {'n_estimators': 124, 'learning_rate': 0.11852555368376647, 'max_depth': 11, 'max_bin': 200, 'num_leaves': 493}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:37,400] Trial 121 finished with value: 0.8641969515909566 and parameters: {'n_estimators': 655, 'learning_rate': 0.08576008312562568, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 590}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:38,444] Trial 122 finished with value: 0.860134771830068 and parameters: {'n_estimators': 638, 'learning_rate': 0.09097631194854768, 'max_depth': 11, 'max_bin': 171, 'num_leaves': 529}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:39,567] Trial 123 finished with value: 0.8633142591883998 and parameters: {'n_estimators': 552, 'learning_rate': 0.08063544136536092, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 624}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:40,292] Trial 124 finished with value: 0.8611045433882367 and parameters: {'n_estimators': 566, 'learning_rate': 0.10224725819970674, 'max_depth': 6, 'max_bin': 221, 'num_leaves': 564}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:41,389] Trial 125 finished with value: 0.8645801540317357 and parameters: {'n_estimators': 718, 'learning_rate': 0.08773635717725296, 'max_depth': 11, 'max_bin': 165, 'num_leaves': 463}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:42,418] Trial 126 finished with value: 0.866383100696738 and parameters: {'n_estimators': 590, 'learning_rate': 0.09678258390764927, 'max_depth': 12, 'max_bin': 180, 'num_leaves': 645}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:43,554] Trial 127 finished with value: 0.8642899931504161 and parameters: {'n_estimators': 620, 'learning_rate': 0.07504342884144868, 'max_depth': 11, 'max_bin': 237, 'num_leaves': 662}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:44,392] Trial 128 finished with value: 0.8658661480295138 and parameters: {'n_estimators': 546, 'learning_rate': 0.09316195549879454, 'max_depth': 8, 'max_bin': 273, 'num_leaves': 548}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:45,275] Trial 129 finished with value: 0.8594970013971526 and parameters: {'n_estimators': 878, 'learning_rate': 0.11273504336011246, 'max_depth': 10, 'max_bin': 158, 'num_leaves': 516}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:46,264] Trial 130 finished with value: 0.8669100141867659 and parameters: {'n_estimators': 474, 'learning_rate': 0.1051267519651412, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 459}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:47,136] Trial 131 finished with value: 0.8590420265689784 and parameters: {'n_estimators': 542, 'learning_rate': 0.09666700522133558, 'max_depth': 8, 'max_bin': 172, 'num_leaves': 507}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:47,924] Trial 132 finished with value: 0.8541107582213867 and parameters: {'n_estimators': 527, 'learning_rate': 0.09920061307526165, 'max_depth': 8, 'max_bin': 177, 'num_leaves': 595}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:48,688] Trial 133 finished with value: 0.8568992416672832 and parameters: {'n_estimators': 574, 'learning_rate': 0.08815095645301724, 'max_depth': 7, 'max_bin': 169, 'num_leaves': 489}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:49,660] Trial 134 finished with value: 0.8632928461790346 and parameters: {'n_estimators': 604, 'learning_rate': 0.08329235276399054, 'max_depth': 9, 'max_bin': 164, 'num_leaves': 407}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:50,490] Trial 135 finished with value: 0.8499215294489162 and parameters: {'n_estimators': 528, 'learning_rate': 0.09535491962252526, 'max_depth': 8, 'max_bin': 150, 'num_leaves': 423}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:51,392] Trial 136 finished with value: 0.8716425797855093 and parameters: {'n_estimators': 563, 'learning_rate': 0.10913192575221091, 'max_depth': 9, 'max_bin': 183, 'num_leaves': 538}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:52,619] Trial 137 finished with value: 0.8735080312492786 and parameters: {'n_estimators': 496, 'learning_rate': 0.06832322232391207, 'max_depth': 12, 'max_bin': 253, 'num_leaves': 442}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:53,500] Trial 138 finished with value: 0.8553781726982528 and parameters: {'n_estimators': 586, 'learning_rate': 0.10227665074321352, 'max_depth': 9, 'max_bin': 172, 'num_leaves': 219}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:54,606] Trial 139 finished with value: 0.8730915260205828 and parameters: {'n_estimators': 793, 'learning_rate': 0.0800234088775242, 'max_depth': 12, 'max_bin': 175, 'num_leaves': 512}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:55,446] Trial 140 finished with value: 0.8644631399585612 and parameters: {'n_estimators': 638, 'learning_rate': 0.09273260514594095, 'max_depth': 8, 'max_bin': 157, 'num_leaves': 360}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:56,356] Trial 141 finished with value: 0.8759881301696059 and parameters: {'n_estimators': 512, 'learning_rate': 0.11647415582542722, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 160}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:57,267] Trial 142 finished with value: 0.8661046155850141 and parameters: {'n_estimators': 573, 'learning_rate': 0.10506874344370416, 'max_depth': 12, 'max_bin': 270, 'num_leaves': 192}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:58,157] Trial 143 finished with value: 0.8571595425925971 and parameters: {'n_estimators': 603, 'learning_rate': 0.11169168848337563, 'max_depth': 11, 'max_bin': 279, 'num_leaves': 115}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:59,115] Trial 144 finished with value: 0.8631920705008186 and parameters: {'n_estimators': 547, 'learning_rate': 0.10031117874101976, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 132}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:37:59,997] Trial 145 finished with value: 0.8598388193234928 and parameters: {'n_estimators': 618, 'learning_rate': 0.1136414546667726, 'max_depth': 9, 'max_bin': 162, 'num_leaves': 390}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:00,922] Trial 146 finished with value: 0.8622515180742294 and parameters: {'n_estimators': 535, 'learning_rate': 0.12267224817979167, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 476}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:01,825] Trial 147 finished with value: 0.8658153314708021 and parameters: {'n_estimators': 665, 'learning_rate': 0.10954636668417257, 'max_depth': 11, 'max_bin': 170, 'num_leaves': 151}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:02,935] Trial 148 finished with value: 0.8692328602293351 and parameters: {'n_estimators': 563, 'learning_rate': 0.07635780534624154, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 455}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:03,861] Trial 149 finished with value: 0.858816856230104 and parameters: {'n_estimators': 468, 'learning_rate': 0.09155464174665888, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 169}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8871\n",
      "\tBest params:\n",
      "\t\tn_estimators: 616\n",
      "\t\tlearning_rate: 0.10205639024483008\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 234\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_2 = lambda trial: objective_lgbm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_lgbm.optimize(func_lgbm_2, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef8fbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   37.000000   46.000000   50.000000\n",
      "1                    TN  190.000000  188.000000  189.000000\n",
      "2                    FP    9.000000   12.000000   12.000000\n",
      "3                    FN   32.000000   22.000000   17.000000\n",
      "4              Accuracy    0.847015    0.873134    0.891791\n",
      "5             Precision    0.804348    0.793103    0.806452\n",
      "6           Sensitivity    0.536232    0.676471    0.746269\n",
      "7           Specificity    0.954800    0.940000    0.940300\n",
      "8              F1 score    0.643478    0.730159    0.775194\n",
      "9   F1 score (weighted)    0.835895    0.869647    0.890359\n",
      "10     F1 score (macro)    0.773046    0.823616    0.851970\n",
      "11    Balanced Accuracy    0.745503    0.808235    0.843284\n",
      "12                  MCC    0.569354    0.651416    0.705000\n",
      "13                  NPV    0.855900    0.895200    0.917500\n",
      "14              ROC_AUC    0.745503    0.808235    0.843284\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_2 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_lgbm_2.fit(X_trainSet2,\n",
    "                Y_trainSet2,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_2 = optimized_lgbm_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_lgbm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_lgbm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_lgbm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_lgbm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_lgbm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_lgbm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_lgbm_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set2'] = Set2\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a48b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:38:04,822] Trial 150 finished with value: 0.861031338787417 and parameters: {'n_estimators': 580, 'learning_rate': 0.09834709409372255, 'max_depth': 10, 'max_bin': 180, 'num_leaves': 279}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:05,726] Trial 151 finished with value: 0.8646910857896446 and parameters: {'n_estimators': 540, 'learning_rate': 0.07199625247797863, 'max_depth': 8, 'max_bin': 175, 'num_leaves': 437}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:06,623] Trial 152 finished with value: 0.8638689869001936 and parameters: {'n_estimators': 598, 'learning_rate': 0.08001962243580302, 'max_depth': 8, 'max_bin': 174, 'num_leaves': 421}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:07,446] Trial 153 finished with value: 0.8682517619434277 and parameters: {'n_estimators': 515, 'learning_rate': 0.08516539019496223, 'max_depth': 7, 'max_bin': 265, 'num_leaves': 444}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:08,290] Trial 154 finished with value: 0.8686362000501925 and parameters: {'n_estimators': 563, 'learning_rate': 0.10312681502215458, 'max_depth': 9, 'max_bin': 244, 'num_leaves': 406}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:09,076] Trial 155 finished with value: 0.8637132886573383 and parameters: {'n_estimators': 280, 'learning_rate': 0.09583031239149539, 'max_depth': 8, 'max_bin': 165, 'num_leaves': 486}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:09,907] Trial 156 finished with value: 0.865404696069176 and parameters: {'n_estimators': 494, 'learning_rate': 0.06291028116194845, 'max_depth': 7, 'max_bin': 287, 'num_leaves': 466}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:10,878] Trial 157 finished with value: 0.8656184560511857 and parameters: {'n_estimators': 530, 'learning_rate': 0.07551351434261092, 'max_depth': 9, 'max_bin': 282, 'num_leaves': 435}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:11,702] Trial 158 finished with value: 0.8747782481679899 and parameters: {'n_estimators': 438, 'learning_rate': 0.08978269987579252, 'max_depth': 8, 'max_bin': 160, 'num_leaves': 610}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:12,653] Trial 159 finished with value: 0.8723416396011672 and parameters: {'n_estimators': 814, 'learning_rate': 0.10696030052211426, 'max_depth': 12, 'max_bin': 168, 'num_leaves': 713}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:13,370] Trial 160 finished with value: 0.8665543363111474 and parameters: {'n_estimators': 215, 'learning_rate': 0.12613300982696227, 'max_depth': 9, 'max_bin': 178, 'num_leaves': 248}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:14,227] Trial 161 finished with value: 0.8607298747631991 and parameters: {'n_estimators': 590, 'learning_rate': 0.09772714217658343, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 213}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:15,134] Trial 162 finished with value: 0.8667500873838829 and parameters: {'n_estimators': 624, 'learning_rate': 0.08817651645002275, 'max_depth': 12, 'max_bin': 220, 'num_leaves': 191}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:15,949] Trial 163 finished with value: 0.8703604818190154 and parameters: {'n_estimators': 550, 'learning_rate': 0.10269084213865201, 'max_depth': 12, 'max_bin': 226, 'num_leaves': 134}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:16,761] Trial 164 finished with value: 0.8651980577518239 and parameters: {'n_estimators': 607, 'learning_rate': 0.11889141823310578, 'max_depth': 12, 'max_bin': 203, 'num_leaves': 178}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:17,780] Trial 165 finished with value: 0.8650071924513487 and parameters: {'n_estimators': 570, 'learning_rate': 0.0929081568491215, 'max_depth': 12, 'max_bin': 207, 'num_leaves': 384}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:18,772] Trial 166 finished with value: 0.8645039381358988 and parameters: {'n_estimators': 700, 'learning_rate': 0.08198289540748925, 'max_depth': 11, 'max_bin': 193, 'num_leaves': 554}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:19,508] Trial 167 finished with value: 0.8628674584876377 and parameters: {'n_estimators': 641, 'learning_rate': 0.09947462452469458, 'max_depth': 8, 'max_bin': 230, 'num_leaves': 500}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:20,388] Trial 168 finished with value: 0.8709975366552382 and parameters: {'n_estimators': 589, 'learning_rate': 0.10896496186735069, 'max_depth': 9, 'max_bin': 222, 'num_leaves': 467}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:21,303] Trial 169 finished with value: 0.8673641088348678 and parameters: {'n_estimators': 853, 'learning_rate': 0.10442101612572327, 'max_depth': 12, 'max_bin': 276, 'num_leaves': 450}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:22,205] Trial 170 finished with value: 0.869803614783109 and parameters: {'n_estimators': 506, 'learning_rate': 0.08657767998622289, 'max_depth': 11, 'max_bin': 216, 'num_leaves': 202}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:23,189] Trial 171 finished with value: 0.8715844153401756 and parameters: {'n_estimators': 483, 'learning_rate': 0.07839220102659877, 'max_depth': 9, 'max_bin': 172, 'num_leaves': 495}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:24,031] Trial 172 finished with value: 0.8626808464421485 and parameters: {'n_estimators': 523, 'learning_rate': 0.09447448282334478, 'max_depth': 9, 'max_bin': 175, 'num_leaves': 532}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:24,999] Trial 173 finished with value: 0.8657859182104384 and parameters: {'n_estimators': 556, 'learning_rate': 0.06863043340505288, 'max_depth': 9, 'max_bin': 187, 'num_leaves': 143}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:25,847] Trial 174 finished with value: 0.8744522899577888 and parameters: {'n_estimators': 539, 'learning_rate': 0.08358410477489558, 'max_depth': 8, 'max_bin': 181, 'num_leaves': 526}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:26,657] Trial 175 finished with value: 0.869747827722434 and parameters: {'n_estimators': 503, 'learning_rate': 0.11389054439745339, 'max_depth': 10, 'max_bin': 174, 'num_leaves': 417}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:27,483] Trial 176 finished with value: 0.864886437743874 and parameters: {'n_estimators': 579, 'learning_rate': 0.09092432288172067, 'max_depth': 9, 'max_bin': 197, 'num_leaves': 168}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:28,430] Trial 177 finished with value: 0.8701858747155768 and parameters: {'n_estimators': 609, 'learning_rate': 0.09636003334244088, 'max_depth': 12, 'max_bin': 211, 'num_leaves': 403}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:29,456] Trial 178 finished with value: 0.8668028731252443 and parameters: {'n_estimators': 523, 'learning_rate': 0.0748001395973872, 'max_depth': 9, 'max_bin': 167, 'num_leaves': 561}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:30,392] Trial 179 finished with value: 0.8682006547463311 and parameters: {'n_estimators': 750, 'learning_rate': 0.08667134281467391, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 518}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:31,200] Trial 180 finished with value: 0.8665833278622902 and parameters: {'n_estimators': 551, 'learning_rate': 0.10127033251434113, 'max_depth': 8, 'max_bin': 296, 'num_leaves': 581}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:32,226] Trial 181 finished with value: 0.8759202409110627 and parameters: {'n_estimators': 657, 'learning_rate': 0.07171976200748355, 'max_depth': 10, 'max_bin': 157, 'num_leaves': 581}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:33,239] Trial 182 finished with value: 0.8685955552839018 and parameters: {'n_estimators': 581, 'learning_rate': 0.0824541825326506, 'max_depth': 10, 'max_bin': 153, 'num_leaves': 622}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:34,081] Trial 183 finished with value: 0.8644543621502632 and parameters: {'n_estimators': 622, 'learning_rate': 0.10738927496099206, 'max_depth': 9, 'max_bin': 170, 'num_leaves': 425}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:35,002] Trial 184 finished with value: 0.8595397558330692 and parameters: {'n_estimators': 105, 'learning_rate': 0.07302446805010634, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 650}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:35,877] Trial 185 finished with value: 0.863857209802535 and parameters: {'n_estimators': 563, 'learning_rate': 0.09753239377003023, 'max_depth': 9, 'max_bin': 163, 'num_leaves': 608}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:37,061] Trial 186 finished with value: 0.8668144938646843 and parameters: {'n_estimators': 768, 'learning_rate': 0.05994021170928383, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 540}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:38,115] Trial 187 finished with value: 0.8604834387922005 and parameters: {'n_estimators': 512, 'learning_rate': 0.06506370102831967, 'max_depth': 10, 'max_bin': 155, 'num_leaves': 442}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:39,074] Trial 188 finished with value: 0.8597280028905351 and parameters: {'n_estimators': 730, 'learning_rate': 0.07799356645974506, 'max_depth': 9, 'max_bin': 177, 'num_leaves': 482}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:40,000] Trial 189 finished with value: 0.8724578827704935 and parameters: {'n_estimators': 532, 'learning_rate': 0.09127033769631614, 'max_depth': 10, 'max_bin': 160, 'num_leaves': 518}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:40,812] Trial 190 finished with value: 0.8775339200016153 and parameters: {'n_estimators': 594, 'learning_rate': 0.11126419935468379, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 231}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:41,635] Trial 191 finished with value: 0.8727306799543932 and parameters: {'n_estimators': 595, 'learning_rate': 0.11571880689979949, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 259}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:42,440] Trial 192 finished with value: 0.8715865821752724 and parameters: {'n_estimators': 629, 'learning_rate': 0.12124268009822164, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 245}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:43,258] Trial 193 finished with value: 0.8563055152558592 and parameters: {'n_estimators': 572, 'learning_rate': 0.11018234598040677, 'max_depth': 11, 'max_bin': 174, 'num_leaves': 211}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:44,120] Trial 194 finished with value: 0.8608463717987191 and parameters: {'n_estimators': 594, 'learning_rate': 0.10552283982855144, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 228}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:45,038] Trial 195 finished with value: 0.8593988985655855 and parameters: {'n_estimators': 548, 'learning_rate': 0.10117364197580939, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 463}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:45,822] Trial 196 finished with value: 0.8732204455299459 and parameters: {'n_estimators': 698, 'learning_rate': 0.11187415380933885, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 182}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:46,620] Trial 197 finished with value: 0.8694436941479552 and parameters: {'n_estimators': 340, 'learning_rate': 0.1318307696873008, 'max_depth': 12, 'max_bin': 183, 'num_leaves': 297}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:50,087] Trial 198 finished with value: 0.4294023967607883 and parameters: {'n_estimators': 613, 'learning_rate': 0.0003749562119740868, 'max_depth': 9, 'max_bin': 280, 'num_leaves': 431}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:51,031] Trial 199 finished with value: 0.8675744895571844 and parameters: {'n_estimators': 371, 'learning_rate': 0.07039801472321089, 'max_depth': 8, 'max_bin': 156, 'num_leaves': 392}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8871\n",
      "\tBest params:\n",
      "\t\tn_estimators: 616\n",
      "\t\tlearning_rate: 0.10205639024483008\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 234\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_3 = lambda trial: objective_lgbm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_lgbm.optimize(func_lgbm_3, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e514e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   37.000000   46.000000   50.000000   47.000000\n",
      "1                    TN  190.000000  188.000000  189.000000  192.000000\n",
      "2                    FP    9.000000   12.000000   12.000000    7.000000\n",
      "3                    FN   32.000000   22.000000   17.000000   22.000000\n",
      "4              Accuracy    0.847015    0.873134    0.891791    0.891791\n",
      "5             Precision    0.804348    0.793103    0.806452    0.870370\n",
      "6           Sensitivity    0.536232    0.676471    0.746269    0.681159\n",
      "7           Specificity    0.954800    0.940000    0.940300    0.964800\n",
      "8              F1 score    0.643478    0.730159    0.775194    0.764228\n",
      "9   F1 score (weighted)    0.835895    0.869647    0.890359    0.887158\n",
      "10     F1 score (macro)    0.773046    0.823616    0.851970    0.847005\n",
      "11    Balanced Accuracy    0.745503    0.808235    0.843284    0.822992\n",
      "12                  MCC    0.569354    0.651416    0.705000    0.704156\n",
      "13                  NPV    0.855900    0.895200    0.917500    0.897200\n",
      "14              ROC_AUC    0.745503    0.808235    0.843284    0.822992\n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_3 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_lgbm_3.fit(X_trainSet3,\n",
    "                Y_trainSet3,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_3 = optimized_lgbm_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_lgbm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_lgbm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_lgbm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_lgbm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_lgbm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_lgbm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_lgbm_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set3'] = Set3\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6528c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:38:52,216] Trial 200 finished with value: 0.8770362458340525 and parameters: {'n_estimators': 570, 'learning_rate': 0.0958268101952349, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 557}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:53,254] Trial 201 finished with value: 0.8835275630440302 and parameters: {'n_estimators': 562, 'learning_rate': 0.09569352843265791, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 593}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:54,310] Trial 202 finished with value: 0.8800669006831013 and parameters: {'n_estimators': 553, 'learning_rate': 0.10033366736065366, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 587}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:55,387] Trial 203 finished with value: 0.8748530519892412 and parameters: {'n_estimators': 540, 'learning_rate': 0.09977460404747772, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 593}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:56,429] Trial 204 finished with value: 0.8708526758767915 and parameters: {'n_estimators': 555, 'learning_rate': 0.1039456102917425, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 599}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:57,516] Trial 205 finished with value: 0.8763032117938199 and parameters: {'n_estimators': 582, 'learning_rate': 0.09274374165381633, 'max_depth': 12, 'max_bin': 152, 'num_leaves': 623}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:58,605] Trial 206 finished with value: 0.8679237727657327 and parameters: {'n_estimators': 527, 'learning_rate': 0.0984315297384901, 'max_depth': 12, 'max_bin': 290, 'num_leaves': 580}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:38:59,709] Trial 207 finished with value: 0.8762814719393601 and parameters: {'n_estimators': 489, 'learning_rate': 0.08728445540975853, 'max_depth': 12, 'max_bin': 284, 'num_leaves': 451}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:00,614] Trial 208 finished with value: 0.8765824957159495 and parameters: {'n_estimators': 600, 'learning_rate': 0.10360824923878177, 'max_depth': 12, 'max_bin': 179, 'num_leaves': 158}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:01,616] Trial 209 finished with value: 0.8666485404549741 and parameters: {'n_estimators': 563, 'learning_rate': 0.10749521019719907, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 567}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:02,406] Trial 210 finished with value: 0.8715306415940608 and parameters: {'n_estimators': 514, 'learning_rate': 0.09607783488108403, 'max_depth': 7, 'max_bin': 166, 'num_leaves': 504}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:03,533] Trial 211 finished with value: 0.878062580999123 and parameters: {'n_estimators': 539, 'learning_rate': 0.0905258095791694, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 579}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:04,609] Trial 212 finished with value: 0.8731056916309026 and parameters: {'n_estimators': 537, 'learning_rate': 0.09119346497270604, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 607}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:05,709] Trial 213 finished with value: 0.8727500044053528 and parameters: {'n_estimators': 557, 'learning_rate': 0.08488766506973981, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 548}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:06,811] Trial 214 finished with value: 0.8746953640473748 and parameters: {'n_estimators': 581, 'learning_rate': 0.09457994630497875, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 573}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:07,840] Trial 215 finished with value: 0.8766481138037902 and parameters: {'n_estimators': 506, 'learning_rate': 0.10127224917149452, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 412}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:08,825] Trial 216 finished with value: 0.8771309472508066 and parameters: {'n_estimators': 539, 'learning_rate': 0.1185760829398678, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 592}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:09,994] Trial 217 finished with value: 0.8759663351746978 and parameters: {'n_estimators': 600, 'learning_rate': 0.08907804420136221, 'max_depth': 12, 'max_bin': 176, 'num_leaves': 636}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:11,000] Trial 218 finished with value: 0.8725622942982822 and parameters: {'n_estimators': 569, 'learning_rate': 0.08122661944675279, 'max_depth': 11, 'max_bin': 152, 'num_leaves': 108}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:11,841] Trial 219 finished with value: 0.8759641586976213 and parameters: {'n_estimators': 552, 'learning_rate': 0.09913689584634648, 'max_depth': 8, 'max_bin': 165, 'num_leaves': 427}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:12,800] Trial 220 finished with value: 0.8722639122726272 and parameters: {'n_estimators': 531, 'learning_rate': 0.11260722828499217, 'max_depth': 12, 'max_bin': 172, 'num_leaves': 232}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:13,730] Trial 221 finished with value: 0.8762616362469144 and parameters: {'n_estimators': 539, 'learning_rate': 0.12380580795208344, 'max_depth': 12, 'max_bin': 158, 'num_leaves': 593}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:14,670] Trial 222 finished with value: 0.8806824146442693 and parameters: {'n_estimators': 519, 'learning_rate': 0.11904186500352633, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 610}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:15,720] Trial 223 finished with value: 0.8687344159190122 and parameters: {'n_estimators': 476, 'learning_rate': 0.11781722328744383, 'max_depth': 12, 'max_bin': 237, 'num_leaves': 625}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:16,772] Trial 224 finished with value: 0.866078238961056 and parameters: {'n_estimators': 513, 'learning_rate': 0.10666559286228443, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 610}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:17,426] Trial 225 finished with value: 0.872696201724193 and parameters: {'n_estimators': 585, 'learning_rate': 0.12936773621219205, 'max_depth': 6, 'max_bin': 168, 'num_leaves': 371}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:18,361] Trial 226 finished with value: 0.870028405950374 and parameters: {'n_estimators': 524, 'learning_rate': 0.11480988001130286, 'max_depth': 9, 'max_bin': 155, 'num_leaves': 575}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:19,454] Trial 227 finished with value: 0.8674544169374441 and parameters: {'n_estimators': 494, 'learning_rate': 0.09401260694659966, 'max_depth': 12, 'max_bin': 274, 'num_leaves': 438}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:20,477] Trial 228 finished with value: 0.8790180892665613 and parameters: {'n_estimators': 558, 'learning_rate': 0.12219680258510576, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 654}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:21,570] Trial 229 finished with value: 0.8693093851246614 and parameters: {'n_estimators': 566, 'learning_rate': 0.12525587031302074, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 653}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:22,579] Trial 230 finished with value: 0.8771459239614604 and parameters: {'n_estimators': 553, 'learning_rate': 0.12159351178791723, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 638}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:23,445] Trial 231 finished with value: 0.8666527306689552 and parameters: {'n_estimators': 582, 'learning_rate': 0.17345331403739905, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 613}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:25,023] Trial 232 finished with value: 0.8811931934832714 and parameters: {'n_estimators': 607, 'learning_rate': 0.04330126473113166, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 410}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:26,651] Trial 233 finished with value: 0.8692657862828547 and parameters: {'n_estimators': 251, 'learning_rate': 0.031799404027320496, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 451}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:28,307] Trial 234 finished with value: 0.8698866470301535 and parameters: {'n_estimators': 615, 'learning_rate': 0.0465189830916125, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 663}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:30,409] Trial 235 finished with value: 0.8716446128643781 and parameters: {'n_estimators': 640, 'learning_rate': 0.024375609300537473, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 690}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:31,453] Trial 236 finished with value: 0.8713393219274661 and parameters: {'n_estimators': 597, 'learning_rate': 0.10983301947147481, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 413}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:32,436] Trial 237 finished with value: 0.8767985508274204 and parameters: {'n_estimators': 606, 'learning_rate': 0.10417280705719258, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 400}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:33,437] Trial 238 finished with value: 0.873541597347154 and parameters: {'n_estimators': 188, 'learning_rate': 0.11920136037830516, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 676}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:36,612] Trial 239 finished with value: 0.8686299233492644 and parameters: {'n_estimators': 571, 'learning_rate': 0.011330829552929328, 'max_depth': 12, 'max_bin': 202, 'num_leaves': 421}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:37,638] Trial 240 finished with value: 0.8728728236899947 and parameters: {'n_estimators': 309, 'learning_rate': 0.09927595070432078, 'max_depth': 12, 'max_bin': 269, 'num_leaves': 196}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:38,919] Trial 241 finished with value: 0.8738549679280906 and parameters: {'n_estimators': 555, 'learning_rate': 0.053346528030787516, 'max_depth': 9, 'max_bin': 169, 'num_leaves': 395}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:39,886] Trial 242 finished with value: 0.8697131133675773 and parameters: {'n_estimators': 522, 'learning_rate': 0.08700822495191463, 'max_depth': 9, 'max_bin': 173, 'num_leaves': 437}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:41,074] Trial 243 finished with value: 0.8724597420403045 and parameters: {'n_estimators': 546, 'learning_rate': 0.06758758937596888, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 477}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:42,270] Trial 244 finished with value: 0.8770902175554266 and parameters: {'n_estimators': 587, 'learning_rate': 0.07653009841100303, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 599}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:43,349] Trial 245 finished with value: 0.8734748456453527 and parameters: {'n_estimators': 505, 'learning_rate': 0.09272386765186738, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 536}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:44,205] Trial 246 finished with value: 0.8745943653780263 and parameters: {'n_estimators': 623, 'learning_rate': 0.11556953269877321, 'max_depth': 9, 'max_bin': 217, 'num_leaves': 171}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:45,097] Trial 247 finished with value: 0.8779211071120482 and parameters: {'n_estimators': 561, 'learning_rate': 0.12810916059881378, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 140}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:45,973] Trial 248 finished with value: 0.8799134344284504 and parameters: {'n_estimators': 571, 'learning_rate': 0.1345745605237438, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 135}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:46,846] Trial 249 finished with value: 0.8793486250239775 and parameters: {'n_estimators': 567, 'learning_rate': 0.13506586896621472, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 141}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8871\n",
      "\tBest params:\n",
      "\t\tn_estimators: 616\n",
      "\t\tlearning_rate: 0.10205639024483008\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 234\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_4 = lambda trial: objective_lgbm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_lgbm.optimize(func_lgbm_4, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b50d2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   37.000000   46.000000   50.000000   47.000000   \n",
      "1                    TN  190.000000  188.000000  189.000000  192.000000   \n",
      "2                    FP    9.000000   12.000000   12.000000    7.000000   \n",
      "3                    FN   32.000000   22.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.847015    0.873134    0.891791    0.891791   \n",
      "5             Precision    0.804348    0.793103    0.806452    0.870370   \n",
      "6           Sensitivity    0.536232    0.676471    0.746269    0.681159   \n",
      "7           Specificity    0.954800    0.940000    0.940300    0.964800   \n",
      "8              F1 score    0.643478    0.730159    0.775194    0.764228   \n",
      "9   F1 score (weighted)    0.835895    0.869647    0.890359    0.887158   \n",
      "10     F1 score (macro)    0.773046    0.823616    0.851970    0.847005   \n",
      "11    Balanced Accuracy    0.745503    0.808235    0.843284    0.822992   \n",
      "12                  MCC    0.569354    0.651416    0.705000    0.704156   \n",
      "13                  NPV    0.855900    0.895200    0.917500    0.897200   \n",
      "14              ROC_AUC    0.745503    0.808235    0.843284    0.822992   \n",
      "\n",
      "          Set4  \n",
      "0    43.000000  \n",
      "1   190.000000  \n",
      "2    10.000000  \n",
      "3    25.000000  \n",
      "4     0.869403  \n",
      "5     0.811321  \n",
      "6     0.632353  \n",
      "7     0.950000  \n",
      "8     0.710744  \n",
      "9     0.863668  \n",
      "10    0.813203  \n",
      "11    0.791176  \n",
      "12    0.636207  \n",
      "13    0.883700  \n",
      "14    0.791176  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_4 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_lgbm_4.fit(X_trainSet4,\n",
    "                Y_trainSet4,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_4 = optimized_lgbm_4.predict(X_testSet4)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_lgbm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_lgbm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_lgbm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_lgbm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_lgbm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_lgbm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_lgbm_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set4'] = Set4\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c56fd97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:39:47,764] Trial 250 finished with value: 0.8619068090748193 and parameters: {'n_estimators': 564, 'learning_rate': 0.14010168260872535, 'max_depth': 12, 'max_bin': 257, 'num_leaves': 123}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:48,600] Trial 251 finished with value: 0.8584812580235324 and parameters: {'n_estimators': 546, 'learning_rate': 0.13339942331185675, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 122}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:49,421] Trial 252 finished with value: 0.8617634238436895 and parameters: {'n_estimators': 574, 'learning_rate': 0.13791432008692245, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 142}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:50,278] Trial 253 finished with value: 0.8735765696317547 and parameters: {'n_estimators': 542, 'learning_rate': 0.1327407416817025, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 137}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:51,125] Trial 254 finished with value: 0.8692770699307731 and parameters: {'n_estimators': 562, 'learning_rate': 0.12928622959113206, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 101}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:51,955] Trial 255 finished with value: 0.8698528682883435 and parameters: {'n_estimators': 532, 'learning_rate': 0.12860959455231338, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 150}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:52,796] Trial 256 finished with value: 0.868895879338629 and parameters: {'n_estimators': 577, 'learning_rate': 0.1260810100165342, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 154}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:53,590] Trial 257 finished with value: 0.8625985236049939 and parameters: {'n_estimators': 555, 'learning_rate': 0.13604564872013106, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 136}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:54,291] Trial 258 finished with value: 0.8579698831464032 and parameters: {'n_estimators': 536, 'learning_rate': 0.14192762626139652, 'max_depth': 8, 'max_bin': 153, 'num_leaves': 116}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:55,222] Trial 259 finished with value: 0.8653959631800536 and parameters: {'n_estimators': 571, 'learning_rate': 0.12407467783429006, 'max_depth': 12, 'max_bin': 152, 'num_leaves': 459}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:55,782] Trial 260 finished with value: 0.844411671014224 and parameters: {'n_estimators': 223, 'learning_rate': 0.13585482036628496, 'max_depth': 3, 'max_bin': 162, 'num_leaves': 95}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:57,112] Trial 261 finished with value: 0.8572945867951104 and parameters: {'n_estimators': 517, 'learning_rate': 0.05806933572954667, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 431}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:57,994] Trial 262 finished with value: 0.8672635024311198 and parameters: {'n_estimators': 586, 'learning_rate': 0.12777829668501822, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 409}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:58,829] Trial 263 finished with value: 0.8621723654699329 and parameters: {'n_estimators': 550, 'learning_rate': 0.12100846219293127, 'max_depth': 12, 'max_bin': 167, 'num_leaves': 130}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:39:59,733] Trial 264 finished with value: 0.8483702016711323 and parameters: {'n_estimators': 606, 'learning_rate': 0.05357445025866253, 'max_depth': 4, 'max_bin': 286, 'num_leaves': 82}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:00,511] Trial 265 finished with value: 0.8569305754946882 and parameters: {'n_estimators': 878, 'learning_rate': 0.0971819308318132, 'max_depth': 7, 'max_bin': 159, 'num_leaves': 160}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:01,632] Trial 266 finished with value: 0.8622717099605367 and parameters: {'n_estimators': 526, 'learning_rate': 0.07807354152309416, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 633}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:02,393] Trial 267 finished with value: 0.8475905049939136 and parameters: {'n_estimators': 841, 'learning_rate': 0.13074780758964857, 'max_depth': 8, 'max_bin': 163, 'num_leaves': 615}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:03,324] Trial 268 finished with value: 0.8652018565231898 and parameters: {'n_estimators': 567, 'learning_rate': 0.10255874553700639, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 379}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:04,329] Trial 269 finished with value: 0.8641404604053319 and parameters: {'n_estimators': 284, 'learning_rate': 0.08965853110805022, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 443}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:05,280] Trial 270 finished with value: 0.8711442015527092 and parameters: {'n_estimators': 548, 'learning_rate': 0.12267636652889065, 'max_depth': 12, 'max_bin': 244, 'num_leaves': 578}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:06,966] Trial 271 finished with value: 0.8642948297714336 and parameters: {'n_estimators': 589, 'learning_rate': 0.03916585601991186, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 424}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:07,928] Trial 272 finished with value: 0.8607390290255076 and parameters: {'n_estimators': 395, 'learning_rate': 0.07257599114611554, 'max_depth': 8, 'max_bin': 159, 'num_leaves': 143}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:08,954] Trial 273 finished with value: 0.8667250326846794 and parameters: {'n_estimators': 561, 'learning_rate': 0.09509557646716506, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 471}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:10,099] Trial 274 finished with value: 0.864003621486822 and parameters: {'n_estimators': 535, 'learning_rate': 0.08368648436781169, 'max_depth': 12, 'max_bin': 262, 'num_leaves': 588}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:10,978] Trial 275 finished with value: 0.8652512706102549 and parameters: {'n_estimators': 608, 'learning_rate': 0.14488218038400424, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 453}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:11,915] Trial 276 finished with value: 0.8599445564955486 and parameters: {'n_estimators': 578, 'learning_rate': 0.10689609650103413, 'max_depth': 11, 'max_bin': 282, 'num_leaves': 558}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:13,090] Trial 277 finished with value: 0.8581848309195292 and parameters: {'n_estimators': 500, 'learning_rate': 0.06417710444798246, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 393}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:13,921] Trial 278 finished with value: 0.8653091896222941 and parameters: {'n_estimators': 416, 'learning_rate': 0.10147515780672209, 'max_depth': 8, 'max_bin': 152, 'num_leaves': 602}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:14,838] Trial 279 finished with value: 0.870281704462813 and parameters: {'n_estimators': 516, 'learning_rate': 0.11752630286129946, 'max_depth': 12, 'max_bin': 170, 'num_leaves': 419}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:15,941] Trial 280 finished with value: 0.856521947042822 and parameters: {'n_estimators': 789, 'learning_rate': 0.06182640658662798, 'max_depth': 9, 'max_bin': 158, 'num_leaves': 333}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:16,924] Trial 281 finished with value: 0.8616886196087377 and parameters: {'n_estimators': 625, 'learning_rate': 0.09112406810240081, 'max_depth': 12, 'max_bin': 191, 'num_leaves': 405}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:17,973] Trial 282 finished with value: 0.8555211232633104 and parameters: {'n_estimators': 655, 'learning_rate': 0.0697398443239697, 'max_depth': 12, 'max_bin': 177, 'num_leaves': 111}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:18,920] Trial 283 finished with value: 0.8645716371430024 and parameters: {'n_estimators': 557, 'learning_rate': 0.09941532070294308, 'max_depth': 12, 'max_bin': 167, 'num_leaves': 440}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:19,815] Trial 284 finished with value: 0.8700282751743693 and parameters: {'n_estimators': 676, 'learning_rate': 0.12668880449020267, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 663}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:20,816] Trial 285 finished with value: 0.8600882593775688 and parameters: {'n_estimators': 594, 'learning_rate': 0.0799825400065339, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 126}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:21,587] Trial 286 finished with value: 0.8547432853637945 and parameters: {'n_estimators': 545, 'learning_rate': 0.11406814378999952, 'max_depth': 8, 'max_bin': 157, 'num_leaves': 647}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:22,512] Trial 287 finished with value: 0.8631329888669395 and parameters: {'n_estimators': 569, 'learning_rate': 0.09543228058502537, 'max_depth': 12, 'max_bin': 174, 'num_leaves': 154}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:23,403] Trial 288 finished with value: 0.854298896899331 and parameters: {'n_estimators': 457, 'learning_rate': 0.10609673166339126, 'max_depth': 9, 'max_bin': 165, 'num_leaves': 465}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:24,354] Trial 289 finished with value: 0.8561140569945758 and parameters: {'n_estimators': 578, 'learning_rate': 0.08734152662852973, 'max_depth': 9, 'max_bin': 171, 'num_leaves': 488}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:25,234] Trial 290 finished with value: 0.8657609820294757 and parameters: {'n_estimators': 541, 'learning_rate': 0.13306372087480428, 'max_depth': 11, 'max_bin': 252, 'num_leaves': 615}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:26,758] Trial 291 finished with value: 0.8704352706508807 and parameters: {'n_estimators': 529, 'learning_rate': 0.04868687898304365, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 569}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:27,925] Trial 292 finished with value: 0.8648445768031632 and parameters: {'n_estimators': 633, 'learning_rate': 0.0652901531051002, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 276}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:28,637] Trial 293 finished with value: 0.8667553650026673 and parameters: {'n_estimators': 605, 'learning_rate': 0.1212700960238453, 'max_depth': 8, 'max_bin': 161, 'num_leaves': 171}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:30,249] Trial 294 finished with value: 0.8586621825746859 and parameters: {'n_estimators': 487, 'learning_rate': 0.040533847858375806, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 429}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:31,152] Trial 295 finished with value: 0.8654851795646128 and parameters: {'n_estimators': 558, 'learning_rate': 0.11063249091060424, 'max_depth': 12, 'max_bin': 185, 'num_leaves': 364}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:31,739] Trial 296 finished with value: 0.844727607850668 and parameters: {'n_estimators': 54, 'learning_rate': 0.10316005589258932, 'max_depth': 11, 'max_bin': 272, 'num_leaves': 417}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:32,695] Trial 297 finished with value: 0.8598080465672087 and parameters: {'n_estimators': 587, 'learning_rate': 0.09113846108527823, 'max_depth': 9, 'max_bin': 276, 'num_leaves': 632}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:33,648] Trial 298 finished with value: 0.8669214931120293 and parameters: {'n_estimators': 816, 'learning_rate': 0.0976557158144779, 'max_depth': 10, 'max_bin': 181, 'num_leaves': 451}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:34,775] Trial 299 finished with value: 0.8625448289651556 and parameters: {'n_estimators': 530, 'learning_rate': 0.08392516402108133, 'max_depth': 12, 'max_bin': 196, 'num_leaves': 586}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8871\n",
      "\tBest params:\n",
      "\t\tn_estimators: 616\n",
      "\t\tlearning_rate: 0.10205639024483008\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 234\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_5 = lambda trial: objective_lgbm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_lgbm.optimize(func_lgbm_5, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef058434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   37.000000   46.000000   50.000000   47.000000   \n",
      "1                    TN  190.000000  188.000000  189.000000  192.000000   \n",
      "2                    FP    9.000000   12.000000   12.000000    7.000000   \n",
      "3                    FN   32.000000   22.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.847015    0.873134    0.891791    0.891791   \n",
      "5             Precision    0.804348    0.793103    0.806452    0.870370   \n",
      "6           Sensitivity    0.536232    0.676471    0.746269    0.681159   \n",
      "7           Specificity    0.954800    0.940000    0.940300    0.964800   \n",
      "8              F1 score    0.643478    0.730159    0.775194    0.764228   \n",
      "9   F1 score (weighted)    0.835895    0.869647    0.890359    0.887158   \n",
      "10     F1 score (macro)    0.773046    0.823616    0.851970    0.847005   \n",
      "11    Balanced Accuracy    0.745503    0.808235    0.843284    0.822992   \n",
      "12                  MCC    0.569354    0.651416    0.705000    0.704156   \n",
      "13                  NPV    0.855900    0.895200    0.917500    0.897200   \n",
      "14              ROC_AUC    0.745503    0.808235    0.843284    0.822992   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    43.000000   51.000000  \n",
      "1   190.000000  192.000000  \n",
      "2    10.000000    8.000000  \n",
      "3    25.000000   17.000000  \n",
      "4     0.869403    0.906716  \n",
      "5     0.811321    0.864407  \n",
      "6     0.632353    0.750000  \n",
      "7     0.950000    0.960000  \n",
      "8     0.710744    0.803150  \n",
      "9     0.863668    0.904437  \n",
      "10    0.813203    0.871012  \n",
      "11    0.791176    0.855000  \n",
      "12    0.636207    0.745639  \n",
      "13    0.883700    0.918700  \n",
      "14    0.791176    0.855000  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_5 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_lgbm_5.fit(X_trainSet5,\n",
    "                Y_trainSet5,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_5 = optimized_lgbm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_lgbm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_lgbm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_lgbm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_lgbm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_lgbm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_lgbm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_lgbm_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set5'] = Set5\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "deb65060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:40:35,690] Trial 300 finished with value: 0.8646883795403782 and parameters: {'n_estimators': 510, 'learning_rate': 0.09370375531616829, 'max_depth': 7, 'max_bin': 164, 'num_leaves': 383}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:37,043] Trial 301 finished with value: 0.8683706355146456 and parameters: {'n_estimators': 568, 'learning_rate': 0.053775880738485035, 'max_depth': 12, 'max_bin': 168, 'num_leaves': 144}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:37,983] Trial 302 finished with value: 0.8625658305218747 and parameters: {'n_estimators': 551, 'learning_rate': 0.10847419393032855, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 437}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:38,822] Trial 303 finished with value: 0.8631403369660975 and parameters: {'n_estimators': 613, 'learning_rate': 0.13885314548210093, 'max_depth': 11, 'max_bin': 153, 'num_leaves': 350}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:39,551] Trial 304 finished with value: 0.8716886606866898 and parameters: {'n_estimators': 521, 'learning_rate': 0.11471683341974619, 'max_depth': 6, 'max_bin': 230, 'num_leaves': 402}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:41,978] Trial 305 finished with value: 0.8575640699026401 and parameters: {'n_estimators': 588, 'learning_rate': 0.01595469303976864, 'max_depth': 9, 'max_bin': 288, 'num_leaves': 475}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:43,768] Trial 306 finished with value: 0.8597313556119712 and parameters: {'n_estimators': 571, 'learning_rate': 0.028743241964530365, 'max_depth': 12, 'max_bin': 234, 'num_leaves': 503}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:44,875] Trial 307 finished with value: 0.8656784755815501 and parameters: {'n_estimators': 900, 'learning_rate': 0.07429396974176589, 'max_depth': 8, 'max_bin': 162, 'num_leaves': 601}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:45,718] Trial 308 finished with value: 0.8635579084203305 and parameters: {'n_estimators': 545, 'learning_rate': 0.12528697808255845, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 132}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:46,596] Trial 309 finished with value: 0.855909420146039 and parameters: {'n_estimators': 137, 'learning_rate': 0.10155175936210312, 'max_depth': 12, 'max_bin': 178, 'num_leaves': 180}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:47,502] Trial 310 finished with value: 0.8569877716219171 and parameters: {'n_estimators': 597, 'learning_rate': 0.11810335800463347, 'max_depth': 10, 'max_bin': 171, 'num_leaves': 456}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:48,541] Trial 311 finished with value: 0.8537597122177466 and parameters: {'n_estimators': 498, 'learning_rate': 0.08867846568774514, 'max_depth': 11, 'max_bin': 166, 'num_leaves': 622}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:49,575] Trial 312 finished with value: 0.853475723928781 and parameters: {'n_estimators': 557, 'learning_rate': 0.09745310576233067, 'max_depth': 12, 'max_bin': 279, 'num_leaves': 567}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:50,456] Trial 313 finished with value: 0.8650653653904467 and parameters: {'n_estimators': 534, 'learning_rate': 0.130993032814996, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 415}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:51,426] Trial 314 finished with value: 0.8736345297024292 and parameters: {'n_estimators': 189, 'learning_rate': 0.06160988835857496, 'max_depth': 8, 'max_bin': 155, 'num_leaves': 167}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:52,127] Trial 315 finished with value: 0.8517529141086755 and parameters: {'n_estimators': 331, 'learning_rate': 0.19915177413454727, 'max_depth': 9, 'max_bin': 163, 'num_leaves': 700}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:53,064] Trial 316 finished with value: 0.8525776728599428 and parameters: {'n_estimators': 569, 'learning_rate': 0.0802520552439421, 'max_depth': 12, 'max_bin': 175, 'num_leaves': 119}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:53,961] Trial 317 finished with value: 0.8698797348788595 and parameters: {'n_estimators': 623, 'learning_rate': 0.1452283074226562, 'max_depth': 12, 'max_bin': 158, 'num_leaves': 433}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:54,974] Trial 318 finished with value: 0.8580227857274239 and parameters: {'n_estimators': 264, 'learning_rate': 0.10294385580791864, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 585}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:55,781] Trial 319 finished with value: 0.8640697796121337 and parameters: {'n_estimators': 645, 'learning_rate': 0.13539294686236902, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 58}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:57,025] Trial 320 finished with value: 0.8737778197451481 and parameters: {'n_estimators': 746, 'learning_rate': 0.0684242995037049, 'max_depth': 12, 'max_bin': 199, 'num_leaves': 389}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:58,499] Trial 321 finished with value: 0.8704415287413619 and parameters: {'n_estimators': 586, 'learning_rate': 0.04107617461858521, 'max_depth': 9, 'max_bin': 161, 'num_leaves': 552}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:40:59,571] Trial 322 finished with value: 0.8605133721049267 and parameters: {'n_estimators': 513, 'learning_rate': 0.09362696154030589, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 644}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:01,054] Trial 323 finished with value: 0.8651811293163669 and parameters: {'n_estimators': 542, 'learning_rate': 0.03540073888916804, 'max_depth': 9, 'max_bin': 283, 'num_leaves': 449}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:01,829] Trial 324 finished with value: 0.8587045173227958 and parameters: {'n_estimators': 560, 'learning_rate': 0.12113980772721163, 'max_depth': 8, 'max_bin': 172, 'num_leaves': 608}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:02,729] Trial 325 finished with value: 0.8551117528835324 and parameters: {'n_estimators': 603, 'learning_rate': 0.10722382408209527, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 153}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:03,625] Trial 326 finished with value: 0.869120696858732 and parameters: {'n_estimators': 523, 'learning_rate': 0.08511089397475273, 'max_depth': 7, 'max_bin': 157, 'num_leaves': 404}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:05,073] Trial 327 finished with value: 0.8687433892915699 and parameters: {'n_estimators': 577, 'learning_rate': 0.04523669347639541, 'max_depth': 10, 'max_bin': 153, 'num_leaves': 426}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:06,154] Trial 328 finished with value: 0.8564197577491248 and parameters: {'n_estimators': 367, 'learning_rate': 0.0739967128645825, 'max_depth': 12, 'max_bin': 294, 'num_leaves': 463}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:07,488] Trial 329 finished with value: 0.8653402267045867 and parameters: {'n_estimators': 469, 'learning_rate': 0.0558712912787922, 'max_depth': 11, 'max_bin': 169, 'num_leaves': 491}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:08,429] Trial 330 finished with value: 0.8621762390661356 and parameters: {'n_estimators': 540, 'learning_rate': 0.11161394175715109, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 306}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:09,460] Trial 331 finished with value: 0.8535148143732254 and parameters: {'n_estimators': 557, 'learning_rate': 0.09844148108353562, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 600}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:10,457] Trial 332 finished with value: 0.8643747753136438 and parameters: {'n_estimators': 580, 'learning_rate': 0.09052349400306263, 'max_depth': 9, 'max_bin': 176, 'num_leaves': 137}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:11,527] Trial 333 finished with value: 0.8652456218262088 and parameters: {'n_estimators': 612, 'learning_rate': 0.07815219114717782, 'max_depth': 11, 'max_bin': 155, 'num_leaves': 446}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:12,527] Trial 334 finished with value: 0.8555643005387896 and parameters: {'n_estimators': 484, 'learning_rate': 0.10484826186796838, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 525}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:14,547] Trial 335 finished with value: 0.8650127282385682 and parameters: {'n_estimators': 529, 'learning_rate': 0.023568566763747, 'max_depth': 12, 'max_bin': 152, 'num_leaves': 103}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:15,717] Trial 336 finished with value: 0.8706145571571687 and parameters: {'n_estimators': 595, 'learning_rate': 0.057861196653671954, 'max_depth': 8, 'max_bin': 166, 'num_leaves': 414}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:17,201] Trial 337 finished with value: 0.8636193028692956 and parameters: {'n_estimators': 717, 'learning_rate': 0.05092063370033814, 'max_depth': 12, 'max_bin': 210, 'num_leaves': 667}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:18,138] Trial 338 finished with value: 0.8578147214591911 and parameters: {'n_estimators': 552, 'learning_rate': 0.12731189739193122, 'max_depth': 12, 'max_bin': 158, 'num_leaves': 575}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:19,097] Trial 339 finished with value: 0.8545192965078456 and parameters: {'n_estimators': 572, 'learning_rate': 0.09519681644304452, 'max_depth': 10, 'max_bin': 267, 'num_leaves': 437}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:19,917] Trial 340 finished with value: 0.865417855392329 and parameters: {'n_estimators': 513, 'learning_rate': 0.11635955901835666, 'max_depth': 9, 'max_bin': 173, 'num_leaves': 158}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:20,947] Trial 341 finished with value: 0.8672556148287527 and parameters: {'n_estimators': 628, 'learning_rate': 0.10038265492729195, 'max_depth': 12, 'max_bin': 180, 'num_leaves': 623}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:21,913] Trial 342 finished with value: 0.8580613497656682 and parameters: {'n_estimators': 546, 'learning_rate': 0.08733421754435289, 'max_depth': 11, 'max_bin': 248, 'num_leaves': 377}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:22,984] Trial 343 finished with value: 0.8631212503251365 and parameters: {'n_estimators': 237, 'learning_rate': 0.08237186972536142, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 653}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:23,742] Trial 344 finished with value: 0.8623488702980906 and parameters: {'n_estimators': 501, 'learning_rate': 0.12290445230943546, 'max_depth': 5, 'max_bin': 163, 'num_leaves': 468}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:24,639] Trial 345 finished with value: 0.8591217172190193 and parameters: {'n_estimators': 588, 'learning_rate': 0.11163089232303694, 'max_depth': 9, 'max_bin': 188, 'num_leaves': 541}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:25,717] Trial 346 finished with value: 0.8630920553644483 and parameters: {'n_estimators': 297, 'learning_rate': 0.048087008787472595, 'max_depth': 7, 'max_bin': 157, 'num_leaves': 479}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:26,565] Trial 347 finished with value: 0.8672724739733738 and parameters: {'n_estimators': 564, 'learning_rate': 0.10462337673519304, 'max_depth': 8, 'max_bin': 168, 'num_leaves': 511}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:27,592] Trial 348 finished with value: 0.8601980014941863 and parameters: {'n_estimators': 533, 'learning_rate': 0.09591155736079364, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 262}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:28,821] Trial 349 finished with value: 0.8750668074353362 and parameters: {'n_estimators': 604, 'learning_rate': 0.061183277391672126, 'max_depth': 11, 'max_bin': 161, 'num_leaves': 189}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.887101\n",
      "\tBest params:\n",
      "\t\tn_estimators: 616\n",
      "\t\tlearning_rate: 0.10205639024483008\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 234\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_6 = lambda trial: objective_lgbm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_lgbm.optimize(func_lgbm_6, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.6f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d232cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   37.000000   46.000000   50.000000   47.000000   \n",
      "1                    TN  190.000000  188.000000  189.000000  192.000000   \n",
      "2                    FP    9.000000   12.000000   12.000000    7.000000   \n",
      "3                    FN   32.000000   22.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.847015    0.873134    0.891791    0.891791   \n",
      "5             Precision    0.804348    0.793103    0.806452    0.870370   \n",
      "6           Sensitivity    0.536232    0.676471    0.746269    0.681159   \n",
      "7           Specificity    0.954800    0.940000    0.940300    0.964800   \n",
      "8              F1 score    0.643478    0.730159    0.775194    0.764228   \n",
      "9   F1 score (weighted)    0.835895    0.869647    0.890359    0.887158   \n",
      "10     F1 score (macro)    0.773046    0.823616    0.851970    0.847005   \n",
      "11    Balanced Accuracy    0.745503    0.808235    0.843284    0.822992   \n",
      "12                  MCC    0.569354    0.651416    0.705000    0.704156   \n",
      "13                  NPV    0.855900    0.895200    0.917500    0.897200   \n",
      "14              ROC_AUC    0.745503    0.808235    0.843284    0.822992   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    43.000000   51.000000   51.000000  \n",
      "1   190.000000  192.000000  192.000000  \n",
      "2    10.000000    8.000000    8.000000  \n",
      "3    25.000000   17.000000   17.000000  \n",
      "4     0.869403    0.906716    0.906716  \n",
      "5     0.811321    0.864407    0.864407  \n",
      "6     0.632353    0.750000    0.750000  \n",
      "7     0.950000    0.960000    0.960000  \n",
      "8     0.710744    0.803150    0.803150  \n",
      "9     0.863668    0.904437    0.904437  \n",
      "10    0.813203    0.871012    0.871012  \n",
      "11    0.791176    0.855000    0.855000  \n",
      "12    0.636207    0.745639    0.745639  \n",
      "13    0.883700    0.918700    0.918700  \n",
      "14    0.791176    0.855000    0.855000  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_6 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_lgbm_6.fit(X_trainSet6,\n",
    "                Y_trainSet6,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_6 = optimized_lgbm_6.predict(X_testSet6)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_lgbm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_lgbm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_lgbm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_lgbm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_lgbm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_lgbm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_lgbm_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[ np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set6'] = Set6\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a5d4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:41:29,793] Trial 350 finished with value: 0.8612800552101886 and parameters: {'n_estimators': 566, 'learning_rate': 0.13002858228637307, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 392}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:30,695] Trial 351 finished with value: 0.8520358179895744 and parameters: {'n_estimators': 522, 'learning_rate': 0.13802751946230438, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 592}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:32,206] Trial 352 finished with value: 0.8468733485764772 and parameters: {'n_estimators': 581, 'learning_rate': 0.03158194707263769, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 122}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:34,147] Trial 353 finished with value: 0.8403825803368701 and parameters: {'n_estimators': 546, 'learning_rate': 0.016888842291581812, 'max_depth': 9, 'max_bin': 239, 'num_leaves': 141}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't set num_leaves and 2^max_depth > num_leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:41:34,810] Trial 354 finished with value: 0.8325464760096853 and parameters: {'n_estimators': 640, 'learning_rate': 0.14854027904694425, 'max_depth': 8, 'max_bin': 285, 'num_leaves': 31}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:35,803] Trial 355 finished with value: 0.8520728469929276 and parameters: {'n_estimators': 436, 'learning_rate': 0.09190255490886208, 'max_depth': 12, 'max_bin': 184, 'num_leaves': 560}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:39,978] Trial 356 finished with value: 0.8462652080151598 and parameters: {'n_estimators': 615, 'learning_rate': 0.005797313022820064, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 432}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:40,845] Trial 357 finished with value: 0.850709242544592 and parameters: {'n_estimators': 665, 'learning_rate': 0.10054245676681425, 'max_depth': 9, 'max_bin': 163, 'num_leaves': 418}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:41,719] Trial 358 finished with value: 0.8439136059787888 and parameters: {'n_estimators': 555, 'learning_rate': 0.11879510265426604, 'max_depth': 12, 'max_bin': 206, 'num_leaves': 446}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:42,624] Trial 359 finished with value: 0.8430564284837307 and parameters: {'n_estimators': 591, 'learning_rate': 0.10765625988881616, 'max_depth': 11, 'max_bin': 156, 'num_leaves': 633}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:43,688] Trial 360 finished with value: 0.8462065601852615 and parameters: {'n_estimators': 501, 'learning_rate': 0.06575302800883273, 'max_depth': 12, 'max_bin': 281, 'num_leaves': 210}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:44,322] Trial 361 finished with value: 0.8349310935981545 and parameters: {'n_estimators': 533, 'learning_rate': 0.1622682523208822, 'max_depth': 8, 'max_bin': 177, 'num_leaves': 614}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:45,356] Trial 362 finished with value: 0.8593199284202303 and parameters: {'n_estimators': 573, 'learning_rate': 0.07568005634495024, 'max_depth': 12, 'max_bin': 169, 'num_leaves': 407}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:46,417] Trial 363 finished with value: 0.8492659819401853 and parameters: {'n_estimators': 556, 'learning_rate': 0.0696333234649255, 'max_depth': 9, 'max_bin': 159, 'num_leaves': 460}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:47,387] Trial 364 finished with value: 0.8522358000537975 and parameters: {'n_estimators': 518, 'learning_rate': 0.08605724093034832, 'max_depth': 10, 'max_bin': 174, 'num_leaves': 581}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:48,194] Trial 365 finished with value: 0.849300698436575 and parameters: {'n_estimators': 160, 'learning_rate': 0.12433073571087916, 'max_depth': 12, 'max_bin': 166, 'num_leaves': 245}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:49,531] Trial 366 finished with value: 0.8480419844739545 and parameters: {'n_estimators': 766, 'learning_rate': 0.04247076774620746, 'max_depth': 12, 'max_bin': 277, 'num_leaves': 143}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:50,544] Trial 367 finished with value: 0.8509257056677153 and parameters: {'n_estimators': 601, 'learning_rate': 0.09855212918958595, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 743}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:51,975] Trial 368 finished with value: 0.8507468468709689 and parameters: {'n_estimators': 570, 'learning_rate': 0.03659489051934924, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 399}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:52,714] Trial 369 finished with value: 0.8434908301870558 and parameters: {'n_estimators': 541, 'learning_rate': 0.11402958266753757, 'max_depth': 8, 'max_bin': 273, 'num_leaves': 111}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:53,645] Trial 370 finished with value: 0.8523490351292609 and parameters: {'n_estimators': 483, 'learning_rate': 0.08929942150715983, 'max_depth': 12, 'max_bin': 255, 'num_leaves': 163}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:54,564] Trial 371 finished with value: 0.8493861393081753 and parameters: {'n_estimators': 583, 'learning_rate': 0.09346583046072217, 'max_depth': 12, 'max_bin': 155, 'num_leaves': 422}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:55,464] Trial 372 finished with value: 0.8400655307388634 and parameters: {'n_estimators': 617, 'learning_rate': 0.10917721994211069, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 494}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:56,309] Trial 373 finished with value: 0.8515271450679609 and parameters: {'n_estimators': 555, 'learning_rate': 0.13383706558022856, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 444}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:57,207] Trial 374 finished with value: 0.8555674987091184 and parameters: {'n_estimators': 527, 'learning_rate': 0.10337471119596782, 'max_depth': 10, 'max_bin': 259, 'num_leaves': 604}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:58,127] Trial 375 finished with value: 0.8477093094885648 and parameters: {'n_estimators': 577, 'learning_rate': 0.0777259860741949, 'max_depth': 9, 'max_bin': 163, 'num_leaves': 479}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:41:59,247] Trial 376 finished with value: 0.849550231623898 and parameters: {'n_estimators': 543, 'learning_rate': 0.08225128831829427, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 683}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:00,437] Trial 377 finished with value: 0.8540524379115355 and parameters: {'n_estimators': 595, 'learning_rate': 0.05722963025135375, 'max_depth': 12, 'max_bin': 166, 'num_leaves': 567}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:01,357] Trial 378 finished with value: 0.850682403544982 and parameters: {'n_estimators': 514, 'learning_rate': 0.09680608745119734, 'max_depth': 11, 'max_bin': 160, 'num_leaves': 425}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:02,298] Trial 379 finished with value: 0.8446460708336094 and parameters: {'n_estimators': 563, 'learning_rate': 0.12025009175327123, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 596}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:03,455] Trial 380 finished with value: 0.8529564660028163 and parameters: {'n_estimators': 632, 'learning_rate': 0.04495338110103168, 'max_depth': 8, 'max_bin': 180, 'num_leaves': 90}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:04,313] Trial 381 finished with value: 0.8451738742816086 and parameters: {'n_estimators': 538, 'learning_rate': 0.1280849064578168, 'max_depth': 9, 'max_bin': 245, 'num_leaves': 379}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:05,126] Trial 382 finished with value: 0.8533922542354206 and parameters: {'n_estimators': 95, 'learning_rate': 0.14085515486129405, 'max_depth': 12, 'max_bin': 174, 'num_leaves': 454}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:06,174] Trial 383 finished with value: 0.84913785758086 and parameters: {'n_estimators': 596, 'learning_rate': 0.07151543109154623, 'max_depth': 12, 'max_bin': 214, 'num_leaves': 175}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:07,052] Trial 384 finished with value: 0.8402078998824919 and parameters: {'n_estimators': 205, 'learning_rate': 0.10148269372970578, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 131}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:07,965] Trial 385 finished with value: 0.8498964432195896 and parameters: {'n_estimators': 505, 'learning_rate': 0.09315707674892548, 'max_depth': 9, 'max_bin': 169, 'num_leaves': 639}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:08,917] Trial 386 finished with value: 0.8535661617915 and parameters: {'n_estimators': 574, 'learning_rate': 0.10546651798723794, 'max_depth': 12, 'max_bin': 288, 'num_leaves': 547}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:09,709] Trial 387 finished with value: 0.8375599906114701 and parameters: {'n_estimators': 557, 'learning_rate': 0.08927250627709786, 'max_depth': 8, 'max_bin': 152, 'num_leaves': 286}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:10,630] Trial 388 finished with value: 0.8531367794831481 and parameters: {'n_estimators': 876, 'learning_rate': 0.11544051335603936, 'max_depth': 11, 'max_bin': 158, 'num_leaves': 525}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:11,592] Trial 389 finished with value: 0.8475968685276094 and parameters: {'n_estimators': 694, 'learning_rate': 0.11176272190500203, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 434}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:12,303] Trial 390 finished with value: 0.8410227420114129 and parameters: {'n_estimators': 530, 'learning_rate': 0.09780829478288706, 'max_depth': 7, 'max_bin': 167, 'num_leaves': 620}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:13,220] Trial 391 finished with value: 0.8570046152104955 and parameters: {'n_estimators': 610, 'learning_rate': 0.08398755146402513, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 154}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:14,017] Trial 392 finished with value: 0.8429447279983668 and parameters: {'n_estimators': 646, 'learning_rate': 0.13124375676377456, 'max_depth': 9, 'max_bin': 227, 'num_leaves': 408}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:15,469] Trial 393 finished with value: 0.8452432123616601 and parameters: {'n_estimators': 583, 'learning_rate': 0.03688709103747568, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 462}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:16,379] Trial 394 finished with value: 0.8586888419570752 and parameters: {'n_estimators': 553, 'learning_rate': 0.12508008910474527, 'max_depth': 11, 'max_bin': 157, 'num_leaves': 320}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:17,318] Trial 395 finished with value: 0.851944621732154 and parameters: {'n_estimators': 797, 'learning_rate': 0.10782807230757575, 'max_depth': 12, 'max_bin': 192, 'num_leaves': 585}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:18,232] Trial 396 finished with value: 0.8479355849588591 and parameters: {'n_estimators': 322, 'learning_rate': 0.10121338406728045, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 367}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:19,278] Trial 397 finished with value: 0.848855968170653 and parameters: {'n_estimators': 829, 'learning_rate': 0.06554289929970508, 'max_depth': 10, 'max_bin': 172, 'num_leaves': 420}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:20,064] Trial 398 finished with value: 0.8449219526786431 and parameters: {'n_estimators': 489, 'learning_rate': 0.11751672964360385, 'max_depth': 9, 'max_bin': 160, 'num_leaves': 122}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:20,975] Trial 399 finished with value: 0.8546113308190287 and parameters: {'n_estimators': 524, 'learning_rate': 0.09435863663494001, 'max_depth': 12, 'max_bin': 167, 'num_leaves': 200}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8871010\n",
      "\tBest params:\n",
      "\t\tn_estimators: 616\n",
      "\t\tlearning_rate: 0.10205639024483008\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 234\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_7 = lambda trial: objective_lgbm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_lgbm.optimize(func_lgbm_7, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.7f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20febb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   37.000000   46.000000   50.000000   47.000000   \n",
      "1                    TN  190.000000  188.000000  189.000000  192.000000   \n",
      "2                    FP    9.000000   12.000000   12.000000    7.000000   \n",
      "3                    FN   32.000000   22.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.847015    0.873134    0.891791    0.891791   \n",
      "5             Precision    0.804348    0.793103    0.806452    0.870370   \n",
      "6           Sensitivity    0.536232    0.676471    0.746269    0.681159   \n",
      "7           Specificity    0.954800    0.940000    0.940300    0.964800   \n",
      "8              F1 score    0.643478    0.730159    0.775194    0.764228   \n",
      "9   F1 score (weighted)    0.835895    0.869647    0.890359    0.887158   \n",
      "10     F1 score (macro)    0.773046    0.823616    0.851970    0.847005   \n",
      "11    Balanced Accuracy    0.745503    0.808235    0.843284    0.822992   \n",
      "12                  MCC    0.569354    0.651416    0.705000    0.704156   \n",
      "13                  NPV    0.855900    0.895200    0.917500    0.897200   \n",
      "14              ROC_AUC    0.745503    0.808235    0.843284    0.822992   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    43.000000   51.000000   51.000000   50.000000  \n",
      "1   190.000000  192.000000  192.000000  192.000000  \n",
      "2    10.000000    8.000000    8.000000    9.000000  \n",
      "3    25.000000   17.000000   17.000000   17.000000  \n",
      "4     0.869403    0.906716    0.906716    0.902985  \n",
      "5     0.811321    0.864407    0.864407    0.847458  \n",
      "6     0.632353    0.750000    0.750000    0.746269  \n",
      "7     0.950000    0.960000    0.960000    0.955200  \n",
      "8     0.710744    0.803150    0.803150    0.793651  \n",
      "9     0.863668    0.904437    0.904437    0.900852  \n",
      "10    0.813203    0.871012    0.871012    0.865118  \n",
      "11    0.791176    0.855000    0.855000    0.850746  \n",
      "12    0.636207    0.745639    0.745639    0.733093  \n",
      "13    0.883700    0.918700    0.918700    0.918700  \n",
      "14    0.791176    0.855000    0.855000    0.850746  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_7 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_lgbm_7.fit(X_trainSet7,\n",
    "                Y_trainSet7,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_7 = optimized_lgbm_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_lgbm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_lgbm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_lgbm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_lgbm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_lgbm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_lgbm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_lgbm_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set7'] = Set7\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2858184a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:42:22,048] Trial 400 finished with value: 0.8605499238188667 and parameters: {'n_estimators': 268, 'learning_rate': 0.079465684903548, 'max_depth': 8, 'max_bin': 178, 'num_leaves': 388}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:23,703] Trial 401 finished with value: 0.8639858014856122 and parameters: {'n_estimators': 568, 'learning_rate': 0.031901623038172025, 'max_depth': 12, 'max_bin': 280, 'num_leaves': 509}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:25,888] Trial 402 finished with value: 0.8560242234809096 and parameters: {'n_estimators': 466, 'learning_rate': 0.01863945677287253, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 145}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:26,915] Trial 403 finished with value: 0.860667576048062 and parameters: {'n_estimators': 542, 'learning_rate': 0.08989647796539488, 'max_depth': 12, 'max_bin': 157, 'num_leaves': 653}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:28,139] Trial 404 finished with value: 0.8596822711780963 and parameters: {'n_estimators': 592, 'learning_rate': 0.05257511132935711, 'max_depth': 11, 'max_bin': 219, 'num_leaves': 439}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:28,843] Trial 405 finished with value: 0.8535988388720082 and parameters: {'n_estimators': 555, 'learning_rate': 0.13684563182836512, 'max_depth': 8, 'max_bin': 176, 'num_leaves': 575}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:29,400] Trial 406 finished with value: 0.8562493403253768 and parameters: {'n_estimators': 616, 'learning_rate': 0.17911133867008572, 'max_depth': 6, 'max_bin': 162, 'num_leaves': 466}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:30,291] Trial 407 finished with value: 0.8557678072642417 and parameters: {'n_estimators': 507, 'learning_rate': 0.10331751639986861, 'max_depth': 9, 'max_bin': 169, 'num_leaves': 610}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:33,096] Trial 408 finished with value: 0.8531268623771868 and parameters: {'n_estimators': 577, 'learning_rate': 0.012282877238383779, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 73}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:33,975] Trial 409 finished with value: 0.8595745086173794 and parameters: {'n_estimators': 538, 'learning_rate': 0.12138423980999948, 'max_depth': 12, 'max_bin': 152, 'num_leaves': 400}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:34,844] Trial 410 finished with value: 0.8507159024221517 and parameters: {'n_estimators': 599, 'learning_rate': 0.09858621379165773, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 483}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:35,642] Trial 411 finished with value: 0.8547030749987954 and parameters: {'n_estimators': 562, 'learning_rate': 0.1512538458899145, 'max_depth': 12, 'max_bin': 183, 'num_leaves': 181}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:36,433] Trial 412 finished with value: 0.8575632876676899 and parameters: {'n_estimators': 521, 'learning_rate': 0.10948693240563895, 'max_depth': 8, 'max_bin': 156, 'num_leaves': 443}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:38,372] Trial 413 finished with value: 0.8606617818684551 and parameters: {'n_estimators': 625, 'learning_rate': 0.021810205066541397, 'max_depth': 10, 'max_bin': 173, 'num_leaves': 594}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:39,490] Trial 414 finished with value: 0.861371257162135 and parameters: {'n_estimators': 577, 'learning_rate': 0.07436164508167996, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 556}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:40,544] Trial 415 finished with value: 0.8568538830137928 and parameters: {'n_estimators': 737, 'learning_rate': 0.06046469560837985, 'max_depth': 9, 'max_bin': 283, 'num_leaves': 415}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:41,367] Trial 416 finished with value: 0.8557323070366685 and parameters: {'n_estimators': 243, 'learning_rate': 0.11408567822300153, 'max_depth': 11, 'max_bin': 263, 'num_leaves': 222}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:42,587] Trial 417 finished with value: 0.8608976092290511 and parameters: {'n_estimators': 550, 'learning_rate': 0.04766011589838864, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 163}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:43,897] Trial 418 finished with value: 0.8648905791338557 and parameters: {'n_estimators': 681, 'learning_rate': 0.042402860037572175, 'max_depth': 10, 'max_bin': 270, 'num_leaves': 430}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:44,910] Trial 419 finished with value: 0.8563588267942904 and parameters: {'n_estimators': 592, 'learning_rate': 0.08624311882804739, 'max_depth': 12, 'max_bin': 201, 'num_leaves': 106}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:45,803] Trial 420 finished with value: 0.8544400997985413 and parameters: {'n_estimators': 350, 'learning_rate': 0.12641380879151853, 'max_depth': 12, 'max_bin': 275, 'num_leaves': 492}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:46,688] Trial 421 finished with value: 0.8625541740475354 and parameters: {'n_estimators': 538, 'learning_rate': 0.09289716891796901, 'max_depth': 9, 'max_bin': 170, 'num_leaves': 627}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:47,492] Trial 422 finished with value: 0.8588963717281342 and parameters: {'n_estimators': 406, 'learning_rate': 0.1059649053302829, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 135}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:48,367] Trial 423 finished with value: 0.8572148357939027 and parameters: {'n_estimators': 559, 'learning_rate': 0.09757806641612723, 'max_depth': 9, 'max_bin': 158, 'num_leaves': 532}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:49,225] Trial 424 finished with value: 0.8594994973950556 and parameters: {'n_estimators': 654, 'learning_rate': 0.08082710832492974, 'max_depth': 7, 'max_bin': 291, 'num_leaves': 151}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:50,884] Trial 425 finished with value: 0.8648816283300335 and parameters: {'n_estimators': 501, 'learning_rate': 0.02921283652102654, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 470}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:51,860] Trial 426 finished with value: 0.8533844088601332 and parameters: {'n_estimators': 574, 'learning_rate': 0.10080523515214805, 'max_depth': 12, 'max_bin': 150, 'num_leaves': 670}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:52,784] Trial 427 finished with value: 0.8597119775294433 and parameters: {'n_estimators': 521, 'learning_rate': 0.07194504820287949, 'max_depth': 8, 'max_bin': 232, 'num_leaves': 352}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:53,648] Trial 428 finished with value: 0.8685852440783222 and parameters: {'n_estimators': 614, 'learning_rate': 0.1320504677939665, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 453}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:54,575] Trial 429 finished with value: 0.8603338691754642 and parameters: {'n_estimators': 545, 'learning_rate': 0.08979733053564701, 'max_depth': 11, 'max_bin': 167, 'num_leaves': 407}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:55,483] Trial 430 finished with value: 0.8549791328427606 and parameters: {'n_estimators': 585, 'learning_rate': 0.14660508963994331, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 570}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:56,276] Trial 431 finished with value: 0.8656858801054215 and parameters: {'n_estimators': 562, 'learning_rate': 0.11855711190952888, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 126}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:57,500] Trial 432 finished with value: 0.8580463208795128 and parameters: {'n_estimators': 539, 'learning_rate': 0.05525001942332744, 'max_depth': 12, 'max_bin': 154, 'num_leaves': 429}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:58,316] Trial 433 finished with value: 0.8607620739974322 and parameters: {'n_estimators': 604, 'learning_rate': 0.1413744128079434, 'max_depth': 9, 'max_bin': 175, 'num_leaves': 586}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:42:59,498] Trial 434 finished with value: 0.8636690268280625 and parameters: {'n_estimators': 481, 'learning_rate': 0.06841008473842249, 'max_depth': 12, 'max_bin': 240, 'num_leaves': 392}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:00,408] Trial 435 finished with value: 0.8614993259501581 and parameters: {'n_estimators': 521, 'learning_rate': 0.1108850982650635, 'max_depth': 10, 'max_bin': 172, 'num_leaves': 445}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:04,192] Trial 436 finished with value: 0.4284925962648115 and parameters: {'n_estimators': 576, 'learning_rate': 0.0004436328797580852, 'max_depth': 11, 'max_bin': 164, 'num_leaves': 645}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:05,181] Trial 437 finished with value: 0.8618722336818158 and parameters: {'n_estimators': 628, 'learning_rate': 0.0953855646409586, 'max_depth': 12, 'max_bin': 300, 'num_leaves': 606}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:06,124] Trial 438 finished with value: 0.8563850291124048 and parameters: {'n_estimators': 562, 'learning_rate': 0.07658141395867081, 'max_depth': 8, 'max_bin': 160, 'num_leaves': 620}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:07,160] Trial 439 finished with value: 0.8649608614406337 and parameters: {'n_estimators': 600, 'learning_rate': 0.08524251988055516, 'max_depth': 12, 'max_bin': 251, 'num_leaves': 424}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:08,018] Trial 440 finished with value: 0.8614151003304013 and parameters: {'n_estimators': 507, 'learning_rate': 0.10355525135744571, 'max_depth': 9, 'max_bin': 156, 'num_leaves': 453}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:08,830] Trial 441 finished with value: 0.8586186088029473 and parameters: {'n_estimators': 449, 'learning_rate': 0.12441701427069726, 'max_depth': 12, 'max_bin': 224, 'num_leaves': 172}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:10,831] Trial 442 finished with value: 0.8606247134580011 and parameters: {'n_estimators': 531, 'learning_rate': 0.024513746969739635, 'max_depth': 11, 'max_bin': 168, 'num_leaves': 633}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:11,776] Trial 443 finished with value: 0.8618608830710098 and parameters: {'n_estimators': 546, 'learning_rate': 0.09160540935808907, 'max_depth': 12, 'max_bin': 178, 'num_leaves': 501}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:12,535] Trial 444 finished with value: 0.8658545428012534 and parameters: {'n_estimators': 568, 'learning_rate': 0.13549571974046626, 'max_depth': 9, 'max_bin': 189, 'num_leaves': 414}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:13,500] Trial 445 finished with value: 0.8560095308915386 and parameters: {'n_estimators': 776, 'learning_rate': 0.09651774086388311, 'max_depth': 12, 'max_bin': 153, 'num_leaves': 600}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:14,453] Trial 446 finished with value: 0.8531827223238861 and parameters: {'n_estimators': 587, 'learning_rate': 0.06395293824347811, 'max_depth': 8, 'max_bin': 195, 'num_leaves': 146}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:15,357] Trial 447 finished with value: 0.8632734323466289 and parameters: {'n_estimators': 384, 'learning_rate': 0.1063863496708873, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 475}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:16,273] Trial 448 finished with value: 0.8655700969414208 and parameters: {'n_estimators': 849, 'learning_rate': 0.12116519480091029, 'max_depth': 12, 'max_bin': 278, 'num_leaves': 513}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:16,846] Trial 449 finished with value: 0.8509258894586159 and parameters: {'n_estimators': 305, 'learning_rate': 0.15202500013133782, 'max_depth': 5, 'max_bin': 170, 'num_leaves': 382}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.88710098\n",
      "\tBest params:\n",
      "\t\tn_estimators: 616\n",
      "\t\tlearning_rate: 0.10205639024483008\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 234\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_8 = lambda trial: objective_lgbm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_lgbm.optimize(func_lgbm_8, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.8f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd869ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   37.000000   46.000000   50.000000   47.000000   \n",
      "1                    TN  190.000000  188.000000  189.000000  192.000000   \n",
      "2                    FP    9.000000   12.000000   12.000000    7.000000   \n",
      "3                    FN   32.000000   22.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.847015    0.873134    0.891791    0.891791   \n",
      "5             Precision    0.804348    0.793103    0.806452    0.870370   \n",
      "6           Sensitivity    0.536232    0.676471    0.746269    0.681159   \n",
      "7           Specificity    0.954800    0.940000    0.940300    0.964800   \n",
      "8              F1 score    0.643478    0.730159    0.775194    0.764228   \n",
      "9   F1 score (weighted)    0.835895    0.869647    0.890359    0.887158   \n",
      "10     F1 score (macro)    0.773046    0.823616    0.851970    0.847005   \n",
      "11    Balanced Accuracy    0.745503    0.808235    0.843284    0.822992   \n",
      "12                  MCC    0.569354    0.651416    0.705000    0.704156   \n",
      "13                  NPV    0.855900    0.895200    0.917500    0.897200   \n",
      "14              ROC_AUC    0.745503    0.808235    0.843284    0.822992   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    43.000000   51.000000   51.000000   50.000000   47.000000  \n",
      "1   190.000000  192.000000  192.000000  192.000000  197.000000  \n",
      "2    10.000000    8.000000    8.000000    9.000000    5.000000  \n",
      "3    25.000000   17.000000   17.000000   17.000000   19.000000  \n",
      "4     0.869403    0.906716    0.906716    0.902985    0.910448  \n",
      "5     0.811321    0.864407    0.864407    0.847458    0.903846  \n",
      "6     0.632353    0.750000    0.750000    0.746269    0.712121  \n",
      "7     0.950000    0.960000    0.960000    0.955200    0.975200  \n",
      "8     0.710744    0.803150    0.803150    0.793651    0.796610  \n",
      "9     0.863668    0.904437    0.904437    0.900852    0.906635  \n",
      "10    0.813203    0.871012    0.871012    0.865118    0.869597  \n",
      "11    0.791176    0.855000    0.855000    0.850746    0.843684  \n",
      "12    0.636207    0.745639    0.745639    0.733093    0.748874  \n",
      "13    0.883700    0.918700    0.918700    0.918700    0.912000  \n",
      "14    0.791176    0.855000    0.855000    0.850746    0.843684  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_8 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_lgbm_8.fit(X_trainSet8,\n",
    "                Y_trainSet8,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_8 = optimized_lgbm_8.predict(X_testSet8)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_lgbm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_lgbm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_lgbm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_lgbm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_lgbm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_lgbm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_lgbm_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set8'] = Set8\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d97912a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:43:18,050] Trial 450 finished with value: 0.8592780921012432 and parameters: {'n_estimators': 495, 'learning_rate': 0.08247764348081905, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 110}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:19,214] Trial 451 finished with value: 0.8546105980121055 and parameters: {'n_estimators': 551, 'learning_rate': 0.0514750104743098, 'max_depth': 7, 'max_bin': 155, 'num_leaves': 242}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:20,140] Trial 452 finished with value: 0.8523337269196055 and parameters: {'n_estimators': 530, 'learning_rate': 0.11461920338932913, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 560}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:21,040] Trial 453 finished with value: 0.8485383850347918 and parameters: {'n_estimators': 584, 'learning_rate': 0.09986613553718346, 'max_depth': 10, 'max_bin': 162, 'num_leaves': 434}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:21,795] Trial 454 finished with value: 0.862141222361615 and parameters: {'n_estimators': 603, 'learning_rate': 0.13052727315962928, 'max_depth': 8, 'max_bin': 285, 'num_leaves': 585}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:22,737] Trial 455 finished with value: 0.8521312554546053 and parameters: {'n_estimators': 566, 'learning_rate': 0.1100247750373081, 'max_depth': 12, 'max_bin': 166, 'num_leaves': 460}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:23,752] Trial 456 finished with value: 0.8601621477031299 and parameters: {'n_estimators': 548, 'learning_rate': 0.08913557685157558, 'max_depth': 9, 'max_bin': 158, 'num_leaves': 401}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:24,420] Trial 457 finished with value: 0.8485499449376487 and parameters: {'n_estimators': 706, 'learning_rate': 0.09423878050310429, 'max_depth': 3, 'max_bin': 156, 'num_leaves': 127}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:27,932] Trial 458 finished with value: 0.8483698997802724 and parameters: {'n_estimators': 635, 'learning_rate': 0.008319262897779851, 'max_depth': 12, 'max_bin': 173, 'num_leaves': 92}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:28,809] Trial 459 finished with value: 0.8569999736270599 and parameters: {'n_estimators': 512, 'learning_rate': 0.10295973155441172, 'max_depth': 12, 'max_bin': 186, 'num_leaves': 156}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:29,823] Trial 460 finished with value: 0.8457153493237044 and parameters: {'n_estimators': 529, 'learning_rate': 0.08614550825415335, 'max_depth': 11, 'max_bin': 181, 'num_leaves': 419}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:31,436] Trial 461 finished with value: 0.8624957807853999 and parameters: {'n_estimators': 576, 'learning_rate': 0.040216674375299584, 'max_depth': 12, 'max_bin': 160, 'num_leaves': 440}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:32,226] Trial 462 finished with value: 0.845915506774363 and parameters: {'n_estimators': 554, 'learning_rate': 0.1440993729349065, 'max_depth': 12, 'max_bin': 267, 'num_leaves': 265}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:33,164] Trial 463 finished with value: 0.8555828297624014 and parameters: {'n_estimators': 592, 'learning_rate': 0.09910834871012855, 'max_depth': 9, 'max_bin': 153, 'num_leaves': 611}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:33,955] Trial 464 finished with value: 0.8553019270088326 and parameters: {'n_estimators': 615, 'learning_rate': 0.11737047570859403, 'max_depth': 8, 'max_bin': 169, 'num_leaves': 201}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:35,233] Trial 465 finished with value: 0.8603663698093278 and parameters: {'n_estimators': 565, 'learning_rate': 0.0603329167644896, 'max_depth': 11, 'max_bin': 259, 'num_leaves': 370}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:36,081] Trial 466 finished with value: 0.8491779351261212 and parameters: {'n_estimators': 540, 'learning_rate': 0.15438610828916188, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 551}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:37,389] Trial 467 finished with value: 0.8606953902396246 and parameters: {'n_estimators': 670, 'learning_rate': 0.04942864742765815, 'max_depth': 12, 'max_bin': 166, 'num_leaves': 138}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:38,296] Trial 468 finished with value: 0.8534111753786705 and parameters: {'n_estimators': 183, 'learning_rate': 0.07989599716589847, 'max_depth': 9, 'max_bin': 205, 'num_leaves': 167}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:38,957] Trial 469 finished with value: 0.850829328875388 and parameters: {'n_estimators': 516, 'learning_rate': 0.1391128182592563, 'max_depth': 4, 'max_bin': 157, 'num_leaves': 535}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:39,862] Trial 470 finished with value: 0.8567109290726258 and parameters: {'n_estimators': 863, 'learning_rate': 0.12305983611051295, 'max_depth': 12, 'max_bin': 176, 'num_leaves': 573}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:40,919] Trial 471 finished with value: 0.851347191635338 and parameters: {'n_estimators': 581, 'learning_rate': 0.07107558741067928, 'max_depth': 12, 'max_bin': 152, 'num_leaves': 186}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:41,709] Trial 472 finished with value: 0.8559360773308704 and parameters: {'n_estimators': 487, 'learning_rate': 0.1281978636156901, 'max_depth': 9, 'max_bin': 160, 'num_leaves': 479}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:42,646] Trial 473 finished with value: 0.8593635438249384 and parameters: {'n_estimators': 601, 'learning_rate': 0.10698427632389507, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 406}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:44,238] Trial 474 finished with value: 0.8647889781674998 and parameters: {'n_estimators': 556, 'learning_rate': 0.044784154101563306, 'max_depth': 12, 'max_bin': 162, 'num_leaves': 652}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:45,387] Trial 475 finished with value: 0.8538558762981789 and parameters: {'n_estimators': 533, 'learning_rate': 0.07677872463689879, 'max_depth': 11, 'max_bin': 154, 'num_leaves': 620}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:46,270] Trial 476 finished with value: 0.8428708048612078 and parameters: {'n_estimators': 282, 'learning_rate': 0.09200488288031095, 'max_depth': 8, 'max_bin': 167, 'num_leaves': 593}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:47,157] Trial 477 finished with value: 0.8573207416719256 and parameters: {'n_estimators': 223, 'learning_rate': 0.11276220271518286, 'max_depth': 10, 'max_bin': 158, 'num_leaves': 445}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:48,116] Trial 478 finished with value: 0.8455974852346054 and parameters: {'n_estimators': 571, 'learning_rate': 0.10199445579361191, 'max_depth': 12, 'max_bin': 164, 'num_leaves': 465}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:49,078] Trial 479 finished with value: 0.8514439377369417 and parameters: {'n_estimators': 620, 'learning_rate': 0.09619589267173738, 'max_depth': 12, 'max_bin': 209, 'num_leaves': 429}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:50,111] Trial 480 finished with value: 0.8513121590597914 and parameters: {'n_estimators': 547, 'learning_rate': 0.08790675874459034, 'max_depth': 12, 'max_bin': 156, 'num_leaves': 393}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:50,941] Trial 481 finished with value: 0.8541201504754552 and parameters: {'n_estimators': 499, 'learning_rate': 0.11846410971489162, 'max_depth': 9, 'max_bin': 175, 'num_leaves': 298}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:51,927] Trial 482 finished with value: 0.8457020395199647 and parameters: {'n_estimators': 648, 'learning_rate': 0.08450688251401998, 'max_depth': 12, 'max_bin': 161, 'num_leaves': 124}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:52,756] Trial 483 finished with value: 0.8600255129250322 and parameters: {'n_estimators': 590, 'learning_rate': 0.1605916492891259, 'max_depth': 11, 'max_bin': 150, 'num_leaves': 490}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:53,454] Trial 484 finished with value: 0.8590083243139874 and parameters: {'n_estimators': 521, 'learning_rate': 0.13345379242837455, 'max_depth': 7, 'max_bin': 280, 'num_leaves': 415}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:54,304] Trial 485 finished with value: 0.8530520376966952 and parameters: {'n_estimators': 564, 'learning_rate': 0.10501906383507428, 'max_depth': 8, 'max_bin': 168, 'num_leaves': 455}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:55,315] Trial 486 finished with value: 0.8561250937379707 and parameters: {'n_estimators': 608, 'learning_rate': 0.09906391810818864, 'max_depth': 12, 'max_bin': 297, 'num_leaves': 521}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:56,352] Trial 487 finished with value: 0.8491916023987608 and parameters: {'n_estimators': 465, 'learning_rate': 0.09257121443521012, 'max_depth': 12, 'max_bin': 289, 'num_leaves': 692}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:57,414] Trial 488 finished with value: 0.8584712837284924 and parameters: {'n_estimators': 538, 'learning_rate': 0.06807773554234989, 'max_depth': 9, 'max_bin': 236, 'num_leaves': 148}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:58,349] Trial 489 finished with value: 0.8496179406178482 and parameters: {'n_estimators': 581, 'learning_rate': 0.10915991882076245, 'max_depth': 11, 'max_bin': 273, 'num_leaves': 579}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:59,341] Trial 490 finished with value: 0.8512345304698081 and parameters: {'n_estimators': 559, 'learning_rate': 0.1144445391742633, 'max_depth': 12, 'max_bin': 181, 'num_leaves': 634}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:43:59,981] Trial 491 finished with value: 0.8618241602862176 and parameters: {'n_estimators': 521, 'learning_rate': 0.19747938015532443, 'max_depth': 8, 'max_bin': 155, 'num_leaves': 598}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:44:00,854] Trial 492 finished with value: 0.8593739935294629 and parameters: {'n_estimators': 632, 'learning_rate': 0.12818299944030884, 'max_depth': 12, 'max_bin': 159, 'num_leaves': 433}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:44:01,767] Trial 493 finished with value: 0.8496941457688809 and parameters: {'n_estimators': 547, 'learning_rate': 0.0949470140781905, 'max_depth': 12, 'max_bin': 165, 'num_leaves': 113}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:44:02,681] Trial 494 finished with value: 0.8573612460430848 and parameters: {'n_estimators': 572, 'learning_rate': 0.12360171529799827, 'max_depth': 12, 'max_bin': 171, 'num_leaves': 448}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:44:03,706] Trial 495 finished with value: 0.8479557922133178 and parameters: {'n_estimators': 144, 'learning_rate': 0.03359001609016143, 'max_depth': 10, 'max_bin': 215, 'num_leaves': 165}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:44:04,370] Trial 496 finished with value: 0.8524987280377531 and parameters: {'n_estimators': 595, 'learning_rate': 0.17064103601661473, 'max_depth': 9, 'max_bin': 153, 'num_leaves': 134}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:44:06,293] Trial 497 finished with value: 0.8488355751489337 and parameters: {'n_estimators': 536, 'learning_rate': 0.028101184504429208, 'max_depth': 12, 'max_bin': 163, 'num_leaves': 564}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:44:07,853] Trial 498 finished with value: 0.8590384191075933 and parameters: {'n_estimators': 581, 'learning_rate': 0.03563078120808155, 'max_depth': 11, 'max_bin': 159, 'num_leaves': 220}. Best is trial 63 with value: 0.8871009773800717.\n",
      "[I 2023-12-05 12:44:08,856] Trial 499 finished with value: 0.8437446075492403 and parameters: {'n_estimators': 120, 'learning_rate': 0.07424224651452606, 'max_depth': 12, 'max_bin': 283, 'num_leaves': 668}. Best is trial 63 with value: 0.8871009773800717.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.887100977\n",
      "\tBest params:\n",
      "\t\tn_estimators: 616\n",
      "\t\tlearning_rate: 0.10205639024483008\n",
      "\t\tmax_depth: 12\n",
      "\t\tmax_bin: 226\n",
      "\t\tnum_leaves: 234\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "#study_lgbm_1 = optuna.create_study(direction='maximize', study_name=\"LGBMClassifier_1\")\n",
    "func_lgbm_9 = lambda trial: objective_lgbm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_lgbm.optimize(func_lgbm_9, n_trials=50)  \n",
    "print(f\"\\tNumber of trials: {len(study_lgbm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_lgbm.best_value:.9f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgbm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a422861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   37.000000   46.000000   50.000000   47.000000   \n",
      "1                    TN  190.000000  188.000000  189.000000  192.000000   \n",
      "2                    FP    9.000000   12.000000   12.000000    7.000000   \n",
      "3                    FN   32.000000   22.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.847015    0.873134    0.891791    0.891791   \n",
      "5             Precision    0.804348    0.793103    0.806452    0.870370   \n",
      "6           Sensitivity    0.536232    0.676471    0.746269    0.681159   \n",
      "7           Specificity    0.954800    0.940000    0.940300    0.964800   \n",
      "8              F1 score    0.643478    0.730159    0.775194    0.764228   \n",
      "9   F1 score (weighted)    0.835895    0.869647    0.890359    0.887158   \n",
      "10     F1 score (macro)    0.773046    0.823616    0.851970    0.847005   \n",
      "11    Balanced Accuracy    0.745503    0.808235    0.843284    0.822992   \n",
      "12                  MCC    0.569354    0.651416    0.705000    0.704156   \n",
      "13                  NPV    0.855900    0.895200    0.917500    0.897200   \n",
      "14              ROC_AUC    0.745503    0.808235    0.843284    0.822992   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    43.000000   51.000000   51.000000   50.000000   47.000000   44.000000  \n",
      "1   190.000000  192.000000  192.000000  192.000000  197.000000  197.000000  \n",
      "2    10.000000    8.000000    8.000000    9.000000    5.000000    4.000000  \n",
      "3    25.000000   17.000000   17.000000   17.000000   19.000000   23.000000  \n",
      "4     0.869403    0.906716    0.906716    0.902985    0.910448    0.899254  \n",
      "5     0.811321    0.864407    0.864407    0.847458    0.903846    0.916667  \n",
      "6     0.632353    0.750000    0.750000    0.746269    0.712121    0.656716  \n",
      "7     0.950000    0.960000    0.960000    0.955200    0.975200    0.980100  \n",
      "8     0.710744    0.803150    0.803150    0.793651    0.796610    0.765217  \n",
      "9     0.863668    0.904437    0.904437    0.900852    0.906635    0.893205  \n",
      "10    0.813203    0.871012    0.871012    0.865118    0.869597    0.850542  \n",
      "11    0.791176    0.855000    0.855000    0.850746    0.843684    0.818408  \n",
      "12    0.636207    0.745639    0.745639    0.733093    0.748874    0.719147  \n",
      "13    0.883700    0.918700    0.918700    0.918700    0.912000    0.895500  \n",
      "14    0.791176    0.855000    0.855000    0.850746    0.843684    0.818408  \n"
     ]
    }
   ],
   "source": [
    "optimized_lgbm_9 = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "#learn\n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_lgbm_9.fit(X_trainSet9,\n",
    "                Y_trainSet9,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "        \n",
    "\n",
    "#predict        \n",
    "y_pred_lgbm_9 = optimized_lgbm_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_lgbm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_lgbm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_lgbm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_lgbm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_lgbm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_lgbm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_lgbm_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_lgbm_test['Set9'] = Set9\n",
    "print(mat_met_lgbm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "812c9364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7929aa59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "plot_param_importances(study_lgbm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea89ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        25.600000     3.062316\n",
      "1                    TN        95.900000     2.766867\n",
      "2                    FP         4.600000     2.756810\n",
      "3                    FN         7.800000     2.898275\n",
      "4              Accuracy         0.907401     0.033759\n",
      "5             Precision         0.851115     0.083880\n",
      "6           Sensitivity         0.766185     0.088308\n",
      "7           Specificity         0.954250     0.027337\n",
      "8              F1 score         0.803969     0.073981\n",
      "9   F1 score (weighted)         0.905610     0.034517\n",
      "10     F1 score (macro)         0.871635     0.047783\n",
      "11    Balanced Accuracy         0.860216     0.049779\n",
      "12                  MCC         0.747269     0.094640\n",
      "13                  NPV         0.925250     0.026136\n",
      "14              ROC_AUC         0.860216     0.049779\n"
     ]
    }
   ],
   "source": [
    "detailed_objective_lgbm_cv(study_lgbm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4e16369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>46.600000</td>\n",
       "      <td>4.402020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>191.900000</td>\n",
       "      <td>3.034981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>2.633122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>21.100000</td>\n",
       "      <td>4.840799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.847015</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.906716</td>\n",
       "      <td>0.906716</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.889925</td>\n",
       "      <td>0.020532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.903846</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.848238</td>\n",
       "      <td>0.043286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.688759</td>\n",
       "      <td>0.068591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.954800</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.940300</td>\n",
       "      <td>0.964800</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.955200</td>\n",
       "      <td>0.975200</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.958040</td>\n",
       "      <td>0.013157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.643478</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.775194</td>\n",
       "      <td>0.764228</td>\n",
       "      <td>0.710744</td>\n",
       "      <td>0.803150</td>\n",
       "      <td>0.803150</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>0.765217</td>\n",
       "      <td>0.758558</td>\n",
       "      <td>0.050931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.835895</td>\n",
       "      <td>0.869647</td>\n",
       "      <td>0.890359</td>\n",
       "      <td>0.887158</td>\n",
       "      <td>0.863668</td>\n",
       "      <td>0.904437</td>\n",
       "      <td>0.904437</td>\n",
       "      <td>0.900852</td>\n",
       "      <td>0.906635</td>\n",
       "      <td>0.893205</td>\n",
       "      <td>0.885629</td>\n",
       "      <td>0.022788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.773046</td>\n",
       "      <td>0.823616</td>\n",
       "      <td>0.851970</td>\n",
       "      <td>0.847005</td>\n",
       "      <td>0.813203</td>\n",
       "      <td>0.871012</td>\n",
       "      <td>0.871012</td>\n",
       "      <td>0.865118</td>\n",
       "      <td>0.869597</td>\n",
       "      <td>0.850542</td>\n",
       "      <td>0.843612</td>\n",
       "      <td>0.031751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.745503</td>\n",
       "      <td>0.808235</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.822992</td>\n",
       "      <td>0.791176</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.843684</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>0.823403</td>\n",
       "      <td>0.034822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.569354</td>\n",
       "      <td>0.651416</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.704156</td>\n",
       "      <td>0.636207</td>\n",
       "      <td>0.745639</td>\n",
       "      <td>0.745639</td>\n",
       "      <td>0.733093</td>\n",
       "      <td>0.748874</td>\n",
       "      <td>0.719147</td>\n",
       "      <td>0.695852</td>\n",
       "      <td>0.059053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.855900</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.897200</td>\n",
       "      <td>0.883700</td>\n",
       "      <td>0.918700</td>\n",
       "      <td>0.918700</td>\n",
       "      <td>0.918700</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.895500</td>\n",
       "      <td>0.901310</td>\n",
       "      <td>0.020396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.745503</td>\n",
       "      <td>0.808235</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.822992</td>\n",
       "      <td>0.791176</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.843684</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>0.823403</td>\n",
       "      <td>0.034822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   37.000000   46.000000   50.000000   47.000000   \n",
       "1                    TN  190.000000  188.000000  189.000000  192.000000   \n",
       "2                    FP    9.000000   12.000000   12.000000    7.000000   \n",
       "3                    FN   32.000000   22.000000   17.000000   22.000000   \n",
       "4              Accuracy    0.847015    0.873134    0.891791    0.891791   \n",
       "5             Precision    0.804348    0.793103    0.806452    0.870370   \n",
       "6           Sensitivity    0.536232    0.676471    0.746269    0.681159   \n",
       "7           Specificity    0.954800    0.940000    0.940300    0.964800   \n",
       "8              F1 score    0.643478    0.730159    0.775194    0.764228   \n",
       "9   F1 score (weighted)    0.835895    0.869647    0.890359    0.887158   \n",
       "10     F1 score (macro)    0.773046    0.823616    0.851970    0.847005   \n",
       "11    Balanced Accuracy    0.745503    0.808235    0.843284    0.822992   \n",
       "12                  MCC    0.569354    0.651416    0.705000    0.704156   \n",
       "13                  NPV    0.855900    0.895200    0.917500    0.897200   \n",
       "14              ROC_AUC    0.745503    0.808235    0.843284    0.822992   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    43.000000   51.000000   51.000000   50.000000   47.000000   44.000000   \n",
       "1   190.000000  192.000000  192.000000  192.000000  197.000000  197.000000   \n",
       "2    10.000000    8.000000    8.000000    9.000000    5.000000    4.000000   \n",
       "3    25.000000   17.000000   17.000000   17.000000   19.000000   23.000000   \n",
       "4     0.869403    0.906716    0.906716    0.902985    0.910448    0.899254   \n",
       "5     0.811321    0.864407    0.864407    0.847458    0.903846    0.916667   \n",
       "6     0.632353    0.750000    0.750000    0.746269    0.712121    0.656716   \n",
       "7     0.950000    0.960000    0.960000    0.955200    0.975200    0.980100   \n",
       "8     0.710744    0.803150    0.803150    0.793651    0.796610    0.765217   \n",
       "9     0.863668    0.904437    0.904437    0.900852    0.906635    0.893205   \n",
       "10    0.813203    0.871012    0.871012    0.865118    0.869597    0.850542   \n",
       "11    0.791176    0.855000    0.855000    0.850746    0.843684    0.818408   \n",
       "12    0.636207    0.745639    0.745639    0.733093    0.748874    0.719147   \n",
       "13    0.883700    0.918700    0.918700    0.918700    0.912000    0.895500   \n",
       "14    0.791176    0.855000    0.855000    0.850746    0.843684    0.818408   \n",
       "\n",
       "           ave       std  \n",
       "0    46.600000  4.402020  \n",
       "1   191.900000  3.034981  \n",
       "2     8.400000  2.633122  \n",
       "3    21.100000  4.840799  \n",
       "4     0.889925  0.020532  \n",
       "5     0.848238  0.043286  \n",
       "6     0.688759  0.068591  \n",
       "7     0.958040  0.013157  \n",
       "8     0.758558  0.050931  \n",
       "9     0.885629  0.022788  \n",
       "10    0.843612  0.031751  \n",
       "11    0.823403  0.034822  \n",
       "12    0.695852  0.059053  \n",
       "13    0.901310  0.020396  \n",
       "14    0.823403  0.034822  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_lgbm_test['ave'] = mat_met_lgbm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test['std'] = mat_met_lgbm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_lgbm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7c3c24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.906951</td>\n",
       "      <td>0.024910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.861611</td>\n",
       "      <td>0.056381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.749292</td>\n",
       "      <td>0.084015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.959422</td>\n",
       "      <td>0.017786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.798892</td>\n",
       "      <td>0.060655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.904348</td>\n",
       "      <td>0.026711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.869142</td>\n",
       "      <td>0.038001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.854355</td>\n",
       "      <td>0.043024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.743905</td>\n",
       "      <td>0.072410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.920636</td>\n",
       "      <td>0.024474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.854355</td>\n",
       "      <td>0.043024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.906951     0.024910\n",
       "1             Precision         0.861611     0.056381\n",
       "2           Sensitivity         0.749292     0.084015\n",
       "3           Specificity         0.959422     0.017786\n",
       "4              F1 score         0.798892     0.060655\n",
       "5   F1 score (weighted)         0.904348     0.026711\n",
       "6      F1 score (macro)         0.869142     0.038001\n",
       "7     Balanced Accuracy         0.854355     0.043024\n",
       "8                   MCC         0.743905     0.072410\n",
       "9                   NPV         0.920636     0.024474\n",
       "10              ROC_AUC         0.854355     0.043024"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_lgbm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_lgbm = lgbm.LGBMClassifier(objective=\"binary\", \n",
    "                                      random_state=1121218, \n",
    "                                      boosting_type =\"gbdt\", \n",
    "                                      subsample=0.8, # also called bagging_fraction\n",
    "                                      subsample_freq=10,\n",
    "                                      n_estimators=study_lgbm.best_params['n_estimators'],\n",
    "                                      learning_rate=study_lgbm.best_params['learning_rate'],\n",
    "                                      max_depth = study_lgbm.best_params['max_depth'],\n",
    "                                      max_bin=study_lgbm.best_params['max_bin'],\n",
    "                                      #lambda_l1 = study_lgbm.best_params['lambda_l1'],\n",
    "                                      #lambda_l2= study_lgbm.best_params['lambda_l2'],\n",
    "                                      num_leaves=study_lgbm.best_params['num_leaves'],\n",
    "                                      #min_child_samples = study_lgbm.best_params['min_child_samples'],\n",
    "                                      #bagging_fraction = study_lgbm.best_params['bagging_fraction'],\n",
    "                                      #bagging_freq = study_lgbm.best_params['bagging_freq'],\n",
    "                                        \n",
    "                                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_lgbm.fit(X_train,\n",
    "                y_train,\n",
    "                eval_set=eval_set,\n",
    "                eval_metric=\"binary_logloss\",     \n",
    "                early_stopping_rounds=50,\n",
    "                verbose = False,\n",
    "                )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_lgbm = optimizedCV_lgbm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_lgbm': y_pred_optimized_lgbm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_lgbm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "       \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_lgbm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_lgbm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_lgbm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_lgbm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_lgbm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_lgbm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_lgbm))\n",
    "        \n",
    "    data_lgbm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_lgbm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_lgbm['y_pred_lgbm' + str(i)] = data_inner['y_pred_lgbm']\n",
    "   # data_lgbm['correct' + str(i)] = correct_value\n",
    "   # data_lgbm['pred' + str(i)] = y_pred_optimized_lgbm\n",
    "\n",
    "mat_met_optimized_lgbm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "\n",
    "mat_met_optimized_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "62e03bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM baseline model f1_score 0.8687 with a standard deviation of 0.0500\n",
      "LightGBM optimized model f1_score 0.8783 with a standard deviation of 0.0424\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized LightGBM \n",
    "fit_params={'early_stopping_rounds': 50, \n",
    "        'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "            'verbose':False,\n",
    "           }\n",
    "#cross valide using this optimized LightGBM \n",
    "lgbm_baseline_CVscore = cross_val_score(lgbm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#f1_cv_lgbm_opt_testSet = cross_val_score(optimized_lgbm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "f1_cv_lgbm_opt = cross_val_score(optimizedCV_lgbm, X, Y, cv=10, scoring=\"f1_macro\", fit_params=fit_params)\n",
    "print(\"LightGBM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(lgbm_baseline_CVscore), np.std(lgbm_baseline_CVscore, ddof=1)))\n",
    "#print(\"LightGBM optimized model (tested on Y_te)f1_score %0.4f with a standard deviation of %0.4f\" % (f1_cv_lgbm_opt_testSet.mean(), f1_cv_lgbm_opt_testSet.std()))\n",
    "print(\"LightGBM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(f1_cv_lgbm_opt), np.std(f1_cv_lgbm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f3cbf6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_lgbm_clf.joblib']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lgbm_clf, \"OUTPUT/lgbm_clf.joblib\")\n",
    "#joblib.dump(optimized_lgbm, \"OUTPUT/optimized_lgbm.joblib\")\n",
    "joblib.dump(optimizedCV_lgbm, \"OUTPUT/optimizedCV_lgbm_clf.joblib\") \n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e710905",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc6f6189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        25.100000     3.446415\n",
      "1                    TN        96.200000     2.936362\n",
      "2                    FP         4.300000     2.750757\n",
      "3                    FN         8.300000     3.301515\n",
      "4              Accuracy         0.905914     0.034109\n",
      "5             Precision         0.857812     0.084222\n",
      "6           Sensitivity         0.751212     0.100444\n",
      "7           Specificity         0.957190     0.027547\n",
      "8              F1 score         0.797588     0.078173\n",
      "9   F1 score (weighted)         0.903455     0.035588\n",
      "10     F1 score (macro)         0.868084     0.049772\n",
      "11    Balanced Accuracy         0.854199     0.053685\n",
      "12                  MCC         0.742068     0.097538\n",
      "13                  NPV         0.921270     0.029074\n",
      "14              ROC_AUC         0.854199     0.053685\n",
      "CPU times: user 7.46 s, sys: 72.2 ms, total: 7.53 s\n",
      "Wall time: 991 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    random_state=1121218,\n",
    "    #n_estimators=10000,  \n",
    "    tree_method=\"hist\",  # enable histogram binning in XGB\n",
    "    subsample=0.8, \n",
    "    n_jobs=8,\n",
    "    )\n",
    "    \n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_clf.fit(X_train,\n",
    "                y_train,\n",
    "    \n",
    "    eval_set=eval_set,\n",
    "    eval_metric=\"logloss\",\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,  # Disable logs\n",
    "               )\n",
    "\n",
    "    y_pred = xgb_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2a7452d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    #y_comb=pd.DataFrame()\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "    \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "            \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38d38cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_xgb_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 900),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-6, 0.1),  \n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),  \n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 1, step=1e-04),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1,40),\n",
    "        #\"alpha\": trial.suggest_float(\"alpha\", 0, 1.0),\n",
    "        #\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 40.0),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 500),\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    TP=np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP= np.empty(10)\n",
    "    FN= np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W=np.empty(10)\n",
    "    f1_scores_M=np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=1121218, booster =\"gbtree\", tree_method='hist',\n",
    "                                  **param_grid,  n_jobs=8, subsample=0.8, )\n",
    "    \n",
    "        eval_set = [(X_test, y_test)]\n",
    "        xgb_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=eval_set,\n",
    "            eval_metric=\"logloss\",    \n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "        \n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        \n",
    "       \n",
    "           \n",
    "        #calculate parameters\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)      \n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })   \n",
    "    \n",
    "    return (mat_met)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec6a49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:44:28,907] A new study created in memory with name: XGBClassifier\n",
      "[I 2023-12-05 12:44:32,955] Trial 0 finished with value: 0.8648618121847859 and parameters: {'n_estimators': 802, 'eta': 0.055813323933700594, 'max_depth': 10, 'alpha': 0.33330000000000004, 'lambda': 16.33524922650445, 'max_bin': 382}. Best is trial 0 with value: 0.8648618121847859.\n",
      "[I 2023-12-05 12:44:35,786] Trial 1 finished with value: 0.8656242755428967 and parameters: {'n_estimators': 232, 'eta': 0.07639076095766455, 'max_depth': 12, 'alpha': 0.5601, 'lambda': 27.515875242572324, 'max_bin': 458}. Best is trial 1 with value: 0.8656242755428967.\n",
      "[I 2023-12-05 12:44:40,920] Trial 2 finished with value: 0.8500999292577301 and parameters: {'n_estimators': 602, 'eta': 0.02533322420375487, 'max_depth': 5, 'alpha': 0.806, 'lambda': 8.917018537263571, 'max_bin': 346}. Best is trial 1 with value: 0.8656242755428967.\n",
      "[I 2023-12-05 12:44:45,049] Trial 3 finished with value: 0.8640380830052251 and parameters: {'n_estimators': 711, 'eta': 0.06304055822654336, 'max_depth': 12, 'alpha': 0.3655, 'lambda': 19.698792321724113, 'max_bin': 469}. Best is trial 1 with value: 0.8656242755428967.\n",
      "[I 2023-12-05 12:44:48,629] Trial 4 finished with value: 0.861914502928285 and parameters: {'n_estimators': 619, 'eta': 0.08417482269992167, 'max_depth': 8, 'alpha': 0.2449, 'lambda': 24.81045719994465, 'max_bin': 330}. Best is trial 1 with value: 0.8656242755428967.\n",
      "[I 2023-12-05 12:44:54,042] Trial 5 finished with value: 0.8483568951252487 and parameters: {'n_estimators': 530, 'eta': 0.02208240508100171, 'max_depth': 7, 'alpha': 0.0951, 'lambda': 20.77980379560941, 'max_bin': 268}. Best is trial 1 with value: 0.8656242755428967.\n",
      "[I 2023-12-05 12:45:00,551] Trial 6 finished with value: 0.8671725209473786 and parameters: {'n_estimators': 512, 'eta': 0.01219356036062499, 'max_depth': 12, 'alpha': 0.0854, 'lambda': 5.071487394343526, 'max_bin': 461}. Best is trial 6 with value: 0.8671725209473786.\n",
      "[I 2023-12-05 12:45:01,795] Trial 7 finished with value: 0.8515367783893127 and parameters: {'n_estimators': 132, 'eta': 0.07640787316814858, 'max_depth': 5, 'alpha': 0.6165, 'lambda': 3.5147967247621335, 'max_bin': 358}. Best is trial 6 with value: 0.8671725209473786.\n",
      "[I 2023-12-05 12:45:07,248] Trial 8 finished with value: 0.8642456332681503 and parameters: {'n_estimators': 718, 'eta': 0.06579920807774416, 'max_depth': 10, 'alpha': 0.7167, 'lambda': 37.94936257597441, 'max_bin': 464}. Best is trial 6 with value: 0.8671725209473786.\n",
      "[I 2023-12-05 12:45:11,221] Trial 9 finished with value: 0.8664019569978507 and parameters: {'n_estimators': 514, 'eta': 0.07490088727798262, 'max_depth': 11, 'alpha': 0.28850000000000003, 'lambda': 28.609498270115825, 'max_bin': 357}. Best is trial 6 with value: 0.8671725209473786.\n",
      "[I 2023-12-05 12:45:14,674] Trial 10 finished with value: 0.8292540696827044 and parameters: {'n_estimators': 302, 'eta': 0.0038025716096466416, 'max_depth': 9, 'alpha': 0.06470000000000001, 'lambda': 3.4889892824531987, 'max_bin': 416}. Best is trial 6 with value: 0.8671725209473786.\n",
      "[I 2023-12-05 12:45:17,254] Trial 11 finished with value: 0.8765404194480633 and parameters: {'n_estimators': 401, 'eta': 0.09828166264030841, 'max_depth': 11, 'alpha': 0.9786, 'lambda': 12.283492100919233, 'max_bin': 296}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:19,988] Trial 12 finished with value: 0.8716103999615112 and parameters: {'n_estimators': 393, 'eta': 0.09172185672471131, 'max_depth': 12, 'alpha': 0.9839, 'lambda': 10.152455033335904, 'max_bin': 256}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:22,666] Trial 13 finished with value: 0.8740541816427629 and parameters: {'n_estimators': 352, 'eta': 0.09926075057771251, 'max_depth': 10, 'alpha': 0.9869, 'lambda': 11.560732340985183, 'max_bin': 256}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:25,415] Trial 14 finished with value: 0.8719881864640634 and parameters: {'n_estimators': 360, 'eta': 0.09918413127522657, 'max_depth': 10, 'alpha': 0.9877, 'lambda': 12.661314431526264, 'max_bin': 301}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:26,902] Trial 15 finished with value: 0.8680749671985366 and parameters: {'n_estimators': 123, 'eta': 0.09913299508817759, 'max_depth': 9, 'alpha': 0.8148000000000001, 'lambda': 12.825308480680377, 'max_bin': 296}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:28,757] Trial 16 finished with value: 0.8698875233504755 and parameters: {'n_estimators': 244, 'eta': 0.08813996191783949, 'max_depth': 7, 'alpha': 0.8685, 'lambda': 1.0504390220605604, 'max_bin': 296}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:32,742] Trial 17 finished with value: 0.8670413689968693 and parameters: {'n_estimators': 414, 'eta': 0.04241254722191903, 'max_depth': 11, 'alpha': 0.7030000000000001, 'lambda': 7.886402805859217, 'max_bin': 274}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:35,342] Trial 18 finished with value: 0.8711846705722082 and parameters: {'n_estimators': 243, 'eta': 0.09861279024579223, 'max_depth': 11, 'alpha': 0.9053, 'lambda': 15.109631271198667, 'max_bin': 251}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:37,949] Trial 19 finished with value: 0.8649650174372011 and parameters: {'n_estimators': 895, 'eta': 0.08711468293479693, 'max_depth': 8, 'alpha': 0.5025000000000001, 'lambda': 8.034910833543558, 'max_bin': 330}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:38,802] Trial 20 finished with value: 0.8312554175263553 and parameters: {'n_estimators': 66, 'eta': 0.04640513962151712, 'max_depth': 10, 'alpha': 0.7141000000000001, 'lambda': 17.126304547331003, 'max_bin': 393}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:41,747] Trial 21 finished with value: 0.8759889911396842 and parameters: {'n_estimators': 368, 'eta': 0.09633879414222057, 'max_depth': 10, 'alpha': 0.9966, 'lambda': 11.949256521793266, 'max_bin': 301}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:44,705] Trial 22 finished with value: 0.870214159659587 and parameters: {'n_estimators': 444, 'eta': 0.08910901243000496, 'max_depth': 11, 'alpha': 0.898, 'lambda': 11.891436327886028, 'max_bin': 316}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:47,596] Trial 23 finished with value: 0.8690708472778953 and parameters: {'n_estimators': 291, 'eta': 0.08272137704449489, 'max_depth': 9, 'alpha': 0.9783000000000001, 'lambda': 12.144425438358864, 'max_bin': 280}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:49,782] Trial 24 finished with value: 0.8718888498552675 and parameters: {'n_estimators': 352, 'eta': 0.0995814889466907, 'max_depth': 10, 'alpha': 0.8029000000000001, 'lambda': 6.884441589904011, 'max_bin': 288}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:52,554] Trial 25 finished with value: 0.8714036277059385 and parameters: {'n_estimators': 478, 'eta': 0.09169674718870024, 'max_depth': 11, 'alpha': 0.9212, 'lambda': 10.309519148388013, 'max_bin': 314}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:55,555] Trial 26 finished with value: 0.8754351957859582 and parameters: {'n_estimators': 315, 'eta': 0.08042714125216266, 'max_depth': 9, 'alpha': 0.8379000000000001, 'lambda': 13.911600256584748, 'max_bin': 265}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:45:57,750] Trial 27 finished with value: 0.8628371869323329 and parameters: {'n_estimators': 198, 'eta': 0.07744030380051346, 'max_depth': 8, 'alpha': 0.6485000000000001, 'lambda': 16.296961567112312, 'max_bin': 314}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:46:00,608] Trial 28 finished with value: 0.8587620886220744 and parameters: {'n_estimators': 302, 'eta': 0.06998353383970561, 'max_depth': 7, 'alpha': 0.7708, 'lambda': 14.52681633985083, 'max_bin': 420}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:46:04,106] Trial 29 finished with value: 0.8653832893577394 and parameters: {'n_estimators': 585, 'eta': 0.08243473567000326, 'max_depth': 9, 'alpha': 0.8598, 'lambda': 18.40676069771884, 'max_bin': 380}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:46:06,157] Trial 30 finished with value: 0.8614117153867674 and parameters: {'n_estimators': 166, 'eta': 0.06151174855198919, 'max_depth': 10, 'alpha': 0.9328000000000001, 'lambda': 14.101988342063532, 'max_bin': 333}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:46:08,971] Trial 31 finished with value: 0.8736423109929049 and parameters: {'n_estimators': 352, 'eta': 0.09263015349555548, 'max_depth': 9, 'alpha': 0.9997, 'lambda': 11.348042808317764, 'max_bin': 262}. Best is trial 11 with value: 0.8765404194480633.\n",
      "[I 2023-12-05 12:46:11,422] Trial 32 finished with value: 0.8767165756130584 and parameters: {'n_estimators': 429, 'eta': 0.09436753600712847, 'max_depth': 10, 'alpha': 0.8571000000000001, 'lambda': 9.579932495677093, 'max_bin': 277}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:14,460] Trial 33 finished with value: 0.8698107869776536 and parameters: {'n_estimators': 445, 'eta': 0.08266016658120837, 'max_depth': 11, 'alpha': 0.8482000000000001, 'lambda': 8.558282735307492, 'max_bin': 277}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:16,856] Trial 34 finished with value: 0.8755278600464548 and parameters: {'n_estimators': 404, 'eta': 0.09276373396837474, 'max_depth': 10, 'alpha': 0.7529, 'lambda': 5.836620235941682, 'max_bin': 307}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:19,767] Trial 35 finished with value: 0.8729412485307831 and parameters: {'n_estimators': 573, 'eta': 0.09322400231832842, 'max_depth': 12, 'alpha': 0.7578, 'lambda': 9.437092672331879, 'max_bin': 306}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:22,056] Trial 36 finished with value: 0.8683231955175927 and parameters: {'n_estimators': 662, 'eta': 0.09312757490097501, 'max_depth': 10, 'alpha': 0.3844, 'lambda': 6.148721685387596, 'max_bin': 343}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:24,593] Trial 37 finished with value: 0.876131960042051 and parameters: {'n_estimators': 410, 'eta': 0.08719826775851104, 'max_depth': 11, 'alpha': 0.5541, 'lambda': 6.771690356753996, 'max_bin': 490}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:27,217] Trial 38 finished with value: 0.8719440991483912 and parameters: {'n_estimators': 474, 'eta': 0.08589674344855568, 'max_depth': 12, 'alpha': 0.4267, 'lambda': 8.629197779758567, 'max_bin': 486}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:29,586] Trial 39 finished with value: 0.8751680777362857 and parameters: {'n_estimators': 528, 'eta': 0.07643630811666699, 'max_depth': 11, 'alpha': 0.5793, 'lambda': 5.0192263302170685, 'max_bin': 440}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:32,067] Trial 40 finished with value: 0.8656990370719967 and parameters: {'n_estimators': 469, 'eta': 0.08662507468807477, 'max_depth': 11, 'alpha': 0.1666, 'lambda': 10.08489001602696, 'max_bin': 364}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:34,221] Trial 41 finished with value: 0.8735491358883858 and parameters: {'n_estimators': 429, 'eta': 0.09449077283150054, 'max_depth': 10, 'alpha': 0.5009, 'lambda': 6.476858634114898, 'max_bin': 284}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:36,426] Trial 42 finished with value: 0.8722438065729211 and parameters: {'n_estimators': 392, 'eta': 0.09489509283831292, 'max_depth': 11, 'alpha': 0.9382, 'lambda': 3.7845252543395667, 'max_bin': 498}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:38,565] Trial 43 finished with value: 0.8690651172747363 and parameters: {'n_estimators': 391, 'eta': 0.0847874915075343, 'max_depth': 12, 'alpha': 0.6805, 'lambda': 1.8013073462245384, 'max_bin': 334}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:41,105] Trial 44 finished with value: 0.8724159641719552 and parameters: {'n_estimators': 559, 'eta': 0.08876391062878317, 'max_depth': 10, 'alpha': 0.5828, 'lambda': 5.622121175258839, 'max_bin': 400}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:44,254] Trial 45 finished with value: 0.8676299088600752 and parameters: {'n_estimators': 497, 'eta': 0.07261244646624904, 'max_depth': 12, 'alpha': 0.7632, 'lambda': 7.405983516340104, 'max_bin': 442}. Best is trial 32 with value: 0.8767165756130584.\n",
      "[I 2023-12-05 12:46:46,572] Trial 46 finished with value: 0.8775134292586342 and parameters: {'n_estimators': 638, 'eta': 0.08050651458994573, 'max_depth': 10, 'alpha': 0.6357, 'lambda': 4.145013106178915, 'max_bin': 322}. Best is trial 46 with value: 0.8775134292586342.\n",
      "[I 2023-12-05 12:46:49,022] Trial 47 finished with value: 0.865021516006729 and parameters: {'n_estimators': 741, 'eta': 0.07834972994876754, 'max_depth': 5, 'alpha': 0.4469, 'lambda': 3.7379514462196486, 'max_bin': 324}. Best is trial 46 with value: 0.8775134292586342.\n",
      "[I 2023-12-05 12:46:51,172] Trial 48 finished with value: 0.8742695001219788 and parameters: {'n_estimators': 658, 'eta': 0.08032115355690597, 'max_depth': 11, 'alpha': 0.5345, 'lambda': 2.3049463222299824, 'max_bin': 350}. Best is trial 46 with value: 0.8775134292586342.\n",
      "[I 2023-12-05 12:46:53,610] Trial 49 finished with value: 0.8691299133938003 and parameters: {'n_estimators': 627, 'eta': 0.09582997584360049, 'max_depth': 10, 'alpha': 0.6358, 'lambda': 9.063785968908524, 'max_bin': 293}. Best is trial 46 with value: 0.8775134292586342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8775\n",
      "\tBest params:\n",
      "\t\tn_estimators: 638\n",
      "\t\teta: 0.08050651458994573\n",
      "\t\tmax_depth: 10\n",
      "\t\talpha: 0.6357\n",
      "\t\tlambda: 4.145013106178915\n",
      "\t\tmax_bin: 322\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_xgb = optuna.create_study(direction='maximize', study_name=\"XGBClassifier\")\n",
    "func_xgb_0 = lambda trial: objective_xgb_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_xgb.optimize(func_xgb_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "584eb50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   43.000000\n",
      "1                    TN  193.000000\n",
      "2                    FP    6.000000\n",
      "3                    FN   26.000000\n",
      "4              Accuracy    0.880597\n",
      "5             Precision    0.877551\n",
      "6           Sensitivity    0.623188\n",
      "7           Specificity    0.969800\n",
      "8              F1 score    0.728814\n",
      "9   F1 score (weighted)    0.873335\n",
      "10     F1 score (macro)    0.826129\n",
      "11    Balanced Accuracy    0.796519\n",
      "12                  MCC    0.670831\n",
      "13                  NPV    0.881300\n",
      "14              ROC_AUC    0.796519\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_xgb_0 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    #learn\n",
    "eval_set = [(X_testSet0, Y_testSet0)]\n",
    "\n",
    "optimized_xgb_0.fit(X_trainSet0,Y_trainSet0, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "   \n",
    "y_pred_xgb_0 = optimized_xgb_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_xgb_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_xgb_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_xgb_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_xgb_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_xgb_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_xgb_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_xgb_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_xgb_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_xgb_0)\n",
    "    \n",
    "\n",
    "mat_met_xgb_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2278de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:46:56,179] Trial 50 finished with value: 0.8696238891263374 and parameters: {'n_estimators': 808, 'eta': 0.09003531366049294, 'max_depth': 6, 'alpha': 0.3286, 'lambda': 4.393300086720213, 'max_bin': 267}. Best is trial 46 with value: 0.8775134292586342.\n",
      "[I 2023-12-05 12:46:58,510] Trial 51 finished with value: 0.8735050091805328 and parameters: {'n_estimators': 278, 'eta': 0.09669330451781431, 'max_depth': 9, 'alpha': 0.6604, 'lambda': 5.962653014813037, 'max_bin': 309}. Best is trial 46 with value: 0.8775134292586342.\n",
      "[I 2023-12-05 12:47:00,693] Trial 52 finished with value: 0.8807837725009504 and parameters: {'n_estimators': 377, 'eta': 0.0898441435613207, 'max_depth': 10, 'alpha': 0.7275, 'lambda': 2.587621833048722, 'max_bin': 301}. Best is trial 52 with value: 0.8807837725009504.\n",
      "[I 2023-12-05 12:47:02,810] Trial 53 finished with value: 0.8814914375544587 and parameters: {'n_estimators': 350, 'eta': 0.09993743545046163, 'max_depth': 10, 'alpha': 0.9534, 'lambda': 2.857104754481703, 'max_bin': 368}. Best is trial 53 with value: 0.8814914375544587.\n",
      "[I 2023-12-05 12:47:04,849] Trial 54 finished with value: 0.8786773982001048 and parameters: {'n_estimators': 326, 'eta': 0.08865585076093971, 'max_depth': 11, 'alpha': 0.6017, 'lambda': 2.520386107091887, 'max_bin': 392}. Best is trial 53 with value: 0.8814914375544587.\n",
      "[I 2023-12-05 12:47:07,032] Trial 55 finished with value: 0.8810869603000103 and parameters: {'n_estimators': 338, 'eta': 0.09830279919503479, 'max_depth': 10, 'alpha': 0.6055, 'lambda': 2.365416992586674, 'max_bin': 399}. Best is trial 53 with value: 0.8814914375544587.\n",
      "[I 2023-12-05 12:47:09,088] Trial 56 finished with value: 0.8793486017997267 and parameters: {'n_estimators': 324, 'eta': 0.08958684309102152, 'max_depth': 10, 'alpha': 0.6098, 'lambda': 1.2293372690639879, 'max_bin': 368}. Best is trial 53 with value: 0.8814914375544587.\n",
      "[I 2023-12-05 12:47:11,146] Trial 57 finished with value: 0.8837729646132795 and parameters: {'n_estimators': 263, 'eta': 0.0898033323636547, 'max_depth': 9, 'alpha': 0.5934, 'lambda': 1.3992662314000142, 'max_bin': 392}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:13,199] Trial 58 finished with value: 0.8780461299188614 and parameters: {'n_estimators': 261, 'eta': 0.09102807535494713, 'max_depth': 8, 'alpha': 0.6074, 'lambda': 1.059917604343517, 'max_bin': 371}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:15,132] Trial 59 finished with value: 0.879816535179418 and parameters: {'n_estimators': 209, 'eta': 0.09979204104939846, 'max_depth': 9, 'alpha': 0.5247, 'lambda': 2.520508038567624, 'max_bin': 393}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:17,033] Trial 60 finished with value: 0.8780674737525697 and parameters: {'n_estimators': 212, 'eta': 0.0992368949659394, 'max_depth': 9, 'alpha': 0.4761, 'lambda': 2.419978676151842, 'max_bin': 414}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:18,891] Trial 61 finished with value: 0.8766947263261125 and parameters: {'n_estimators': 176, 'eta': 0.09963196918916527, 'max_depth': 8, 'alpha': 0.5431, 'lambda': 3.0288692552064456, 'max_bin': 393}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:20,872] Trial 62 finished with value: 0.878471197076131 and parameters: {'n_estimators': 326, 'eta': 0.09014692519775071, 'max_depth': 9, 'alpha': 0.605, 'lambda': 1.242659771739475, 'max_bin': 406}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:23,161] Trial 63 finished with value: 0.8809620367201134 and parameters: {'n_estimators': 330, 'eta': 0.09616131682592989, 'max_depth': 9, 'alpha': 0.6852, 'lambda': 2.75365973461021, 'max_bin': 382}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:25,188] Trial 64 finished with value: 0.8806548412451829 and parameters: {'n_estimators': 223, 'eta': 0.09641289924417537, 'max_depth': 9, 'alpha': 0.6844, 'lambda': 2.9477889462124702, 'max_bin': 371}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:26,561] Trial 65 finished with value: 0.8771341738452565 and parameters: {'n_estimators': 115, 'eta': 0.09625979929170875, 'max_depth': 8, 'alpha': 0.6975, 'lambda': 4.64182110024999, 'max_bin': 384}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:28,662] Trial 66 finished with value: 0.8822076130407615 and parameters: {'n_estimators': 261, 'eta': 0.09650221324030152, 'max_depth': 9, 'alpha': 0.7318, 'lambda': 2.8642062242341026, 'max_bin': 424}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:30,802] Trial 67 finished with value: 0.8792060364896919 and parameters: {'n_estimators': 265, 'eta': 0.09642495408917216, 'max_depth': 9, 'alpha': 0.7256, 'lambda': 3.254336352608303, 'max_bin': 428}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:32,939] Trial 68 finished with value: 0.8750277757699779 and parameters: {'n_estimators': 236, 'eta': 0.08562414262938399, 'max_depth': 8, 'alpha': 0.8115, 'lambda': 4.5365571086484895, 'max_bin': 384}. Best is trial 57 with value: 0.8837729646132795.\n",
      "[I 2023-12-05 12:47:34,974] Trial 69 finished with value: 0.8840944545088343 and parameters: {'n_estimators': 290, 'eta': 0.0966020417130898, 'max_depth': 9, 'alpha': 0.0002, 'lambda': 3.2092975337367795, 'max_bin': 407}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:37,145] Trial 70 finished with value: 0.8783651931740192 and parameters: {'n_estimators': 282, 'eta': 0.0920977006809807, 'max_depth': 9, 'alpha': 0.2336, 'lambda': 5.067748749726189, 'max_bin': 428}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:39,033] Trial 71 finished with value: 0.8746033143739117 and parameters: {'n_estimators': 354, 'eta': 0.09618888586241732, 'max_depth': 9, 'alpha': 0.007200000000000001, 'lambda': 3.1687312130070944, 'max_bin': 404}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:40,980] Trial 72 finished with value: 0.8684925485921117 and parameters: {'n_estimators': 256, 'eta': 0.09668604683780836, 'max_depth': 9, 'alpha': 0.7250000000000001, 'lambda': 2.1898285268514717, 'max_bin': 412}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:42,773] Trial 73 finished with value: 0.8824159097216884 and parameters: {'n_estimators': 301, 'eta': 0.09996059293571063, 'max_depth': 8, 'alpha': 0.6755, 'lambda': 1.0198057965578393, 'max_bin': 361}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:44,842] Trial 74 finished with value: 0.8791956516385515 and parameters: {'n_estimators': 372, 'eta': 0.09159398202619679, 'max_depth': 7, 'alpha': 0.6625, 'lambda': 1.3715081256060329, 'max_bin': 354}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:47,558] Trial 75 finished with value: 0.8814071902878678 and parameters: {'n_estimators': 304, 'eta': 0.08403747282967093, 'max_depth': 8, 'alpha': 0.7828, 'lambda': 7.695164347065565, 'max_bin': 361}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:49,858] Trial 76 finished with value: 0.8783262762019568 and parameters: {'n_estimators': 300, 'eta': 0.09306590162607332, 'max_depth': 8, 'alpha': 0.7963, 'lambda': 7.622796556586222, 'max_bin': 377}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:51,635] Trial 77 finished with value: 0.8685631746840772 and parameters: {'n_estimators': 179, 'eta': 0.09994631398912013, 'max_depth': 7, 'alpha': 0.7885000000000001, 'lambda': 5.334194465540599, 'max_bin': 361}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:53,872] Trial 78 finished with value: 0.8795006767711673 and parameters: {'n_estimators': 331, 'eta': 0.08453467099652731, 'max_depth': 8, 'alpha': 0.8846, 'lambda': 3.740731180124692, 'max_bin': 344}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:56,443] Trial 79 finished with value: 0.8785489019915824 and parameters: {'n_estimators': 299, 'eta': 0.09368857887638875, 'max_depth': 8, 'alpha': 0.5807, 'lambda': 6.651434570647444, 'max_bin': 423}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:47:58,845] Trial 80 finished with value: 0.8691145762925986 and parameters: {'n_estimators': 342, 'eta': 0.08662328962516148, 'max_depth': 8, 'alpha': 0.8305, 'lambda': 4.383054993077238, 'max_bin': 441}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:01,085] Trial 81 finished with value: 0.880404455241667 and parameters: {'n_estimators': 276, 'eta': 0.0977813296095494, 'max_depth': 9, 'alpha': 0.7415, 'lambda': 2.081443124687617, 'max_bin': 386}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:03,290] Trial 82 finished with value: 0.8766468243011165 and parameters: {'n_estimators': 382, 'eta': 0.09428747582698094, 'max_depth': 10, 'alpha': 0.6804, 'lambda': 3.5352164491510294, 'max_bin': 409}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:05,242] Trial 83 finished with value: 0.879203744229916 and parameters: {'n_estimators': 306, 'eta': 0.08868515373582762, 'max_depth': 10, 'alpha': 0.6312, 'lambda': 1.0581322661597292, 'max_bin': 401}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:07,462] Trial 84 finished with value: 0.8745559814357113 and parameters: {'n_estimators': 238, 'eta': 0.0909074744393099, 'max_depth': 9, 'alpha': 0.7109000000000001, 'lambda': 5.349968382895723, 'max_bin': 399}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:09,734] Trial 85 finished with value: 0.8767014410723022 and parameters: {'n_estimators': 367, 'eta': 0.08344242497542632, 'max_depth': 8, 'alpha': 0.6677000000000001, 'lambda': 1.9010748067225547, 'max_bin': 451}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:11,989] Trial 86 finished with value: 0.883867976854327 and parameters: {'n_estimators': 342, 'eta': 0.0938022467932445, 'max_depth': 9, 'alpha': 0.7704000000000001, 'lambda': 3.033513844102958, 'max_bin': 378}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:13,700] Trial 87 finished with value: 0.8800562006160499 and parameters: {'n_estimators': 145, 'eta': 0.09775814339797471, 'max_depth': 9, 'alpha': 0.7788, 'lambda': 7.115837276219436, 'max_bin': 356}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:15,638] Trial 88 finished with value: 0.8770828319436179 and parameters: {'n_estimators': 198, 'eta': 0.09402550971063221, 'max_depth': 7, 'alpha': 0.5674, 'lambda': 5.542938653438956, 'max_bin': 376}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:17,716] Trial 89 finished with value: 0.8800461471969498 and parameters: {'n_estimators': 250, 'eta': 0.09822669365765858, 'max_depth': 9, 'alpha': 0.17, 'lambda': 3.870399771184995, 'max_bin': 388}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:20,322] Trial 90 finished with value: 0.8774981425254393 and parameters: {'n_estimators': 343, 'eta': 0.08760927357379461, 'max_depth': 8, 'alpha': 0.6474000000000001, 'lambda': 8.019288963162948, 'max_bin': 364}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:22,548] Trial 91 finished with value: 0.8785691174960446 and parameters: {'n_estimators': 315, 'eta': 0.0914147372121944, 'max_depth': 10, 'alpha': 0.7239, 'lambda': 2.9147710485485123, 'max_bin': 381}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:24,634] Trial 92 finished with value: 0.8831464811483599 and parameters: {'n_estimators': 424, 'eta': 0.09491551219931167, 'max_depth': 10, 'alpha': 0.7508, 'lambda': 2.103279530343506, 'max_bin': 339}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:26,718] Trial 93 finished with value: 0.874475144585914 and parameters: {'n_estimators': 417, 'eta': 0.09494175031113375, 'max_depth': 9, 'alpha': 0.9599000000000001, 'lambda': 1.7828621003377008, 'max_bin': 352}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:29,103] Trial 94 finished with value: 0.8828665601136138 and parameters: {'n_estimators': 288, 'eta': 0.09460947047081093, 'max_depth': 9, 'alpha': 0.7486, 'lambda': 4.477380125257676, 'max_bin': 341}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:31,497] Trial 95 finished with value: 0.8760242914394514 and parameters: {'n_estimators': 287, 'eta': 0.09976423816296927, 'max_depth': 10, 'alpha': 0.7549, 'lambda': 4.043368654015957, 'max_bin': 338}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:33,967] Trial 96 finished with value: 0.8769539708843869 and parameters: {'n_estimators': 442, 'eta': 0.09417190846859214, 'max_depth': 8, 'alpha': 0.8285, 'lambda': 6.194285643942642, 'max_bin': 341}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:36,250] Trial 97 finished with value: 0.8787093952328087 and parameters: {'n_estimators': 271, 'eta': 0.09786486296113633, 'max_depth': 9, 'alpha': 0.894, 'lambda': 4.422006208783966, 'max_bin': 346}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:38,314] Trial 98 finished with value: 0.8766044727882354 and parameters: {'n_estimators': 227, 'eta': 0.09232723664338129, 'max_depth': 10, 'alpha': 0.7772, 'lambda': 1.7062191840249512, 'max_bin': 328}. Best is trial 69 with value: 0.8840944545088343.\n",
      "[I 2023-12-05 12:48:40,694] Trial 99 finished with value: 0.8849347158637322 and parameters: {'n_estimators': 310, 'eta': 0.08977155105733017, 'max_depth': 9, 'alpha': 0.8714000000000001, 'lambda': 3.3899853772465174, 'max_bin': 366}. Best is trial 99 with value: 0.8849347158637322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8849\n",
      "\tBest params:\n",
      "\t\tn_estimators: 310\n",
      "\t\teta: 0.08977155105733017\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.8714000000000001\n",
      "\t\tlambda: 3.3899853772465174\n",
      "\t\tmax_bin: 366\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_xgb_1 = lambda trial: objective_xgb_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_xgb.optimize(func_xgb_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "565b2677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   43.000000   48.000000\n",
      "1                    TN  193.000000  189.000000\n",
      "2                    FP    6.000000   11.000000\n",
      "3                    FN   26.000000   20.000000\n",
      "4              Accuracy    0.880597    0.884328\n",
      "5             Precision    0.877551    0.813559\n",
      "6           Sensitivity    0.623188    0.705882\n",
      "7           Specificity    0.969800    0.945000\n",
      "8              F1 score    0.728814    0.755906\n",
      "9   F1 score (weighted)    0.873335    0.881502\n",
      "10     F1 score (macro)    0.826129    0.840055\n",
      "11    Balanced Accuracy    0.796519    0.825441\n",
      "12                  MCC    0.670831    0.683554\n",
      "13                  NPV    0.881300    0.904300\n",
      "14              ROC_AUC    0.796519    0.825441\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_1 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet1, Y_testSet1)]\n",
    "optimized_xgb_1.fit(X_trainSet1,Y_trainSet1, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_1 = optimized_xgb_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_xgb_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_xgb_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_xgb_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_xgb_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_xgb_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_xgb_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_xgb_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_xgb_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_xgb_1)\n",
    "\n",
    "\n",
    "set1 = pd.DataFrame({ 'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set1'] =set1\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33fb1804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:48:43,303] Trial 100 finished with value: 0.8708402374891877 and parameters: {'n_estimators': 312, 'eta': 0.08762674398415206, 'max_depth': 9, 'alpha': 0.8541000000000001, 'lambda': 5.117903470250692, 'max_bin': 360}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:48:45,050] Trial 101 finished with value: 0.8602040406171231 and parameters: {'n_estimators': 295, 'eta': 0.09518543479241434, 'max_depth': 9, 'alpha': 0.9186000000000001, 'lambda': 1.0183336310672595, 'max_bin': 368}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:48:47,127] Trial 102 finished with value: 0.8732466924885809 and parameters: {'n_estimators': 352, 'eta': 0.08971212706033141, 'max_depth': 10, 'alpha': 0.9539000000000001, 'lambda': 3.352262838517836, 'max_bin': 349}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:48:49,070] Trial 103 finished with value: 0.868108479910698 and parameters: {'n_estimators': 254, 'eta': 0.0921365216426475, 'max_depth': 9, 'alpha': 0.8689, 'lambda': 2.6354177037703117, 'max_bin': 375}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:48:51,183] Trial 104 finished with value: 0.8731222642901606 and parameters: {'n_estimators': 395, 'eta': 0.09778550445301962, 'max_depth': 8, 'alpha': 0.8151, 'lambda': 1.9609657846266848, 'max_bin': 396}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:48:53,345] Trial 105 finished with value: 0.8616546236472237 and parameters: {'n_estimators': 287, 'eta': 0.08601010333898895, 'max_depth': 9, 'alpha': 0.7063, 'lambda': 4.568462682845035, 'max_bin': 419}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:48:55,684] Trial 106 finished with value: 0.8647926857532309 and parameters: {'n_estimators': 330, 'eta': 0.09435524480154908, 'max_depth': 10, 'alpha': 0.7429, 'lambda': 6.435568350514753, 'max_bin': 338}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:48:57,807] Trial 107 finished with value: 0.8667899255384823 and parameters: {'n_estimators': 368, 'eta': 0.09057986564427455, 'max_depth': 9, 'alpha': 0.7954, 'lambda': 3.5677241008201452, 'max_bin': 389}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:00,041] Trial 108 finished with value: 0.8661649344836603 and parameters: {'n_estimators': 270, 'eta': 0.09824347681518716, 'max_depth': 8, 'alpha': 0.875, 'lambda': 5.986096643020967, 'max_bin': 364}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:01,895] Trial 109 finished with value: 0.8682814770419165 and parameters: {'n_estimators': 343, 'eta': 0.08798758606840275, 'max_depth': 6, 'alpha': 0.7678, 'lambda': 1.9538309080438037, 'max_bin': 434}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:04,253] Trial 110 finished with value: 0.8664621032486302 and parameters: {'n_estimators': 458, 'eta': 0.08147718008202393, 'max_depth': 10, 'alpha': 0.9071, 'lambda': 2.9135527803080343, 'max_bin': 372}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:06,295] Trial 111 finished with value: 0.8720292536258896 and parameters: {'n_estimators': 314, 'eta': 0.09596072177496744, 'max_depth': 9, 'alpha': 0.7029000000000001, 'lambda': 2.722480571595267, 'max_bin': 380}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:08,326] Trial 112 finished with value: 0.8694416899579673 and parameters: {'n_estimators': 334, 'eta': 0.09992566556187801, 'max_depth': 9, 'alpha': 0.6783, 'lambda': 4.007803619146124, 'max_bin': 358}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:10,515] Trial 113 finished with value: 0.8606857575127694 and parameters: {'n_estimators': 292, 'eta': 0.09625201258586807, 'max_depth': 9, 'alpha': 0.743, 'lambda': 5.133347025266385, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:12,873] Trial 114 finished with value: 0.8682698836573776 and parameters: {'n_estimators': 421, 'eta': 0.0844464671212903, 'max_depth': 8, 'alpha': 0.6914, 'lambda': 3.2516814159466207, 'max_bin': 397}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:14,832] Trial 115 finished with value: 0.8664659670538668 and parameters: {'n_estimators': 252, 'eta': 0.09305689539625879, 'max_depth': 9, 'alpha': 0.8355, 'lambda': 1.8329043312575066, 'max_bin': 375}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:17,024] Trial 116 finished with value: 0.8683544367304824 and parameters: {'n_estimators': 357, 'eta': 0.09748065219132493, 'max_depth': 9, 'alpha': 0.6254000000000001, 'lambda': 4.7371611376412766, 'max_bin': 405}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:18,755] Trial 117 finished with value: 0.8684958324142148 and parameters: {'n_estimators': 214, 'eta': 0.0954410679508548, 'max_depth': 10, 'alpha': 0.658, 'lambda': 1.0812351102614253, 'max_bin': 409}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:20,742] Trial 118 finished with value: 0.8679800672138137 and parameters: {'n_estimators': 381, 'eta': 0.0910919726289153, 'max_depth': 9, 'alpha': 0.8104, 'lambda': 2.5498535944985212, 'max_bin': 350}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:22,949] Trial 119 finished with value: 0.863864634628014 and parameters: {'n_estimators': 321, 'eta': 0.09320032354491037, 'max_depth': 8, 'alpha': 0.5218, 'lambda': 7.153557315906555, 'max_bin': 391}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:25,267] Trial 120 finished with value: 0.873620550161563 and parameters: {'n_estimators': 271, 'eta': 0.089408560783087, 'max_depth': 10, 'alpha': 0.7389, 'lambda': 3.776535675684374, 'max_bin': 382}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:27,491] Trial 121 finished with value: 0.8661704792675305 and parameters: {'n_estimators': 377, 'eta': 0.08954100234346499, 'max_depth': 10, 'alpha': 0.7745000000000001, 'lambda': 2.4598103359764787, 'max_bin': 320}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:29,626] Trial 122 finished with value: 0.8784544927884552 and parameters: {'n_estimators': 310, 'eta': 0.0999545041519948, 'max_depth': 9, 'alpha': 0.7249, 'lambda': 3.0380656413947795, 'max_bin': 368}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:31,876] Trial 123 finished with value: 0.8668045414101833 and parameters: {'n_estimators': 340, 'eta': 0.09498249051596133, 'max_depth': 10, 'alpha': 0.6463, 'lambda': 4.29582457847116, 'max_bin': 355}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:33,814] Trial 124 finished with value: 0.8720551191020938 and parameters: {'n_estimators': 394, 'eta': 0.08605997837447664, 'max_depth': 10, 'alpha': 0.5917, 'lambda': 1.0251092795957584, 'max_bin': 379}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:35,723] Trial 125 finished with value: 0.86606990527787 and parameters: {'n_estimators': 361, 'eta': 0.09706977452260004, 'max_depth': 9, 'alpha': 0.752, 'lambda': 2.103759810450167, 'max_bin': 387}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:37,863] Trial 126 finished with value: 0.8694285368520422 and parameters: {'n_estimators': 236, 'eta': 0.08311936236767153, 'max_depth': 9, 'alpha': 0.7025, 'lambda': 5.710166771958518, 'max_bin': 418}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:39,920] Trial 127 finished with value: 0.8634062810364138 and parameters: {'n_estimators': 408, 'eta': 0.09240913756475214, 'max_depth': 8, 'alpha': 0.06380000000000001, 'lambda': 3.335729067143654, 'max_bin': 371}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:41,959] Trial 128 finished with value: 0.8720317588658248 and parameters: {'n_estimators': 297, 'eta': 0.09088934888061592, 'max_depth': 10, 'alpha': 0.7869, 'lambda': 1.782166447336824, 'max_bin': 426}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:44,098] Trial 129 finished with value: 0.8724731232987255 and parameters: {'n_estimators': 318, 'eta': 0.0980242838246644, 'max_depth': 9, 'alpha': 0.6772, 'lambda': 4.792942658408338, 'max_bin': 361}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:45,938] Trial 130 finished with value: 0.8696181773567669 and parameters: {'n_estimators': 281, 'eta': 0.094292494326119, 'max_depth': 11, 'alpha': 0.29200000000000004, 'lambda': 2.6631276990788857, 'max_bin': 396}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:47,912] Trial 131 finished with value: 0.8755988699331189 and parameters: {'n_estimators': 193, 'eta': 0.09614057421469058, 'max_depth': 9, 'alpha': 0.7344, 'lambda': 3.820639772375782, 'max_bin': 413}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:49,993] Trial 132 finished with value: 0.8758300402542358 and parameters: {'n_estimators': 257, 'eta': 0.09692132608967217, 'max_depth': 9, 'alpha': 0.6188, 'lambda': 3.1327047058894912, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:51,933] Trial 133 finished with value: 0.8696511190577925 and parameters: {'n_estimators': 236, 'eta': 0.08779813717132307, 'max_depth': 9, 'alpha': 0.6875, 'lambda': 1.7671002370288926, 'max_bin': 402}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:53,949] Trial 134 finished with value: 0.8696291284016227 and parameters: {'n_estimators': 220, 'eta': 0.09295601477741706, 'max_depth': 10, 'alpha': 0.7082, 'lambda': 2.568183721156433, 'max_bin': 376}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:55,956] Trial 135 finished with value: 0.8695856052180393 and parameters: {'n_estimators': 303, 'eta': 0.09803332367859063, 'max_depth': 8, 'alpha': 0.7632, 'lambda': 4.263579637004987, 'max_bin': 383}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:57,553] Trial 136 finished with value: 0.862065873388557 and parameters: {'n_estimators': 346, 'eta': 0.09517051915106119, 'max_depth': 9, 'alpha': 0.5658000000000001, 'lambda': 1.0429057154610746, 'max_bin': 302}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:49:59,990] Trial 137 finished with value: 0.8685460062908632 and parameters: {'n_estimators': 329, 'eta': 0.08919658350481052, 'max_depth': 9, 'alpha': 0.6622, 'lambda': 4.9884260235795495, 'max_bin': 359}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:01,886] Trial 138 finished with value: 0.8734589705419827 and parameters: {'n_estimators': 494, 'eta': 0.09242705743982152, 'max_depth': 8, 'alpha': 0.41650000000000004, 'lambda': 3.4451748026308215, 'max_bin': 332}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:04,686] Trial 139 finished with value: 0.8694425162345881 and parameters: {'n_estimators': 433, 'eta': 0.07860472895368308, 'max_depth': 10, 'alpha': 0.7139, 'lambda': 5.94537790469067, 'max_bin': 346}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:06,583] Trial 140 finished with value: 0.8656515655508812 and parameters: {'n_estimators': 277, 'eta': 0.09096379652976176, 'max_depth': 10, 'alpha': 0.9436, 'lambda': 2.3256959051386428, 'max_bin': 326}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:08,575] Trial 141 finished with value: 0.8703004756845963 and parameters: {'n_estimators': 264, 'eta': 0.09799536218249741, 'max_depth': 9, 'alpha': 0.7396, 'lambda': 2.2673361101348517, 'max_bin': 387}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:10,301] Trial 142 finished with value: 0.8652414707346547 and parameters: {'n_estimators': 281, 'eta': 0.09596487877502886, 'max_depth': 9, 'alpha': 0.8169000000000001, 'lambda': 1.5977965948260275, 'max_bin': 373}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:12,387] Trial 143 finished with value: 0.872860148170067 and parameters: {'n_estimators': 367, 'eta': 0.09925708181470078, 'max_depth': 9, 'alpha': 0.7546, 'lambda': 3.04831158843342, 'max_bin': 386}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:14,491] Trial 144 finished with value: 0.8624945698688158 and parameters: {'n_estimators': 297, 'eta': 0.09386042974065653, 'max_depth': 9, 'alpha': 0.686, 'lambda': 3.887454084024939, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:16,341] Trial 145 finished with value: 0.8607717078206492 and parameters: {'n_estimators': 244, 'eta': 0.0971228848591448, 'max_depth': 9, 'alpha': 0.7255, 'lambda': 1.7177826586857334, 'max_bin': 392}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:18,399] Trial 146 finished with value: 0.8658415131534134 and parameters: {'n_estimators': 314, 'eta': 0.09487979834657884, 'max_depth': 9, 'alpha': 0.7971, 'lambda': 4.996103826592814, 'max_bin': 434}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:20,243] Trial 147 finished with value: 0.8643181823518713 and parameters: {'n_estimators': 896, 'eta': 0.09978844064766036, 'max_depth': 8, 'alpha': 0.7706000000000001, 'lambda': 2.6926356939010097, 'max_bin': 354}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:22,756] Trial 148 finished with value: 0.8646364298307609 and parameters: {'n_estimators': 335, 'eta': 0.09152085090646157, 'max_depth': 10, 'alpha': 0.638, 'lambda': 10.763596091766583, 'max_bin': 288}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:25,568] Trial 149 finished with value: 0.8679191443513685 and parameters: {'n_estimators': 265, 'eta': 0.05332753963303556, 'max_depth': 9, 'alpha': 0.9769000000000001, 'lambda': 4.114044893993585, 'max_bin': 379}. Best is trial 99 with value: 0.8849347158637322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8849\n",
      "\tBest params:\n",
      "\t\tn_estimators: 310\n",
      "\t\teta: 0.08977155105733017\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.8714000000000001\n",
      "\t\tlambda: 3.3899853772465174\n",
      "\t\tmax_bin: 366\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_2 = lambda trial: objective_xgb_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_xgb.optimize(func_xgb_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4c671e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   43.000000   48.000000   50.000000\n",
      "1                    TN  193.000000  189.000000  190.000000\n",
      "2                    FP    6.000000   11.000000   11.000000\n",
      "3                    FN   26.000000   20.000000   17.000000\n",
      "4              Accuracy    0.880597    0.884328    0.895522\n",
      "5             Precision    0.877551    0.813559    0.819672\n",
      "6           Sensitivity    0.623188    0.705882    0.746269\n",
      "7           Specificity    0.969800    0.945000    0.945300\n",
      "8              F1 score    0.728814    0.755906    0.781250\n",
      "9   F1 score (weighted)    0.873335    0.881502    0.893842\n",
      "10     F1 score (macro)    0.826129    0.840055    0.856311\n",
      "11    Balanced Accuracy    0.796519    0.825441    0.845771\n",
      "12                  MCC    0.670831    0.683554    0.714174\n",
      "13                  NPV    0.881300    0.904300    0.917900\n",
      "14              ROC_AUC    0.796519    0.825441    0.845771\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_2 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet2, Y_testSet2)]\n",
    "optimized_xgb_2.fit(X_trainSet2,Y_trainSet2, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_2 = optimized_xgb_2.predict(X_testSet2)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_xgb_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_xgb_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_xgb_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_xgb_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_xgb_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_xgb_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_xgb_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_xgb_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_xgb_2)\n",
    "\n",
    "\n",
    "Set2 = pd.DataFrame({ 'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set2'] =Set2\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c547ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:50:28,283] Trial 150 finished with value: 0.872948344240881 and parameters: {'n_estimators': 390, 'eta': 0.08514780734556135, 'max_depth': 9, 'alpha': 0.6734, 'lambda': 6.539082104824078, 'max_bin': 364}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:30,076] Trial 151 finished with value: 0.864074778355965 and parameters: {'n_estimators': 172, 'eta': 0.09813790835597427, 'max_depth': 9, 'alpha': 0.7932, 'lambda': 3.31245166910102, 'max_bin': 357}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:31,330] Trial 152 finished with value: 0.8591861006399185 and parameters: {'n_estimators': 99, 'eta': 0.0967052027709819, 'max_depth': 9, 'alpha': 0.8479, 'lambda': 7.460190037339687, 'max_bin': 340}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:33,272] Trial 153 finished with value: 0.8619485318231117 and parameters: {'n_estimators': 189, 'eta': 0.09415383408592695, 'max_depth': 9, 'alpha': 0.7748, 'lambda': 8.888237172943905, 'max_bin': 350}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:34,765] Trial 154 finished with value: 0.8656979621369982 and parameters: {'n_estimators': 149, 'eta': 0.09993843617664426, 'max_depth': 10, 'alpha': 0.7526, 'lambda': 1.007779829493075, 'max_bin': 372}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:35,674] Trial 155 finished with value: 0.8654794988628014 and parameters: {'n_estimators': 68, 'eta': 0.09729597998373589, 'max_depth': 9, 'alpha': 0.4656, 'lambda': 2.388138701419802, 'max_bin': 362}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:38,375] Trial 156 finished with value: 0.8669569238270066 and parameters: {'n_estimators': 288, 'eta': 0.06353084501008921, 'max_depth': 8, 'alpha': 0.7186, 'lambda': 8.029645925218317, 'max_bin': 399}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:40,952] Trial 157 finished with value: 0.8578861382099447 and parameters: {'n_estimators': 354, 'eta': 0.08903036033270502, 'max_depth': 9, 'alpha': 0.6988000000000001, 'lambda': 13.392425911644036, 'max_bin': 384}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:42,954] Trial 158 finished with value: 0.8668864222182439 and parameters: {'n_estimators': 214, 'eta': 0.09307747132234816, 'max_depth': 9, 'alpha': 0.7335, 'lambda': 5.7291705278816965, 'max_bin': 409}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:45,171] Trial 159 finished with value: 0.8686383260506467 and parameters: {'n_estimators': 308, 'eta': 0.07472772269588047, 'max_depth': 10, 'alpha': 0.8139000000000001, 'lambda': 4.307516153058778, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:47,004] Trial 160 finished with value: 0.8656164171664116 and parameters: {'n_estimators': 330, 'eta': 0.09575308768184583, 'max_depth': 9, 'alpha': 0.7862, 'lambda': 1.8419661399489011, 'max_bin': 357}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:48,553] Trial 161 finished with value: 0.8649093837188774 and parameters: {'n_estimators': 138, 'eta': 0.09726789739529004, 'max_depth': 9, 'alpha': 0.1588, 'lambda': 3.555951501608707, 'max_bin': 387}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:50,116] Trial 162 finished with value: 0.8632928559211619 and parameters: {'n_estimators': 248, 'eta': 0.09806861856273957, 'max_depth': 9, 'alpha': 0.016300000000000002, 'lambda': 2.9355433256107446, 'max_bin': 391}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:52,157] Trial 163 finished with value: 0.8642278049126813 and parameters: {'n_estimators': 228, 'eta': 0.09358519153884928, 'max_depth': 9, 'alpha': 0.1905, 'lambda': 7.018231539701581, 'max_bin': 376}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:54,076] Trial 164 finished with value: 0.8672356515196968 and parameters: {'n_estimators': 273, 'eta': 0.09517108670450179, 'max_depth': 8, 'alpha': 0.0358, 'lambda': 5.294267174583011, 'max_bin': 396}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:55,660] Trial 165 finished with value: 0.8689708744221635 and parameters: {'n_estimators': 158, 'eta': 0.09111528279283329, 'max_depth': 9, 'alpha': 0.0966, 'lambda': 3.8191020293961015, 'max_bin': 381}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:57,059] Trial 166 finished with value: 0.866910644500835 and parameters: {'n_estimators': 253, 'eta': 0.09990289804768784, 'max_depth': 9, 'alpha': 0.0816, 'lambda': 2.1374159417607657, 'max_bin': 475}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:50:59,287] Trial 167 finished with value: 0.8716396186483424 and parameters: {'n_estimators': 290, 'eta': 0.0686702504727339, 'max_depth': 10, 'alpha': 0.13620000000000002, 'lambda': 4.550495764629417, 'max_bin': 376}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:01,000] Trial 168 finished with value: 0.8675195992089304 and parameters: {'n_estimators': 310, 'eta': 0.098358978223722, 'max_depth': 9, 'alpha': 0.3527, 'lambda': 3.0318297986903784, 'max_bin': 406}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:02,699] Trial 169 finished with value: 0.8663970343709891 and parameters: {'n_estimators': 344, 'eta': 0.09563613122862435, 'max_depth': 10, 'alpha': 0.7526, 'lambda': 1.9303815274061478, 'max_bin': 391}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:05,802] Trial 170 finished with value: 0.8653171668557823 and parameters: {'n_estimators': 369, 'eta': 0.08981105470535808, 'max_depth': 11, 'alpha': 0.6924, 'lambda': 22.455428890320036, 'max_bin': 353}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:07,495] Trial 171 finished with value: 0.861973714499819 and parameters: {'n_estimators': 228, 'eta': 0.09990666973411533, 'max_depth': 9, 'alpha': 0.5517000000000001, 'lambda': 2.6085525444521114, 'max_bin': 401}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:09,322] Trial 172 finished with value: 0.8666117864542235 and parameters: {'n_estimators': 206, 'eta': 0.09786434414833454, 'max_depth': 9, 'alpha': 0.5918, 'lambda': 3.4947508984670086, 'max_bin': 395}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:10,910] Trial 173 finished with value: 0.8752985188885685 and parameters: {'n_estimators': 246, 'eta': 0.0928001861595032, 'max_depth': 9, 'alpha': 0.21130000000000002, 'lambda': 1.0002682533608744, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:12,694] Trial 174 finished with value: 0.8672396436222864 and parameters: {'n_estimators': 269, 'eta': 0.0957718382901886, 'max_depth': 7, 'alpha': 0.2913, 'lambda': 4.140288697753891, 'max_bin': 315}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:14,621] Trial 175 finished with value: 0.8716751624449122 and parameters: {'n_estimators': 286, 'eta': 0.08713711807224608, 'max_depth': 9, 'alpha': 0.5219, 'lambda': 2.241599975743579, 'max_bin': 387}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:16,370] Trial 176 finished with value: 0.863817541814537 and parameters: {'n_estimators': 203, 'eta': 0.09819043376746242, 'max_depth': 8, 'alpha': 0.49350000000000005, 'lambda': 2.8475218674961384, 'max_bin': 380}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:18,258] Trial 177 finished with value: 0.8657223259207365 and parameters: {'n_estimators': 853, 'eta': 0.0940209520320371, 'max_depth': 9, 'alpha': 0.2654, 'lambda': 5.044654004300092, 'max_bin': 258}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:21,854] Trial 178 finished with value: 0.8602539068302896 and parameters: {'n_estimators': 327, 'eta': 0.04207272676282625, 'max_depth': 10, 'alpha': 0.6551, 'lambda': 10.071723528567217, 'max_bin': 337}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:23,529] Trial 179 finished with value: 0.8682736881454394 and parameters: {'n_estimators': 307, 'eta': 0.09663379369630348, 'max_depth': 9, 'alpha': 0.6182000000000001, 'lambda': 1.7891946838188104, 'max_bin': 344}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:25,446] Trial 180 finished with value: 0.8646181004196063 and parameters: {'n_estimators': 262, 'eta': 0.091620420710692, 'max_depth': 9, 'alpha': 0.7269, 'lambda': 3.437568651760316, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:27,459] Trial 181 finished with value: 0.8641621070379557 and parameters: {'n_estimators': 325, 'eta': 0.08493194549866107, 'max_depth': 8, 'alpha': 0.9014000000000001, 'lambda': 3.829639687113053, 'max_bin': 343}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:30,192] Trial 182 finished with value: 0.8657650809006737 and parameters: {'n_estimators': 352, 'eta': 0.08142650315292727, 'max_depth': 8, 'alpha': 0.9195000000000001, 'lambda': 11.820098968243324, 'max_bin': 359}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:32,814] Trial 183 finished with value: 0.8661799029979628 and parameters: {'n_estimators': 299, 'eta': 0.08760344021326201, 'max_depth': 8, 'alpha': 0.8639, 'lambda': 16.407898650890097, 'max_bin': 333}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:34,657] Trial 184 finished with value: 0.8653231393822212 and parameters: {'n_estimators': 376, 'eta': 0.09983468795556544, 'max_depth': 8, 'alpha': 0.8802000000000001, 'lambda': 2.527560725545671, 'max_bin': 346}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:36,414] Trial 185 finished with value: 0.8696055644871106 and parameters: {'n_estimators': 333, 'eta': 0.09511330944147482, 'max_depth': 9, 'alpha': 0.7738, 'lambda': 1.5352143086318848, 'max_bin': 350}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:38,673] Trial 186 finished with value: 0.8656917337531965 and parameters: {'n_estimators': 277, 'eta': 0.08316333227649755, 'max_depth': 8, 'alpha': 0.9769000000000001, 'lambda': 4.6090000284475, 'max_bin': 373}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:40,592] Trial 187 finished with value: 0.8581870988811012 and parameters: {'n_estimators': 315, 'eta': 0.09757092356350364, 'max_depth': 10, 'alpha': 0.8397, 'lambda': 3.3579126660220373, 'max_bin': 449}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:41,966] Trial 188 finished with value: 0.8665022833448776 and parameters: {'n_estimators': 109, 'eta': 0.09250289956589874, 'max_depth': 9, 'alpha': 0.7157, 'lambda': 6.630836587938047, 'max_bin': 390}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:44,279] Trial 189 finished with value: 0.8730904696718783 and parameters: {'n_estimators': 407, 'eta': 0.08997566604641997, 'max_depth': 9, 'alpha': 0.7396, 'lambda': 5.656406040714419, 'max_bin': 382}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:46,243] Trial 190 finished with value: 0.8716142928408847 and parameters: {'n_estimators': 239, 'eta': 0.09599789856568769, 'max_depth': 9, 'alpha': 0.7582, 'lambda': 4.053790996862621, 'max_bin': 362}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:48,222] Trial 191 finished with value: 0.8688829934571045 and parameters: {'n_estimators': 346, 'eta': 0.07967499632667227, 'max_depth': 10, 'alpha': 0.5718, 'lambda': 1.3374632170005072, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:50,182] Trial 192 finished with value: 0.8629947485413487 and parameters: {'n_estimators': 299, 'eta': 0.08620502148145953, 'max_depth': 10, 'alpha': 0.6197, 'lambda': 2.3394451092776833, 'max_bin': 270}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:52,115] Trial 193 finished with value: 0.873005609754601 and parameters: {'n_estimators': 319, 'eta': 0.08859407081708046, 'max_depth': 10, 'alpha': 0.6057, 'lambda': 2.8840410675559087, 'max_bin': 356}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:53,670] Trial 194 finished with value: 0.8688085514767488 and parameters: {'n_estimators': 362, 'eta': 0.09370198861066603, 'max_depth': 10, 'alpha': 0.6457, 'lambda': 1.0073931069023232, 'max_bin': 369}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:55,451] Trial 195 finished with value: 0.8695263710630053 and parameters: {'n_estimators': 290, 'eta': 0.0983527843695244, 'max_depth': 9, 'alpha': 0.664, 'lambda': 1.8833792509587188, 'max_bin': 377}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:57,481] Trial 196 finished with value: 0.87143436957226 and parameters: {'n_estimators': 335, 'eta': 0.09130527729431627, 'max_depth': 9, 'alpha': 0.7010000000000001, 'lambda': 3.005116427435021, 'max_bin': 403}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:51:59,365] Trial 197 finished with value: 0.867692829180769 and parameters: {'n_estimators': 221, 'eta': 0.09505516048473088, 'max_depth': 10, 'alpha': 0.7927000000000001, 'lambda': 3.815840231664028, 'max_bin': 395}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:01,139] Trial 198 finished with value: 0.8741729940877244 and parameters: {'n_estimators': 258, 'eta': 0.09987371593860309, 'max_depth': 9, 'alpha': 0.9992000000000001, 'lambda': 1.8466967014945355, 'max_bin': 385}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:03,022] Trial 199 finished with value: 0.8629854731357675 and parameters: {'n_estimators': 183, 'eta': 0.09711646999440475, 'max_depth': 9, 'alpha': 0.5298, 'lambda': 4.669486950156433, 'max_bin': 416}. Best is trial 99 with value: 0.8849347158637322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8849\n",
      "\tBest params:\n",
      "\t\tn_estimators: 310\n",
      "\t\teta: 0.08977155105733017\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.8714000000000001\n",
      "\t\tlambda: 3.3899853772465174\n",
      "\t\tmax_bin: 366\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_3 = lambda trial: objective_xgb_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_xgb.optimize(func_xgb_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0b40dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   43.000000   48.000000   50.000000   47.000000\n",
      "1                    TN  193.000000  189.000000  190.000000  191.000000\n",
      "2                    FP    6.000000   11.000000   11.000000    8.000000\n",
      "3                    FN   26.000000   20.000000   17.000000   22.000000\n",
      "4              Accuracy    0.880597    0.884328    0.895522    0.888060\n",
      "5             Precision    0.877551    0.813559    0.819672    0.854545\n",
      "6           Sensitivity    0.623188    0.705882    0.746269    0.681159\n",
      "7           Specificity    0.969800    0.945000    0.945300    0.959800\n",
      "8              F1 score    0.728814    0.755906    0.781250    0.758065\n",
      "9   F1 score (weighted)    0.873335    0.881502    0.893842    0.883642\n",
      "10     F1 score (macro)    0.826129    0.840055    0.856311    0.842624\n",
      "11    Balanced Accuracy    0.796519    0.825441    0.845771    0.820479\n",
      "12                  MCC    0.670831    0.683554    0.714174    0.693921\n",
      "13                  NPV    0.881300    0.904300    0.917900    0.896700\n",
      "14              ROC_AUC    0.796519    0.825441    0.845771    0.820479\n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_3 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet3, Y_testSet3)]\n",
    "optimized_xgb_3.fit(X_trainSet3,Y_trainSet3, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_3 = optimized_xgb_3.predict(X_testSet3)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_xgb_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_xgb_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_xgb_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_xgb_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_xgb_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_xgb_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_xgb_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_xgb_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_xgb_3)\n",
    "\n",
    "\n",
    "Set3 = pd.DataFrame({ 'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set3'] =Set3\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c5e7f6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:52:05,258] Trial 200 finished with value: 0.8748211732450277 and parameters: {'n_estimators': 281, 'eta': 0.09348144721176312, 'max_depth': 8, 'alpha': 0.6809000000000001, 'lambda': 2.685309078681551, 'max_bin': 362}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:07,470] Trial 201 finished with value: 0.8756839415757753 and parameters: {'n_estimators': 258, 'eta': 0.09604621050469633, 'max_depth': 9, 'alpha': 0.7284, 'lambda': 3.4136755645638366, 'max_bin': 435}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:09,505] Trial 202 finished with value: 0.874357360103257 and parameters: {'n_estimators': 317, 'eta': 0.09805519724909188, 'max_depth': 9, 'alpha': 0.743, 'lambda': 2.2723326069103447, 'max_bin': 372}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:11,665] Trial 203 finished with value: 0.8701236919861106 and parameters: {'n_estimators': 277, 'eta': 0.09070601280584946, 'max_depth': 9, 'alpha': 0.7735000000000001, 'lambda': 3.3952464890523526, 'max_bin': 353}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:14,293] Trial 204 finished with value: 0.8754843228822693 and parameters: {'n_estimators': 244, 'eta': 0.09484983745707116, 'max_depth': 9, 'alpha': 0.8858, 'lambda': 18.790655582235495, 'max_bin': 424}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:17,179] Trial 205 finished with value: 0.8751678003854954 and parameters: {'n_estimators': 294, 'eta': 0.07679674015881659, 'max_depth': 9, 'alpha': 0.9294, 'lambda': 8.536937522020663, 'max_bin': 423}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:20,179] Trial 206 finished with value: 0.8703920229143842 and parameters: {'n_estimators': 331, 'eta': 0.09631228647623474, 'max_depth': 10, 'alpha': 0.7206, 'lambda': 14.510712995193597, 'max_bin': 412}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:22,280] Trial 207 finished with value: 0.8757321530271744 and parameters: {'n_estimators': 302, 'eta': 0.08471525696378861, 'max_depth': 9, 'alpha': 0.4041, 'lambda': 1.621315801248015, 'max_bin': 431}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:24,873] Trial 208 finished with value: 0.8674899406244692 and parameters: {'n_estimators': 383, 'eta': 0.09250384308171264, 'max_depth': 10, 'alpha': 0.5881000000000001, 'lambda': 5.078460122007861, 'max_bin': 388}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:27,186] Trial 209 finished with value: 0.8702467839538428 and parameters: {'n_estimators': 267, 'eta': 0.08921956001825906, 'max_depth': 8, 'alpha': 0.7031000000000001, 'lambda': 4.149881269099593, 'max_bin': 379}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:29,248] Trial 210 finished with value: 0.873418949186866 and parameters: {'n_estimators': 350, 'eta': 0.09992284890583132, 'max_depth': 9, 'alpha': 0.9567, 'lambda': 1.0116163772438893, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:31,472] Trial 211 finished with value: 0.8786132177694416 and parameters: {'n_estimators': 314, 'eta': 0.08792256378790814, 'max_depth': 10, 'alpha': 0.6419, 'lambda': 2.481529218086712, 'max_bin': 399}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:34,146] Trial 212 finished with value: 0.8716032415480784 and parameters: {'n_estimators': 300, 'eta': 0.09767665473996494, 'max_depth': 10, 'alpha': 0.6782, 'lambda': 13.109674395849161, 'max_bin': 404}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:36,355] Trial 213 finished with value: 0.8751977020561652 and parameters: {'n_estimators': 325, 'eta': 0.08156001791364864, 'max_depth': 10, 'alpha': 0.6325000000000001, 'lambda': 1.6651953097117902, 'max_bin': 394}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:38,503] Trial 214 finished with value: 0.8764272684949146 and parameters: {'n_estimators': 280, 'eta': 0.08667954729781781, 'max_depth': 10, 'alpha': 0.6042000000000001, 'lambda': 2.9754937765140297, 'max_bin': 408}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:40,497] Trial 215 finished with value: 0.8742396282817463 and parameters: {'n_estimators': 342, 'eta': 0.09401917901069613, 'max_depth': 9, 'alpha': 0.1192, 'lambda': 2.1627941894476432, 'max_bin': 400}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:42,464] Trial 216 finished with value: 0.8722843477659605 and parameters: {'n_estimators': 221, 'eta': 0.08985470449800276, 'max_depth': 10, 'alpha': 0.7604000000000001, 'lambda': 1.0387308540022373, 'max_bin': 390}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:45,018] Trial 217 finished with value: 0.8745738561741409 and parameters: {'n_estimators': 259, 'eta': 0.0834856870174967, 'max_depth': 9, 'alpha': 0.5546, 'lambda': 9.133522283064814, 'max_bin': 383}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:47,942] Trial 218 finished with value: 0.8736929661210115 and parameters: {'n_estimators': 362, 'eta': 0.06010953282967993, 'max_depth': 7, 'alpha': 0.6584, 'lambda': 3.232219792923987, 'max_bin': 362}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:50,827] Trial 219 finished with value: 0.867986577406365 and parameters: {'n_estimators': 302, 'eta': 0.0740905663820057, 'max_depth': 6, 'alpha': 0.6244000000000001, 'lambda': 27.482033507442182, 'max_bin': 337}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:53,374] Trial 220 finished with value: 0.8788590569143453 and parameters: {'n_estimators': 238, 'eta': 0.09652879006593294, 'max_depth': 9, 'alpha': 0.8206, 'lambda': 15.184803064734291, 'max_bin': 373}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:55,410] Trial 221 finished with value: 0.871981053835747 and parameters: {'n_estimators': 383, 'eta': 0.09205089939337942, 'max_depth': 7, 'alpha': 0.6753, 'lambda': 1.705074473803488, 'max_bin': 356}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:52:58,361] Trial 222 finished with value: 0.8684597214342116 and parameters: {'n_estimators': 364, 'eta': 0.09075520582937088, 'max_depth': 6, 'alpha': 0.6915, 'lambda': 11.279562620246375, 'max_bin': 352}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:00,436] Trial 223 finished with value: 0.8697826380262932 and parameters: {'n_estimators': 339, 'eta': 0.09439702730539505, 'max_depth': 7, 'alpha': 0.6587000000000001, 'lambda': 2.417955685997295, 'max_bin': 348}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:02,463] Trial 224 finished with value: 0.8736670620664725 and parameters: {'n_estimators': 318, 'eta': 0.0882573441054889, 'max_depth': 6, 'alpha': 0.7411, 'lambda': 1.701439027046002, 'max_bin': 358}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:04,498] Trial 225 finished with value: 0.8786527957385981 and parameters: {'n_estimators': 289, 'eta': 0.0981483441169533, 'max_depth': 9, 'alpha': 0.6318, 'lambda': 2.7983721116667066, 'max_bin': 430}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:06,389] Trial 226 finished with value: 0.8753311806938454 and parameters: {'n_estimators': 276, 'eta': 0.0919704428783231, 'max_depth': 10, 'alpha': 0.7146, 'lambda': 1.0167061423147477, 'max_bin': 420}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:08,614] Trial 227 finished with value: 0.8766755567738196 and parameters: {'n_estimators': 399, 'eta': 0.09600568895030685, 'max_depth': 9, 'alpha': 0.6003000000000001, 'lambda': 3.8660867519190285, 'max_bin': 344}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:11,032] Trial 228 finished with value: 0.875071755679053 and parameters: {'n_estimators': 307, 'eta': 0.09998370591173589, 'max_depth': 9, 'alpha': 0.4539, 'lambda': 9.65031993255649, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:12,143] Trial 229 finished with value: 0.8668227509193727 and parameters: {'n_estimators': 85, 'eta': 0.09365447598468844, 'max_depth': 9, 'alpha': 0.8012, 'lambda': 2.3395423743070074, 'max_bin': 397}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:14,837] Trial 230 finished with value: 0.8674005909880986 and parameters: {'n_estimators': 353, 'eta': 0.08615979448528921, 'max_depth': 5, 'alpha': 0.7744000000000001, 'lambda': 7.645309928885719, 'max_bin': 378}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:17,100] Trial 231 finished with value: 0.8699109989028443 and parameters: {'n_estimators': 234, 'eta': 0.09714127268396762, 'max_depth': 9, 'alpha': 0.8232, 'lambda': 6.029990534009532, 'max_bin': 373}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:19,738] Trial 232 finished with value: 0.8729914691416576 and parameters: {'n_estimators': 248, 'eta': 0.09617450761933534, 'max_depth': 9, 'alpha': 0.8485, 'lambda': 12.92751884120884, 'max_bin': 373}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:21,932] Trial 233 finished with value: 0.8752418889894342 and parameters: {'n_estimators': 202, 'eta': 0.0984348058271275, 'max_depth': 9, 'alpha': 0.7568, 'lambda': 13.869522576631214, 'max_bin': 363}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:24,034] Trial 234 finished with value: 0.8767506451584474 and parameters: {'n_estimators': 268, 'eta': 0.0952123426180015, 'max_depth': 9, 'alpha': 0.7393000000000001, 'lambda': 3.4022466130277924, 'max_bin': 392}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:26,540] Trial 235 finished with value: 0.8683680597249136 and parameters: {'n_estimators': 244, 'eta': 0.07805550070585622, 'max_depth': 7, 'alpha': 0.7854, 'lambda': 11.12564150657348, 'max_bin': 385}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:28,776] Trial 236 finished with value: 0.8718267053808925 and parameters: {'n_estimators': 322, 'eta': 0.09679345331688481, 'max_depth': 9, 'alpha': 0.8112, 'lambda': 4.378849187652531, 'max_bin': 439}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:32,018] Trial 237 finished with value: 0.8791334875316075 and parameters: {'n_estimators': 449, 'eta': 0.09241297646762814, 'max_depth': 9, 'alpha': 0.7125, 'lambda': 17.53598105264116, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:33,647] Trial 238 finished with value: 0.8740568281705242 and parameters: {'n_estimators': 129, 'eta': 0.08923537294076993, 'max_depth': 10, 'alpha': 0.7100000000000001, 'lambda': 10.616090489433205, 'max_bin': 359}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:37,046] Trial 239 finished with value: 0.8739112965529987 and parameters: {'n_estimators': 475, 'eta': 0.0924381108589043, 'max_depth': 9, 'alpha': 0.6861, 'lambda': 16.911958958642558, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:42,531] Trial 240 finished with value: 0.8715865768129077 and parameters: {'n_estimators': 439, 'eta': 0.011671432853886499, 'max_depth': 10, 'alpha': 0.7232000000000001, 'lambda': 1.8054159798066043, 'max_bin': 328}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:45,050] Trial 241 finished with value: 0.8693462346890655 and parameters: {'n_estimators': 455, 'eta': 0.09430126755677445, 'max_depth': 9, 'alpha': 0.05, 'lambda': 7.083339724219763, 'max_bin': 375}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:48,149] Trial 242 finished with value: 0.8696328404350693 and parameters: {'n_estimators': 451, 'eta': 0.09844362936441375, 'max_depth': 9, 'alpha': 0.7526, 'lambda': 14.724684459144544, 'max_bin': 371}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:50,912] Trial 243 finished with value: 0.8673415158823389 and parameters: {'n_estimators': 423, 'eta': 0.09107283512514214, 'max_depth': 9, 'alpha': 0.7001000000000001, 'lambda': 12.355054460016198, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:53,699] Trial 244 finished with value: 0.8694823099794837 and parameters: {'n_estimators': 287, 'eta': 0.09641269510036092, 'max_depth': 9, 'alpha': 0.5792, 'lambda': 15.488027997731578, 'max_bin': 309}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:56,044] Trial 245 finished with value: 0.872756002091774 and parameters: {'n_estimators': 228, 'eta': 0.09320863828052951, 'max_depth': 9, 'alpha': 0.5034000000000001, 'lambda': 15.417973656016287, 'max_bin': 379}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:53:58,362] Trial 246 finished with value: 0.8695581272595362 and parameters: {'n_estimators': 338, 'eta': 0.08439641331493511, 'max_depth': 9, 'alpha': 0.6658000000000001, 'lambda': 2.847862466134087, 'max_bin': 384}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:01,233] Trial 247 finished with value: 0.8718383241695369 and parameters: {'n_estimators': 261, 'eta': 0.08876631416387312, 'max_depth': 9, 'alpha': 0.7265, 'lambda': 17.40578623369904, 'max_bin': 353}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:04,355] Trial 248 finished with value: 0.8736137758639394 and parameters: {'n_estimators': 375, 'eta': 0.09813613164553003, 'max_depth': 9, 'alpha': 0.7747, 'lambda': 19.603292957920626, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:07,302] Trial 249 finished with value: 0.8780534824087944 and parameters: {'n_estimators': 499, 'eta': 0.09505140818049852, 'max_depth': 8, 'alpha': 0.7514000000000001, 'lambda': 10.137907829859063, 'max_bin': 250}. Best is trial 99 with value: 0.8849347158637322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8849\n",
      "\tBest params:\n",
      "\t\tn_estimators: 310\n",
      "\t\teta: 0.08977155105733017\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.8714000000000001\n",
      "\t\tlambda: 3.3899853772465174\n",
      "\t\tmax_bin: 366\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_4 = lambda trial: objective_xgb_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_xgb.optimize(func_xgb_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ea2f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   43.000000   48.000000   50.000000   47.000000   \n",
      "1                    TN  193.000000  189.000000  190.000000  191.000000   \n",
      "2                    FP    6.000000   11.000000   11.000000    8.000000   \n",
      "3                    FN   26.000000   20.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.880597    0.884328    0.895522    0.888060   \n",
      "5             Precision    0.877551    0.813559    0.819672    0.854545   \n",
      "6           Sensitivity    0.623188    0.705882    0.746269    0.681159   \n",
      "7           Specificity    0.969800    0.945000    0.945300    0.959800   \n",
      "8              F1 score    0.728814    0.755906    0.781250    0.758065   \n",
      "9   F1 score (weighted)    0.873335    0.881502    0.893842    0.883642   \n",
      "10     F1 score (macro)    0.826129    0.840055    0.856311    0.842624   \n",
      "11    Balanced Accuracy    0.796519    0.825441    0.845771    0.820479   \n",
      "12                  MCC    0.670831    0.683554    0.714174    0.693921   \n",
      "13                  NPV    0.881300    0.904300    0.917900    0.896700   \n",
      "14              ROC_AUC    0.796519    0.825441    0.845771    0.820479   \n",
      "\n",
      "          Set4  \n",
      "0    42.000000  \n",
      "1   190.000000  \n",
      "2    10.000000  \n",
      "3    26.000000  \n",
      "4     0.865672  \n",
      "5     0.807692  \n",
      "6     0.617647  \n",
      "7     0.950000  \n",
      "8     0.700000  \n",
      "9     0.859300  \n",
      "10    0.806731  \n",
      "11    0.783824  \n",
      "12    0.624625  \n",
      "13    0.879600  \n",
      "14    0.783824  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_4 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet4, Y_testSet4)]\n",
    "optimized_xgb_4.fit(X_trainSet4,Y_trainSet4, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_4 = optimized_xgb_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_xgb_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_xgb_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_xgb_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_xgb_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_xgb_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_xgb_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_xgb_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_xgb_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_xgb_4)\n",
    "\n",
    "\n",
    "Set4 = pd.DataFrame({ 'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set4'] =Set4\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1955a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:54:09,975] Trial 250 finished with value: 0.873623860553846 and parameters: {'n_estimators': 304, 'eta': 0.07115865698593535, 'max_depth': 10, 'alpha': 0.6469, 'lambda': 3.4477208041827296, 'max_bin': 369}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:13,617] Trial 251 finished with value: 0.8563911187182048 and parameters: {'n_estimators': 323, 'eta': 0.04390416695324575, 'max_depth': 9, 'alpha': 0.7985, 'lambda': 18.36950556942433, 'max_bin': 361}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:15,833] Trial 252 finished with value: 0.8774091874011457 and parameters: {'n_estimators': 698, 'eta': 0.09037313282292596, 'max_depth': 10, 'alpha': 0.6188, 'lambda': 2.149643293108689, 'max_bin': 342}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:18,127] Trial 253 finished with value: 0.8658391655827724 and parameters: {'n_estimators': 264, 'eta': 0.09252298239163079, 'max_depth': 9, 'alpha': 0.912, 'lambda': 8.193366078121773, 'max_bin': 320}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:20,351] Trial 254 finished with value: 0.8722257298973262 and parameters: {'n_estimators': 414, 'eta': 0.09976892478670844, 'max_depth': 9, 'alpha': 0.8309000000000001, 'lambda': 2.791354802668657, 'max_bin': 390}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:23,322] Trial 255 finished with value: 0.8501039658716614 and parameters: {'n_estimators': 286, 'eta': 0.06734581281284113, 'max_depth': 8, 'alpha': 0.8673000000000001, 'lambda': 21.422355210625803, 'max_bin': 282}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:25,821] Trial 256 finished with value: 0.8621218808013694 and parameters: {'n_estimators': 214, 'eta': 0.03648068126248014, 'max_depth': 9, 'alpha': 0.6988000000000001, 'lambda': 1.562582876448148, 'max_bin': 376}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:28,465] Trial 257 finished with value: 0.869058179235604 and parameters: {'n_estimators': 338, 'eta': 0.09646017762226136, 'max_depth': 10, 'alpha': 0.7348, 'lambda': 11.932980505036136, 'max_bin': 400}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:30,485] Trial 258 finished with value: 0.865506184035091 and parameters: {'n_estimators': 362, 'eta': 0.09441037471237039, 'max_depth': 9, 'alpha': 0.672, 'lambda': 3.9723416310948587, 'max_bin': 348}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:32,888] Trial 259 finished with value: 0.869413193154713 and parameters: {'n_estimators': 308, 'eta': 0.05855709313105814, 'max_depth': 9, 'alpha': 0.7654000000000001, 'lambda': 1.0158033681917664, 'max_bin': 356}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:35,277] Trial 260 finished with value: 0.8717332880876582 and parameters: {'n_estimators': 251, 'eta': 0.08738270638797666, 'max_depth': 10, 'alpha': 0.6457, 'lambda': 5.3010282653534695, 'max_bin': 382}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:37,241] Trial 261 finished with value: 0.8633664562190442 and parameters: {'n_estimators': 159, 'eta': 0.05287291095891514, 'max_depth': 9, 'alpha': 0.7231000000000001, 'lambda': 2.655456559430533, 'max_bin': 395}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:40,228] Trial 262 finished with value: 0.8596027013057114 and parameters: {'n_estimators': 275, 'eta': 0.06352829999914672, 'max_depth': 9, 'alpha': 0.7864, 'lambda': 15.794800307864381, 'max_bin': 405}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:43,406] Trial 263 finished with value: 0.8698663261246395 and parameters: {'n_estimators': 295, 'eta': 0.06467670181883095, 'max_depth': 11, 'alpha': 0.8875000000000001, 'lambda': 9.412631343072839, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:46,208] Trial 264 finished with value: 0.8728336838823239 and parameters: {'n_estimators': 325, 'eta': 0.07323699348997895, 'max_depth': 8, 'alpha': 0.6935, 'lambda': 4.581516371934858, 'max_bin': 388}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:48,618] Trial 265 finished with value: 0.8692909390353754 and parameters: {'n_estimators': 352, 'eta': 0.0798593557426233, 'max_depth': 10, 'alpha': 0.7393000000000001, 'lambda': 3.613408280089922, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:51,059] Trial 266 finished with value: 0.8639368894805702 and parameters: {'n_estimators': 239, 'eta': 0.09747683547771058, 'max_depth': 9, 'alpha': 0.7070000000000001, 'lambda': 14.258997247486, 'max_bin': 378}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:52,939] Trial 267 finished with value: 0.8725296218251083 and parameters: {'n_estimators': 389, 'eta': 0.09994562993644672, 'max_depth': 9, 'alpha': 0.5941000000000001, 'lambda': 2.2178105154972685, 'max_bin': 337}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:55,749] Trial 268 finished with value: 0.8646196819468797 and parameters: {'n_estimators': 313, 'eta': 0.06990201187959252, 'max_depth': 7, 'alpha': 0.6159, 'lambda': 6.448563935765209, 'max_bin': 412}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:54:57,645] Trial 269 finished with value: 0.8719223581117912 and parameters: {'n_estimators': 191, 'eta': 0.0915802904256455, 'max_depth': 9, 'alpha': 0.33290000000000003, 'lambda': 1.7210930116027396, 'max_bin': 359}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:00,843] Trial 270 finished with value: 0.8680882694084309 and parameters: {'n_estimators': 469, 'eta': 0.09521889230503706, 'max_depth': 8, 'alpha': 0.7643000000000001, 'lambda': 13.576386840981643, 'max_bin': 291}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:02,917] Trial 271 finished with value: 0.8733923281705842 and parameters: {'n_estimators': 278, 'eta': 0.09753989818142692, 'max_depth': 10, 'alpha': 0.5671, 'lambda': 3.1464731533967853, 'max_bin': 426}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:05,808] Trial 272 finished with value: 0.865700110445224 and parameters: {'n_estimators': 295, 'eta': 0.08554722959569308, 'max_depth': 9, 'alpha': 0.6669, 'lambda': 17.70025935326416, 'max_bin': 374}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:08,348] Trial 273 finished with value: 0.8663638001802232 and parameters: {'n_estimators': 230, 'eta': 0.08246522072607854, 'max_depth': 9, 'alpha': 0.6346, 'lambda': 16.029574018894873, 'max_bin': 446}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:10,899] Trial 274 finished with value: 0.8756328012454357 and parameters: {'n_estimators': 339, 'eta': 0.08970016072252777, 'max_depth': 10, 'alpha': 0.8200000000000001, 'lambda': 3.9321335695243897, 'max_bin': 348}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:12,783] Trial 275 finished with value: 0.8700414290761784 and parameters: {'n_estimators': 257, 'eta': 0.09366787443385899, 'max_depth': 9, 'alpha': 0.24680000000000002, 'lambda': 2.398318022601634, 'max_bin': 354}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:15,129] Trial 276 finished with value: 0.86318329751633 and parameters: {'n_estimators': 375, 'eta': 0.09580232778341505, 'max_depth': 10, 'alpha': 0.9371, 'lambda': 4.59164948001467, 'max_bin': 298}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:18,123] Trial 277 finished with value: 0.8738685340082337 and parameters: {'n_estimators': 324, 'eta': 0.09845414083825009, 'max_depth': 9, 'alpha': 0.7182000000000001, 'lambda': 16.850944773119917, 'max_bin': 383}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:20,038] Trial 278 finished with value: 0.868329982593616 and parameters: {'n_estimators': 269, 'eta': 0.0925988588022307, 'max_depth': 9, 'alpha': 0.5404, 'lambda': 1.496822034433861, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:22,485] Trial 279 finished with value: 0.8739610277961833 and parameters: {'n_estimators': 355, 'eta': 0.07488754830968064, 'max_depth': 8, 'alpha': 0.7458, 'lambda': 3.0189721596815975, 'max_bin': 395}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:25,804] Trial 280 finished with value: 0.873397623207061 and parameters: {'n_estimators': 426, 'eta': 0.09655920145506126, 'max_depth': 9, 'alpha': 0.7925000000000001, 'lambda': 20.02551470274489, 'max_bin': 387}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:28,554] Trial 281 finished with value: 0.8657505819069025 and parameters: {'n_estimators': 309, 'eta': 0.08807768797394896, 'max_depth': 10, 'alpha': 0.6779000000000001, 'lambda': 8.61663530481922, 'max_bin': 361}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:30,823] Trial 282 finished with value: 0.8754807132070688 and parameters: {'n_estimators': 292, 'eta': 0.09111524040083091, 'max_depth': 9, 'alpha': 0.4909, 'lambda': 2.310504372109596, 'max_bin': 375}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:33,777] Trial 283 finished with value: 0.8742496493858862 and parameters: {'n_estimators': 334, 'eta': 0.09422408889042644, 'max_depth': 9, 'alpha': 0.0051, 'lambda': 14.853282299519552, 'max_bin': 403}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:35,781] Trial 284 finished with value: 0.8694467573252395 and parameters: {'n_estimators': 243, 'eta': 0.0999665117518239, 'max_depth': 9, 'alpha': 0.7689, 'lambda': 3.455660558363041, 'max_bin': 369}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:37,568] Trial 285 finished with value: 0.8671882430856688 and parameters: {'n_estimators': 397, 'eta': 0.09795855776991676, 'max_depth': 8, 'alpha': 0.6916, 'lambda': 1.0022887273939942, 'max_bin': 418}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:39,643] Trial 286 finished with value: 0.8597597282371578 and parameters: {'n_estimators': 209, 'eta': 0.08649849651995302, 'max_depth': 7, 'alpha': 0.7327, 'lambda': 5.697686200464132, 'max_bin': 392}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:41,985] Trial 287 finished with value: 0.8682902662013904 and parameters: {'n_estimators': 277, 'eta': 0.06638651206641819, 'max_depth': 10, 'alpha': 0.6514, 'lambda': 1.8802634369854232, 'max_bin': 340}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:44,638] Trial 288 finished with value: 0.8736381187206262 and parameters: {'n_estimators': 312, 'eta': 0.07633298601379736, 'max_depth': 9, 'alpha': 0.6064, 'lambda': 5.111123028709447, 'max_bin': 456}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:46,648] Trial 289 finished with value: 0.8697521559536353 and parameters: {'n_estimators': 175, 'eta': 0.0952017798872322, 'max_depth': 10, 'alpha': 0.3758, 'lambda': 12.931313879027961, 'max_bin': 380}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:49,274] Trial 290 finished with value: 0.8659415586780004 and parameters: {'n_estimators': 346, 'eta': 0.08942781996504179, 'max_depth': 9, 'alpha': 0.1882, 'lambda': 7.947666096285538, 'max_bin': 332}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:51,377] Trial 291 finished with value: 0.8785759124168798 and parameters: {'n_estimators': 292, 'eta': 0.09359774442714712, 'max_depth': 9, 'alpha': 0.7152000000000001, 'lambda': 2.923114094160659, 'max_bin': 351}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:53,721] Trial 292 finished with value: 0.8667744812113127 and parameters: {'n_estimators': 555, 'eta': 0.09699081834982295, 'max_depth': 9, 'alpha': 0.8420000000000001, 'lambda': 4.280680393976228, 'max_bin': 398}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:56,551] Trial 293 finished with value: 0.8704275737909806 and parameters: {'n_estimators': 369, 'eta': 0.09167934394384009, 'max_depth': 8, 'alpha': 0.7544000000000001, 'lambda': 10.53268388888902, 'max_bin': 363}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:55:59,285] Trial 294 finished with value: 0.8688051166187893 and parameters: {'n_estimators': 260, 'eta': 0.09815995679806314, 'max_depth': 10, 'alpha': 0.8032, 'lambda': 22.703966506999492, 'max_bin': 436}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:01,465] Trial 295 finished with value: 0.87269247616708 and parameters: {'n_estimators': 224, 'eta': 0.08473471528735599, 'max_depth': 9, 'alpha': 0.9608000000000001, 'lambda': 2.303007774472164, 'max_bin': 357}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:02,861] Trial 296 finished with value: 0.8409226434324681 and parameters: {'n_estimators': 142, 'eta': 0.048378381590776794, 'max_depth': 5, 'alpha': 0.8658, 'lambda': 6.847648175522796, 'max_bin': 373}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:05,096] Trial 297 finished with value: 0.8713322644664947 and parameters: {'n_estimators': 325, 'eta': 0.09528223352928662, 'max_depth': 9, 'alpha': 0.7799, 'lambda': 3.802607487693688, 'max_bin': 384}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:08,149] Trial 298 finished with value: 0.8748550752685583 and parameters: {'n_estimators': 302, 'eta': 0.09056282898945135, 'max_depth': 10, 'alpha': 0.5810000000000001, 'lambda': 19.237010604543126, 'max_bin': 430}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:10,057] Trial 299 finished with value: 0.8694682010427919 and parameters: {'n_estimators': 246, 'eta': 0.09288025141438859, 'max_depth': 9, 'alpha': 0.10600000000000001, 'lambda': 1.0195013982058692, 'max_bin': 390}. Best is trial 99 with value: 0.8849347158637322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8849\n",
      "\tBest params:\n",
      "\t\tn_estimators: 310\n",
      "\t\teta: 0.08977155105733017\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.8714000000000001\n",
      "\t\tlambda: 3.3899853772465174\n",
      "\t\tmax_bin: 366\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_5 = lambda trial: objective_xgb_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_xgb.optimize(func_xgb_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "072752d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   43.000000   48.000000   50.000000   47.000000   \n",
      "1                    TN  193.000000  189.000000  190.000000  191.000000   \n",
      "2                    FP    6.000000   11.000000   11.000000    8.000000   \n",
      "3                    FN   26.000000   20.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.880597    0.884328    0.895522    0.888060   \n",
      "5             Precision    0.877551    0.813559    0.819672    0.854545   \n",
      "6           Sensitivity    0.623188    0.705882    0.746269    0.681159   \n",
      "7           Specificity    0.969800    0.945000    0.945300    0.959800   \n",
      "8              F1 score    0.728814    0.755906    0.781250    0.758065   \n",
      "9   F1 score (weighted)    0.873335    0.881502    0.893842    0.883642   \n",
      "10     F1 score (macro)    0.826129    0.840055    0.856311    0.842624   \n",
      "11    Balanced Accuracy    0.796519    0.825441    0.845771    0.820479   \n",
      "12                  MCC    0.670831    0.683554    0.714174    0.693921   \n",
      "13                  NPV    0.881300    0.904300    0.917900    0.896700   \n",
      "14              ROC_AUC    0.796519    0.825441    0.845771    0.820479   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    42.000000   53.000000  \n",
      "1   190.000000  191.000000  \n",
      "2    10.000000    9.000000  \n",
      "3    26.000000   15.000000  \n",
      "4     0.865672    0.910448  \n",
      "5     0.807692    0.854839  \n",
      "6     0.617647    0.779412  \n",
      "7     0.950000    0.955000  \n",
      "8     0.700000    0.815385  \n",
      "9     0.859300    0.909043  \n",
      "10    0.806731    0.878136  \n",
      "11    0.783824    0.867206  \n",
      "12    0.624625    0.757844  \n",
      "13    0.879600    0.927200  \n",
      "14    0.783824    0.867206  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_5 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet5, Y_testSet5)]\n",
    "optimized_xgb_5.fit(X_trainSet5,Y_trainSet5, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_5 = optimized_xgb_5.predict(X_testSet5)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_xgb_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_xgb_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_xgb_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_xgb_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_xgb_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_xgb_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_xgb_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_xgb_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_xgb_5)\n",
    "\n",
    "\n",
    "Set5 = pd.DataFrame({ 'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set5'] =Set5\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "88297c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:56:13,621] Trial 300 finished with value: 0.8580791742053744 and parameters: {'n_estimators': 436, 'eta': 0.08096594789646977, 'max_depth': 10, 'alpha': 0.6868000000000001, 'lambda': 18.01064953769213, 'max_bin': 407}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:16,352] Trial 301 finished with value: 0.8652039141987519 and parameters: {'n_estimators': 278, 'eta': 0.09648292003512839, 'max_depth': 12, 'alpha': 0.6297, 'lambda': 18.767479201818503, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:18,826] Trial 302 finished with value: 0.8583106848155685 and parameters: {'n_estimators': 347, 'eta': 0.0682405646788174, 'max_depth': 9, 'alpha': 0.7348, 'lambda': 3.034655636780557, 'max_bin': 380}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:21,068] Trial 303 finished with value: 0.8605643417102412 and parameters: {'n_estimators': 488, 'eta': 0.07097529930666016, 'max_depth': 9, 'alpha': 0.6629, 'lambda': 1.7981432708323362, 'max_bin': 358}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:23,154] Trial 304 finished with value: 0.8553772353055977 and parameters: {'n_estimators': 324, 'eta': 0.09997529305848232, 'max_depth': 8, 'alpha': 0.7092, 'lambda': 4.582787831551075, 'max_bin': 345}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:26,262] Trial 305 finished with value: 0.8562652500020059 and parameters: {'n_estimators': 264, 'eta': 0.06202900455872115, 'max_depth': 9, 'alpha': 0.7447, 'lambda': 38.82708748263868, 'max_bin': 401}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:28,311] Trial 306 finished with value: 0.8592415011862645 and parameters: {'n_estimators': 288, 'eta': 0.08810912548880749, 'max_depth': 11, 'alpha': 0.7622, 'lambda': 2.2547565097699716, 'max_bin': 374}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:32,968] Trial 307 finished with value: 0.8556504661228134 and parameters: {'n_estimators': 406, 'eta': 0.02675789631939079, 'max_depth': 9, 'alpha': 0.43060000000000004, 'lambda': 16.93366167096664, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:35,532] Trial 308 finished with value: 0.867684950886477 and parameters: {'n_estimators': 462, 'eta': 0.0828319326353546, 'max_depth': 10, 'alpha': 0.0714, 'lambda': 11.65358560560973, 'max_bin': 362}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:37,990] Trial 309 finished with value: 0.8640196772879485 and parameters: {'n_estimators': 365, 'eta': 0.06008994442186652, 'max_depth': 9, 'alpha': 0.1451, 'lambda': 3.448475690484294, 'max_bin': 395}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:40,384] Trial 310 finished with value: 0.8575201332996729 and parameters: {'n_estimators': 308, 'eta': 0.09423704384710893, 'max_depth': 9, 'alpha': 0.8121, 'lambda': 7.463120574652937, 'max_bin': 387}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:42,074] Trial 311 finished with value: 0.8551113901360206 and parameters: {'n_estimators': 239, 'eta': 0.09829288888412327, 'max_depth': 8, 'alpha': 0.6456000000000001, 'lambda': 1.7004099641068042, 'max_bin': 352}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:42,814] Trial 312 finished with value: 0.8372144938643838 and parameters: {'n_estimators': 50, 'eta': 0.09625363596905898, 'max_depth': 10, 'alpha': 0.9064000000000001, 'lambda': 9.40722725862405, 'max_bin': 378}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:44,947] Trial 313 finished with value: 0.8596127063431223 and parameters: {'n_estimators': 332, 'eta': 0.09252078300595512, 'max_depth': 9, 'alpha': 0.7130000000000001, 'lambda': 2.758237661433772, 'max_bin': 366}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:47,132] Trial 314 finished with value: 0.8594560821023922 and parameters: {'n_estimators': 379, 'eta': 0.09044075396219933, 'max_depth': 9, 'alpha': 0.6776, 'lambda': 3.766267469756375, 'max_bin': 410}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:49,822] Trial 315 finished with value: 0.8557062671321815 and parameters: {'n_estimators': 215, 'eta': 0.05686911349589954, 'max_depth': 10, 'alpha': 0.7857000000000001, 'lambda': 20.419560932615777, 'max_bin': 275}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:51,946] Trial 316 finished with value: 0.8509045730380025 and parameters: {'n_estimators': 280, 'eta': 0.09811658749172127, 'max_depth': 9, 'alpha': 0.6018, 'lambda': 5.732640061285928, 'max_bin': 420}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:53,666] Trial 317 finished with value: 0.852921029645672 and parameters: {'n_estimators': 255, 'eta': 0.0951164301446701, 'max_depth': 8, 'alpha': 0.7531, 'lambda': 1.0256484309049014, 'max_bin': 356}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:56,458] Trial 318 finished with value: 0.8636219611947299 and parameters: {'n_estimators': 306, 'eta': 0.08671779258670108, 'max_depth': 9, 'alpha': 0.6964, 'lambda': 13.765008041221536, 'max_bin': 383}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:56:59,101] Trial 319 finished with value: 0.8581217950635038 and parameters: {'n_estimators': 347, 'eta': 0.09319707354178011, 'max_depth': 6, 'alpha': 0.7334, 'lambda': 12.29841814663117, 'max_bin': 391}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:01,367] Trial 320 finished with value: 0.856236256548381 and parameters: {'n_estimators': 322, 'eta': 0.07751250514178715, 'max_depth': 7, 'alpha': 0.6258, 'lambda': 2.949113084972406, 'max_bin': 362}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:04,051] Trial 321 finished with value: 0.8609610362025932 and parameters: {'n_estimators': 290, 'eta': 0.09641123278819776, 'max_depth': 10, 'alpha': 0.8848, 'lambda': 15.005744337839825, 'max_bin': 372}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:07,168] Trial 322 finished with value: 0.8663069297873911 and parameters: {'n_estimators': 362, 'eta': 0.08875930113564108, 'max_depth': 9, 'alpha': 0.8366, 'lambda': 16.469178430156262, 'max_bin': 398}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:09,247] Trial 323 finished with value: 0.8611478032924941 and parameters: {'n_estimators': 190, 'eta': 0.0797151867910081, 'max_depth': 9, 'alpha': 0.5617, 'lambda': 4.98141141140859, 'max_bin': 347}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:12,407] Trial 324 finished with value: 0.865632627049244 and parameters: {'n_estimators': 338, 'eta': 0.09815079133176577, 'max_depth': 9, 'alpha': 0.024800000000000003, 'lambda': 32.231472162901774, 'max_bin': 304}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:15,608] Trial 325 finished with value: 0.8571863826443964 and parameters: {'n_estimators': 271, 'eta': 0.053491035179174966, 'max_depth': 10, 'alpha': 0.7683, 'lambda': 17.748679197793642, 'max_bin': 335}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:17,614] Trial 326 finished with value: 0.859770799945497 and parameters: {'n_estimators': 302, 'eta': 0.09026554023020446, 'max_depth': 8, 'alpha': 0.6587000000000001, 'lambda': 2.170881334546914, 'max_bin': 378}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:19,800] Trial 327 finished with value: 0.8607541170555211 and parameters: {'n_estimators': 234, 'eta': 0.08407473512737025, 'max_depth': 9, 'alpha': 0.7169, 'lambda': 4.062416768374711, 'max_bin': 415}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:21,677] Trial 328 finished with value: 0.8603852641933152 and parameters: {'n_estimators': 255, 'eta': 0.09460955021641743, 'max_depth': 10, 'alpha': 0.7958000000000001, 'lambda': 1.7197386292972456, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:23,485] Trial 329 finished with value: 0.8558415421389254 and parameters: {'n_estimators': 388, 'eta': 0.09181356697540313, 'max_depth': 9, 'alpha': 0.687, 'lambda': 2.6482251623945627, 'max_bin': 403}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:25,509] Trial 330 finished with value: 0.8617856366611456 and parameters: {'n_estimators': 318, 'eta': 0.09960432792492706, 'max_depth': 9, 'alpha': 0.8588, 'lambda': 3.4233465284656344, 'max_bin': 340}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:27,966] Trial 331 finished with value: 0.8576158078876928 and parameters: {'n_estimators': 519, 'eta': 0.0656960237921449, 'max_depth': 10, 'alpha': 0.7395, 'lambda': 1.748126803367021, 'max_bin': 385}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:29,837] Trial 332 finished with value: 0.8602200711937295 and parameters: {'n_estimators': 270, 'eta': 0.09666752313119659, 'max_depth': 9, 'alpha': 0.7034, 'lambda': 4.475801017745366, 'max_bin': 352}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:32,169] Trial 333 finished with value: 0.8541183914474642 and parameters: {'n_estimators': 295, 'eta': 0.09428422543212546, 'max_depth': 8, 'alpha': 0.6165, 'lambda': 8.490091400716585, 'max_bin': 426}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:34,159] Trial 334 finished with value: 0.86006165862144 and parameters: {'n_estimators': 336, 'eta': 0.09999529592998085, 'max_depth': 9, 'alpha': 0.778, 'lambda': 2.50147883579932, 'max_bin': 375}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:37,028] Trial 335 finished with value: 0.8671030324401846 and parameters: {'n_estimators': 357, 'eta': 0.08619994229018424, 'max_depth': 9, 'alpha': 0.8219000000000001, 'lambda': 15.945693052789252, 'max_bin': 468}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:39,314] Trial 336 finished with value: 0.8581594916575627 and parameters: {'n_estimators': 421, 'eta': 0.09279132204764981, 'max_depth': 10, 'alpha': 0.5874, 'lambda': 5.981269756784071, 'max_bin': 361}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:41,474] Trial 337 finished with value: 0.8555173527803481 and parameters: {'n_estimators': 220, 'eta': 0.0967532696905384, 'max_depth': 7, 'alpha': 0.5221, 'lambda': 11.296805323208574, 'max_bin': 324}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:43,736] Trial 338 finished with value: 0.8514987801149425 and parameters: {'n_estimators': 450, 'eta': 0.0897323748367669, 'max_depth': 9, 'alpha': 0.6433, 'lambda': 3.7652339482958146, 'max_bin': 393}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:45,780] Trial 339 finished with value: 0.859012381271096 and parameters: {'n_estimators': 315, 'eta': 0.08800274753703723, 'max_depth': 9, 'alpha': 0.6678000000000001, 'lambda': 3.031259789946332, 'max_bin': 372}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:47,643] Trial 340 finished with value: 0.8598739240669289 and parameters: {'n_estimators': 283, 'eta': 0.09815589956722248, 'max_depth': 9, 'alpha': 0.7211000000000001, 'lambda': 1.5825624308161976, 'max_bin': 366}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:50,496] Trial 341 finished with value: 0.7973704226939315 and parameters: {'n_estimators': 251, 'eta': 0.002139134677985134, 'max_depth': 11, 'alpha': 0.7643000000000001, 'lambda': 19.28409841592932, 'max_bin': 311}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:52,635] Trial 342 finished with value: 0.8597733762583302 and parameters: {'n_estimators': 306, 'eta': 0.09422390584694436, 'max_depth': 10, 'alpha': 0.9339000000000001, 'lambda': 4.732997874942067, 'max_bin': 387}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:53,935] Trial 343 finished with value: 0.8542407219622448 and parameters: {'n_estimators': 101, 'eta': 0.09187664195966594, 'max_depth': 9, 'alpha': 0.9832000000000001, 'lambda': 2.370998729348475, 'max_bin': 381}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:55,445] Trial 344 finished with value: 0.8516820263359142 and parameters: {'n_estimators': 125, 'eta': 0.0952083027700344, 'max_depth': 8, 'alpha': 0.7502000000000001, 'lambda': 10.409210926635868, 'max_bin': 358}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:57,522] Trial 345 finished with value: 0.8575764003903485 and parameters: {'n_estimators': 208, 'eta': 0.09764053812161344, 'max_depth': 9, 'alpha': 0.6872, 'lambda': 7.263308524012178, 'max_bin': 399}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:57:59,522] Trial 346 finished with value: 0.8595856250268328 and parameters: {'n_estimators': 343, 'eta': 0.08465921675037694, 'max_depth': 10, 'alpha': 0.787, 'lambda': 1.0429433049023271, 'max_bin': 346}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:01,649] Trial 347 finished with value: 0.8580907329108168 and parameters: {'n_estimators': 265, 'eta': 0.0962132432825079, 'max_depth': 9, 'alpha': 0.4685, 'lambda': 6.5232363846289045, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:04,044] Trial 348 finished with value: 0.8608850551276508 and parameters: {'n_estimators': 375, 'eta': 0.06224872165787486, 'max_depth': 8, 'alpha': 0.0477, 'lambda': 3.5455127346854014, 'max_bin': 391}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:06,463] Trial 349 finished with value: 0.8547034179764822 and parameters: {'n_estimators': 325, 'eta': 0.09986181422152242, 'max_depth': 10, 'alpha': 0.7278, 'lambda': 9.321842408937798, 'max_bin': 406}. Best is trial 99 with value: 0.8849347158637322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8849\n",
      "\tBest params:\n",
      "\t\tn_estimators: 310\n",
      "\t\teta: 0.08977155105733017\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.8714000000000001\n",
      "\t\tlambda: 3.3899853772465174\n",
      "\t\tmax_bin: 366\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_6 = lambda trial: objective_xgb_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_xgb.optimize(func_xgb_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ea8e79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   43.000000   48.000000   50.000000   47.000000   \n",
      "1                    TN  193.000000  189.000000  190.000000  191.000000   \n",
      "2                    FP    6.000000   11.000000   11.000000    8.000000   \n",
      "3                    FN   26.000000   20.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.880597    0.884328    0.895522    0.888060   \n",
      "5             Precision    0.877551    0.813559    0.819672    0.854545   \n",
      "6           Sensitivity    0.623188    0.705882    0.746269    0.681159   \n",
      "7           Specificity    0.969800    0.945000    0.945300    0.959800   \n",
      "8              F1 score    0.728814    0.755906    0.781250    0.758065   \n",
      "9   F1 score (weighted)    0.873335    0.881502    0.893842    0.883642   \n",
      "10     F1 score (macro)    0.826129    0.840055    0.856311    0.842624   \n",
      "11    Balanced Accuracy    0.796519    0.825441    0.845771    0.820479   \n",
      "12                  MCC    0.670831    0.683554    0.714174    0.693921   \n",
      "13                  NPV    0.881300    0.904300    0.917900    0.896700   \n",
      "14              ROC_AUC    0.796519    0.825441    0.845771    0.820479   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    42.000000   53.000000   52.000000  \n",
      "1   190.000000  191.000000  193.000000  \n",
      "2    10.000000    9.000000    7.000000  \n",
      "3    26.000000   15.000000   16.000000  \n",
      "4     0.865672    0.910448    0.914179  \n",
      "5     0.807692    0.854839    0.881356  \n",
      "6     0.617647    0.779412    0.764706  \n",
      "7     0.950000    0.955000    0.965000  \n",
      "8     0.700000    0.815385    0.818898  \n",
      "9     0.859300    0.909043    0.912082  \n",
      "10    0.806731    0.878136    0.881331  \n",
      "11    0.783824    0.867206    0.864853  \n",
      "12    0.624625    0.757844    0.766334  \n",
      "13    0.879600    0.927200    0.923400  \n",
      "14    0.783824    0.867206    0.864853  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_6 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet6, Y_testSet6)]\n",
    "optimized_xgb_6.fit(X_trainSet6,Y_trainSet6, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_6 = optimized_xgb_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_xgb_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_xgb_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_xgb_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_xgb_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_xgb_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_xgb_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_xgb_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_xgb_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_xgb_6)\n",
    "\n",
    "\n",
    "Set6 = pd.DataFrame({ 'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set6'] =Set6\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be1838b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 12:58:08,891] Trial 350 finished with value: 0.8717316819173145 and parameters: {'n_estimators': 284, 'eta': 0.09090742110774597, 'max_depth': 9, 'alpha': 0.6321, 'lambda': 2.8460553570628915, 'max_bin': 378}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:10,960] Trial 351 finished with value: 0.8653089705621589 and parameters: {'n_estimators': 232, 'eta': 0.0931475214487417, 'max_depth': 9, 'alpha': 0.7045, 'lambda': 5.568775942772099, 'max_bin': 355}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:13,222] Trial 352 finished with value: 0.8682272892401061 and parameters: {'n_estimators': 401, 'eta': 0.06402350491231108, 'max_depth': 9, 'alpha': 0.31570000000000004, 'lambda': 2.056965126310556, 'max_bin': 363}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:17,281] Trial 353 finished with value: 0.8619732248026601 and parameters: {'n_estimators': 774, 'eta': 0.07591093076049442, 'max_depth': 9, 'alpha': 0.6714, 'lambda': 18.18780342560417, 'max_bin': 342}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:19,163] Trial 354 finished with value: 0.8659236881623702 and parameters: {'n_estimators': 163, 'eta': 0.08705651924170887, 'max_depth': 10, 'alpha': 0.5468000000000001, 'lambda': 4.169320916861361, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:21,849] Trial 355 finished with value: 0.8708973689480238 and parameters: {'n_estimators': 302, 'eta': 0.09544850819333953, 'max_depth': 9, 'alpha': 0.6029, 'lambda': 12.695873376282869, 'max_bin': 376}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:23,796] Trial 356 finished with value: 0.8737019131757853 and parameters: {'n_estimators': 359, 'eta': 0.09782188149119278, 'max_depth': 8, 'alpha': 0.8148000000000001, 'lambda': 1.0299365771553173, 'max_bin': 385}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:26,581] Trial 357 finished with value: 0.8628950823252323 and parameters: {'n_estimators': 241, 'eta': 0.08218869970348221, 'max_depth': 10, 'alpha': 0.2164, 'lambda': 21.54320088659364, 'max_bin': 396}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:29,767] Trial 358 finished with value: 0.8683190505835825 and parameters: {'n_estimators': 334, 'eta': 0.07142289042685221, 'max_depth': 9, 'alpha': 0.7466, 'lambda': 14.468031213415687, 'max_bin': 350}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:31,858] Trial 359 finished with value: 0.8707129262233279 and parameters: {'n_estimators': 273, 'eta': 0.08863605090619703, 'max_depth': 9, 'alpha': 0.6538, 'lambda': 3.1367532882959464, 'max_bin': 262}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:34,526] Trial 360 finished with value: 0.8645328711242037 and parameters: {'n_estimators': 287, 'eta': 0.0917019907059964, 'max_depth': 10, 'alpha': 0.765, 'lambda': 17.21236398676314, 'max_bin': 329}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:36,515] Trial 361 finished with value: 0.876834774233172 and parameters: {'n_estimators': 322, 'eta': 0.09402690527087147, 'max_depth': 9, 'alpha': 0.7307, 'lambda': 2.042514250963735, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:39,512] Trial 362 finished with value: 0.8217288766985582 and parameters: {'n_estimators': 261, 'eta': 0.004629655952212111, 'max_depth': 9, 'alpha': 0.89, 'lambda': 5.0145872117426435, 'max_bin': 357}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:41,672] Trial 363 finished with value: 0.8658304814710218 and parameters: {'n_estimators': 350, 'eta': 0.09666448567387549, 'max_depth': 8, 'alpha': 0.8029000000000001, 'lambda': 3.437934657773633, 'max_bin': 389}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:43,585] Trial 364 finished with value: 0.8737790962463338 and parameters: {'n_estimators': 306, 'eta': 0.09990263807754374, 'max_depth': 7, 'alpha': 0.5786, 'lambda': 2.5020169699129284, 'max_bin': 445}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:45,867] Trial 365 finished with value: 0.8713643281536466 and parameters: {'n_estimators': 201, 'eta': 0.05810745953245948, 'max_depth': 9, 'alpha': 0.6982, 'lambda': 1.8228545736553683, 'max_bin': 422}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:48,906] Trial 366 finished with value: 0.868340032675102 and parameters: {'n_estimators': 294, 'eta': 0.08990222348764278, 'max_depth': 10, 'alpha': 0.8514, 'lambda': 20.689500984591184, 'max_bin': 434}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:50,797] Trial 367 finished with value: 0.8544218005276798 and parameters: {'n_estimators': 249, 'eta': 0.0981229207715794, 'max_depth': 6, 'alpha': 0.5021, 'lambda': 4.133274527596762, 'max_bin': 380}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:52,927] Trial 368 finished with value: 0.863811210486728 and parameters: {'n_estimators': 381, 'eta': 0.09540918152628688, 'max_depth': 9, 'alpha': 0.9583, 'lambda': 3.019421032927613, 'max_bin': 372}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:56,742] Trial 369 finished with value: 0.8601944472522914 and parameters: {'n_estimators': 317, 'eta': 0.048956258483294636, 'max_depth': 11, 'alpha': 0.9123, 'lambda': 19.891949721730064, 'max_bin': 364}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:58:59,481] Trial 370 finished with value: 0.8667538789841137 and parameters: {'n_estimators': 228, 'eta': 0.03677570956965807, 'max_depth': 9, 'alpha': 0.7289, 'lambda': 1.5587112799798533, 'max_bin': 412}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:01,655] Trial 371 finished with value: 0.8650448997089419 and parameters: {'n_estimators': 445, 'eta': 0.0929302634528901, 'max_depth': 10, 'alpha': 0.771, 'lambda': 3.7566849679458363, 'max_bin': 403}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:04,725] Trial 372 finished with value: 0.8725142582525705 and parameters: {'n_estimators': 335, 'eta': 0.07959788828085555, 'max_depth': 8, 'alpha': 0.6767000000000001, 'lambda': 15.537509237772417, 'max_bin': 394}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:05,821] Trial 373 finished with value: 0.8510609834932383 and parameters: {'n_estimators': 82, 'eta': 0.08521657285332808, 'max_depth': 9, 'alpha': 0.6205, 'lambda': 7.652794549782838, 'max_bin': 359}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:08,246] Trial 374 finished with value: 0.8613547095411731 and parameters: {'n_estimators': 275, 'eta': 0.07348143735142527, 'max_depth': 9, 'alpha': 0.1736, 'lambda': 5.134787801887593, 'max_bin': 383}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:11,479] Trial 375 finished with value: 0.8619071520604802 and parameters: {'n_estimators': 354, 'eta': 0.09789392231027134, 'max_depth': 9, 'alpha': 0.7154, 'lambda': 24.472843811980233, 'max_bin': 318}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:13,934] Trial 376 finished with value: 0.8700006377160812 and parameters: {'n_estimators': 294, 'eta': 0.09508593774965833, 'max_depth': 10, 'alpha': 0.7475, 'lambda': 6.305656869456543, 'max_bin': 440}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:17,000] Trial 377 finished with value: 0.8599153432794224 and parameters: {'n_estimators': 314, 'eta': 0.07844393915858205, 'max_depth': 9, 'alpha': 0.7852, 'lambda': 13.626958104357033, 'max_bin': 375}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:20,332] Trial 378 finished with value: 0.8619117662794299 and parameters: {'n_estimators': 409, 'eta': 0.06635045790709974, 'max_depth': 9, 'alpha': 0.651, 'lambda': 10.960338837941062, 'max_bin': 334}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:22,688] Trial 379 finished with value: 0.8676183725160728 and parameters: {'n_estimators': 476, 'eta': 0.08796094224531695, 'max_depth': 10, 'alpha': 0.6954, 'lambda': 8.65010542423473, 'max_bin': 399}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:24,587] Trial 380 finished with value: 0.8687464495851305 and parameters: {'n_estimators': 596, 'eta': 0.09110069279015477, 'max_depth': 8, 'alpha': 0.8239000000000001, 'lambda': 1.0135629777496198, 'max_bin': 353}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:26,615] Trial 381 finished with value: 0.8727683948301435 and parameters: {'n_estimators': 255, 'eta': 0.09671559722419287, 'max_depth': 9, 'alpha': 0.7575000000000001, 'lambda': 2.489462697288955, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:28,773] Trial 382 finished with value: 0.8657078916015853 and parameters: {'n_estimators': 371, 'eta': 0.0930810666382329, 'max_depth': 9, 'alpha': 0.7140000000000001, 'lambda': 2.281264365988523, 'max_bin': 342}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:31,036] Trial 383 finished with value: 0.8622683370250253 and parameters: {'n_estimators': 272, 'eta': 0.08293777172517776, 'max_depth': 8, 'alpha': 0.5966, 'lambda': 4.412292183160378, 'max_bin': 386}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:33,441] Trial 384 finished with value: 0.8627055845139673 and parameters: {'n_estimators': 334, 'eta': 0.06888658340017467, 'max_depth': 9, 'alpha': 0.6275000000000001, 'lambda': 3.2811521541144457, 'max_bin': 349}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:36,248] Trial 385 finished with value: 0.8695373189892482 and parameters: {'n_estimators': 430, 'eta': 0.09840484352103507, 'max_depth': 9, 'alpha': 0.673, 'lambda': 12.235134464620728, 'max_bin': 360}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:38,239] Trial 386 finished with value: 0.8669888449012397 and parameters: {'n_estimators': 243, 'eta': 0.09989504424919554, 'max_depth': 10, 'alpha': 0.7767000000000001, 'lambda': 1.624733051341878, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:40,839] Trial 387 finished with value: 0.8612644991505605 and parameters: {'n_estimators': 218, 'eta': 0.0724663996566454, 'max_depth': 10, 'alpha': 0.7401, 'lambda': 16.680902148205277, 'max_bin': 390}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:42,897] Trial 388 finished with value: 0.858603552818661 and parameters: {'n_estimators': 178, 'eta': 0.08968851181766364, 'max_depth': 9, 'alpha': 0.8051, 'lambda': 10.307010263094924, 'max_bin': 379}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:46,287] Trial 389 finished with value: 0.8497750266992146 and parameters: {'n_estimators': 301, 'eta': 0.05307026517093785, 'max_depth': 9, 'alpha': 0.5691, 'lambda': 18.651239394816425, 'max_bin': 394}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:48,372] Trial 390 finished with value: 0.872330999806253 and parameters: {'n_estimators': 286, 'eta': 0.09450919650732827, 'max_depth': 9, 'alpha': 0.6471, 'lambda': 2.6263103205547926, 'max_bin': 408}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:52,756] Trial 391 finished with value: 0.8545605944028176 and parameters: {'n_estimators': 349, 'eta': 0.01927423077603729, 'max_depth': 10, 'alpha': 0.6984, 'lambda': 3.927175498905943, 'max_bin': 375}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:55,314] Trial 392 finished with value: 0.8633719136296687 and parameters: {'n_estimators': 393, 'eta': 0.0957810137036025, 'max_depth': 7, 'alpha': 0.8412000000000001, 'lambda': 5.338963121413292, 'max_bin': 286}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 12:59:57,857] Trial 393 finished with value: 0.8664406736648139 and parameters: {'n_estimators': 328, 'eta': 0.09198756882296896, 'max_depth': 9, 'alpha': 0.8692000000000001, 'lambda': 8.054551915132127, 'max_bin': 366}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:00,530] Trial 394 finished with value: 0.8643858299197585 and parameters: {'n_estimators': 312, 'eta': 0.08718495490639341, 'max_depth': 8, 'alpha': 0.727, 'lambda': 6.904755531479424, 'max_bin': 416}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:02,914] Trial 395 finished with value: 0.8598962797903722 and parameters: {'n_estimators': 273, 'eta': 0.09701139755247096, 'max_depth': 10, 'alpha': 0.3961, 'lambda': 9.916128682765398, 'max_bin': 356}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:04,787] Trial 396 finished with value: 0.8735251269280255 and parameters: {'n_estimators': 257, 'eta': 0.0938265373593909, 'max_depth': 9, 'alpha': 0.44110000000000005, 'lambda': 1.7898169685580485, 'max_bin': 362}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:07,560] Trial 397 finished with value: 0.8640817296686285 and parameters: {'n_estimators': 233, 'eta': 0.046939430566067905, 'max_depth': 9, 'alpha': 0.7591, 'lambda': 3.0885841505385745, 'max_bin': 382}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:09,432] Trial 398 finished with value: 0.8645382144912297 and parameters: {'n_estimators': 360, 'eta': 0.0813116600810099, 'max_depth': 9, 'alpha': 0.25880000000000003, 'lambda': 1.0137138431706165, 'max_bin': 429}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:11,910] Trial 399 finished with value: 0.8734279451919109 and parameters: {'n_estimators': 197, 'eta': 0.061728464999631846, 'max_depth': 11, 'alpha': 0.6837000000000001, 'lambda': 4.272538445748621, 'max_bin': 372}. Best is trial 99 with value: 0.8849347158637322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8849\n",
      "\tBest params:\n",
      "\t\tn_estimators: 310\n",
      "\t\teta: 0.08977155105733017\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.8714000000000001\n",
      "\t\tlambda: 3.3899853772465174\n",
      "\t\tmax_bin: 366\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_7 = lambda trial: objective_xgb_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_xgb.optimize(func_xgb_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35af308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   43.000000   48.000000   50.000000   47.000000   \n",
      "1                    TN  193.000000  189.000000  190.000000  191.000000   \n",
      "2                    FP    6.000000   11.000000   11.000000    8.000000   \n",
      "3                    FN   26.000000   20.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.880597    0.884328    0.895522    0.888060   \n",
      "5             Precision    0.877551    0.813559    0.819672    0.854545   \n",
      "6           Sensitivity    0.623188    0.705882    0.746269    0.681159   \n",
      "7           Specificity    0.969800    0.945000    0.945300    0.959800   \n",
      "8              F1 score    0.728814    0.755906    0.781250    0.758065   \n",
      "9   F1 score (weighted)    0.873335    0.881502    0.893842    0.883642   \n",
      "10     F1 score (macro)    0.826129    0.840055    0.856311    0.842624   \n",
      "11    Balanced Accuracy    0.796519    0.825441    0.845771    0.820479   \n",
      "12                  MCC    0.670831    0.683554    0.714174    0.693921   \n",
      "13                  NPV    0.881300    0.904300    0.917900    0.896700   \n",
      "14              ROC_AUC    0.796519    0.825441    0.845771    0.820479   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    42.000000   53.000000   52.000000   49.000000  \n",
      "1   190.000000  191.000000  193.000000  190.000000  \n",
      "2    10.000000    9.000000    7.000000   11.000000  \n",
      "3    26.000000   15.000000   16.000000   18.000000  \n",
      "4     0.865672    0.910448    0.914179    0.891791  \n",
      "5     0.807692    0.854839    0.881356    0.816667  \n",
      "6     0.617647    0.779412    0.764706    0.731343  \n",
      "7     0.950000    0.955000    0.965000    0.945300  \n",
      "8     0.700000    0.815385    0.818898    0.771654  \n",
      "9     0.859300    0.909043    0.912082    0.889735  \n",
      "10    0.806731    0.878136    0.881331    0.850374  \n",
      "11    0.783824    0.867206    0.864853    0.838308  \n",
      "12    0.624625    0.757844    0.766334    0.702863  \n",
      "13    0.879600    0.927200    0.923400    0.913500  \n",
      "14    0.783824    0.867206    0.864853    0.838308  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_7 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet7, Y_testSet7)]\n",
    "optimized_xgb_7.fit(X_trainSet7,Y_trainSet7, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_7 = optimized_xgb_7.predict(X_testSet7)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_xgb_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_xgb_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_xgb_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_xgb_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_xgb_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_xgb_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_xgb_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_xgb_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_xgb_7)\n",
    "\n",
    "\n",
    "Set7 = pd.DataFrame({ 'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set7'] =Set7\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f4cebba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:00:14,185] Trial 400 finished with value: 0.8615215194465933 and parameters: {'n_estimators': 300, 'eta': 0.08961596164979325, 'max_depth': 9, 'alpha': 0.7441, 'lambda': 2.4404968853605045, 'max_bin': 479}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:16,946] Trial 401 finished with value: 0.8609673623546561 and parameters: {'n_estimators': 323, 'eta': 0.09995942336986631, 'max_depth': 10, 'alpha': 0.6121, 'lambda': 14.081624902280781, 'max_bin': 400}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:18,921] Trial 402 finished with value: 0.8595904659312099 and parameters: {'n_estimators': 286, 'eta': 0.08535118116891781, 'max_depth': 9, 'alpha': 0.3594, 'lambda': 3.259419873611073, 'max_bin': 347}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:22,006] Trial 403 finished with value: 0.8658600519733668 and parameters: {'n_estimators': 340, 'eta': 0.09678431457804054, 'max_depth': 8, 'alpha': 0.6629, 'lambda': 19.306876437960092, 'max_bin': 338}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:23,868] Trial 404 finished with value: 0.8554735491083274 and parameters: {'n_estimators': 372, 'eta': 0.09208901670450795, 'max_depth': 9, 'alpha': 0.5406000000000001, 'lambda': 1.9653190734567318, 'max_bin': 390}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:26,729] Trial 405 finished with value: 0.8628008543496624 and parameters: {'n_estimators': 263, 'eta': 0.05988843557192092, 'max_depth': 10, 'alpha': 0.7831, 'lambda': 6.2194995903276595, 'max_bin': 353}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:30,114] Trial 406 finished with value: 0.8573340867795487 and parameters: {'n_estimators': 313, 'eta': 0.09472566414064434, 'max_depth': 9, 'alpha': 0.9206000000000001, 'lambda': 34.252616727110066, 'max_bin': 368}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:31,987] Trial 407 finished with value: 0.8653308438426965 and parameters: {'n_estimators': 243, 'eta': 0.09847653386407178, 'max_depth': 8, 'alpha': 0.1296, 'lambda': 4.834587306470746, 'max_bin': 378}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:33,782] Trial 408 finished with value: 0.8547866280018205 and parameters: {'n_estimators': 143, 'eta': 0.07510635149514494, 'max_depth': 10, 'alpha': 0.7151000000000001, 'lambda': 3.77866757509908, 'max_bin': 363}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:35,971] Trial 409 finished with value: 0.8654309808973523 and parameters: {'n_estimators': 284, 'eta': 0.09614380675677857, 'max_depth': 9, 'alpha': 0.0966, 'lambda': 8.910599610165615, 'max_bin': 386}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:38,797] Trial 410 finished with value: 0.8582502246206334 and parameters: {'n_estimators': 350, 'eta': 0.09070087561048555, 'max_depth': 9, 'alpha': 0.6297, 'lambda': 16.080065233248167, 'max_bin': 404}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:40,777] Trial 411 finished with value: 0.8618269434023969 and parameters: {'n_estimators': 220, 'eta': 0.08790318179313306, 'max_depth': 9, 'alpha': 0.7011000000000001, 'lambda': 2.684974796897854, 'max_bin': 395}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:42,749] Trial 412 finished with value: 0.8638981371727634 and parameters: {'n_estimators': 328, 'eta': 0.09342766204913357, 'max_depth': 10, 'alpha': 0.734, 'lambda': 1.5893429384163795, 'max_bin': 373}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:45,149] Trial 413 finished with value: 0.8632339539492252 and parameters: {'n_estimators': 297, 'eta': 0.0773398711173565, 'max_depth': 9, 'alpha': 0.9999, 'lambda': 3.134466791827669, 'max_bin': 383}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:47,655] Trial 414 finished with value: 0.8548602564815224 and parameters: {'n_estimators': 264, 'eta': 0.06422206786293574, 'max_depth': 6, 'alpha': 0.7689, 'lambda': 11.539963744787638, 'max_bin': 360}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:49,661] Trial 415 finished with value: 0.8665937463861914 and parameters: {'n_estimators': 460, 'eta': 0.09817527809861087, 'max_depth': 9, 'alpha': 0.8967, 'lambda': 2.1970356689816697, 'max_bin': 343}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:51,799] Trial 416 finished with value: 0.8587839772253666 and parameters: {'n_estimators': 416, 'eta': 0.09521803963243684, 'max_depth': 10, 'alpha': 0.8024, 'lambda': 4.350566962179824, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:55,122] Trial 417 finished with value: 0.8672362514292417 and parameters: {'n_estimators': 311, 'eta': 0.045055118378081294, 'max_depth': 9, 'alpha': 0.5883, 'lambda': 5.474458914221932, 'max_bin': 353}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:57,067] Trial 418 finished with value: 0.8621560960583166 and parameters: {'n_estimators': 389, 'eta': 0.08615885420197579, 'max_depth': 8, 'alpha': 0.6807000000000001, 'lambda': 1.0213770564934586, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:00:59,768] Trial 419 finished with value: 0.8617396072506487 and parameters: {'n_estimators': 278, 'eta': 0.09153780510889983, 'max_depth': 9, 'alpha': 0.6401, 'lambda': 15.451568448294596, 'max_bin': 425}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:03,281] Trial 420 finished with value: 0.8620668466512263 and parameters: {'n_estimators': 346, 'eta': 0.054500583609021463, 'max_depth': 7, 'alpha': 0.7487, 'lambda': 12.934077953972025, 'max_bin': 377}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:06,215] Trial 421 finished with value: 0.8672129407149626 and parameters: {'n_estimators': 246, 'eta': 0.03887192990477355, 'max_depth': 9, 'alpha': 0.9521000000000001, 'lambda': 3.6911637621693503, 'max_bin': 388}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:09,166] Trial 422 finished with value: 0.8637037869345114 and parameters: {'n_estimators': 294, 'eta': 0.09814736331950881, 'max_depth': 10, 'alpha': 0.7127, 'lambda': 21.952419277821903, 'max_bin': 397}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:13,033] Trial 423 finished with value: 0.849939962114696 and parameters: {'n_estimators': 328, 'eta': 0.02728564202047944, 'max_depth': 9, 'alpha': 0.48410000000000003, 'lambda': 18.082418970775795, 'max_bin': 357}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:15,094] Trial 424 finished with value: 0.866480860152477 and parameters: {'n_estimators': 368, 'eta': 0.08362095874508099, 'max_depth': 9, 'alpha': 0.6642, 'lambda': 2.618061980885468, 'max_bin': 497}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:17,207] Trial 425 finished with value: 0.8692162965119532 and parameters: {'n_estimators': 312, 'eta': 0.09316919266399823, 'max_depth': 10, 'alpha': 0.8366, 'lambda': 1.695131582655183, 'max_bin': 408}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:20,125] Trial 426 finished with value: 0.85828345651932 and parameters: {'n_estimators': 266, 'eta': 0.08919515548763293, 'max_depth': 9, 'alpha': 0.7857000000000001, 'lambda': 26.50301112066083, 'max_bin': 379}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:22,465] Trial 427 finished with value: 0.8625093895298157 and parameters: {'n_estimators': 223, 'eta': 0.051882597160491505, 'max_depth': 8, 'alpha': 0.7312000000000001, 'lambda': 3.2924608466617356, 'max_bin': 369}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:24,456] Trial 428 finished with value: 0.8668405684953981 and parameters: {'n_estimators': 248, 'eta': 0.09637247021823428, 'max_depth': 9, 'alpha': 0.6899000000000001, 'lambda': 2.289390183687827, 'max_bin': 391}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:26,750] Trial 429 finished with value: 0.8623816331705741 and parameters: {'n_estimators': 202, 'eta': 0.09993331240713474, 'max_depth': 12, 'alpha': 0.6123000000000001, 'lambda': 14.623077474326335, 'max_bin': 433}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:29,381] Trial 430 finished with value: 0.8648585871975445 and parameters: {'n_estimators': 284, 'eta': 0.06692408790713583, 'max_depth': 10, 'alpha': 0.7587, 'lambda': 4.586226344472847, 'max_bin': 293}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:31,970] Trial 431 finished with value: 0.864519088483567 and parameters: {'n_estimators': 436, 'eta': 0.09466845205953456, 'max_depth': 8, 'alpha': 0.7984, 'lambda': 9.899053564819635, 'max_bin': 415}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:34,330] Trial 432 finished with value: 0.8678835829637375 and parameters: {'n_estimators': 335, 'eta': 0.09736858948462482, 'max_depth': 9, 'alpha': 0.8742000000000001, 'lambda': 7.761137601719811, 'max_bin': 346}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:36,222] Trial 433 finished with value: 0.8670126172829244 and parameters: {'n_estimators': 355, 'eta': 0.09122009174404053, 'max_depth': 9, 'alpha': 0.5191, 'lambda': 1.6611071411381482, 'max_bin': 402}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:38,681] Trial 434 finished with value: 0.8616058322062484 and parameters: {'n_estimators': 316, 'eta': 0.06906094182907149, 'max_depth': 10, 'alpha': 0.6531, 'lambda': 2.9104031620053625, 'max_bin': 279}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:41,658] Trial 435 finished with value: 0.8633712149618752 and parameters: {'n_estimators': 303, 'eta': 0.09364783790132757, 'max_depth': 9, 'alpha': 0.5701, 'lambda': 22.47491353642724, 'max_bin': 361}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:44,591] Trial 436 finished with value: 0.8683555456719002 and parameters: {'n_estimators': 271, 'eta': 0.07105778449129026, 'max_depth': 9, 'alpha': 0.7174, 'lambda': 17.266886591892764, 'max_bin': 269}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:46,695] Trial 437 finished with value: 0.8645628278197682 and parameters: {'n_estimators': 230, 'eta': 0.09587769012836103, 'max_depth': 9, 'alpha': 0.7511, 'lambda': 5.70573535510884, 'max_bin': 375}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:52,136] Trial 438 finished with value: 0.8516013568297514 and parameters: {'n_estimators': 487, 'eta': 0.03348649796954322, 'max_depth': 10, 'alpha': 0.9717, 'lambda': 20.90981873373288, 'max_bin': 384}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:55,380] Trial 439 finished with value: 0.8672643193538663 and parameters: {'n_estimators': 381, 'eta': 0.0551960812632964, 'max_depth': 9, 'alpha': 0.8167000000000001, 'lambda': 3.5831063003165857, 'max_bin': 335}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:01:57,431] Trial 440 finished with value: 0.859914435488958 and parameters: {'n_estimators': 186, 'eta': 0.05814370497588836, 'max_depth': 9, 'alpha': 0.9306000000000001, 'lambda': 2.065730615485019, 'max_bin': 352}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:00,708] Trial 441 finished with value: 0.8599118651172274 and parameters: {'n_estimators': 402, 'eta': 0.0435702745846494, 'max_depth': 8, 'alpha': 0.7772, 'lambda': 1.021756479322422, 'max_bin': 367}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:02,253] Trial 442 finished with value: 0.8625402509550252 and parameters: {'n_estimators': 118, 'eta': 0.08139363711364123, 'max_depth': 10, 'alpha': 0.7028, 'lambda': 13.502882182697874, 'max_bin': 393}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:05,807] Trial 443 finished with value: 0.8642761707656093 and parameters: {'n_estimators': 338, 'eta': 0.040808751391079895, 'max_depth': 9, 'alpha': 0.7277, 'lambda': 6.710507514686755, 'max_bin': 381}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:08,426] Trial 444 finished with value: 0.8617598950810406 and parameters: {'n_estimators': 256, 'eta': 0.09826493019229682, 'max_depth': 8, 'alpha': 0.764, 'lambda': 20.633760487509964, 'max_bin': 373}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:11,402] Trial 445 finished with value: 0.8578800952471939 and parameters: {'n_estimators': 294, 'eta': 0.08872501072377995, 'max_depth': 9, 'alpha': 0.6781, 'lambda': 23.010078807820364, 'max_bin': 324}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:13,528] Trial 446 finished with value: 0.8616513591346001 and parameters: {'n_estimators': 321, 'eta': 0.08642891098588688, 'max_depth': 7, 'alpha': 0.2798, 'lambda': 4.673674338844464, 'max_bin': 360}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:15,462] Trial 447 finished with value: 0.8632714374688112 and parameters: {'n_estimators': 357, 'eta': 0.09267712852884953, 'max_depth': 10, 'alpha': 0.6076, 'lambda': 3.8353175451305868, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:17,283] Trial 448 finished with value: 0.8685116698288375 and parameters: {'n_estimators': 277, 'eta': 0.09994320825542435, 'max_depth': 9, 'alpha': 0.6349, 'lambda': 2.4772537341155547, 'max_bin': 388}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:19,324] Trial 449 finished with value: 0.8635857179689838 and parameters: {'n_estimators': 300, 'eta': 0.09648817675489821, 'max_depth': 11, 'alpha': 0.6548, 'lambda': 3.018014143314934, 'max_bin': 300}. Best is trial 99 with value: 0.8849347158637322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8849\n",
      "\tBest params:\n",
      "\t\tn_estimators: 310\n",
      "\t\teta: 0.08977155105733017\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.8714000000000001\n",
      "\t\tlambda: 3.3899853772465174\n",
      "\t\tmax_bin: 366\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_8 = lambda trial: objective_xgb_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_xgb.optimize(func_xgb_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b9ad3192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   43.000000   48.000000   50.000000   47.000000   \n",
      "1                    TN  193.000000  189.000000  190.000000  191.000000   \n",
      "2                    FP    6.000000   11.000000   11.000000    8.000000   \n",
      "3                    FN   26.000000   20.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.880597    0.884328    0.895522    0.888060   \n",
      "5             Precision    0.877551    0.813559    0.819672    0.854545   \n",
      "6           Sensitivity    0.623188    0.705882    0.746269    0.681159   \n",
      "7           Specificity    0.969800    0.945000    0.945300    0.959800   \n",
      "8              F1 score    0.728814    0.755906    0.781250    0.758065   \n",
      "9   F1 score (weighted)    0.873335    0.881502    0.893842    0.883642   \n",
      "10     F1 score (macro)    0.826129    0.840055    0.856311    0.842624   \n",
      "11    Balanced Accuracy    0.796519    0.825441    0.845771    0.820479   \n",
      "12                  MCC    0.670831    0.683554    0.714174    0.693921   \n",
      "13                  NPV    0.881300    0.904300    0.917900    0.896700   \n",
      "14              ROC_AUC    0.796519    0.825441    0.845771    0.820479   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    42.000000   53.000000   52.000000   49.000000   49.000000  \n",
      "1   190.000000  191.000000  193.000000  190.000000  195.000000  \n",
      "2    10.000000    9.000000    7.000000   11.000000    7.000000  \n",
      "3    26.000000   15.000000   16.000000   18.000000   17.000000  \n",
      "4     0.865672    0.910448    0.914179    0.891791    0.910448  \n",
      "5     0.807692    0.854839    0.881356    0.816667    0.875000  \n",
      "6     0.617647    0.779412    0.764706    0.731343    0.742424  \n",
      "7     0.950000    0.955000    0.965000    0.945300    0.965300  \n",
      "8     0.700000    0.815385    0.818898    0.771654    0.803279  \n",
      "9     0.859300    0.909043    0.912082    0.889735    0.907859  \n",
      "10    0.806731    0.878136    0.881331    0.850374    0.872654  \n",
      "11    0.783824    0.867206    0.864853    0.838308    0.853885  \n",
      "12    0.624625    0.757844    0.766334    0.702863    0.750029  \n",
      "13    0.879600    0.927200    0.923400    0.913500    0.919800  \n",
      "14    0.783824    0.867206    0.864853    0.838308    0.853885  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_8 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet8, Y_testSet8)]\n",
    "optimized_xgb_8.fit(X_trainSet8,Y_trainSet8, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_8 = optimized_xgb_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_xgb_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_xgb_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_xgb_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_xgb_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_xgb_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_xgb_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_xgb_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_xgb_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_xgb_8)\n",
    "\n",
    "\n",
    "Set8 = pd.DataFrame({ 'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set8'] =Set8\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5d985847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:02:22,479] Trial 450 finished with value: 0.8516424354181339 and parameters: {'n_estimators': 341, 'eta': 0.09065947408036634, 'max_depth': 9, 'alpha': 0.7394000000000001, 'lambda': 18.716691588108347, 'max_bin': 421}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:26,272] Trial 451 finished with value: 0.8584927182411072 and parameters: {'n_estimators': 826, 'eta': 0.09460265980828841, 'max_depth': 9, 'alpha': 0.6959000000000001, 'lambda': 29.64910303803928, 'max_bin': 411}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:28,799] Trial 452 finished with value: 0.8577541459848474 and parameters: {'n_estimators': 240, 'eta': 0.07894393816704885, 'max_depth': 10, 'alpha': 0.5895, 'lambda': 14.975724033850106, 'max_bin': 331}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:30,706] Trial 453 finished with value: 0.8546474203794208 and parameters: {'n_estimators': 160, 'eta': 0.09773986705493663, 'max_depth': 9, 'alpha': 0.5567, 'lambda': 23.40884359379624, 'max_bin': 398}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:33,432] Trial 454 finished with value: 0.8587308870080317 and parameters: {'n_estimators': 373, 'eta': 0.0881273093161839, 'max_depth': 8, 'alpha': 0.7883, 'lambda': 16.58047735166269, 'max_bin': 355}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:35,892] Trial 455 finished with value: 0.8556954792344594 and parameters: {'n_estimators': 260, 'eta': 0.09259519663343671, 'max_depth': 9, 'alpha': 0.8614, 'lambda': 12.040857485864535, 'max_bin': 376}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:38,416] Trial 456 finished with value: 0.8561133585166031 and parameters: {'n_estimators': 323, 'eta': 0.09557130179439202, 'max_depth': 10, 'alpha': 0.7226, 'lambda': 9.309845836927638, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:40,740] Trial 457 finished with value: 0.8655437930706922 and parameters: {'n_estimators': 289, 'eta': 0.08448431670779813, 'max_depth': 9, 'alpha': 0.0044, 'lambda': 10.65166837786318, 'max_bin': 348}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:43,163] Trial 458 finished with value: 0.859033397933727 and parameters: {'n_estimators': 312, 'eta': 0.09691752267947741, 'max_depth': 9, 'alpha': 0.6703, 'lambda': 8.51216723277105, 'max_bin': 382}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:45,617] Trial 459 finished with value: 0.8489119943405852 and parameters: {'n_estimators': 209, 'eta': 0.06035502272395299, 'max_depth': 9, 'alpha': 0.8320000000000001, 'lambda': 17.526628915673673, 'max_bin': 394}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:47,569] Trial 460 finished with value: 0.867829649844782 and parameters: {'n_estimators': 274, 'eta': 0.09074457198030637, 'max_depth': 10, 'alpha': 0.895, 'lambda': 1.6799241174207133, 'max_bin': 364}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:49,569] Trial 461 finished with value: 0.8543774965333508 and parameters: {'n_estimators': 341, 'eta': 0.0940632514265856, 'max_depth': 8, 'alpha': 0.7605000000000001, 'lambda': 3.9063023788367395, 'max_bin': 456}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:51,879] Trial 462 finished with value: 0.8654583189308317 and parameters: {'n_estimators': 230, 'eta': 0.057420559456314246, 'max_depth': 9, 'alpha': 0.7091000000000001, 'lambda': 2.698441195980958, 'max_bin': 359}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:54,109] Trial 463 finished with value: 0.8577985126467871 and parameters: {'n_estimators': 300, 'eta': 0.09828040323850677, 'max_depth': 10, 'alpha': 0.7462000000000001, 'lambda': 7.052454008747912, 'max_bin': 429}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:56,338] Trial 464 finished with value: 0.8642313111653127 and parameters: {'n_estimators': 364, 'eta': 0.07607868374625947, 'max_depth': 9, 'alpha': 0.0721, 'lambda': 5.315733876551698, 'max_bin': 343}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:02:58,087] Trial 465 finished with value: 0.8527357643582283 and parameters: {'n_estimators': 259, 'eta': 0.09994916757194017, 'max_depth': 7, 'alpha': 0.6363, 'lambda': 2.0763512807905133, 'max_bin': 386}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:00,169] Trial 466 finished with value: 0.8564856801357316 and parameters: {'n_estimators': 420, 'eta': 0.08936463984414442, 'max_depth': 9, 'alpha': 0.6821, 'lambda': 3.2815855947327104, 'max_bin': 438}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:03,369] Trial 467 finished with value: 0.8566051044911027 and parameters: {'n_estimators': 687, 'eta': 0.0954909648225437, 'max_depth': 9, 'alpha': 0.046900000000000004, 'lambda': 23.505136481839816, 'max_bin': 405}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:07,523] Trial 468 finished with value: 0.8320964954613856 and parameters: {'n_estimators': 461, 'eta': 0.032816032224238775, 'max_depth': 5, 'alpha': 0.7767000000000001, 'lambda': 37.31689719225194, 'max_bin': 338}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:10,141] Trial 469 finished with value: 0.869437737862633 and parameters: {'n_estimators': 324, 'eta': 0.04660676123734905, 'max_depth': 8, 'alpha': 0.8163, 'lambda': 1.0435245917947182, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:13,134] Trial 470 finished with value: 0.8672082545667552 and parameters: {'n_estimators': 390, 'eta': 0.050689873300058474, 'max_depth': 9, 'alpha': 0.7346, 'lambda': 2.488929808444481, 'max_bin': 401}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:15,343] Trial 471 finished with value: 0.859810775628414 and parameters: {'n_estimators': 287, 'eta': 0.08321680371847824, 'max_depth': 10, 'alpha': 0.6227, 'lambda': 4.361648870683802, 'max_bin': 374}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:18,056] Trial 472 finished with value: 0.8698158706550126 and parameters: {'n_estimators': 442, 'eta': 0.05455501714675301, 'max_depth': 9, 'alpha': 0.1486, 'lambda': 1.635350343257738, 'max_bin': 390}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:21,096] Trial 473 finished with value: 0.851350740828319 and parameters: {'n_estimators': 311, 'eta': 0.08662989159983825, 'max_depth': 10, 'alpha': 0.7079000000000001, 'lambda': 19.784719956955556, 'max_bin': 379}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:24,124] Trial 474 finished with value: 0.8439797745077342 and parameters: {'n_estimators': 249, 'eta': 0.016562788831396377, 'max_depth': 9, 'alpha': 0.216, 'lambda': 5.749790494427325, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:27,290] Trial 475 finished with value: 0.8658495469710962 and parameters: {'n_estimators': 349, 'eta': 0.04464425294009635, 'max_depth': 9, 'alpha': 0.7961, 'lambda': 3.1051283273342714, 'max_bin': 351}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:30,619] Trial 476 finished with value: 0.849212416249981 and parameters: {'n_estimators': 331, 'eta': 0.06793980878098399, 'max_depth': 10, 'alpha': 0.6668000000000001, 'lambda': 12.749160246266477, 'max_bin': 358}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:32,626] Trial 477 finished with value: 0.855996557682064 and parameters: {'n_estimators': 279, 'eta': 0.09287079964236662, 'max_depth': 8, 'alpha': 0.8513000000000001, 'lambda': 4.0638522457569, 'max_bin': 395}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:34,644] Trial 478 finished with value: 0.8566838842740809 and parameters: {'n_estimators': 209, 'eta': 0.09767681507854688, 'max_depth': 9, 'alpha': 0.5949, 'lambda': 4.90410772998035, 'max_bin': 370}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:37,246] Trial 479 finished with value: 0.8592257730944061 and parameters: {'n_estimators': 304, 'eta': 0.08051567641453232, 'max_depth': 9, 'alpha': 0.6931, 'lambda': 11.550589783602133, 'max_bin': 384}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:40,214] Trial 480 finished with value: 0.8558664089366218 and parameters: {'n_estimators': 271, 'eta': 0.07166157217640536, 'max_depth': 10, 'alpha': 0.7587, 'lambda': 20.089967515136724, 'max_bin': 363}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:42,185] Trial 481 finished with value: 0.8590191977959754 and parameters: {'n_estimators': 362, 'eta': 0.09138429933603885, 'max_depth': 7, 'alpha': 0.7265, 'lambda': 1.870887644252726, 'max_bin': 375}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:48,503] Trial 482 finished with value: 0.834373407989121 and parameters: {'n_estimators': 545, 'eta': 0.009535631290475487, 'max_depth': 9, 'alpha': 0.7429, 'lambda': 26.32527790797124, 'max_bin': 355}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:51,195] Trial 483 finished with value: 0.849186115744746 and parameters: {'n_estimators': 234, 'eta': 0.05568202550270452, 'max_depth': 9, 'alpha': 0.11800000000000001, 'lambda': 21.19016211104288, 'max_bin': 418}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:54,026] Trial 484 finished with value: 0.8563225518803458 and parameters: {'n_estimators': 291, 'eta': 0.0496845553430867, 'max_depth': 9, 'alpha': 0.7739, 'lambda': 3.4699895425962506, 'max_bin': 399}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:55,551] Trial 485 finished with value: 0.8598986191416265 and parameters: {'n_estimators': 172, 'eta': 0.07383035934524226, 'max_depth': 6, 'alpha': 0.6479, 'lambda': 1.0093521789802065, 'max_bin': 349}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:03:58,279] Trial 486 finished with value: 0.8599935894042192 and parameters: {'n_estimators': 331, 'eta': 0.09512816409703378, 'max_depth': 11, 'alpha': 0.9441, 'lambda': 14.295828393979239, 'max_bin': 379}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:01,229] Trial 487 finished with value: 0.86594353545192 and parameters: {'n_estimators': 252, 'eta': 0.025185733630169856, 'max_depth': 9, 'alpha': 0.6103000000000001, 'lambda': 2.3128540876237467, 'max_bin': 390}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:04,142] Trial 488 finished with value: 0.8532443130488362 and parameters: {'n_estimators': 321, 'eta': 0.09634603992469147, 'max_depth': 9, 'alpha': 0.7123, 'lambda': 21.572482534618597, 'max_bin': 408}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:06,806] Trial 489 finished with value: 0.8697324522351912 and parameters: {'n_estimators': 381, 'eta': 0.06369920968689743, 'max_depth': 10, 'alpha': 0.8804000000000001, 'lambda': 3.1215891620343212, 'max_bin': 341}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:08,994] Trial 490 finished with value: 0.8563661965055834 and parameters: {'n_estimators': 403, 'eta': 0.09354120900188585, 'max_depth': 8, 'alpha': 0.4202, 'lambda': 6.3704203912190644, 'max_bin': 365}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:11,268] Trial 491 finished with value: 0.854167527609075 and parameters: {'n_estimators': 348, 'eta': 0.09854867845255787, 'max_depth': 9, 'alpha': 0.5101, 'lambda': 7.620869593976217, 'max_bin': 386}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:14,457] Trial 492 finished with value: 0.8528421094602686 and parameters: {'n_estimators': 304, 'eta': 0.06536173354458645, 'max_depth': 10, 'alpha': 0.9122, 'lambda': 16.439002386918016, 'max_bin': 368}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:16,996] Trial 493 finished with value: 0.8591237671977285 and parameters: {'n_estimators': 265, 'eta': 0.08792431191198283, 'max_depth': 8, 'alpha': 0.46900000000000003, 'lambda': 18.863636054355887, 'max_bin': 423}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:18,847] Trial 494 finished with value: 0.8611941368357604 and parameters: {'n_estimators': 216, 'eta': 0.0999804934714187, 'max_depth': 9, 'alpha': 0.8038000000000001, 'lambda': 2.5317118392871025, 'max_bin': 376}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:22,065] Trial 495 finished with value: 0.869297472737359 and parameters: {'n_estimators': 746, 'eta': 0.048065587599503766, 'max_depth': 10, 'alpha': 0.5346000000000001, 'lambda': 3.697655584374008, 'max_bin': 360}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:24,657] Trial 496 finished with value: 0.8622478530409486 and parameters: {'n_estimators': 282, 'eta': 0.05116748507780749, 'max_depth': 9, 'alpha': 0.6869000000000001, 'lambda': 1.6265113799559678, 'max_bin': 382}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:26,917] Trial 497 finished with value: 0.8580854895615582 and parameters: {'n_estimators': 339, 'eta': 0.09153777436297027, 'max_depth': 9, 'alpha': 0.6659, 'lambda': 4.535531050356415, 'max_bin': 316}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:29,448] Trial 498 finished with value: 0.8599758060671178 and parameters: {'n_estimators': 246, 'eta': 0.08960340898081069, 'max_depth': 9, 'alpha': 0.7526, 'lambda': 18.176636600427397, 'max_bin': 328}. Best is trial 99 with value: 0.8849347158637322.\n",
      "[I 2023-12-05 13:04:32,172] Trial 499 finished with value: 0.855155507874335 and parameters: {'n_estimators': 308, 'eta': 0.08509834961546472, 'max_depth': 10, 'alpha': 0.1942, 'lambda': 15.85799708674896, 'max_bin': 395}. Best is trial 99 with value: 0.8849347158637322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8849\n",
      "\tBest params:\n",
      "\t\tn_estimators: 310\n",
      "\t\teta: 0.08977155105733017\n",
      "\t\tmax_depth: 9\n",
      "\t\talpha: 0.8714000000000001\n",
      "\t\tlambda: 3.3899853772465174\n",
      "\t\tmax_bin: 366\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_xgb_9 = lambda trial: objective_xgb_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_xgb.optimize(func_xgb_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_xgb.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_xgb.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_xgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e9f6fc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   43.000000   48.000000   50.000000   47.000000   \n",
      "1                    TN  193.000000  189.000000  190.000000  191.000000   \n",
      "2                    FP    6.000000   11.000000   11.000000    8.000000   \n",
      "3                    FN   26.000000   20.000000   17.000000   22.000000   \n",
      "4              Accuracy    0.880597    0.884328    0.895522    0.888060   \n",
      "5             Precision    0.877551    0.813559    0.819672    0.854545   \n",
      "6           Sensitivity    0.623188    0.705882    0.746269    0.681159   \n",
      "7           Specificity    0.969800    0.945000    0.945300    0.959800   \n",
      "8              F1 score    0.728814    0.755906    0.781250    0.758065   \n",
      "9   F1 score (weighted)    0.873335    0.881502    0.893842    0.883642   \n",
      "10     F1 score (macro)    0.826129    0.840055    0.856311    0.842624   \n",
      "11    Balanced Accuracy    0.796519    0.825441    0.845771    0.820479   \n",
      "12                  MCC    0.670831    0.683554    0.714174    0.693921   \n",
      "13                  NPV    0.881300    0.904300    0.917900    0.896700   \n",
      "14              ROC_AUC    0.796519    0.825441    0.845771    0.820479   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    42.000000   53.000000   52.000000   49.000000   49.000000   45.000000  \n",
      "1   190.000000  191.000000  193.000000  190.000000  195.000000  196.000000  \n",
      "2    10.000000    9.000000    7.000000   11.000000    7.000000    5.000000  \n",
      "3    26.000000   15.000000   16.000000   18.000000   17.000000   22.000000  \n",
      "4     0.865672    0.910448    0.914179    0.891791    0.910448    0.899254  \n",
      "5     0.807692    0.854839    0.881356    0.816667    0.875000    0.900000  \n",
      "6     0.617647    0.779412    0.764706    0.731343    0.742424    0.671642  \n",
      "7     0.950000    0.955000    0.965000    0.945300    0.965300    0.975100  \n",
      "8     0.700000    0.815385    0.818898    0.771654    0.803279    0.769231  \n",
      "9     0.859300    0.909043    0.912082    0.889735    0.907859    0.893978  \n",
      "10    0.806731    0.878136    0.881331    0.850374    0.872654    0.852396  \n",
      "11    0.783824    0.867206    0.864853    0.838308    0.853885    0.823383  \n",
      "12    0.624625    0.757844    0.766334    0.702863    0.750029    0.718902  \n",
      "13    0.879600    0.927200    0.923400    0.913500    0.919800    0.899100  \n",
      "14    0.783824    0.867206    0.864853    0.838308    0.853885    0.823383  \n"
     ]
    }
   ],
   "source": [
    "optimized_xgb_9 = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "    \n",
    "eval_set = [(X_testSet9, Y_testSet9)]\n",
    "optimized_xgb_9.fit(X_trainSet9,Y_trainSet9, \n",
    "                          eval_set=eval_set,\n",
    "                          eval_metric=[\"logloss\"],\n",
    "                          early_stopping_rounds=50,\n",
    "                          verbose= False,\n",
    "                  )\n",
    "\n",
    "#predict        \n",
    "y_pred_xgb_9 = optimized_xgb_9.predict(X_testSet9)\n",
    "\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_xgb_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_xgb_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_xgb_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_xgb_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_xgb_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_xgb_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_xgb_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_xgb_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_xgb_9)\n",
    "\n",
    "\n",
    "Set9 = pd.DataFrame({ 'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "            \n",
    "                       })    \n",
    "\n",
    "mat_met_xgb_test['Set9'] =Set9\n",
    "print(mat_met_xgb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c1317b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_optimization_history(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b90d1484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_param_importances(study_xgb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b46bd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>3.098387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>2.867442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.838231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.018462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.914129</td>\n",
       "      <td>0.037739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.881816</td>\n",
       "      <td>0.095218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.760387</td>\n",
       "      <td>0.091534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.965190</td>\n",
       "      <td>0.028195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.814631</td>\n",
       "      <td>0.084099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.911807</td>\n",
       "      <td>0.038997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.879345</td>\n",
       "      <td>0.054122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.862787</td>\n",
       "      <td>0.054089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.764212</td>\n",
       "      <td>0.107698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.924150</td>\n",
       "      <td>0.027346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.862787</td>\n",
       "      <td>0.054089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP        25.400000     3.098387\n",
       "1                    TN        97.000000     2.867442\n",
       "2                    FP         3.500000     2.838231\n",
       "3                    FN         8.000000     3.018462\n",
       "4              Accuracy         0.914129     0.037739\n",
       "5             Precision         0.881816     0.095218\n",
       "6           Sensitivity         0.760387     0.091534\n",
       "7           Specificity         0.965190     0.028195\n",
       "8              F1 score         0.814631     0.084099\n",
       "9   F1 score (weighted)         0.911807     0.038997\n",
       "10     F1 score (macro)         0.879345     0.054122\n",
       "11    Balanced Accuracy         0.862787     0.054089\n",
       "12                  MCC         0.764212     0.107698\n",
       "13                  NPV         0.924150     0.027346\n",
       "14              ROC_AUC         0.862787     0.054089"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_xgb_CV(study_xgb.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fc89d739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>47.800000</td>\n",
       "      <td>3.614784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>191.800000</td>\n",
       "      <td>2.347576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>2.223611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>3.984693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.884328</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.888060</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.914179</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.894030</td>\n",
       "      <td>0.015253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.850088</td>\n",
       "      <td>0.033433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.623188</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>0.671642</td>\n",
       "      <td>0.706367</td>\n",
       "      <td>0.056653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.969800</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.945300</td>\n",
       "      <td>0.959800</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.945300</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>0.975100</td>\n",
       "      <td>0.957560</td>\n",
       "      <td>0.011055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.755906</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.818898</td>\n",
       "      <td>0.771654</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.770248</td>\n",
       "      <td>0.037454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.873335</td>\n",
       "      <td>0.881502</td>\n",
       "      <td>0.893842</td>\n",
       "      <td>0.883642</td>\n",
       "      <td>0.859300</td>\n",
       "      <td>0.909043</td>\n",
       "      <td>0.912082</td>\n",
       "      <td>0.889735</td>\n",
       "      <td>0.907859</td>\n",
       "      <td>0.893978</td>\n",
       "      <td>0.890432</td>\n",
       "      <td>0.016769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.826129</td>\n",
       "      <td>0.840055</td>\n",
       "      <td>0.856311</td>\n",
       "      <td>0.842624</td>\n",
       "      <td>0.806731</td>\n",
       "      <td>0.878136</td>\n",
       "      <td>0.881331</td>\n",
       "      <td>0.850374</td>\n",
       "      <td>0.872654</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.850674</td>\n",
       "      <td>0.023388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.796519</td>\n",
       "      <td>0.825441</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.820479</td>\n",
       "      <td>0.783824</td>\n",
       "      <td>0.867206</td>\n",
       "      <td>0.864853</td>\n",
       "      <td>0.838308</td>\n",
       "      <td>0.853885</td>\n",
       "      <td>0.823383</td>\n",
       "      <td>0.831967</td>\n",
       "      <td>0.027602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.670831</td>\n",
       "      <td>0.683554</td>\n",
       "      <td>0.714174</td>\n",
       "      <td>0.693921</td>\n",
       "      <td>0.624625</td>\n",
       "      <td>0.757844</td>\n",
       "      <td>0.766334</td>\n",
       "      <td>0.702863</td>\n",
       "      <td>0.750029</td>\n",
       "      <td>0.718902</td>\n",
       "      <td>0.708308</td>\n",
       "      <td>0.043416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.881300</td>\n",
       "      <td>0.904300</td>\n",
       "      <td>0.917900</td>\n",
       "      <td>0.896700</td>\n",
       "      <td>0.879600</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>0.923400</td>\n",
       "      <td>0.913500</td>\n",
       "      <td>0.919800</td>\n",
       "      <td>0.899100</td>\n",
       "      <td>0.906280</td>\n",
       "      <td>0.016944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.796519</td>\n",
       "      <td>0.825441</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.820479</td>\n",
       "      <td>0.783824</td>\n",
       "      <td>0.867206</td>\n",
       "      <td>0.864853</td>\n",
       "      <td>0.838308</td>\n",
       "      <td>0.853885</td>\n",
       "      <td>0.823383</td>\n",
       "      <td>0.831967</td>\n",
       "      <td>0.027602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   43.000000   48.000000   50.000000   47.000000   \n",
       "1                    TN  193.000000  189.000000  190.000000  191.000000   \n",
       "2                    FP    6.000000   11.000000   11.000000    8.000000   \n",
       "3                    FN   26.000000   20.000000   17.000000   22.000000   \n",
       "4              Accuracy    0.880597    0.884328    0.895522    0.888060   \n",
       "5             Precision    0.877551    0.813559    0.819672    0.854545   \n",
       "6           Sensitivity    0.623188    0.705882    0.746269    0.681159   \n",
       "7           Specificity    0.969800    0.945000    0.945300    0.959800   \n",
       "8              F1 score    0.728814    0.755906    0.781250    0.758065   \n",
       "9   F1 score (weighted)    0.873335    0.881502    0.893842    0.883642   \n",
       "10     F1 score (macro)    0.826129    0.840055    0.856311    0.842624   \n",
       "11    Balanced Accuracy    0.796519    0.825441    0.845771    0.820479   \n",
       "12                  MCC    0.670831    0.683554    0.714174    0.693921   \n",
       "13                  NPV    0.881300    0.904300    0.917900    0.896700   \n",
       "14              ROC_AUC    0.796519    0.825441    0.845771    0.820479   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    42.000000   53.000000   52.000000   49.000000   49.000000   45.000000   \n",
       "1   190.000000  191.000000  193.000000  190.000000  195.000000  196.000000   \n",
       "2    10.000000    9.000000    7.000000   11.000000    7.000000    5.000000   \n",
       "3    26.000000   15.000000   16.000000   18.000000   17.000000   22.000000   \n",
       "4     0.865672    0.910448    0.914179    0.891791    0.910448    0.899254   \n",
       "5     0.807692    0.854839    0.881356    0.816667    0.875000    0.900000   \n",
       "6     0.617647    0.779412    0.764706    0.731343    0.742424    0.671642   \n",
       "7     0.950000    0.955000    0.965000    0.945300    0.965300    0.975100   \n",
       "8     0.700000    0.815385    0.818898    0.771654    0.803279    0.769231   \n",
       "9     0.859300    0.909043    0.912082    0.889735    0.907859    0.893978   \n",
       "10    0.806731    0.878136    0.881331    0.850374    0.872654    0.852396   \n",
       "11    0.783824    0.867206    0.864853    0.838308    0.853885    0.823383   \n",
       "12    0.624625    0.757844    0.766334    0.702863    0.750029    0.718902   \n",
       "13    0.879600    0.927200    0.923400    0.913500    0.919800    0.899100   \n",
       "14    0.783824    0.867206    0.864853    0.838308    0.853885    0.823383   \n",
       "\n",
       "           ave       std  \n",
       "0    47.800000  3.614784  \n",
       "1   191.800000  2.347576  \n",
       "2     8.500000  2.223611  \n",
       "3    19.900000  3.984693  \n",
       "4     0.894030  0.015253  \n",
       "5     0.850088  0.033433  \n",
       "6     0.706367  0.056653  \n",
       "7     0.957560  0.011055  \n",
       "8     0.770248  0.037454  \n",
       "9     0.890432  0.016769  \n",
       "10    0.850674  0.023388  \n",
       "11    0.831967  0.027602  \n",
       "12    0.708308  0.043416  \n",
       "13    0.906280  0.016944  \n",
       "14    0.831967  0.027602  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_xgb_test['ave'] = mat_met_xgb_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test['std'] = mat_met_xgb_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_xgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "01de6232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.910829</td>\n",
       "      <td>0.021558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.873710</td>\n",
       "      <td>0.050177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.752712</td>\n",
       "      <td>0.069414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.963398</td>\n",
       "      <td>0.015591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.806939</td>\n",
       "      <td>0.050839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.908308</td>\n",
       "      <td>0.022817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.874456</td>\n",
       "      <td>0.032137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.858053</td>\n",
       "      <td>0.036121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.754312</td>\n",
       "      <td>0.062008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.921778</td>\n",
       "      <td>0.020479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.858053</td>\n",
       "      <td>0.036121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.910829     0.021558\n",
       "1             Precision         0.873710     0.050177\n",
       "2           Sensitivity         0.752712     0.069414\n",
       "3           Specificity         0.963398     0.015591\n",
       "4              F1 score         0.806939     0.050839\n",
       "5   F1 score (weighted)         0.908308     0.022817\n",
       "6      F1 score (macro)         0.874456     0.032137\n",
       "7     Balanced Accuracy         0.858053     0.036121\n",
       "8                   MCC         0.754312     0.062008\n",
       "9                   NPV         0.921778     0.020479\n",
       "10              ROC_AUC         0.858053     0.036121"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_xgb=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_xgb = xgb.XGBClassifier(objective=\"binary:logistic\",\n",
    "                                    random_state=1121218, \n",
    "                                    booster =\"gbtree\", \n",
    "                                    tree_method='hist', \n",
    "                                    n_estimators = study_xgb.best_params['n_estimators'], \n",
    "                                    eta = study_xgb.best_params['eta'],\n",
    "                                    max_depth = study_xgb.best_params['max_depth'], \n",
    "                                    max_bin = study_xgb.best_params['max_bin'], \n",
    "                                    reg_lambda = study_xgb.best_params['lambda'], \n",
    "                                    alpha =study_xgb.best_params['alpha'],  \n",
    "                                    n_jobs=8,\n",
    "                                    subsample=0.8, \n",
    "                                   )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        eval_set = [(X_test, y_test)]\n",
    "        optimizedCV_xgb.fit(X_train,y_train, \n",
    "            eval_set=eval_set,\n",
    "            eval_metric=[\"logloss\"],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose= False,\n",
    "                  )\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_xgb = optimizedCV_xgb.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_xgb': y_pred_optimized_xgb } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_xgb)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_xgb))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_xgb))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_xgb))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_xgb, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_xgb))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_xgb))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_xgb))\n",
    "        \n",
    "    data_xgb['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_xgb['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_xgb['y_pred_xgb' + str(i)] = data_inner['y_pred_xgb']\n",
    "   # data_xgb['correct' + str(i)] = correct_value\n",
    "   # data_xgb['pred' + str(i)] = y_pred_optimized_xgb\n",
    "\n",
    "mat_met_optimized_xgb = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "mat_met_optimized_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "eac08484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:04:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:04:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:05:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:05:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:05:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:05:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:05:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:05:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:05:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost baseline model f1_score 0.8792 with a standard deviation of 0.0337\n",
      "XGBoost optimized model f1_score 0.8746 with a standard deviation of 0.0382\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized XGBoost \n",
    "fit_params = {'early_stopping_rounds': 50, \n",
    "            'eval_set': [(X_tr, Y_tr), (X_te, Y_te)],\n",
    "              'verbose' : False,\n",
    "             }\n",
    "\n",
    "xgb_baseline_CVscore = cross_val_score(xgb_clf, X, Y, cv=10, scoring=\"f1_macro\", )\n",
    "#cv_xgb_opt_testSet = cross_val_score(optimized_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "cv_xgb_opt = cross_val_score(optimizedCV_xgb, X, Y, cv=10, scoring=\"f1_macro\", fit_params = fit_params)\n",
    "print(\"XGBoost baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(xgb_baseline_CVscore), np.std(xgb_baseline_CVscore, ddof=1)))\n",
    "#print(\"XGBoost optimized model (tested with Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_xgb_opt_testSet.mean(), cv_xgb_opt_testSet.std()))\n",
    "print(\"XGBoost optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_xgb_opt), np.std(cv_xgb_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7db6158b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_xgb_clf.joblib']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(xgb_clf, \"OUTPUT/xgb_clf.joblib\")\n",
    "#joblib.dump(optimized_xgb, \"OUTPUT/optimized_xgb.joblib\")\n",
    "joblib.dump(optimizedCV_xgb, \"OUTPUT/optimizedCV_xgb_clf.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4b54e",
   "metadata": {},
   "source": [
    "## KNeighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6f757a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        25.900000     2.233582\n",
      "1                    TN        93.100000     3.784471\n",
      "2                    FP         7.400000     3.657564\n",
      "3                    FN         7.500000     2.068279\n",
      "4              Accuracy         0.888733     0.030113\n",
      "5             Precision         0.785618     0.085761\n",
      "6           Sensitivity         0.775286     0.061937\n",
      "7           Specificity         0.926350     0.036577\n",
      "8              F1 score         0.777486     0.054745\n",
      "9   F1 score (weighted)         0.888733     0.028943\n",
      "10     F1 score (macro)         0.851575     0.037433\n",
      "11    Balanced Accuracy         0.850815     0.034572\n",
      "12                  MCC         0.705909     0.074972\n",
      "13                  NPV         0.925800     0.018875\n",
      "14              ROC_AUC         0.850815     0.034572\n",
      "CPU times: user 857 ms, sys: 3.2 s, total: 4.06 s\n",
      "Wall time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    knn_clf = KNeighborsClassifier()\n",
    "    \n",
    "    knn_clf.fit(X_train,y_train)\n",
    "\n",
    "    y_pred = knn_clf.predict(X_test) \n",
    "    \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6c405f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 5, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "   \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3a83374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_knn_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"n_neighbors\" : trial.suggest_int(\"n_neighbors\", 1, 30),\n",
    "        \"weights\" :trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
    "        \"metric\" : trial.suggest_categorical(\"metric\", ['euclidean', 'manhattan', 'minkowski']),\n",
    "        \"leaf_size\": trial.suggest_int(\"leaf_size\", 20, 100)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),      \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=1121218)\n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        knn_model = KNeighborsClassifier(**param_grid, n_jobs=8)\n",
    "        knn_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "16e1ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:05:03,973] A new study created in memory with name: KNNClassifier\n",
      "[I 2023-12-05 13:05:04,196] Trial 0 finished with value: 0.8100831475282806 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 0 with value: 0.8100831475282806.\n",
      "[I 2023-12-05 13:05:04,350] Trial 1 finished with value: 0.8690343698307672 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:04,510] Trial 2 finished with value: 0.7911903528041556 and parameters: {'n_neighbors': 26, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:04,661] Trial 3 finished with value: 0.8081496410337131 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 71}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:04,813] Trial 4 finished with value: 0.8677930904124441 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:04,964] Trial 5 finished with value: 0.8215285980151524 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 39}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:05,123] Trial 6 finished with value: 0.8412020757183548 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 71}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:05,294] Trial 7 finished with value: 0.7939610842549569 and parameters: {'n_neighbors': 23, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:05,458] Trial 8 finished with value: 0.8412020757183548 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 65}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:05,610] Trial 9 finished with value: 0.8215285980151524 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 61}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:05,788] Trial 10 finished with value: 0.8093056460028748 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 21}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:05,944] Trial 11 finished with value: 0.8613593242073815 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 1 with value: 0.8690343698307672.\n",
      "[I 2023-12-05 13:05:06,110] Trial 12 finished with value: 0.8759093489450274 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:06,271] Trial 13 finished with value: 0.8522398802141812 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:06,430] Trial 14 finished with value: 0.8425679769141693 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:06,588] Trial 15 finished with value: 0.8678411461133624 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:06,750] Trial 16 finished with value: 0.8234160421171639 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 31}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:06,946] Trial 17 finished with value: 0.8678411461133624 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:07,125] Trial 18 finished with value: 0.8100831475282806 and parameters: {'n_neighbors': 11, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 83}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:07,292] Trial 19 finished with value: 0.8335177893822779 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:07,444] Trial 20 finished with value: 0.8540176966404041 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:07,596] Trial 21 finished with value: 0.8678411461133624 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:07,761] Trial 22 finished with value: 0.8702120201263721 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:07,930] Trial 23 finished with value: 0.8702120201263721 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:08,112] Trial 24 finished with value: 0.8702120201263721 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:08,290] Trial 25 finished with value: 0.8759093489450274 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:08,458] Trial 26 finished with value: 0.8093056460028748 and parameters: {'n_neighbors': 13, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:08,623] Trial 27 finished with value: 0.8759093489450274 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:08,782] Trial 28 finished with value: 0.8410319535370567 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:08,956] Trial 29 finished with value: 0.8158956771841911 and parameters: {'n_neighbors': 12, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 24}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:09,136] Trial 30 finished with value: 0.8522398802141812 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:09,308] Trial 31 finished with value: 0.8731030672189549 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:09,489] Trial 32 finished with value: 0.8731030672189549 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:09,661] Trial 33 finished with value: 0.8731030672189549 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:09,844] Trial 34 finished with value: 0.8522398802141812 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:09,996] Trial 35 finished with value: 0.8533965492739763 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 34}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:10,161] Trial 36 finished with value: 0.8690343698307672 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:10,323] Trial 37 finished with value: 0.8497562305214956 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 41}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:10,475] Trial 38 finished with value: 0.8759093489450274 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:10,632] Trial 39 finished with value: 0.8475536379714658 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 28}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:10,790] Trial 40 finished with value: 0.8286596695038562 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:10,961] Trial 41 finished with value: 0.8759093489450274 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:11,125] Trial 42 finished with value: 0.8759093489450274 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 75}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:11,298] Trial 43 finished with value: 0.8690343698307672 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:11,460] Trial 44 finished with value: 0.8677930904124441 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 57}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:11,626] Trial 45 finished with value: 0.8702120201263721 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:11,796] Trial 46 finished with value: 0.8315415341054807 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:11,948] Trial 47 finished with value: 0.8677930904124441 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 65}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:12,123] Trial 48 finished with value: 0.8425679769141693 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 93}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:12,300] Trial 49 finished with value: 0.8475536379714658 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 31}. Best is trial 12 with value: 0.8759093489450274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8759\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 28\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_knn = optuna.create_study(direction='maximize', study_name=\"KNNClassifier\")\n",
    "func_knn_0 = lambda trial: objective_knn_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_knn.optimize(func_knn_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ac43f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   44.000000\n",
      "1                    TN  189.000000\n",
      "2                    FP   10.000000\n",
      "3                    FN   25.000000\n",
      "4              Accuracy    0.869403\n",
      "5             Precision    0.814815\n",
      "6           Sensitivity    0.637681\n",
      "7           Specificity    0.949700\n",
      "8              F1 score    0.715447\n",
      "9   F1 score (weighted)    0.863811\n",
      "10     F1 score (macro)    0.815351\n",
      "11    Balanced Accuracy    0.793715\n",
      "12                  MCC    0.640329\n",
      "13                  NPV    0.883200\n",
      "14              ROC_AUC    0.793715\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_0 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_0.fit(X_trainSet0,Y_trainSet0, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_0 = optimized_knn_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_knn_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_knn_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_knn_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_knn_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_knn_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_knn_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_knn_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_knn_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_knn_0)\n",
    "    \n",
    "\n",
    "mat_met_knn_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "13d758f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:05:12,491] Trial 50 finished with value: 0.8179238247119061 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:12,646] Trial 51 finished with value: 0.864888506287657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 78}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:12,815] Trial 52 finished with value: 0.8554092208027987 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 76}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:12,987] Trial 53 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 64}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:13,188] Trial 54 finished with value: 0.864888506287657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 91}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:13,374] Trial 55 finished with value: 0.8643727683259922 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:13,526] Trial 56 finished with value: 0.8347380531622427 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 72}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:13,693] Trial 57 finished with value: 0.8213675647433882 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:13,880] Trial 58 finished with value: 0.864888506287657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:14,042] Trial 59 finished with value: 0.8432176759280494 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:14,203] Trial 60 finished with value: 0.8298013004131242 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 68}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:14,375] Trial 61 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:14,550] Trial 62 finished with value: 0.8643727683259922 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:14,729] Trial 63 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 33}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:14,924] Trial 64 finished with value: 0.8554092208027987 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:15,083] Trial 65 finished with value: 0.864888506287657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 57}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:15,255] Trial 66 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 36}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:15,410] Trial 67 finished with value: 0.8577597073298067 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 28}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:15,620] Trial 68 finished with value: 0.8213675647433882 and parameters: {'n_neighbors': 8, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:15,775] Trial 69 finished with value: 0.8307841709002289 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:15,941] Trial 70 finished with value: 0.8469204824681384 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:16,097] Trial 71 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:16,285] Trial 72 finished with value: 0.852242743391318 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:16,454] Trial 73 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:16,624] Trial 74 finished with value: 0.8643727683259922 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:16,797] Trial 75 finished with value: 0.864888506287657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:16,961] Trial 76 finished with value: 0.8432176759280494 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:17,126] Trial 77 finished with value: 0.864888506287657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:17,329] Trial 78 finished with value: 0.8262483430289717 and parameters: {'n_neighbors': 9, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 84}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:17,484] Trial 79 finished with value: 0.8107397588928345 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:17,653] Trial 80 finished with value: 0.8554092208027987 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 34}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:17,821] Trial 81 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:17,979] Trial 82 finished with value: 0.8643727683259922 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:18,141] Trial 83 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:18,302] Trial 84 finished with value: 0.864888506287657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:18,461] Trial 85 finished with value: 0.8491452124539391 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 58}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:18,632] Trial 86 finished with value: 0.8643727683259922 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:18,789] Trial 87 finished with value: 0.8554092208027987 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 61}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:18,948] Trial 88 finished with value: 0.864888506287657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:19,108] Trial 89 finished with value: 0.8374721710004926 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 26}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:19,290] Trial 90 finished with value: 0.800260817063371 and parameters: {'n_neighbors': 22, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 22}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:19,470] Trial 91 finished with value: 0.8643727683259922 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:19,642] Trial 92 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:19,802] Trial 93 finished with value: 0.8302279888823785 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:19,962] Trial 94 finished with value: 0.864888506287657 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:20,138] Trial 95 finished with value: 0.8661661989231533 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:20,299] Trial 96 finished with value: 0.8643727683259922 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 100}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:20,458] Trial 97 finished with value: 0.8347380531622427 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:20,612] Trial 98 finished with value: 0.864706621931181 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 43}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:20,772] Trial 99 finished with value: 0.8643727683259922 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 12 with value: 0.8759093489450274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8759\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 28\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_1 = lambda trial: objective_knn_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_knn.optimize(func_knn_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1d7f3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   44.000000   53.000000\n",
      "1                    TN  189.000000  178.000000\n",
      "2                    FP   10.000000   22.000000\n",
      "3                    FN   25.000000   15.000000\n",
      "4              Accuracy    0.869403    0.861940\n",
      "5             Precision    0.814815    0.706667\n",
      "6           Sensitivity    0.637681    0.779412\n",
      "7           Specificity    0.949700    0.890000\n",
      "8              F1 score    0.715447    0.741259\n",
      "9   F1 score (weighted)    0.863811    0.864090\n",
      "10     F1 score (macro)    0.815351    0.823556\n",
      "11    Balanced Accuracy    0.793715    0.834706\n",
      "12                  MCC    0.640329    0.648864\n",
      "13                  NPV    0.883200    0.922300\n",
      "14              ROC_AUC    0.793715    0.834706\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_1 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_1.fit(X_trainSet1,Y_trainSet1, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_1 = optimized_knn_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_knn_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_knn_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_knn_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_knn_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_knn_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_knn_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_knn_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_knn_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_knn_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set1'] = set1\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "92d3e174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:05:20,991] Trial 100 finished with value: 0.8408904205461937 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 20}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:21,156] Trial 101 finished with value: 0.8646099551005471 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:21,343] Trial 102 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 32}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:21,519] Trial 103 finished with value: 0.8684825297108973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:21,697] Trial 104 finished with value: 0.8592238468077043 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 24}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:21,866] Trial 105 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 29}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:22,030] Trial 106 finished with value: 0.8646099551005471 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:22,221] Trial 107 finished with value: 0.8477416181738784 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:22,393] Trial 108 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:22,554] Trial 109 finished with value: 0.8684825297108973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:22,712] Trial 110 finished with value: 0.8646099551005471 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:22,891] Trial 111 finished with value: 0.8684825297108973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 30}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:23,059] Trial 112 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:23,244] Trial 113 finished with value: 0.8592238468077043 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:23,449] Trial 114 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 23}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:23,619] Trial 115 finished with value: 0.8646099551005471 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:23,779] Trial 116 finished with value: 0.8525905362420934 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 32}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:23,931] Trial 117 finished with value: 0.8597629641421701 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 55}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:24,100] Trial 118 finished with value: 0.8686201316019359 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:24,260] Trial 119 finished with value: 0.8410156944687012 and parameters: {'n_neighbors': 28, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:24,530] Trial 120 finished with value: 0.8490623840498938 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:24,706] Trial 121 finished with value: 0.8684825297108973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:24,872] Trial 122 finished with value: 0.8646099551005471 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 35}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:25,055] Trial 123 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:25,240] Trial 124 finished with value: 0.8684825297108973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:25,459] Trial 125 finished with value: 0.8477416181738784 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:25,632] Trial 126 finished with value: 0.8592238468077043 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 25}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:25,810] Trial 127 finished with value: 0.8646099551005471 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:25,974] Trial 128 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 73}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:26,164] Trial 129 finished with value: 0.8684825297108973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:26,379] Trial 130 finished with value: 0.8646099551005471 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 21}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:26,560] Trial 131 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:26,742] Trial 132 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:26,941] Trial 133 finished with value: 0.8684825297108973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:27,127] Trial 134 finished with value: 0.8592238468077043 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:27,315] Trial 135 finished with value: 0.8592238468077043 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:27,468] Trial 136 finished with value: 0.8600794268159684 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 29}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:27,622] Trial 137 finished with value: 0.8597629641421701 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 33}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:27,793] Trial 138 finished with value: 0.8646099551005471 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:27,963] Trial 139 finished with value: 0.8640680865051825 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:28,151] Trial 140 finished with value: 0.8477416181738784 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 94}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:28,319] Trial 141 finished with value: 0.8638241382487056 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 59}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:28,480] Trial 142 finished with value: 0.8686201316019359 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:28,649] Trial 143 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:28,823] Trial 144 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 67}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:29,001] Trial 145 finished with value: 0.8684825297108973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:29,178] Trial 146 finished with value: 0.8646099551005471 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:29,369] Trial 147 finished with value: 0.8684825297108973 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 63}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:29,543] Trial 148 finished with value: 0.8659878640713286 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:29,718] Trial 149 finished with value: 0.8686201316019359 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8759\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 28\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_2 = lambda trial: objective_knn_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_knn.optimize(func_knn_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "455b4e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   44.000000   53.000000   48.000000\n",
      "1                    TN  189.000000  178.000000  184.000000\n",
      "2                    FP   10.000000   22.000000   17.000000\n",
      "3                    FN   25.000000   15.000000   19.000000\n",
      "4              Accuracy    0.869403    0.861940    0.865672\n",
      "5             Precision    0.814815    0.706667    0.738462\n",
      "6           Sensitivity    0.637681    0.779412    0.716418\n",
      "7           Specificity    0.949700    0.890000    0.915400\n",
      "8              F1 score    0.715447    0.741259    0.727273\n",
      "9   F1 score (weighted)    0.863811    0.864090    0.864986\n",
      "10     F1 score (macro)    0.815351    0.823556    0.819082\n",
      "11    Balanced Accuracy    0.793715    0.834706    0.815920\n",
      "12                  MCC    0.640329    0.648864    0.638320\n",
      "13                  NPV    0.883200    0.922300    0.906400\n",
      "14              ROC_AUC    0.793715    0.834706    0.815920\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_2 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_2.fit(X_trainSet2,Y_trainSet2, )\n",
    "#predict\n",
    "y_pred_knn_2 = optimized_knn_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_knn_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_knn_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_knn_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_knn_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_knn_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_knn_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_knn_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_knn_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_knn_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set2'] = Set2\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5425d357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:05:29,930] Trial 150 finished with value: 0.8545814049385919 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 27}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:30,094] Trial 151 finished with value: 0.8553017046893026 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:30,269] Trial 152 finished with value: 0.8642640372466938 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:30,444] Trial 153 finished with value: 0.8675327178837874 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:30,612] Trial 154 finished with value: 0.8627481580660836 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:30,777] Trial 155 finished with value: 0.8642640372466938 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:30,948] Trial 156 finished with value: 0.8649778222310271 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:31,111] Trial 157 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:31,272] Trial 158 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:31,452] Trial 159 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:31,624] Trial 160 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:31,801] Trial 161 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:32,006] Trial 162 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:32,210] Trial 163 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:32,407] Trial 164 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:32,583] Trial 165 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:32,742] Trial 166 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:32,940] Trial 167 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:33,099] Trial 168 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:33,275] Trial 169 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:33,443] Trial 170 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:33,624] Trial 171 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:33,803] Trial 172 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:33,979] Trial 173 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:34,178] Trial 174 finished with value: 0.8627481580660836 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:34,343] Trial 175 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:34,505] Trial 176 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:34,663] Trial 177 finished with value: 0.8643314423775911 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 57}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:34,838] Trial 178 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:34,990] Trial 179 finished with value: 0.8643314423775911 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 48}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:35,162] Trial 180 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:35,336] Trial 181 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:35,516] Trial 182 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:35,681] Trial 183 finished with value: 0.8627481580660836 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:35,846] Trial 184 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:36,012] Trial 185 finished with value: 0.8627481580660836 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:36,204] Trial 186 finished with value: 0.8466162519443712 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:36,382] Trial 187 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 58}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:36,554] Trial 188 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:36,733] Trial 189 finished with value: 0.8627481580660836 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:36,925] Trial 190 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:37,086] Trial 191 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:37,247] Trial 192 finished with value: 0.8627481580660836 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:37,415] Trial 193 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:37,575] Trial 194 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 56}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:37,740] Trial 195 finished with value: 0.8627481580660836 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:37,908] Trial 196 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 60}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:38,091] Trial 197 finished with value: 0.8627481580660836 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:38,264] Trial 198 finished with value: 0.8734453366857743 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 12 with value: 0.8759093489450274.\n",
      "[I 2023-12-05 13:05:38,437] Trial 199 finished with value: 0.8627481580660836 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 55}. Best is trial 12 with value: 0.8759093489450274.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8759\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 6\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 28\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_3 = lambda trial: objective_knn_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_knn.optimize(func_knn_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0558b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   44.000000   53.000000   48.000000   49.000000\n",
      "1                    TN  189.000000  178.000000  184.000000  192.000000\n",
      "2                    FP   10.000000   22.000000   17.000000    7.000000\n",
      "3                    FN   25.000000   15.000000   19.000000   20.000000\n",
      "4              Accuracy    0.869403    0.861940    0.865672    0.899254\n",
      "5             Precision    0.814815    0.706667    0.738462    0.875000\n",
      "6           Sensitivity    0.637681    0.779412    0.716418    0.710145\n",
      "7           Specificity    0.949700    0.890000    0.915400    0.964800\n",
      "8              F1 score    0.715447    0.741259    0.727273    0.784000\n",
      "9   F1 score (weighted)    0.863811    0.864090    0.864986    0.895608\n",
      "10     F1 score (macro)    0.815351    0.823556    0.819082    0.859153\n",
      "11    Balanced Accuracy    0.793715    0.834706    0.815920    0.837485\n",
      "12                  MCC    0.640329    0.648864    0.638320    0.725894\n",
      "13                  NPV    0.883200    0.922300    0.906400    0.905700\n",
      "14              ROC_AUC    0.793715    0.834706    0.815920    0.837485\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_3 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_3.fit(X_trainSet3,Y_trainSet3, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_3 = optimized_knn_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_knn_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_knn_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_knn_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_knn_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_knn_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_knn_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_knn_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_knn_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_knn_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set3'] = Set3\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "353f5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:05:38,681] Trial 200 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:38,857] Trial 201 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:39,020] Trial 202 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:39,201] Trial 203 finished with value: 0.8699658670298082 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:39,379] Trial 204 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:39,559] Trial 205 finished with value: 0.8699658670298082 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:39,757] Trial 206 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:39,931] Trial 207 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:40,095] Trial 208 finished with value: 0.8699658670298082 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:40,263] Trial 209 finished with value: 0.8699658670298082 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:40,421] Trial 210 finished with value: 0.864833050946916 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:40,588] Trial 211 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:40,774] Trial 212 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:40,953] Trial 213 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:41,143] Trial 214 finished with value: 0.8699658670298082 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:41,322] Trial 215 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:41,517] Trial 216 finished with value: 0.8699658670298082 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:41,705] Trial 217 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:41,886] Trial 218 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:42,060] Trial 219 finished with value: 0.8551083988456705 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:42,228] Trial 220 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:42,427] Trial 221 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:42,588] Trial 222 finished with value: 0.8699658670298082 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:42,751] Trial 223 finished with value: 0.8789452602969373 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:42,936] Trial 224 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:43,116] Trial 225 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:43,293] Trial 226 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:43,466] Trial 227 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:43,629] Trial 228 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:43,789] Trial 229 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:43,956] Trial 230 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:44,123] Trial 231 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:44,294] Trial 232 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:44,457] Trial 233 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:44,619] Trial 234 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:44,782] Trial 235 finished with value: 0.8613021420225907 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:44,958] Trial 236 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:45,129] Trial 237 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:45,310] Trial 238 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:45,502] Trial 239 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:45,692] Trial 240 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:45,873] Trial 241 finished with value: 0.8613021420225907 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:46,054] Trial 242 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:46,237] Trial 243 finished with value: 0.8613021420225907 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:46,403] Trial 244 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:46,583] Trial 245 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:46,776] Trial 246 finished with value: 0.8396969447223617 and parameters: {'n_neighbors': 17, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:46,957] Trial 247 finished with value: 0.8613021420225907 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:47,139] Trial 248 finished with value: 0.8374379115154744 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:47,321] Trial 249 finished with value: 0.8784679212799518 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8789\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_4 = lambda trial: objective_knn_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_knn.optimize(func_knn_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "09d47487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   53.000000   48.000000   49.000000   \n",
      "1                    TN  189.000000  178.000000  184.000000  192.000000   \n",
      "2                    FP   10.000000   22.000000   17.000000    7.000000   \n",
      "3                    FN   25.000000   15.000000   19.000000   20.000000   \n",
      "4              Accuracy    0.869403    0.861940    0.865672    0.899254   \n",
      "5             Precision    0.814815    0.706667    0.738462    0.875000   \n",
      "6           Sensitivity    0.637681    0.779412    0.716418    0.710145   \n",
      "7           Specificity    0.949700    0.890000    0.915400    0.964800   \n",
      "8              F1 score    0.715447    0.741259    0.727273    0.784000   \n",
      "9   F1 score (weighted)    0.863811    0.864090    0.864986    0.895608   \n",
      "10     F1 score (macro)    0.815351    0.823556    0.819082    0.859153   \n",
      "11    Balanced Accuracy    0.793715    0.834706    0.815920    0.837485   \n",
      "12                  MCC    0.640329    0.648864    0.638320    0.725894   \n",
      "13                  NPV    0.883200    0.922300    0.906400    0.905700   \n",
      "14              ROC_AUC    0.793715    0.834706    0.815920    0.837485   \n",
      "\n",
      "          Set4  \n",
      "0    50.000000  \n",
      "1   182.000000  \n",
      "2    18.000000  \n",
      "3    18.000000  \n",
      "4     0.865672  \n",
      "5     0.735294  \n",
      "6     0.735294  \n",
      "7     0.910000  \n",
      "8     0.735294  \n",
      "9     0.865672  \n",
      "10    0.822647  \n",
      "11    0.822647  \n",
      "12    0.645294  \n",
      "13    0.910000  \n",
      "14    0.822647  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_4 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_4.fit(X_trainSet4,Y_trainSet4, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_4 = optimized_knn_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_knn_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_knn_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_knn_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_knn_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_knn_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_knn_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_knn_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_knn_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_knn_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set4'] = Set4\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6089e60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:05:47,561] Trial 250 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:47,743] Trial 251 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:47,922] Trial 252 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:48,112] Trial 253 finished with value: 0.8559011467970172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:48,291] Trial 254 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:48,459] Trial 255 finished with value: 0.8559011467970172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:48,642] Trial 256 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:48,820] Trial 257 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:49,000] Trial 258 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:49,174] Trial 259 finished with value: 0.8248232738118286 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:49,331] Trial 260 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:49,503] Trial 261 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:49,658] Trial 262 finished with value: 0.8426884960084099 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:49,827] Trial 263 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:50,025] Trial 264 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:50,200] Trial 265 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:50,374] Trial 266 finished with value: 0.8559011467970172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:50,545] Trial 267 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:50,730] Trial 268 finished with value: 0.8410202309053307 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:50,911] Trial 269 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:51,079] Trial 270 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:51,256] Trial 271 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:51,436] Trial 272 finished with value: 0.8341598032647152 and parameters: {'n_neighbors': 24, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:51,624] Trial 273 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:51,786] Trial 274 finished with value: 0.8559011467970172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:51,950] Trial 275 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:52,120] Trial 276 finished with value: 0.8400614206082313 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:52,284] Trial 277 finished with value: 0.8374154265663478 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:52,463] Trial 278 finished with value: 0.8465380391830509 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:52,625] Trial 279 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:52,786] Trial 280 finished with value: 0.8558969328235403 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:52,955] Trial 281 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:53,136] Trial 282 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:53,290] Trial 283 finished with value: 0.8558969328235403 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:53,461] Trial 284 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:53,633] Trial 285 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:53,808] Trial 286 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:53,969] Trial 287 finished with value: 0.8559011467970172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:54,147] Trial 288 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:54,320] Trial 289 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:54,480] Trial 290 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:54,656] Trial 291 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:54,835] Trial 292 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:55,025] Trial 293 finished with value: 0.8608221433637968 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:55,204] Trial 294 finished with value: 0.8602230438070922 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:55,384] Trial 295 finished with value: 0.861303249683431 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:55,565] Trial 296 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:55,739] Trial 297 finished with value: 0.8248232738118286 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:55,906] Trial 298 finished with value: 0.8559011467970172 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:56,070] Trial 299 finished with value: 0.8622941393957518 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 200 with value: 0.8789452602969373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8789\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_5 = lambda trial: objective_knn_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_knn.optimize(func_knn_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "29b6d99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   53.000000   48.000000   49.000000   \n",
      "1                    TN  189.000000  178.000000  184.000000  192.000000   \n",
      "2                    FP   10.000000   22.000000   17.000000    7.000000   \n",
      "3                    FN   25.000000   15.000000   19.000000   20.000000   \n",
      "4              Accuracy    0.869403    0.861940    0.865672    0.899254   \n",
      "5             Precision    0.814815    0.706667    0.738462    0.875000   \n",
      "6           Sensitivity    0.637681    0.779412    0.716418    0.710145   \n",
      "7           Specificity    0.949700    0.890000    0.915400    0.964800   \n",
      "8              F1 score    0.715447    0.741259    0.727273    0.784000   \n",
      "9   F1 score (weighted)    0.863811    0.864090    0.864986    0.895608   \n",
      "10     F1 score (macro)    0.815351    0.823556    0.819082    0.859153   \n",
      "11    Balanced Accuracy    0.793715    0.834706    0.815920    0.837485   \n",
      "12                  MCC    0.640329    0.648864    0.638320    0.725894   \n",
      "13                  NPV    0.883200    0.922300    0.906400    0.905700   \n",
      "14              ROC_AUC    0.793715    0.834706    0.815920    0.837485   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    50.000000   58.000000  \n",
      "1   182.000000  189.000000  \n",
      "2    18.000000   11.000000  \n",
      "3    18.000000   10.000000  \n",
      "4     0.865672    0.921642  \n",
      "5     0.735294    0.840580  \n",
      "6     0.735294    0.852941  \n",
      "7     0.910000    0.945000  \n",
      "8     0.735294    0.846715  \n",
      "9     0.865672    0.921830  \n",
      "10    0.822647    0.897042  \n",
      "11    0.822647    0.898971  \n",
      "12    0.645294    0.794126  \n",
      "13    0.910000    0.949700  \n",
      "14    0.822647    0.898971  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_5 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_5.fit(X_trainSet5,Y_trainSet5, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_5 = optimized_knn_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_knn_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_knn_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_knn_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_knn_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_knn_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_knn_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_knn_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_knn_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_knn_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set5'] = Set5\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "baa41e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:05:56,295] Trial 300 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:56,467] Trial 301 finished with value: 0.841004915424854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:56,629] Trial 302 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:56,791] Trial 303 finished with value: 0.8365313258885042 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:56,958] Trial 304 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:57,129] Trial 305 finished with value: 0.841004915424854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:57,291] Trial 306 finished with value: 0.8239251482213816 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:57,471] Trial 307 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:57,656] Trial 308 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:57,843] Trial 309 finished with value: 0.841004915424854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:58,017] Trial 310 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:58,192] Trial 311 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:58,368] Trial 312 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:58,529] Trial 313 finished with value: 0.841004915424854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:58,704] Trial 314 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:58,876] Trial 315 finished with value: 0.8317950998227992 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:59,040] Trial 316 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:59,227] Trial 317 finished with value: 0.8208683916135607 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:59,420] Trial 318 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:59,599] Trial 319 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:59,787] Trial 320 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:05:59,967] Trial 321 finished with value: 0.8317950998227992 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:00,147] Trial 322 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:00,325] Trial 323 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:00,492] Trial 324 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:00,662] Trial 325 finished with value: 0.841004915424854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:00,822] Trial 326 finished with value: 0.8347261745792096 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:00,981] Trial 327 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:01,155] Trial 328 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:01,310] Trial 329 finished with value: 0.829003337618178 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:01,466] Trial 330 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:01,628] Trial 331 finished with value: 0.8363713713519996 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:01,792] Trial 332 finished with value: 0.8317950998227992 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:01,954] Trial 333 finished with value: 0.841004915424854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:02,120] Trial 334 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:02,327] Trial 335 finished with value: 0.8251530840430344 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:02,501] Trial 336 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:02,693] Trial 337 finished with value: 0.8128266415260459 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:02,866] Trial 338 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:03,031] Trial 339 finished with value: 0.841004915424854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:03,210] Trial 340 finished with value: 0.8317950998227992 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:03,380] Trial 341 finished with value: 0.8388243464231957 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:03,555] Trial 342 finished with value: 0.8350610293697255 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:03,720] Trial 343 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:03,897] Trial 344 finished with value: 0.841004915424854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:04,074] Trial 345 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:04,258] Trial 346 finished with value: 0.841226789562473 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:04,415] Trial 347 finished with value: 0.8365313258885042 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:04,574] Trial 348 finished with value: 0.841004915424854 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:04,744] Trial 349 finished with value: 0.8317950998227992 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8789\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_6 = lambda trial: objective_knn_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_knn.optimize(func_knn_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1946b7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   53.000000   48.000000   49.000000   \n",
      "1                    TN  189.000000  178.000000  184.000000  192.000000   \n",
      "2                    FP   10.000000   22.000000   17.000000    7.000000   \n",
      "3                    FN   25.000000   15.000000   19.000000   20.000000   \n",
      "4              Accuracy    0.869403    0.861940    0.865672    0.899254   \n",
      "5             Precision    0.814815    0.706667    0.738462    0.875000   \n",
      "6           Sensitivity    0.637681    0.779412    0.716418    0.710145   \n",
      "7           Specificity    0.949700    0.890000    0.915400    0.964800   \n",
      "8              F1 score    0.715447    0.741259    0.727273    0.784000   \n",
      "9   F1 score (weighted)    0.863811    0.864090    0.864986    0.895608   \n",
      "10     F1 score (macro)    0.815351    0.823556    0.819082    0.859153   \n",
      "11    Balanced Accuracy    0.793715    0.834706    0.815920    0.837485   \n",
      "12                  MCC    0.640329    0.648864    0.638320    0.725894   \n",
      "13                  NPV    0.883200    0.922300    0.906400    0.905700   \n",
      "14              ROC_AUC    0.793715    0.834706    0.815920    0.837485   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    50.000000   58.000000   57.000000  \n",
      "1   182.000000  189.000000  192.000000  \n",
      "2    18.000000   11.000000    8.000000  \n",
      "3    18.000000   10.000000   11.000000  \n",
      "4     0.865672    0.921642    0.929104  \n",
      "5     0.735294    0.840580    0.876923  \n",
      "6     0.735294    0.852941    0.838235  \n",
      "7     0.910000    0.945000    0.960000  \n",
      "8     0.735294    0.846715    0.857143  \n",
      "9     0.865672    0.921830    0.928569  \n",
      "10    0.822647    0.897042    0.904998  \n",
      "11    0.822647    0.898971    0.899118  \n",
      "12    0.645294    0.794126    0.810393  \n",
      "13    0.910000    0.949700    0.945800  \n",
      "14    0.822647    0.898971    0.899118  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_6 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_6.fit(X_trainSet6,Y_trainSet6, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_6 = optimized_knn_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_knn_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_knn_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_knn_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_knn_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_knn_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_knn_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_knn_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_knn_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_knn_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set6'] = Set6\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "869b61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:06:04,973] Trial 350 finished with value: 0.8576924153394095 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:05,135] Trial 351 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:05,299] Trial 352 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:05,484] Trial 353 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:05,656] Trial 354 finished with value: 0.8333568650612813 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:05,866] Trial 355 finished with value: 0.8389510119186576 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:06,042] Trial 356 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:06,216] Trial 357 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:06,401] Trial 358 finished with value: 0.8379672043655348 and parameters: {'n_neighbors': 22, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:06,571] Trial 359 finished with value: 0.8541772333147503 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:06,737] Trial 360 finished with value: 0.8535268511200764 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:06,916] Trial 361 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:07,083] Trial 362 finished with value: 0.8494269255169453 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:07,247] Trial 363 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:07,424] Trial 364 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:07,590] Trial 365 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:07,766] Trial 366 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:07,960] Trial 367 finished with value: 0.8535268511200764 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:08,141] Trial 368 finished with value: 0.8493607276525241 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:08,310] Trial 369 finished with value: 0.8576924153394095 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:08,507] Trial 370 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:08,671] Trial 371 finished with value: 0.8535268511200764 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:08,843] Trial 372 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:09,032] Trial 373 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:09,209] Trial 374 finished with value: 0.8480553636893443 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:09,396] Trial 375 finished with value: 0.846157342701989 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:09,580] Trial 376 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 96}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:09,748] Trial 377 finished with value: 0.8494269255169453 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:09,921] Trial 378 finished with value: 0.8434533737549877 and parameters: {'n_neighbors': 13, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:10,099] Trial 379 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:10,283] Trial 380 finished with value: 0.8535268511200764 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:10,474] Trial 381 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:10,640] Trial 382 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:10,819] Trial 383 finished with value: 0.8337249870111778 and parameters: {'n_neighbors': 29, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:10,997] Trial 384 finished with value: 0.8494269255169453 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:11,170] Trial 385 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:11,355] Trial 386 finished with value: 0.8374203346904304 and parameters: {'n_neighbors': 26, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:11,521] Trial 387 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:11,692] Trial 388 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:11,860] Trial 389 finished with value: 0.8535268511200764 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:12,040] Trial 390 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:12,209] Trial 391 finished with value: 0.8576924153394095 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:12,402] Trial 392 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:12,582] Trial 393 finished with value: 0.8389510119186576 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:12,741] Trial 394 finished with value: 0.8508272238720748 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:12,906] Trial 395 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:13,076] Trial 396 finished with value: 0.8607210019005251 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:13,252] Trial 397 finished with value: 0.8541772333147503 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:13,421] Trial 398 finished with value: 0.8535268511200764 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:13,593] Trial 399 finished with value: 0.8624848381018826 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8789\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_7 = lambda trial: objective_knn_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_knn.optimize(func_knn_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "40066dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   53.000000   48.000000   49.000000   \n",
      "1                    TN  189.000000  178.000000  184.000000  192.000000   \n",
      "2                    FP   10.000000   22.000000   17.000000    7.000000   \n",
      "3                    FN   25.000000   15.000000   19.000000   20.000000   \n",
      "4              Accuracy    0.869403    0.861940    0.865672    0.899254   \n",
      "5             Precision    0.814815    0.706667    0.738462    0.875000   \n",
      "6           Sensitivity    0.637681    0.779412    0.716418    0.710145   \n",
      "7           Specificity    0.949700    0.890000    0.915400    0.964800   \n",
      "8              F1 score    0.715447    0.741259    0.727273    0.784000   \n",
      "9   F1 score (weighted)    0.863811    0.864090    0.864986    0.895608   \n",
      "10     F1 score (macro)    0.815351    0.823556    0.819082    0.859153   \n",
      "11    Balanced Accuracy    0.793715    0.834706    0.815920    0.837485   \n",
      "12                  MCC    0.640329    0.648864    0.638320    0.725894   \n",
      "13                  NPV    0.883200    0.922300    0.906400    0.905700   \n",
      "14              ROC_AUC    0.793715    0.834706    0.815920    0.837485   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    50.000000   58.000000   57.000000   53.000000  \n",
      "1   182.000000  189.000000  192.000000  188.000000  \n",
      "2    18.000000   11.000000    8.000000   13.000000  \n",
      "3    18.000000   10.000000   11.000000   14.000000  \n",
      "4     0.865672    0.921642    0.929104    0.899254  \n",
      "5     0.735294    0.840580    0.876923    0.803030  \n",
      "6     0.735294    0.852941    0.838235    0.791045  \n",
      "7     0.910000    0.945000    0.960000    0.935300  \n",
      "8     0.735294    0.846715    0.857143    0.796992  \n",
      "9     0.865672    0.921830    0.928569    0.899000  \n",
      "10    0.822647    0.897042    0.904998    0.864997  \n",
      "11    0.822647    0.898971    0.899118    0.863184  \n",
      "12    0.645294    0.794126    0.810393    0.730037  \n",
      "13    0.910000    0.949700    0.945800    0.930700  \n",
      "14    0.822647    0.898971    0.899118    0.863184  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_7 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_7.fit(X_trainSet7,Y_trainSet7, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_7 = optimized_knn_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_knn_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_knn_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_knn_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_knn_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_knn_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_knn_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_knn_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_knn_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_knn_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set7'] = Set7\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "18e519f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:06:13,820] Trial 400 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:14,006] Trial 401 finished with value: 0.8507596490039401 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:14,197] Trial 402 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:14,376] Trial 403 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:14,558] Trial 404 finished with value: 0.8447567183217345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:14,741] Trial 405 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:14,908] Trial 406 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:15,081] Trial 407 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:15,276] Trial 408 finished with value: 0.8250904940849881 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:15,471] Trial 409 finished with value: 0.8222976512651311 and parameters: {'n_neighbors': 20, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:15,639] Trial 410 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:15,811] Trial 411 finished with value: 0.8507596490039401 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:15,997] Trial 412 finished with value: 0.8107167029762214 and parameters: {'n_neighbors': 5, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:16,164] Trial 413 finished with value: 0.8447567183217345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 87}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:16,323] Trial 414 finished with value: 0.8263824273032221 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:16,486] Trial 415 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:16,689] Trial 416 finished with value: 0.8507596490039401 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:16,875] Trial 417 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:17,036] Trial 418 finished with value: 0.8331491484173105 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:17,207] Trial 419 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:17,381] Trial 420 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:17,560] Trial 421 finished with value: 0.8507596490039401 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:17,738] Trial 422 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:17,907] Trial 423 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:18,081] Trial 424 finished with value: 0.8507596490039401 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:18,264] Trial 425 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:18,431] Trial 426 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:18,597] Trial 427 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:18,776] Trial 428 finished with value: 0.8430666335881446 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:18,978] Trial 429 finished with value: 0.8496875012090728 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 62}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:19,160] Trial 430 finished with value: 0.8263854292668865 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:19,354] Trial 431 finished with value: 0.8104752865507493 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:19,531] Trial 432 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:19,706] Trial 433 finished with value: 0.8447567183217345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 66}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:19,877] Trial 434 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:20,058] Trial 435 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:20,221] Trial 436 finished with value: 0.8263824273032221 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:20,396] Trial 437 finished with value: 0.8507596490039401 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:20,583] Trial 438 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:20,746] Trial 439 finished with value: 0.816359210515242 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:20,933] Trial 440 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:21,112] Trial 441 finished with value: 0.8507596490039401 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:21,291] Trial 442 finished with value: 0.8447567183217345 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:21,488] Trial 443 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:21,668] Trial 444 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:21,853] Trial 445 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:22,026] Trial 446 finished with value: 0.8507596490039401 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:22,241] Trial 447 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:22,428] Trial 448 finished with value: 0.8323419268773689 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:22,614] Trial 449 finished with value: 0.8432587254704986 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 79}. Best is trial 200 with value: 0.8789452602969373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8789\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_8 = lambda trial: objective_knn_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_knn.optimize(func_knn_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dc63e372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   53.000000   48.000000   49.000000   \n",
      "1                    TN  189.000000  178.000000  184.000000  192.000000   \n",
      "2                    FP   10.000000   22.000000   17.000000    7.000000   \n",
      "3                    FN   25.000000   15.000000   19.000000   20.000000   \n",
      "4              Accuracy    0.869403    0.861940    0.865672    0.899254   \n",
      "5             Precision    0.814815    0.706667    0.738462    0.875000   \n",
      "6           Sensitivity    0.637681    0.779412    0.716418    0.710145   \n",
      "7           Specificity    0.949700    0.890000    0.915400    0.964800   \n",
      "8              F1 score    0.715447    0.741259    0.727273    0.784000   \n",
      "9   F1 score (weighted)    0.863811    0.864090    0.864986    0.895608   \n",
      "10     F1 score (macro)    0.815351    0.823556    0.819082    0.859153   \n",
      "11    Balanced Accuracy    0.793715    0.834706    0.815920    0.837485   \n",
      "12                  MCC    0.640329    0.648864    0.638320    0.725894   \n",
      "13                  NPV    0.883200    0.922300    0.906400    0.905700   \n",
      "14              ROC_AUC    0.793715    0.834706    0.815920    0.837485   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    50.000000   58.000000   57.000000   53.000000   53.000000  \n",
      "1   182.000000  189.000000  192.000000  188.000000  190.000000  \n",
      "2    18.000000   11.000000    8.000000   13.000000   12.000000  \n",
      "3    18.000000   10.000000   11.000000   14.000000   13.000000  \n",
      "4     0.865672    0.921642    0.929104    0.899254    0.906716  \n",
      "5     0.735294    0.840580    0.876923    0.803030    0.815385  \n",
      "6     0.735294    0.852941    0.838235    0.791045    0.803030  \n",
      "7     0.910000    0.945000    0.960000    0.935300    0.940600  \n",
      "8     0.735294    0.846715    0.857143    0.796992    0.809160  \n",
      "9     0.865672    0.921830    0.928569    0.899000    0.906476  \n",
      "10    0.822647    0.897042    0.904998    0.864997    0.873716  \n",
      "11    0.822647    0.898971    0.899118    0.863184    0.871812  \n",
      "12    0.645294    0.794126    0.810393    0.730037    0.747475  \n",
      "13    0.910000    0.949700    0.945800    0.930700    0.936000  \n",
      "14    0.822647    0.898971    0.899118    0.863184    0.871812  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_8 =  KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_8.fit(X_trainSet8,Y_trainSet8, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_8 = optimized_knn_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_knn_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_knn_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_knn_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_knn_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_knn_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_knn_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_knn_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_knn_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_knn_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set8'] = Set8\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "70af445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:06:22,896] Trial 450 finished with value: 0.8396926478878972 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:23,062] Trial 451 finished with value: 0.8502503059496478 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:23,235] Trial 452 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:23,426] Trial 453 finished with value: 0.8477240364029729 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 37}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:23,601] Trial 454 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:23,777] Trial 455 finished with value: 0.8502503059496478 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:23,952] Trial 456 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:24,138] Trial 457 finished with value: 0.8285502450064562 and parameters: {'n_neighbors': 27, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:24,298] Trial 458 finished with value: 0.8511167648360238 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:24,462] Trial 459 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:24,647] Trial 460 finished with value: 0.8502503059496478 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 52}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:24,806] Trial 461 finished with value: 0.8499004877256905 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:24,981] Trial 462 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:25,160] Trial 463 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:25,331] Trial 464 finished with value: 0.8259671145603494 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:25,514] Trial 465 finished with value: 0.8307552198454496 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:25,692] Trial 466 finished with value: 0.8477240364029729 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 54}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:25,889] Trial 467 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 69}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:26,056] Trial 468 finished with value: 0.841848342723386 and parameters: {'n_neighbors': 9, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:26,239] Trial 469 finished with value: 0.8373874842640336 and parameters: {'n_neighbors': 7, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:26,436] Trial 470 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 50}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:26,624] Trial 471 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:26,800] Trial 472 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 39}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:26,988] Trial 473 finished with value: 0.8502503059496478 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:27,168] Trial 474 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:27,349] Trial 475 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:27,529] Trial 476 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:27,692] Trial 477 finished with value: 0.8477240364029729 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:27,894] Trial 478 finished with value: 0.8502503059496478 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:28,073] Trial 479 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:28,231] Trial 480 finished with value: 0.8511167648360238 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'minkowski', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:28,405] Trial 481 finished with value: 0.8477240364029729 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:28,599] Trial 482 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 40}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:28,768] Trial 483 finished with value: 0.8502503059496478 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:28,933] Trial 484 finished with value: 0.8499004877256905 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'euclidean', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:29,098] Trial 485 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 53}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:29,277] Trial 486 finished with value: 0.8502503059496478 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:29,443] Trial 487 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 42}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:29,646] Trial 488 finished with value: 0.8396926478878972 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:29,833] Trial 489 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:30,009] Trial 490 finished with value: 0.8477240364029729 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 48}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:30,187] Trial 491 finished with value: 0.8502503059496478 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 43}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:30,376] Trial 492 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 46}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:30,564] Trial 493 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 49}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:30,736] Trial 494 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 38}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:30,926] Trial 495 finished with value: 0.8529595681036678 and parameters: {'n_neighbors': 6, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 44}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:31,115] Trial 496 finished with value: 0.8502503059496478 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 41}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:31,291] Trial 497 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 47}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:31,475] Trial 498 finished with value: 0.8369878076543049 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 45}. Best is trial 200 with value: 0.8789452602969373.\n",
      "[I 2023-12-05 13:06:31,663] Trial 499 finished with value: 0.8558378054019966 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'metric': 'manhattan', 'leaf_size': 51}. Best is trial 200 with value: 0.8789452602969373.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8789\n",
      "\tBest params:\n",
      "\t\tn_neighbors: 5\n",
      "\t\tweights: distance\n",
      "\t\tmetric: manhattan\n",
      "\t\tleaf_size: 51\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "\n",
    "func_knn_9 = lambda trial: objective_knn_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_knn.optimize(func_knn_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_knn.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_knn.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_knn.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ae930c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   44.000000   53.000000   48.000000   49.000000   \n",
      "1                    TN  189.000000  178.000000  184.000000  192.000000   \n",
      "2                    FP   10.000000   22.000000   17.000000    7.000000   \n",
      "3                    FN   25.000000   15.000000   19.000000   20.000000   \n",
      "4              Accuracy    0.869403    0.861940    0.865672    0.899254   \n",
      "5             Precision    0.814815    0.706667    0.738462    0.875000   \n",
      "6           Sensitivity    0.637681    0.779412    0.716418    0.710145   \n",
      "7           Specificity    0.949700    0.890000    0.915400    0.964800   \n",
      "8              F1 score    0.715447    0.741259    0.727273    0.784000   \n",
      "9   F1 score (weighted)    0.863811    0.864090    0.864986    0.895608   \n",
      "10     F1 score (macro)    0.815351    0.823556    0.819082    0.859153   \n",
      "11    Balanced Accuracy    0.793715    0.834706    0.815920    0.837485   \n",
      "12                  MCC    0.640329    0.648864    0.638320    0.725894   \n",
      "13                  NPV    0.883200    0.922300    0.906400    0.905700   \n",
      "14              ROC_AUC    0.793715    0.834706    0.815920    0.837485   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    50.000000   58.000000   57.000000   53.000000   53.000000   52.000000  \n",
      "1   182.000000  189.000000  192.000000  188.000000  190.000000  190.000000  \n",
      "2    18.000000   11.000000    8.000000   13.000000   12.000000   11.000000  \n",
      "3    18.000000   10.000000   11.000000   14.000000   13.000000   15.000000  \n",
      "4     0.865672    0.921642    0.929104    0.899254    0.906716    0.902985  \n",
      "5     0.735294    0.840580    0.876923    0.803030    0.815385    0.825397  \n",
      "6     0.735294    0.852941    0.838235    0.791045    0.803030    0.776119  \n",
      "7     0.910000    0.945000    0.960000    0.935300    0.940600    0.945300  \n",
      "8     0.735294    0.846715    0.857143    0.796992    0.809160    0.800000  \n",
      "9     0.865672    0.921830    0.928569    0.899000    0.906476    0.901970  \n",
      "10    0.822647    0.897042    0.904998    0.864997    0.873716    0.867980  \n",
      "11    0.822647    0.898971    0.899118    0.863184    0.871812    0.860697  \n",
      "12    0.645294    0.794126    0.810393    0.730037    0.747475    0.736648  \n",
      "13    0.910000    0.949700    0.945800    0.930700    0.936000    0.926800  \n",
      "14    0.822647    0.898971    0.899118    0.863184    0.871812    0.860697  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_knn_9 = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "    \n",
    "#learn\n",
    "\n",
    "\n",
    "optimized_knn_9.fit(X_trainSet9,Y_trainSet9, )\n",
    "\n",
    "# predict\n",
    "y_pred_knn_9 = optimized_knn_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_knn_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_knn_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_knn_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_knn_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_knn_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_knn_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_knn_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_knn_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_knn_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                      })   \n",
    "\n",
    "mat_met_knn_test['Set9'] = Set9\n",
    "print(mat_met_knn_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b3879852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d16d4a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAncAAAHJCAYAAADn4h/6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAABrFklEQVR4nO3deVxN+f8H8NdtL+1KexQVUiE7I/tOjGRfsszYhsEwE4MYy1jGNsMYDJElNHZDjLEzdhKylZCSaFXpVuf3h1/n6+pG97bpej0fjx4z95zPOed93vfSy9muRBAEAURERESkEtTKugAiIiIiKj4Md0REREQqhOGOiIiISIUw3BERERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIwx0RERGRCmG4IyIiIlIhDHdEREREKoTh7jMmkUggkUg+OKZKlSqQSCR49OhR6RRFn5wWLVp89HNSWoYMGQKJRILAwMCyLqXEfUp9J6LyheGOiIiISIUw3BERERGpEIY7UkhiYiL09PRQtWpVCIIgd0yXLl0gkUhw5coVAMCjR48gkUgwZMgQREREoHv37jA1NUWFChXQrFkzHDlypMDtbdu2DS1btoSJiQl0dHRQo0YNzJkzB2/evMk3ViKRoEWLFnj27Bn8/PxgZWUFdXV18RRe3im9yMhILFmyBNWrV4eOjg5sbW0xYcIEpKSk5Fvn8ePH8dVXX6FmzZowNDSErq4uXF1dMXPmTGRkZOQbHxAQAIlEghMnTmDTpk2oX78+KlSogCpVqohjAgMD0bNnTzg6OkJXVxeGhoZo2rQpNm3aJLcHeafnpFIpZs+ejapVq0JHRwcuLi5Yu3atOG7lypWoVasWdHV1YWtri4CAAOTm5spd54ULF+Dj4wNLS0toaWnBzs4OX3/9NZ49eyaOyXvfTp48KfY376dFixYy63v69CnGjh0LR0dHaGtro2LFiujWrRsuXbqkVI8UVZw9UvbzmpmZifnz58PNzQ16enowNDTEF198geDg4Hxj39+Gj48PzM3NoaamhsDAwEL1vSifzZCQEDRo0AB6enowNTVF79698fTpU7n79erVK0ybNg21atWCnp4ejIyM4OHhgR9++AGvX7/ON9bf3x81atSArq4ujIyM0Lp1a7k9e/PmDZYuXYo6derAxMQEenp6sLOzQ9euXXH06FG5tRBR4WiUdQFUvpiYmKBPnz7YsGED/vnnH7Rt21Zm/pMnT3Do0CF4enrC09NTZl5UVBQaN26MWrVq4euvv0ZsbCy2b9+Ojh07YuvWrejdu7fM+GHDhmH9+vWws7NDz549YWRkhP/++w/Tp0/HsWPHcOTIEWhqasos8/LlSzRu3BgGBgbw8fGBIAioVKmSzJgJEybg1KlT8PX1hbe3N0JDQ7Fs2TKcPn0aZ86cgY6Ojjh2wYIFiIiIQJMmTdC5c2dkZGTg7NmzmD17No4fP45///0XGhr5/xgtXrwY//zzD7p27YpWrVohKSlJnDdq1CjUrFkTzZs3h5WVFRISEnDw4EEMHjwYERERmDdvntze9+nTBxcuXECnTp2gqamJkJAQfPXVV9DS0sLly5exdetWdOnSBW3atMH+/fsxa9Ys6Orq4vvvv5dZz4YNGzBixAjo6OigW7dusLW1xf3797Fu3Trs378f//33H+zt7WFsbIyZM2ciMDAQ0dHRmDlzpriOd4PY1atX0a5dO7x69Qrt27fHl19+iYSEBOzZswfNmjXD7t270alTJ4V6pKzi6hGg2Oc1KysL7dq1w+nTp1GzZk2MGTMG6enp2LlzJ/r27Ytr165hwYIF+bbx4MEDNGrUCC4uLhgwYADS0tLg5uZWqL4r+9lctWoV9u3bh27dusHLywsXLlzAjh07cP36dYSFhUFbW1umBy1btkR0dDQ8PT0xatQo5Obm4u7du1i6dClGjhyJChUqAACio6PRokULPHr0CM2bN0fHjh2RlpaGAwcOoEOHDli9ejW++uorcd2DBg3Cjh07UKtWLQwaNAi6urp49uwZzpw5g9DQ0Hx/txCRAgT6bAEQAAgzZ84s8MfIyEgAIERFRYnLXb58WQAg9OzZM986p0+fLgAQ1qxZI06LiooSt/Xdd9/JjL906ZKgoaEhGBsbC8nJyeL0DRs2CAAEHx8fISMjQ2aZmTNnCgCEpUuXyt2fgQMHClKpNF9tgwcPFgAIFStWFB49eiROz8nJEb788ksBgDB79myZZR4+fCjk5ubmW5e/v78AQNi2bZvc2vT09ISrV6/mW04QBOHBgwf5pmVmZgotWrQQNDQ0hCdPnsjM8/LyEgAI9erVExITE2Vq09TUFIyMjIQqVaoIT58+FeclJSUJZmZmgpmZmUwv7t69K2hqagpOTk7Cs2fPZLZz7NgxQU1NTfD29pa7fXmkUqlQtWpVQUdHRzh9+rTMvJiYGMHa2lqwsLCQeQ8L06OC5L2HGzZskFtjcfRImc/r3LlzBQBCly5dZNYVFxcn2NnZCQBk+vPuNvz9/eXu64f6nrdvynw2DQwMhLCwMJl5ffv2FQAIwcHBMtObNGkiABDmzZuXbzsvXryQeV+9vLwEiUQi7NixQ2ZcYmKi4OHhIejo6AixsbGCILztvUQiETw9PYXs7Ox8605ISChwv4no4xjuPmN5v1wK8/NuuBMEQahfv76gqakpxMXFidOys7MFa2trwcDAQEhLSxOn5/0iMzIyElJSUvLVkfcLOzAwUJxWu3ZtQVNTU+YX9bvbqVixolCvXr18+6OlpSU8f/5c7v7mbef9ACcIb39RqqmpCVWqVJG77PsSEhIEAIKfn5/M9LxfoOPHjy/Uet4VEhIiABA2btwoMz3vl/yxY8fyLdOyZUsBgPDnn3/mm+fn5ycAkAmy3377rQBAOHjwoNwaunfvLqipqckElw+FjD179ggAhMmTJ8udv2zZMgGAcODAAXFaUXr0sXBXHD1S5vNatWpVQSKRCHfv3s03fs2aNfk+K3nbsLCwEDIzM+Xu68fCXUE+9tn88ccf8y3z77//CgCESZMmidPy/hFXu3ZtIScn54PbvH79ugBA6NWrl9z5eZ+T3377TRAEQUhJSREACE2aNJEbUImoaHhalgq8dg54exooOjo63/TRo0fDz88P69evh7+/PwBg//79ePbsGUaNGiWeqnlX3bp1YWBgkG96ixYtsHHjRly7dg2DBw9Geno6bty4ATMzMyxbtkxuXdra2oiIiJBb7/unYd/n5eWVb5qjoyPs7Ozw6NEjJCUlwdjYGADw+vVrLF++HLt378a9e/eQmpoq06+YmBi522jYsGGB23/8+DEWLFiAY8eO4fHjx/mujypone+f5gYAa2vrj857+vQpKleuDAA4f/48AODEiRO4ePFivmXi4+ORm5uL+/fvy13n+/LW9+jRIwQEBOSbf//+fQBAREQEOnfuLDPvQz1SVnH0KE9hP6+pqal4+PAhbG1t4ezsnG98mzZtALw9ff0+Dw8PmdOgilD2s1mvXr180+zs7AC8vaY2z3///QcAaN++PdTUPnx5dt7nICkpSe7n4MWLFwAg/pk1MDBA165dsX//ftSpUwc9e/ZEs2bN0LBhQ+jp6X1wW0T0cQx3pJTevXtj0qRJWLduHX744QdIJBL88ccfAICRI0fKXcbCwkLudEtLSwBAcnIygLe/YARBwIsXLzBr1iyF6spb14d8qI7o6GgkJyfD2NgYUqkUrVq1wsWLF1GrVi307t0b5ubm4nV+s2bNkntjx4fqiIyMRIMGDZCYmIgvvvgC7dq1g5GREdTV1fHo0SNs3LixwHUaGRnlm5Z3TdWH5kmlUnHay5cvAQCLFi2Su408aWlpH5z//vp27typ8PoK814pqjh6lKewn9e8/xa0P1ZWVjLj5K1LUUX5bH6oDzk5OeK0vGsgbWxsPlpP3ufg6NGjH7wZ4t3Pwfbt27FgwQJs3boVM2bMAADo6OjA19cXixcvhrm5+Ue3S0TyMdyRUnR1dTFkyBAsWbIER48ehbOzM44cOYJGjRrB3d1d7jLPnz+XOz0uLg7A/37p5P23Tp06co92fEhhHvr6/PlzuLi4fLSOvXv34uLFixg8eHC+h+bGxsZ+MHgWVMeSJUvw8uVLbNiwAUOGDJGZt23bNmzcuPGj9RdF3r4lJyfD0NCw2Na3d+9edOvWTaFlP/UH9Cr6ec2b/r7Y2FiZce9StgdF+WwWVt7R64KOAL4rb9+WL1+OcePGFWr9urq6CAgIQEBAAJ48eYJTp04hMDAQmzZtwqNHj8S7hYlIcXwUCilt1KhR4hG7tWvXIjc3F19//XWB469evYrU1NR800+cOAHgbZgDAH19fbi6uuLWrVt49epVsdct75dGZGQknjx5gipVqoi/1B48eAAA6NmzZ6HWURglsU5FNGrUCABw+vTpQi+jrq4OQPaoTlHWV14U9vNqYGCAqlWrIiYmRjwN/a7jx48DeHuaVxEf6ntpfI7y3tujR49+8NKNd8cq+zmws7ND//79ERoaCicnJ5w6dapE/uwTfS4Y7khp1apVQ9u2bbFv3z6sWbMGxsbG+R5n8q7k5GTMnj1bZtrly5exZcsWGBkZoUePHuL0iRMnIisrC0OHDpX7iIzExESFj+rlWb58ucx1hLm5uZg8eTJyc3Ph5+cnTs977ETeL+c8kZGRch+dURgFrTM0NBTr1q1Tap2KGDt2LDQ1NTFhwgTcu3cv3/ysrKx8v6ArVqwI4O1jbt7n7e2NqlWrYuXKlfj777/lbvP8+fNIT08vhupLlyKf16FDh0IQBEyePFkmjCUkJOCnn34SxyjiQ30vic/m+zw9PdGkSRNcvXoVixcvzjf/5cuXyMzMBPD2Or4vvvgCu3btwvr16+Wu7+bNm4iPjwfw9hq8Cxcu5Bvz+vVrpKamQl1dXe5jXIiocPinh4pk1KhROHLkCBISEjBu3Djo6uoWOLZ58+ZYt24dLly4gKZNm4rPDcvNzcUff/whc5pw6NChuHLlClatWoWqVauiffv2sLe3x6tXrxAVFYVTp07Bz88Pq1evVrjmZs2aoXbt2ujduzeMjIwQGhqKGzduwNPTE1OmTBHHde3aFdWqVcPSpUsRHh6OOnXq4PHjxzhw4AA6d+6Mx48fK7zt0aNHY8OGDfD19UXPnj1hY2OD8PBwHD58GL6+vti+fbvC61RE9erVsX79egwdOhSurq7o0KEDnJ2dIZVK8fjxY5w+fRrm5uYyN6u0bt0aO3fuxJdffomOHTtCV1cXlStXxsCBA6GpqYldu3ahffv26Ny5M5o0aYLatWtDT08PT548waVLlxAZGYnY2Nhyd6G8Ip/X7777DocOHcLevXvh4eGBTp06ic+5i4+Px5QpU9CsWTOFtv+hvpfEZ1OezZs3o0WLFpgyZQp27NgBLy8vCIKA+/fv48iRI4iIiBCD5tatW9GqVSsMGzYMK1asQMOGDWFsbIynT58iLCwM4eHhOH/+PCpVqoSYmBg0atQINWrUQN26dWFnZ4eUlBQcOHAAcXFxGDt2bLFcNkD02SrDO3WpjOH/H3PyIZUrV5b7KJQ82dnZgpmZmQBAuHXrltwxeY99GDx4sHDnzh2hW7dugrGxsaCrqys0adJEOHz4cIHb379/v9C5c2fB3Nxc0NTUFCwsLIT69esL06ZNE+7cuZNvf7y8vApcV94jLB4+fCgsXrxYcHFxEbS1tQVra2th/PjxMo//yPP48WOhX79+grW1taCjoyPUrFlTWLBggSCVSuVuL+9xE8ePHy+wjrNnzwotW7YUjI2NBX19faFp06bC7t27hePHj4vPHXzXhx6JkbdP8t6fD9USFhYmDB48WLC3txe0tLQEExMTwdXVVfjqq6/yPU4kOztb8Pf3FxwcHAQNDQ25+/38+XPh+++/F1xdXQVdXV2hQoUKQrVq1YSePXsKQUFBMs9+K0yPCvKxR6F8aJnC9kjZz2tGRoYwd+5cwdXVVdDR0RHf261bt+Yb++42CvKxvhfnZ/ND9SQkJAhTpkwRnJ2dBW1tbcHIyEjw8PAQpk6dKrx+/VpmbEpKijB37lyhbt26QoUKFQQdHR2hSpUqQqdOnYQ//vhDfERSYmKiMGvWLKFly5aCtbW1oKWlJVhaWgpeXl7C1q1b+XgUoiKSCMJHLqYg+oCHDx/CyckJzZo1w6lTp+SOefToERwcHORe/F2ahgwZgo0bNyIqKqpIX3VFqu1T+bwSESmL19xRkSxatAiCIGDs2LFlXQoRERGB19yREqKjoxEUFIT79+8jKCgIderUgY+PT1mXRURERGC4IyVERUVh+vTpqFChAtq3b4/ff//9o0+wJyIiotLBa+6IiIiIVAgPtxARERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQI75b9TCUmJiI7O7usyyg3zM3N8eLFi7Iuo9xh3xTHnimOPVMO+6a4suyZhoYGTExMCje2hGuhT1R2djakUmlZl1EuSCQSAG97xpvLC499Uxx7pjj2TDnsm+LKU894WpaIiIhIhTDcEREREakQhjsiIiIiFcJwR0RERKRCGO6IiIiIVAjDHREREZEKYbgjIiIiUiEMd0REREQqhOGOiIiISIUw3BERERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIwx0RERGRCtEo6wKobIzfE4WIuLSyLqMcuVPWBZRT7Jvi2DPFsWfKUe2+HRhWvaxLKDM8ckdERESkQhjuiIiIiFQIwx0RERGRCmG4IyIiIlIhDHdEREREKoThjoiIiEiFMNwRERERqRCGOyIiIiIVwnBHREREpEIY7oiIiIhUCMMdERERkQphuCMiIiJSIQx3RERERCqE4Y6IiIhIhTDcEREREakQhjsiIiIiFcJwR0RERKRCGO6IiIiIVAjDHREREZEKYbgjIiIiUiEMd0REREQqhOGOiIiISIUw3BERERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIwx0RERGRCmG4IyIiIlIhDHdEREREKuSzD3cBAQEIDAxUaBlfX19cvHixwPm3bt2Cr68vXr9+XcTqiIiIqKgCAwPRqFEjODo6okOHDrhw4UKBY8+dOwcbG5t8P/fv3xfH+Pj4yB0zcODA0tidj9Io6wLK2nfffQd1dfWyLoOIiIhKwN69exEQEIB58+ahfv36CAoKwoABA3DixAnY2NgUuNypU6dgYGAgvjYzMxP/f+3atZBKpeLrxMREtG3bFl26dCmZnVDQZ3/kTl9fH7q6umVdRqFkZ2eXdQlERETlytq1a9GnTx/069cPTk5OmD17NqytrbFp06YPLmdmZoZKlSqJP+8eCDIxMZGZd+rUKejq6qJr164lvTuFUuZH7gICAmBvbw8tLS0cO3YMGhoaaNu2LXx9fT+6rK+vL77++mtcvXoVN27cgKmpKQYNGoR69eqJY54+fYqgoCDcvn0bOjo6cHd3x+DBg2FoaChuv0qVKhgyZAiAt+l79erVCA8Ph7GxMfr27Ytt27ahU6dO6Ny5s7je1NRULFq0qMDtAsDdu3exbds2PHv2DJUrV8bIkSNhb28vzv/vv/+wY8cOxMXFwcTEBB06dJD5YIwZMwatWrVCXFwcLl68iPr162PkyJHYuHEjLly4gNevX8PY2Bht2rRBjx49lOo/ERGRqsrKykJYWBjGjBkjM93LywuXL1/+4LLt27fHmzdv4OTkhPHjx6NZs2YFjg0ODoa3tzf09PSKpe6i+iSO3J08eRLa2tqYN28eBgwYgL/++gthYWGFWjYkJASNGzfG4sWLUadOHaxYsQJpaWkA3ga1mTNnonLlyvj5558xdepUJCcnY+nSpQWu77fffkNiYiICAgIwadIk/PPPP0hOTlZou3mCgoIwcOBAzJ8/H4aGhliwYIF49C0yMhJLly5FkyZNsHjxYvTq1Qvbt2/HiRMnZNaxb98+2NnZYcGCBfDx8cHff/+Ny5cvY8KECVi2bBm++eYbmJubF7g/UqkU6enp4k9GRkah+kpERFSeSSQSJCYmIicnB+bm5pBIJOKPubk54uPjZabl/VhYWGDRokVYu3Yt1q1bh2rVqqF3797477//xPW++3P9+nVERESgX79+ctdXXD+KKPMjdwBQuXJl9OrVCwBgZWWFw4cP4+bNm3B3d//osl5eXmKa7tu3Lw4fPowHDx6gdu3aOHLkCBwdHdGvXz9x/KhRozBq1Cg8e/YM1tbWMuuKiYnBzZs3MX/+fFStWhUAMHLkSIwbN06h7ebp1auXuA9jx47FyJEjcfHiRTRp0gQHDhyAm5sbfHx8AADW1tZ4+vQp9u3bhxYtWojrqFWrFrp16ya+TkhIgJWVFapXry5+QD9k9+7dCAkJEV87ODhgwYIFH1yGiIiovLOysoIgCAAAc3NzWFlZifP09fWhqakpM+3d5b744gvxddeuXfHy5Uts2LABPXr0gKWlpcz4gIAA1KpVC506dSqhPVHcJxHu3j1VCbw9ly3vaJk8lStXFv9fR0cHOjo64rKRkZEIDw+Xe/fK8+fP84W7Z8+eQV1dHQ4ODuI0S0tLVKhQQaHt5nF2dhb/X19fH9bW1oiJiQHwNki+fxrXxcUFBw8eRG5uLtTU3h5UzQuZeVq0aIE5c+bg22+/hYeHBzw9PeHh4SGnM2/16NFD5gJPRdM/ERFReRQbGwupVAp1dXXcuXMHVapUEedFRUXBxMQEsbGxhVqXq6sr/vrrLwBAXFycGBrT09Oxbds2TJ48udDrUpaGhsZHD+iIY0u0kkLS0MhfRl7jPub9O10lEom4rCAI8PT0xIABA/ItZ2xsrPQ2P7bdD8kLV4Ig5Ata8pbX1taWee3o6IjffvsN169fR1hYGJYuXQo3NzdMmjRJ7vY0NTWhqan50bqIiIhUiSAI0NTUhLu7O06ePIkOHTqI806dOoX27dsX+vf+zZs3YWFhIa43b7l9+/YhKysLPXr0UChDlLRPItyVFAcHB1y4cAHm5uaFetyJjY0NcnJy8OjRIzg6OgJ4m9CVfV7dvXv3xFun09LSEBsbKx4ttLW1RURERL7x1tbW4lG7gujp6aFJkyZo0qQJGjVqhHnz5iEtLQ36+vpK1UlERKSqRowYgfHjx4tnuzZv3oyYmBjxrN78+fMRGxuLFStWAHh7d62dnR2cnZ0hlUqxa9cu/P3331i3bl2+dQcHB6N9+/YwNTUt1X36GJUOd+3bt8exY8ewfPlydOvWDQYGBoiLi8PZs2cxcuTIfCHKxsYGbm5u+OOPPzBixAioq6tj06ZN0NLSUup05l9//QUDAwMYGRkhODgYBgYGaNCgAQCgS5cu8Pf3R0hICJo0aYJ79+7h8OHDGD58+AfXeeDAAZiYmKBKlSqQSCT477//YGxs/MncoUNERPQp8fb2RmJiIpYuXYr4+Hi4uLggKCgItra2AN5epvXs2TNxvFQqxU8//YS4uDjo6OjA2dkZmzZtQps2bWTW+/DhQ1y8eBHbtm0r1f0pDJUOd6ampvjpp5+wZcsWzJ07F1KpFObm5vDw8CgwrI0dOxarV6/GzJkzxUehPH36VKlTm/369UNgYCBiY2NRuXJlTJkyRTwF7ejoiAkTJmDHjh3466+/YGJiAl9fX5mbKeTR0dHB3r17ERsbCzU1NVSrVg3+/v4fPdpHRET0uRoyZIj4yLP3LVu2TOb16NGjMXr06I+us2rVquJ19J8aifApnST+BL18+RKjRo3C9OnT4ebmVtblFJt+ay8iIi7t4wOJiIjKoQPDqhfr+iQSCaysrBAbG1sm19dpamqWrxsqPiXh4eHIzMyEvb09EhMTsXnzZpibm6NGjRplXRoRERHRR32y4e706dNYs2aN3Hnm5uZYsmRJiWw3Ozsb27Ztw/Pnz6GrqwtnZ2eMGzdO7h29RERERJ+aTzax1KtXD05OTnLnFebOV2XVrl1b5kHEREREROXJJxvudHV1oaurW9ZlEBEREZUrvMWSiIiISIUw3BERERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIwx0RERGRCmG4IyIiIlIhDHdEREREKoThjoiIiEiFMNwRERERqRCGOyIiIiIVwnBHREREpEIY7oiIiIhUCMMdERERkQphuCMiIiJSIQx3RERERCqE4Y6IiIhIhTDcEREREakQhjsiIiIiFcJwR0RERKRCGO6IiIiIVAjDHREREZEKYbgjIiIiUiEMd0REREQqhOGOiIiISIVolHUBVDaWd3eAVCot6zLKBYlEAisrK8TGxkIQhLIup9xg3xTHnimOPVMO+6baeOSOiIiISIUw3BERERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIwx0RERGRCmG4IyIiIlIhDHdEREREKoThjoiIiEiFMNwRERERqRCGOyIiIiIVwnBHREREpEIY7oiIiIhUCMMdERERkQphuCMiIiJSIQx3RERERCpEo6wLoLIxfk8UIuLSyrqMcuROWRdQTrFviiufPTswrHpZl0BE/49H7oiIiIhUCMMdERERkQphuCMiIiJSIQx3RERERCqE4Y6IiIhIhTDcEREREakQhjsiIiIiFcJwR0RERKRCGO6IiIiIVAjDHREREZEKYbgjIiIiUiEMd0REREQqRKlwl5WVhX/++QdPnz4t7nqIiIiIqAiUCndaWlrYsGEDUlJSirseIiIiIioCpU/LVqpUCUlJScVYChEREREVldLhrlOnTtizZw/S09OLsx4iIiIiKgINZRd88uQJUlNTMWbMGNSqVQsmJiYy8yUSCfz8/IpcIBEREREVntLhLjQ0VPz/ixcvyh3DcEdERERUupQOd9u3by/OOoiIiIioGPA5d0REREQqROkjd3muX7+O27dvIyUlBT4+PjAzM8ODBw9QqVIlGBoaFkeNRERERFRISoe7N2/eYOHChQgPDxentWvXDmZmZti/fz8qVqyIQYMGFUuRRERERFQ4Sp+W3bZtGyIjIzFp0iRs3LhRZp6Hhwdu3rxZ5OKIiIiISDFKH7n777//0Lt3bzRo0AC5ubky88zMzJCQkFDk4oiIiIhIMUofuUtJSYGtra3ceRKJBFlZWUoXRURERETKUTrcmZqa4vHjx3LnRUdHo1KlSkoXRURERETKUTrcNWjQALt370ZUVJQ4TSKR4MWLFzh48CAaN25cLAUSERERUeEpfc1dr169EB4ejqlTp8LOzg4AsGrVKjx//hzW1tbo3r17cdVIRERERIWkdLjT1dXFnDlz8Pfff+Pq1auwtLSEtrY2unfvjs6dO0NLS6s46yQiIiKiQijSN1RoaWmhe/fumD17NpYvX445c+bgyy+/hLa2dnHVV26MGTMGBw8eLPT4+Ph4+Pr64tGjRyVXFBFRGQgMDESjRo3g6OiIDh064MKFCwWOvXjxIry9veHq6oqqVauiefPmWLNmTb5xycnJmDp1KurUqQNHR0d4eXnh2LFjJbkbROWW0uFu7NixBQaTx48fY+zYscquulyaP38+2rRpU6zrPHHiBIYMGVKs6yQiKkl79+5FQEAAxo0bh9DQUDRo0AADBgxATEyM3PF6enrw8/PDrl27cOLECYwfPx4LFy7E5s2bxTFZWVno27cvnjx5gjVr1uDUqVNYtGgRLC0tS2u3iMoVpU/LvnjxAtnZ2XLnSaVSvHjxQumiyiN+1RoREbB27Vr06dMH/fr1AwDMnj0bJ0+exKZNm+Dv759vfK1atVCrVi3xtZ2dHQ4dOoQLFy5gwIABAIDg4GAkJSVh79690NTUBIACH8VFREU8LVuQ58+fQ1dXtyRWXWwuX76MIUOGiA9gfvToEXx9fREUFCSOWbNmDZYtWwYAuHv3LmbOnIn+/ftj1KhRWL9+PTIzM8Wx75+WjYmJwfTp09G/f39MmDABYWFh8PX1xcWLF2XqeP78OWbNmoUBAwZg8uTJuHfvHgDg1q1bWLVqFdLT0+Hr6wtfX1/s2LEDABAaGopx48ahf//+GDFiBH755ZcS6RERkSKysrIQFhYGLy8vmeleXl64fPlyodYRHh6Oy5cvyzxx4ejRo/D09MS0adPg4eGBVq1aYcWKFcjJySnW+olUhUJH7k6cOIGTJ0+Kr9etW5cvxGVlZSE6Oho1a9YsngpLSM2aNZGRkYFHjx7B0dERt2/fhoGBAW7fvi2OuXXrFjp37ozHjx9j7ty56N27N0aOHImUlBSsX78e69evx+jRo/OtOzc3F4sWLYKZmRnmzp2LzMxMbNq0SW4dwcHBGDhwICwtLREcHIzly5djxYoVcHFxwZAhQ7B9+3YsX74cAKCjo4OHDx9iw4YNGDt2LFxcXJCWloY7d+6UTJOIiBTw6tUr5OTkwMzMTGa6mZkZ4uPjP7isp6cnXr16hezsbEycOFE88ge8fXbq2bNn0aNHDwQFBSEqKgpTp05FTk4OJk6cWCL7QlSeKRTusrKykJKSIr5+/fo1pFKpzBhNTU00adIEvr6+xVNhCdHT00OVKlVw69YtODo6ikEuJCQEGRkZePPmDWJjY+Hq6ordu3ejWbNm6Ny5MwDAysoKfn5+mDlzJoYPH57vzuCwsDA8f/4cAQEBMDY2BgD06dMHc+bMyVdH165dUbduXQCAr68vJk6ciLi4ONjY2EBPTw8SiURcBwAkJCRAW1sbnp6e0NXVhbm5ORwcHArcT6lUKvMeSSSST/6oKhGVPxKJBBKJBACgpqYm/r+8+fLs2bMHr1+/xtWrVzFv3jw4ODigR48eAABBEFCxYkUsWrQI6urq8PDwwPPnz/H7779j0qRJ4vqp8PL6xb4VXnnqmULhrl27dmjXrh2At6chJ02ahCpVqpREXaXC1dUVt27dQpcuXRAREYE+ffrgwoULiIiIwOvXr2FkZAQbGxtERkYiLi4Op0+fllleEATEx8fnu/bj2bNnqFixokwoq1atmtwa7O3txf/PG5+cnAwbGxu5493d3WFubo6xY8eidu3aqF27Nho0aFDgHcq7d+9GSEiI+NrBwQELFiwosCdERMqwsrJCxYoVoa6ujuzsbFhZWYnzMjIyYGNjIzNN3vIA0KpVK2RmZmL58uXimRFbW1toamrK/F3bsGFDzJo1C6ampgDAmyuUxL4prjz0TOkbKlauXFmcdZSJmjVr4t9//0V0dDQkEglsbW1Rs2ZN3L59G69fvxZPLQuCgDZt2qBTp0751vH+6Ye88YVN9hoa/3sL8pYRBKHA8bq6uliwYAFu3bqFsLAw7NixAzt37sT8+fNRoUKFfON79OiBLl265NsGEVFxio2NBfD2H6B79+5Fo0aNxHmHDh1C+/btxTEfk5KSgvT0dHG8h4cHdu/ejZiYGKipvb1U/PLly7CwsMCrV69gaWmJuLi4D/7dSbIkEgn7pqCy7pmGhgbMzc0LN7YoG5JKpThx4gRu3bqF1NRUDB8+HFZWVrh06RLs7e1hYWFRlNWXuLzr7g4ePIiaNWtCIpGgZs2a2LNnD9LS0sQw5+DggKdPnxY6rdvY2CAhIQFJSUni0biHDx8qXJ+GhoZ4w8e71NXV4e7uDnd3d/j4+MDPzw/h4eFo2LBhvrGampri3WVERCUl75fdiBEjMH78eLi7u8PT0xObN29GTEwMBg4cCEEQMH/+fMTGxmLFihUA3j4Tz9raWjy7cenSJaxevRp+fn7iOgcOHIj169dj+vTp8PPzQ1RUFFasWIGhQ4eKYwRBYEhRAvumuPLQM6XDXUpKCmbNmoWnT5/C2NgYSUlJyMjIAPD2D+eNGzcwfPjwYiu0JORdd3f69GnxeXI1atTAkiVLkJOTA1dXVwCAt7c3pk2bhnXr1qFNmzbQ1tZGTEwMwsLCMHTo0HzrdXd3h4WFBVauXIkBAwYgIyMDwcHBABQ7cmZubo7MzEzcvHkTlStXhra2NsLDw/H8+XPUrFkTFSpUwLVr15Cbmwtra+uiN4SIqIi8vb2RmJiIpUuXIj4+Hi4uLggKChJPqT5//hzPnj0Tx+fm5uLnn3/G48ePoaGhgcqVK8Pf3x8DBw4Ux9jY2GDr1q0ICAhA27ZtYWlpiWHDhmHMmDGlvn9E5YHS4W7z5s1IT0/H/PnzUblyZZk7m1xdXbF3795iKbCkubq6IioqSgxy+vr6sLW1RWJionjdW+XKlREQEIDg4GDMmDEDgiDA0tJS5lb9d6mpqWHy5MlYvXo1/P39YWFhgQEDBmDBggUKHUVzcXFB27ZtsWzZMqSmpsLHxwfu7u64ePEidu7cCalUCisrK4wfP178fl8iorI2ZMiQAh/Anvd4qTxDhw6V+4/k99WrVw8HDhwohuqIVJ9EUPLY4vDhw9G/f3+0bNkSubm56Nu3L+bPnw9HR0eEh4dj0aJF2LhxY3HXW25FRERgxowZWLFixSdxMWa/tRcREZdW1mUQkYo4MKx6mWxXIpHAysoKsbGxn/ypsk8J+6a4su6ZpqZmyV9zl5GRUeBGsrOz5V4r9jm5ePEidHR0xIsvAwMD4eLi8kkEOyIiIlJdSoe7SpUq4d69ezJfG5PnwYMHn/01YBkZGdi8eTNevnwJAwMDuLm5YdCgQWVdFhEREak4pcNds2bNsHfvXtjZ2YkP4ZVIJHjw4AEOHTokPnzyc+Xl5ZXvK3iIiIiISprS4c7b2xt3797F4sWLxeerzZ07F6mpqahdu7bcZ8IRERERUclSOtxpaGjA398f586dw9WrV5GcnAwDAwN4enqiSZMm4oMmiYiIiKj0FOkhxhKJBE2bNkXTpk2Lqx4iIiIiKgIeXiMiIiJSIUofucvNzcWhQ4dw5swZvHjxAlKpNN8YPueOiIiIqHQpHe62bNmCAwcOoEqVKnB3d4eGRpHO8BIRERFRMVA6kZ05cwbe3t4yXztGRERERGVL6WvusrKy4O7uXpy1EBEREVERKR3u3N3dcf/+/eKshYiIiIiKSOnTsn5+fvj555+hra2NunXrQl9fP98YedOIiIiIqOQoHe709PRgbW2NjRs3FnhX7Pbt25UujIiIiIgUp3S4W7NmDc6fP4/69evDxsaGd8sSERERfQKUTmSXLl1C37590a1bt+Ksh4iIiIiKQOkbKjQ0NODg4FCctRARERFRESkd7ho0aIAbN24UZy1EREREVERKn5Zt2rQp/vjjD2RnZxd4t6yjo2ORiiMiIiIixSgd7n766ScAwKFDh3Do0CG5Y3i3LBEREVHpUjrcjRo1qjjrICIiIqJioHS4a9GiRTGWQURERETFQekbKoiIiIjo01OkJw+npaXhzJkzePr0KbKysmTmSSQSnrolIiIiKmVKh7uEhAT4+/vjzZs3ePPmDQwNDZGWlobc3FxUqFABenp6xVknERERERWC0qdlt2zZAltbW6xduxYA4O/vj6CgIPj5+UFTUxM//PBDsRVJRERERIWjdLi7d+8e2rVrB01NTXGahoYGOnTogFatWmHz5s3FUiARERERFZ7S4S45ORkmJiZQU1ODmpoa0tPTxXk1a9ZEREREsRRIRERERIWndLgzMjJCWloaAMDc3ByRkZHivBcvXkBdXb3o1RERERGRQpS+ocLJyQlRUVGoV68eGjRogJCQEEilUmhoaGDfvn1wdXUtzjqpmC3v7gCpVFrWZZQLEokEVlZWiI2NhSAIZV1OucG+KY49I6LioHS469atG+Lj4wEAPj4+iImJwY4dOwAANWrUgJ+fX/FUSERERESFpnS4c3R0hKOjIwBAR0cH33//PdLT0yGRSKCrq1tsBRIRERFR4Sl1zV1WVha+/vprXL58WWa6np4egx0RERFRGVIq3GlpaSErKws6OjrFXQ8RERERFYHSd8u6ubkhLCysOGshIiIioiJS+pq7Hj164JdffoGWlhYaNGgAExMTSCQSmTH6+vpFLpCIiIiICk/pcJf39WI7d+7Ezp075Y7Zvn27sqsnIiIiIiUoHe569uyZ70gdEREREZUtpcOdr69vcdZBRERERMVA6RsqiIiIiOjTo/SROwDIzc3FtWvXEBMTg6ysrHzzfXx8irJ6IiIiIlKQ0uEuNTUVM2bMwLNnzwocw3BHREREVLqUPi27bds2aGlpYeXKlQCAuXPnYvny5ejSpQusra3x+++/F1uRRERERFQ4Soe78PBwdO7cGaampm9XpKYGS0tLDBw4EG5ubti0aVOxFUlEREREhaN0uHv58iUqVaoENTU1SCQSZGZmivM8PT1x8+bNYimQiIiIiApP6XBnaGiI9PR0AICJiQmePHkizktLS0NOTk7RqyMiIiIihSh9Q4WDgwOePHmCunXrok6dOggJCYGuri40NDSwbds2ODk5FWedRERERFQISoe7Dh064Pnz5wCAPn364P79++LNFRYWFvDz8yueCqlEjN8ThYi4tLIuQyEHhlUv6xKIiIg+eUqHO3d3d/H/DQ0NsXDhQvHUrI2NDdTV1YteHREREREppEgPMX6XRCKBvb19ca2OiIiIiJRQpHCXnp6O0NBQ3Lp1C6mpqTAwMICrqyvatWuHChUqFFeNRERERFRISoe7+Ph4zJo1CwkJCTAzM4OxsTFiY2Nx8+ZNHD16FDNnzoSFhUVx1kpEREREH6F0uNuwYQOysrLw008/wdnZWZx+9+5dLF68GIGBgfj++++LpUgiIiIiKpwifUNF3759ZYIdALi4uKBPnz4IDw8vcnFEREREpBilw52mpiYqVqwod56ZmRk0NTWVLoqIiIiIlKN0uKtXrx7Onz8vd9758+dRt25dpYsiIiIiIuUofc1ds2bNsHr1aixZsgTNmjWDsbExkpKScPr0aURGRmLkyJGIjIwUxzs6OhZLwURERERUMKXD3dy5cwEAL1++xIULF/LNnzNnjszr7du3K7spIiIiIiokpcPdqFGjirMOIiIiIioGSoW73NxcODs7w8jIiA8rJiIiIvqEKHVDhSAImDhxIu7du1fc9RARERFRESgV7tTV1WFsbAxBEIq7HiIiIiIqAqUfhdKkSROcPHmyOGshIiIioiJS+oaKKlWq4Pz585g1axYaNmwIY2NjSCQSmTENGzYscoFEREREVHhKh7uVK1cCAF69eoXbt2/LHcPHnxARERGVLqXD3cyZM4uzDiIiIiIqBkqHu5o1axZnHURERERUDJQOd3nS09Nx7949pKamok6dOtDX1y+OuoiIiIhICUUKdyEhIdi7dy+ysrIAAPPnz4e+vj5mz54Nd3d3dO/evThqJCIiIqJCUvpRKKGhoQgJCUHLli3xww8/yMyrW7curl69WuTiiIiIiEgxSh+5O3z4MLp06YIBAwYgNzdXZp6VlRViY2OLXBwRERERKUbpI3fx8fHw8PCQO09XVxfp6elKF0VEREREylE63Onp6SE5OVnuvPj4eBgaGipdFBEREREpR+lwV6tWLezduxeZmZniNIlEgpycHBw9erTAo3pEREREVHKUvuaud+/e8Pf3x8SJE9GgQQMAb6/De/ToERISEjBhwoRiK5KIiIiICkfpI3eWlpb46aefYGNjg9DQUADAqVOnYGBggFmzZsHMzKzYiiQiIiKiwinSc+5sbW0xbdo0SKVSpKamQl9fH1paWsVVG5FcgYGBWL16NeLj4+Hs7IxZs2ahYcOGBY4/f/48Zs2ahXv37sHCwgKjRo3CoEGDZMYkJydjwYIFOHToEJKTk2FnZ4cZM2agdevWJb07RERExUrpI3fv0tDQgK6uLjQ1NYtjdfSOHTt2YPLkyWVdxidj7969CAgIwLhx4xAaGooGDRpgwIABiImJkTv+8ePHGDhwIBo0aIDQ0FB88803mDFjBg4ePCiOycrKQt++ffHkyROsWbMGp06dwqJFi2BpaVlau0VERFRsinTk7v79+9ixYwdu376N7OxsaGhooGbNmujVqxecnZ2Lq0aVExAQgCpVqmDIkCEfHdutWzd07Nix5IsqJ9auXYs+ffqgX79+AIDZs2fj5MmT2LRpE/z9/fONDwoKgo2NDWbPng0AcHJywo0bN7B69Wp07twZABAcHIykpCTs3btX/AeKra1tKe0RERFR8VL6yF14eDhmzpyJyMhING3aFN7e3mjatCkiIyMREBCAmzdvFmednx1BEJCTkwMdHR0YGBiUdTmfhKysLISFhcHLy0tmupeXFy5fvix3mStXruQb36JFC4SFhUEqlQIAjh49Ck9PT0ybNg0eHh5o1aoVVqxYgZycnJLZESIiohKk9JG7LVu2wMHBAdOnT4eOjo44PSMjA7Nnz8bWrVsxf/78YimyLAUEBMDe3h5qamo4efIkNDQ00Lt3bzRr1gzr16/Hf//9ByMjIwwdOhR16tQBADx9+hRBQUG4ffs2dHR04O7ujsGDB8PQ0BArV67E7du3cfv2bfz9998AgN9++w0vXrzArFmzMHXqVAQHByM6OhrTpk3D7du3cenSJSxatEis6d9//8WBAwcQFxcHfX19NGzYEMOGDSuT/pSmV69eIScnJ9/NOmZmZoiPj5e7THx8vNzx2dnZePXqFSwsLBAdHY2zZ8+iR48eCAoKQlRUFKZOnYqcnBze9U1EROWO0uHu8ePHGDdunEywA95+O4W3tzd+/fXXIhf3qTh58iS6deuGefPm4dy5c1i7di0uXbqE+vXro0ePHjh48CB+++03rFq1Cunp6Zg5cyZat26NQYMGISsrC1u2bMHSpUsxc+ZM+Pn5ITY2FnZ2dujduzcAwNDQEC9evADwNjQPHDgQlSpVQoUKFXD79m2ZWo4cOYKNGzeif//+qF27NtLT03H37t0Ca5dKpeIRKuDtswh1dXVLoEslTyKRAADU1NTE/3933vvT8qbLG//uegRBQMWKFbFo0SKoq6vDw8MDz58/x++//46JEyeKy8pbBxWMfVMce6Y49kw57JviylPPlA53RkZGBe6gmpqaSn1DReXKldGzZ08AQI8ePbBnzx4YGBigTZs2AAAfHx8cOXIE0dHRuHbtGhwdHcVrwgBg1KhRGDVqFJ49ewZra2toaGhAW1sbxsbG+bbl6+sLd3f3Amv566+/0LVrV3Tq1EmcVq1atQLH7969GyEhIeJrBwcHLFiwoND7/ilxdXWFuro6srOzYWVlJU7PyMiAjY2NzLQ8NjY2eP36tcy83Nxc8fpQTU1N2Nraiv/N07BhQ8yaNQsVK1YU7wDnDRbKYd8Ux54pjj1TDvumuPLQM6XDXZs2bXDw4EHUrVsXGhr/W012djYOHjwoBh9VYG9vL/6/mpoaDAwMZKYZGRkBAFJSUhAZGYnw8HAMHDgw33qeP38Oa2vrD26ratWqBc5LTk5GYmIiatWqVejae/TogS5duoivy8O/OAry8uVLuLu7Y+/evWjUqJE4/dChQ2jfvj1iY2PzLePm5oZDhw7hhx9+EKft2bMHHh4eSEhIAAB4eHhg9+7diImJgZra28tQL1++DAsLC7x8+RISiQSWlpaIi4uDIAglvJeqg31THHumOPZMOeyb4sq6ZxoaGjA3Ny/c2KJs5MWLF/jmm2/QoEEDGBsbIykpCRcvXoSamho0NTVx4MABcfy7AaO8eTe8Am/fYHV1dZnXwNsjQoIgwNPTEwMGDMi3HnlH6t6nra1d4DxlniGoqampMo+oEQQBI0aMwPjx4+Hu7g5PT09s3rwZMTExGDhwIARBwPz58xEbG4sVK1YAAAYOHIgNGzZg5syZ6N+/P65cuYJt27Zh5cqV4h/OgQMHYv369Zg+fTr8/PwQFRWFFStWYOjQoTJ/gAVB4F+CSmDfFMeeKY49Uw77prjy0LMi3VCR5/Dhwx+cD5TvcKcIBwcHXLhwAebm5jIB8F0aGhrIzc1VeN26urowNzdHeHi4QkfvVIm3tzcSExOxdOlSxMfHw8XFBUFBQeIp1efPn+PZs2fieHt7ewQFBSEgIAAbN26EhYUFZs+eLT4GBXh76nbr1q0ICAhA27ZtYWlpiWHDhmHMmDGlvn9ERERFpXS4++2334qzDpXRvn17HDt2DMuXL0e3bt1gYGCAuLg4nD17FiNHjoSamhrMzc1x//59xMfHQ0dHB/r6+oVef69evbB27VoYGhqiTp06yMjIwN27dz+rZ+ENGTKkwGcELlu2LN+0xo0bi1+RV5B69erJHGkmIiIqr5QOd4U97/u5MTU1xU8//YQtW7Zg7ty5kEqlMDc3h4eHh3j6tmvXrli5ciUmTpyIrKwshYJyixYtIJVKcfDgQQQFBcHQ0PCDX71FREREnxeJoOSJ459//hkdOnRA7dq1i7kkKg391l5ERFxaWZehkAPDqpfJdiUSCaysrBAbG/vJX2fxKWHfFMeeKY49Uw77priy7pmmpmbJ31ARExOD+fPnw9LSEu3bt0eLFi2gp6en7OqIiIiIqBgoHe5+/fVXXL16FaGhodi4cSOCg4PRrFkzdOjQQeYxIURERERUepQOdwBQt25d1K1bF3FxcQgNDcWJEydw7Ngx1KhRAx06dECDBg3E54YRERERUckrUrjLY2lpicGDB6Nnz55YsmQJbt26hTt37sDU1BTdunVDhw4dyvXDc4mIiIjKi2IJdy9fvsTRo0dx7NgxpKSkoHbt2mjSpAkuXbqEwMBAPHv27LP4YnsiIiKislakcBceHo7Dhw/jypUr0NLSgpeXFzp27Ch+j6eXlxf+/vtv7Ny5k+GOiIiIqBQoHe4mTJiAZ8+eoVKlShgwYABatmwp927ZatWqIT09vUhFEhEREVHhKB3uTE1N0b9/f3h6en7wejpHR0d+mwURERFRKVE63E2fPr1wG9DQ4LdZEBEREZUShcLd2LFjCz1WIpHg119/VbggIiIiIlKeQuHO1tY237Rr166hevXq0NXVLbaiiIiIiEg5CoW7H374QeZ1Tk4O+vXrh8GDB8PR0bFYCyMiIiIixRXp6yP4YGIiIiKiTwu/G4yIiIhIhTDcEREREakQhjsiIiIiFaLQDRWRkZEyr3NzcwEAz549kzueN1kQERERlS6Fwp2/v7/c6QU9z2779u2KV0RERERESlMo3I0aNaqk6iAiIiKiYqBQuGvRokUJlUFERERExYE3VBARERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIwx0RERGRCmG4IyIiIlIhDHdEREREKoThjoiIiEiFMNwRERERqRCFvqGCVMfy7g6QSqVlXQYREREVMx65IyIiIlIhDHdEREREKoThjoiIiEiFMNwRERERqRCGOyIiIiIVwnBHREREpEIY7oiIiIhUCMMdERERkQphuCMiIiJSIQx3RERERCqE4Y6IiIhIhTDcEREREakQhjsiIiIiFcJwR0RERKRCGO6IiIiIVAjDHREREZEK0SjrAqhsjN8ThYi4NJlpB4ZVL6NqiIiIqLjwyB0RERGRCmG4IyIiIlIhDHdEREREKoThjoiIiEiFMNwRERERqRCGOyIiIiIVwnBHREREpEIY7oiIiIhUCMMdERERkQphuCMiIiJSIQx3RERERCqE4Y6IiIhIhTDcEREREakQhjsiIiIiFcJwR0RERKRCGO6IiIiIVAjDHREREZEKYbgjIiIiUiEMd0REREQqhOGOiIiISIUw3BERERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIwx0RERGRCmG4IyIiIlIhDHdEREREKoThjoiIiEiFMNwRERERqZDPItwFBAQgMDCw2NYnCAL++OMP+Pn5wdfXF48ePVJ6XStXrsTChQuLrbbSlpSUhG+++QbVq1dH9erV8c033yA5OfmDywiCgF9++QV169ZF1apV4ePjg7t378qM2bx5M3x8fODi4gIbG5uPrpOIiIje+izCXXG7fv06Tpw4gR9++AFr1qyBnZ2d0uvy8/PDmDFjirG6kpeUlITXr18DAMaOHYvbt29j8+bN2Lx5M27fvo1x48Z9cPlVq1ZhzZo1mDNnDg4ePAhzc3P07dsXaWlp4piMjAy0aNEC33zzTYnuCxERkarRKOsCyqPnz5/DxMQELi4uRV6Xnp5eMVRU8rKzs3HixAns3LkTR48exf79+6GlpYXjx49j//79qFu3LgBg4cKF6NatGx48eIBq1arlW48gCFi3bh3GjRuHTp06AQCWLVuG2rVrY/fu3Rg4cCAAYMSIEQCAc+fOldIeEhERqYbPLtxlZ2cjODgYp0+fRnp6Ouzs7NC/f3+4uroCAFJTU/Hnn38iIiICaWlpsLCwQI8ePdCsWTMAb0+jnjx5EgDg6+sLc3NzrFy58oPb/O+//7Bz507ExcVBW1sbDg4OmDx5MnR0dLBy5Uq8fv0aU6ZMQXx8PMaOHZtv+Zo1ayIgIAAAcPfuXWzduhUPHjyAoaEh6tevj379+kFHR6cYu/Q/d+7cwc6dO7Fr1y5IpVJ07doVO3bsgKurK4KDg2FoaCgGOwDw9PSEoaEhrly5IjfcPX78GPHx8fDy8hKnaWtro1GjRrh8+bIY7oiIiEg5n124W7VqFV68eIFvv/0WJiYmuHjxIubNm4fFixfDysoKUqkUjo6O6N69O3R1dXH16lX89ttvsLCwgJOTE/z8/GBhYYFjx45h/vz5UFP78JntxMRELF++HP3790eDBg2QmZmJO3fuyB1rZmaGNWvWiK+TkpLw008/oUaNGgDeBqO5c+eid+/eGDlyJFJSUrB+/XqsX78eo0ePLrYevXr1Crt378aOHTtw7949tGzZEvPmzUObNm2gpaUljouPj0fFihXzLV+xYkXEx8fLXXfedDMzM5np5ubmePr0abHtAxER0efqswp3cXFxOHv2LH7//XeYmpoCALp164YbN27g+PHj6NevH0xNTdGtWzdxmY4dO+L69es4f/48nJycoKenB11dXaipqcHY2Pij20xMTEROTg4aNmwIc3NzAIC9vb3cse+uMysrC4sWLYKTkxN69eoFANi3bx+aNWuGzp07AwCsrKzg5+eHmTNnYvjw4TLBK49UKoVUKhVfSyQS6Orqyt2+RCIBAGzYsAFLlixBw4YNcfbsWdjY2BQ4Pu+noHkFbUNNTU1mviAIcpfJe13Q+krDuzVQ4bFvimPPFMeeKYd9U1x56tlnFe6ioqIgCALGjx8vMz07Oxv6+voAgNzcXOzZswfnzp3Dq1evIJVKkZ2dDW1tbaW2WaVKFbi5ueG7776Dh4cH3N3d0ahRI3F7BVm9ejUyMjLw448/ikcHIyMjERcXh9OnT8uMFQQB8fHxsLW1zbee3bt3IyQkRHzt4OCABQsWyN2mlZUVAGDSpEkwNTXFxo0b0bJlS/Ts2RMDBw5Ey5YtZY5UOjk54eXLl+JyeV69egUnJ6d80wGgVq1aYs3vzk9LS4O9vX2+ZfKODFpaWhYqTJckS0vLMt1+ecW+KY49Uxx7phz2TXHloWefVbgTBAFqampYsGBBvtOpedes7d+/HwcPHsTgwYNhb28PHR0dBAYGIjs7W6ltqqmp4ccff8Tdu3cRFhaGw4cPIzg4GPPmzUOlSpXkLvPXX3/h+vXrmDdvnsxRNkEQ0KZNG/FGhHe9f5ozT48ePdClSxfx9Yf+xREbGyuOGTp0KIYOHYpLly5h586d+PLLL1GhQgV8+eWX4iNKqlWrhuTkZPz999+oU6cOAODq1atITk5GtWrVxPW9S0dHB5UqVcJff/0l/gHJysrCiRMnMG3atHzLvHz5EsDbo64ZGRkF1l6SJBIJLC0tERcXB0EQyqSG8oh9Uxx7pjj2TDnsm+LKumcaGhriGcCPji3hWj4pVapUQW5uLpKTk8Xr2N53584d1KtXD82bNwfw9khebGxsgacmC0MikYjPgfPx8cHo0aNx8eJFmdCV57///kNISAimTp2a718HDg4OePr0qUL/atDU1ISmpmahxsr7sNarVw/16tXDrFmzEBoaip07d6JNmzYIDQ1FjRo10LJlS3z33Xfi0cDvv/8ebdq0QdWqVcX1NW/eHP7+/ujYsSMAYPjw4fj111/h4OAABwcH/Prrr9DV1UX37t3FZeLj4xEfH4+oqCgAb9+XChUqwMbGBiYmJoXe/+IkCAL/ElQC+6Y49kxx7Jly2DfFlYeefVbhztraGs2aNcNvv/2GQYMGwcHBASkpKQgPD4e9vT3q1q0LS0tLXLhwAXfv3kWFChVw4MABJCUlKR3u7t+/j5s3b8LDwwNGRka4f/8+UlJS5K7v8ePHWLlyJby9vWFnZ4ekpCQAb9O6vr4+vL29MW3aNKxbtw5t2rSBtrY2YmJiEBYWhqFDhxalNR+lo6MDb29veHt7Iy4uDhUqVAAA/Prrr5gxYwb69esHAGjXrh3mzJkjs+zDhw+RkpIivh49ejQyMzMxdepUJCcno06dOti6davMqeqgoCAsWbJEfP3ll18CAJYsWYLevXuX2H4SERGVd59VuAPeBotdu3Zh06ZNePXqFQwMDODs7Cw+zsPHxwfx8fGYO3cutLW10bp1a9SvXx/p6elKbU9XVxd37tzB33//jYyMDJiZmWHQoEHiacx3RUZG4s2bN9i1axd27dolTs97FErlypUREBCA4OBgzJgxA4IgwNLSEo0bN1auGUp698ihiYkJfv311w+Oj4mJkXktkUgwadIkTJo0qcBlPjafiIiI5JMIn/qxRSoR/dZeRERcmsy0A8Oql1E1nzaJRAIrKyvExsZ+8ofiPyXsm+LYM8WxZ8ph3xRX1j3T1NQs9DV3/PoxIiIiIhXy2Z2WLW4JCQmYMGFCgfOXLl1a4J2sRERERMWN4a6ITExMsGjRog/OJyIiIiotDHdFpK6uXi4eaEhERESfB15zR0RERKRCGO6IiIiIVAjDHREREZEKYbgjIiIiUiEMd0REREQqhOGOiIiISIUw3BERERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIv36M8nnz5g3evHlT1mV8UjIyMpCVlVXWZZQ7hembRCKBvr4+JBJJKVVFRKTaGO5IxuvXryGRSGBgYMBftu/Q1NSEVCot6zLKncL0LSsrC2lpaTAwMCilqoiIVBtPy5KM7Oxs6OnpMdhRqdHS0oIgCGVdBhGRymC4IxkMdUREROUbwx0RERGRCmG4o89Kw4YNsXbt2iKPKart27ejRo0aJbqN4lBe6iQiov9huCOVEBMTg0mTJqFu3bqoUqUKGjRogBkzZuDVq1cKr+vvv//GgAEDiq02eWGxW7duOH36dLFt430HDx6EnZ0dYmJi5M5v3rw5pk+fXmLbJyKissO7ZalQuvwZUWrbOjCsukLjo6Oj0a1bNzg6OmLlypWwt7fH3bt3MWfOHPz777/Yv38/TExMCr2+ihUrKlqywnR1daGrq1ti62/Xrh1MTEywY8cOTJgwQWbepUuX8PDhQ/z+++8ltn0iIio7PHJH5d60adOgqamJrVu3onHjxrCxsUGrVq0QHByMuLg4LFiwQGZ8WloaxowZAycnJ9StWxfr16+Xmf/+kbaUlBRMmjQJ7u7ucHFxQa9evXDr1i2ZZY4cOYKOHTvC0dERtWrVwvDhwwEAPj4+ePr0KQICAmBjYwMbGxsAsqc7Hzx4ABsbGzx48EBmnX/88QcaNmwo3kl67949DBw4EE5OTvDw8MA333xT4JFJTU1N9OzZEzt37sx3J2pwcDDc3d3h6uqKP/74A61bt0a1atVQr149+Pv74/Xr1wX2+ttvv8XQoUNlps2YMQM+Pj7ia0EQsGrVKjRu3Bj29vZo06YNDhw4UOA6iYioeDHcUbmWmJiIEydOYPDgwfmOhFWqVAlffvkl9u/fLxNwVq9ejRo1auDw4cMYO3YsAgICcOrUKbnrFwQBgwYNQnx8PIKCgnDo0CG4ubmhd+/eSExMBAD8888/GD58OFq3bo3Q0FBs374d7u7uAIC1a9fCysoK3333Ha5du4Zr167l20a1atXg7u6OXbt2yUzfs2cPunfvDolEgufPn6Nnz56oWbMmDh06hC1btiAhIQFff/11gb3p27cvoqOjcf78eXFaeno69u/fjz59+gAA1NTUMHv2bPz7779YtmwZzp49izlz5nyo5R+1YMECbN++HfPnz8epU6cwYsQIjBs3TqYOIiIqOTwtS+VaVFQUBEGAk5OT3PnVqlVDUlISXr58CTMzMwBA/fr1MXbsWABA1apVcenSJaxduxbNmzfPt/zZs2cRERGB27dvQ03t7b+FZsyYgdDQUBw8eBADBgzAihUr4O3tje+++05cztXVFQBgYmICdXV16Ovro1KlSgXuR48ePRAYGIgpU6YAAB4+fIiwsDAsX74cALBp0ya4ubnB399fXOaXX35B/fr18fDhQ1StWjXfOp2dnVGnTh1s374dTZo0AQDs378fOTk56N69OwBgxIgR4nh7e3tMnjwZ/v7+mD9/foG1fkh6ejrWrl2L7du3o169etDU1ISNjQ0uXbqEzZs3o3Hjxkqtl4iICo/hjlRa3hG7d5/f5+npKTPG09MT69atk7v8zZs38fr1a7i4uMhMz8zMRHR0NADg1q1b6N+/f5Hq9Pb2xpw5c3DlyhV4enpi9+7dcHV1hbOzMwAgLCwM586dkxtio6Oj5YY74O3Ru5kzZ2Lu3LnQ19dHcHAwOnXqBCMjIwBvw+uvv/6K+/fvIzU1FTk5OcjMzER6ejr09PQU3o979+4hMzMTffv2lZkulUpRq1YthddHRESKY7ijcq1KlSqQSCS4d+8eOnTokG/+w4cPYWxsDFNT0w+up6CHN+fm5qJSpUrYs2cPsrOzZeblBSQdHR0lq/8fCwsLNGnSBHv27IGnpyf27Nkjc8euIAho27Ytpk6dKnfZgnh7eyMgIAD79u1D48aNcfHiRfEI49OnTzFo0CAMGDAAkydPhrGxMS5duoRJkyYV+JVhampq+a7he7cvubm5AN4eabS0tISGhoY4X0tLq5DdICKiomC4o3LN1NQUzZs3x8aNGzFixAiZ6+7i4+Oxa9cu+Pj4yIS3q1evyqzj6tWrqFatmtz1u7m54cWLF9DQ0ICdnZ3cMTVq1MCZM2fQu3dvufM1NTWRk5Pz0X3p0aMH5s2bB29vb0RHR8Pb21ucV6tWLfz999+ws7ODhkbh/9jq6+ujS5cu2L59O6Kjo1G5cmXxFO2NGzeQnZ2NmTNniqec9+/f/8H1VaxYEXfv3pWZduvWLWhqagJ4eypYW1sbMTExaNy4Mb+Tl4ioDPCGCir35syZg6ysLPTv3x///fcfYmJicPz4cfTt2xeWlpb4/vvvZcZfunQJq1atwsOHDxEYGIgDBw5g2LBhctf9xRdfwNPTE4MHD8aJEyfw5MkTXLp0CQsWLMCNGzcAABMnTsSePXuwePFi3L9/H3fu3MGqVavEddjZ2eHChQuIjY394HP3OnXqhLS0NPj7+6NJkyawsrIS5w0ZMgRJSUkYPXo0rl27hujoaJw8eRITJ078aHDs27cvLl++jKCgIPTu3VsMupUrV0Z2djbWr1+P6OhohISEICgo6IPratq0KW7cuIGdO3ciMjISixcvlgl7+vr6+PrrrxEQEIAdO3YgKioK4eHhCAwMxI4dOz64biIiKh48cveZWt7dQWWOqDg6OuLQoUP45ZdfMGrUKCQmJsLc3BwdOnTAhAkT8j3j7uuvv0ZYWBiWLFkCfX19zJgxAy1atJC7bolEgqCgICxatAiTJk3Cy5cvYW5ujkaNGok3aDRp0gR//PEHli1bhpUrV0JfXx+NGjUS1/Hdd9/h+++/R9OmTfHmzZsCHyxsYGAgPjZkyZIlMvMsLS2xZ88ezJs3D/3798ebN29ga2uLFi1aiEfdCtKgQQNUrVoVUVFR6NWrlzi9Vq1amDlzJlatWoX58+ejUaNG8Pf3x/jx4wtcV4sWLfDtt99i7ty5ePPmDXr37g0fHx9ERPzvOYhTpkyBmZkZfvvtN0yZMgWGhoZwc3PDN99888E6iYioeEiE9y+goc/Cixcv5Ia7lJQUGBoalkFFn446depg8uTJ6NevnziNpxeVU9i+8XP3lkQigZWVFWJjY/Nd20jysWfKYd8UV9Y909TUhLm5eaHG8sgd0f/LyMjApUuX8OLFC/EuVSIiovKG19wR/b/Nmzdj1KhRGD58OOrVq1fW5RARESmFR+6I/t+IESNkHupLRERUHvHIHREREZEKYbgjIiIiUiEMd0REREQqhOGO8sn7Cimi0sDHMBARFS+GO5Khp6eH1NRUBjwqNenp6dDW1i7rMoiIVAbvliUZGhoaqFChAtLS0sq6lE+KlpYWsrKyyrqMcudjfRMEARoaGgx3RETFiOGO8tHQ0OC3BbyjrJ9KXl6xb0REZYOnZYmIiIhUCMMdERERkQphuCMiIiJSIQx3RERERCqEN1R8pjQ0+NYrij1TDvumOPZMceyZctg3xZVVzxTZrkTgbWyfFalUCk1NzbIug4iIiEoIT8t+ZqRSKZYvX46MjIyyLqXcyMjIwPfff8+eKYh9Uxx7pjj2TDnsm+LKU88Y7j5DZ8+e5XPHFCAIAqKiotgzBbFvimPPFMeeKYd9U1x56hnDHREREZEKYbgjIiIiUiEMd58ZTU1N+Pj48KYKBbBnymHfFMeeKY49Uw77prjy1DPeLUtERESkQnjkjoiIiEiFMNwRERERqRCGOyIiIiIVwnBHREREpEL4pXIqKDQ0FPv27UNSUhJsbW0xZMgQ1KhRo8Dxt2/fxsaNG/H06VOYmJigW7duaNeuXSlWXPYU6VliYiI2bdqEyMhIxMXFoWPHjhgyZEjpFvwJUKRnFy5cwJEjR/Do0SNkZ2fD1tYWvXr1Qu3atUu36E+AIn2LiIjAli1bEBMTgzdv3sDc3Bxt2rRBly5dSrnqsqXo32l5IiIiEBAQADs7OyxatKgUKv10KNKzW7duYdasWfmmL126FDY2NiVd6idF0c+aVCpFSEgITp8+jaSkJFSsWBE9evRAq1atSrHq/BjuVMy5c+cQGBiI4cOHw8XFBf/88w/mzZuHpUuXwszMLN/4+Ph4zJ8/H61bt8Y333yDu3fvYt26dTA0NESjRo3KYA9Kn6I9k0qlMDQ0xJdffomDBw+WQcVlT9Ge3blzB+7u7ujbty8qVKiA48ePY8GCBZg3bx4cHBzKYA/KhqJ909bWRvv27VG5cmVoa2sjIiICa9euhY6ODtq0aVMGe1D6FO1ZnvT0dKxcuRJubm5ISkoqvYI/Acr2bNmyZdDT0xNfGxoalka5nwxl+rZ06VIkJydj5MiRsLS0REpKCnJyckq58vx4WlbFHDhwAK1atULr1q3Ff3WYmZnhyJEjcscfOXIEZmZmGDJkCGxtbdG6dWu0bNkS+/fvL+XKy46iPatUqRL8/Pzg5eUl8xfh50TRng0ZMgTe3t6oVq0arKys0K9fP1hZWeHKlSulXHnZUrRvDg4OaNasGezs7FCpUiU0b94cHh4euHPnTilXXnYU7VmeNWvWoGnTpnByciqlSj8dyvbMyMgIxsbG4o+a2ucVERTt2/Xr13H79m34+/vD3d0dlSpVQrVq1eDi4lLKlef3eb1zKi47OxuRkZHw8PCQme7u7o67d+/KXeb+/ftwd3eXmVa7dm1ERkYiOzu7xGr9VCjTs89dcfQsNzcXGRkZ0NfXL4kSP0nF0beoqCjcvXsXNWvWLIkSPznK9uz48eN4/vw5evXqVdIlfnKK8jmbMmUKvvrqK8yePRvh4eElWeYnR5m+Xb58GVWrVsXevXvx9ddfY/z48di0aROysrJKo+QP4mlZFZKSkoLc3FwYGRnJTDcyMirwtERSUpLc8Tk5OUhNTYWJiUlJlftJUKZnn7vi6NmBAwfw5s0bNG7cuAQq/DQVpW8jR44UT/f06tULrVu3LsFKPx3K9Cw2NhZbt27FrFmzoK6uXgpVflqU6ZmJiQm++uorODo6Ijs7G6dOncJPP/2EmTNnfjb/kFCmb8+fP0dERAQ0NTUxefJkpKSk4M8//0RaWhpGjx5dClUXjOFOBUkkkkJNK2he3peWfGgZVaNoz0j5np05cwY7d+7E5MmT8/1F+jlQpm+zZ89GZmYm7t27h61bt8LS0hLNmjUrqRI/OYXtWW5uLlasWIFevXrB2tq6NEr7ZCnyObO2tpbpl7OzMxISErB///7PJtzlUaRveb8rx40bJ16iI5VKsWTJEgwfPhxaWlolV+hHMNypEENDQ6ipqeX7V0ZycnKBv0SNjY3zjU9JSYG6uvpnccpMmZ597orSs3PnzmH16tWYOHFivssBVF1R+lapUiUAgL29PZKTk7Fz587PItwp2rOMjAw8fPgQUVFRWL9+PYC3v4AFQUCfPn3w448/olatWqVRepkprr/TnJ2dcfr06WKu7tOl7O9PU1NTmWuvbWxsIAgCXr58CSsrq5Is+YN4zZ0K0dDQgKOjI8LCwmSmh4WFFXiBp5OTU77xN27cgKOjIzQ0VD/7K9Ozz52yPTtz5gxWrlyJcePGoW7duiVd5ienuD5rgiB8FtfDAor3TFdXF4sXL8bChQvFn7Zt28La2hoLFy5EtWrVSqv0MlNcn7OoqCgYGxsXc3WfLmX6Vr16dSQmJiIzM1OcFhsbC4lEgooVK5ZovR/DcKdiunTpgmPHjuHff//F06dPERgYiISEBLRt2xYAsHXrVvz222/i+Hbt2iEhIUF8zt2///6Lf//9F127di2rXSh1ivYMAB49eoRHjx4hMzMTKSkpePToEZ4+fVoW5ZcJRXuWF+wGDRoEZ2dnJCUlISkpCenp6WW1C2VC0b4dPnwYly9fRmxsLGJjY3H8+HHs378fX3zxRVntQqlTpGdqamqwt7eX+TE0NISmpibs7e2ho6NTlrtSahT9nB08eBAXL15EbGwsnjx5gq1bt+LChQvo0KFDWe1CmVC0b82aNYOBgQFWrVqFp0+f4vbt29i8eTNatmxZpqdkAZ6WVTlNmjRBamoq/vrrLyQmJsLOzg7+/v4wNzcH8PYBvAkJCeL4SpUqwd/fHxs3bkRoaChMTEzg5+f32TzjDlC8Z8Dbu8ryREZG4syZMzA3N8fKlStLtfayomjP/vnnH+Tk5ODPP//En3/+KU738vLCmDFjSr3+sqJo3wRBwLZt2xAfHw81NTVYWlqif//+n80z7gDl/nx+7hTtWXZ2NoKCgvDq1StoaWnBzs4OP/zww2d3hF3Rvuno6ODHH3/E+vXr8cMPP8DAwACNGzdGnz59ymoXRBIh74pAIiIiIir3eFqWiIiISIUw3BERERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIwx0RERGRCmG4IyIiIlIhDHdEn6ETJ07A19cXDx8+lDv/559//qweLlyehYaG4sSJE6W6zYCAAEyaNKlUt1mc3rx5gx07duDWrVtlXQpRiWC4IyIqx44cOVLq4a68e/PmDUJCQhjuSGUx3BFRuZOdnY2cnJxS296bN29KbVufAkEQkJWVVdZlFDtV3S+i9/G7ZYnoo2bPno1Xr15h6dKlkEgk4nRBEDBu3DhYW1vD398f8fHxGDt2LPr374+cnBwcPXoUKSkpsLOzQ//+/eHm5iaz3tjYWOzYsQM3b95Eeno6LCws0L59e5kvLL916xZmzZqFsWPH4tGjRzh79iySkpKwZMkS3L9/H6tWrcKPP/6IM2fO4NKlS8jOzoarqyv8/PxgYWEhricsLAyHDx9GZGQkUlNTYWpqCjc3N/Tp0weGhobiuB07diAkJAQ///wzdu/ejfDwcGhqamLNmjV4+PAh9u/fj/v37yMpKQnGxsZwcnJC//79xe+fBN6e9l61ahVmzJiBM2fO4OLFi8jJyUH9+vUxfPhwZGZmYv369QgLC4OWlhaaNWuGfv36QUPjf38lZ2dnY+/evTh9+jTi4+Ohq6sLT09PDBgwQKx3zJgxePHiBQDA19cXAGS+4zg9PR0hISG4cOECXr16BUNDQ/G7L3V0dMRt+fr6on379rCzs8OhQ4cQFxcHPz8/tGvXrtCfkbx1ODo6Ys+ePUhISICdnR2GDh0KJycn7N+/H6GhoUhJSUG1atXw9ddfw9LSUlw+ICAAqampGD58ODZv3oxHjx5BX18fLVu2hK+vL9TU/ncsIi0tDcHBwbh06RJSUlJQsWJFNG3aFD4+PtDU1Pzofq1btw4AEBISgpCQEAD/+57juLg47Nq1CxEREXj16hUqVKgABwcH9OvXD/b29vk+l+PGjcOTJ09w4sQJZGZmolq1ahg2bBisra1l+nP9+nXs27cPDx8+RE5ODszNzdG8eXP06NFDHPPw4UOEhIQgIiICWVlZsLGxQffu3dGkSZNCvw9EAMMd0WctNzdX7hGw979yulOnTli4cCFu3rwJd3d3cfq1a9fw/Plz+Pn5yYw/fPgwzM3NMWTIEAiCgL1792LevHmYNWsWnJ2dAQBPnz7Fjz/+CDMzMwwaNAjGxsa4fv06NmzYgNTUVPTq1UtmnVu3boWzszNGjBgBNTU1GBkZifN+//13uLu7Y/z48UhISMD27dsREBCAxYsXo0KFCgCAuLg4ODs7o1WrVtDT08OLFy9w4MABzJgxA4sXL5YJVgDwyy+/oEmTJmjbtq145O7FixewtrZGkyZNoK+vj6SkJBw5cgT+/v5YsmSJTEgEgNWrV6NBgwb49ttvERUVhW3btiEnJwfPnj1Dw4YN0aZNG9y8eRN79+6FqakpunTpIr4vCxcuxJ07d+Dt7Q1nZ2ckJCRgx44dCAgIwM8//wwtLS189913WLJkCfT09DBs2DAAEMPNmzdvEBAQgJcvX6JHjx6oXLkynjx5gh07duDx48eYPn26TFC/dOkSIiIi0LNnTxgbG8v0t7CuXr2KR48eoX///gCALVu24Oeff4aXlxeeP3+OYcOGIT09HRs3bsQvv/yChQsXytSQlJSEZcuWoXv37vD19cXVq1exa9cuvH79Wty/rKwszJo1C3FxcfD19UXlypVx584d7NmzB48ePYK/v79MTe/vl76+PqZOnYp58+ahVatWaNWqFQCI792rV6+gr6+Pfv36wdDQEGlpaTh58iSmTp2KhQsX5gtt27Ztg4uLC77++mtkZGRgy5YtWLBgAZYuXSoG0n///Rd//PEHatasiREjRsDIyAixsbF4/PixuJ7w8HDMmzcPTk5OGDFiBPT09HDu3DksW7YMWVlZaNGihcLvB32+GO6IPmPTpk0rcN67R6Lq1q0LCwsLHD58WCbchYaGwsLCAnXq1JFZNjc3Fz/++CO0tLQAAB4eHhgzZgy2b9+O6dOnAwA2btwIXV1dzJ49G3p6egAAd3d3ZGdnY8+ePejYsSP09fXFdVpYWGDixIlya61atSpGjRolvrazs8P06dMRGhqKL7/8EgBkjkIJggAXFxe4urpi9OjRuH79OurVqyezTi8vL/FoWJ5GjRqhUaNGMvtZt25djBgxAmfOnEGnTp1kxtetWxeDBg0S9+3evXs4e/YsBg0aJAY5d3d33LhxA6dPnxannT9/HtevX8ekSZPQsGFDcX2VK1eGv78/Tpw4gXbt2sHBwQFaWlrQ1dUVQ3OeQ4cOITo6GvPmzUPVqlUBAG5ubjA1NcWSJUtw/fp1mfctMzMTixcvlum5oqRSKaZNmyYeFZRIJFi0aBFu3bqFBQsWiEEuJSUFgYGBePLkiczRsNTUVEyZMkV8Lzw8PJCVlYUjR47A29sbZmZmOHnyJKKjozFhwgQ0btxY7KGOjg62bNmCsLAwmc+ovP1KSUkBAJiamubrW82aNVGzZk3xdd57PGnSJBw9ehSDBw+WGW9ra4tx48aJr9XU1LB06VI8ePAAzs7OyMzMxMaNG+Hi4oIZM2aIPXj/KPaff/4JOzs7zJgxA+rq6gCA2rVrIyUlBdu2bUPz5s1ljl4SfQjDHdFnbOzYsbCxsck3fePGjXj58qX4Wk1NDe3bt8fmzZuRkJAAMzMzxMXF4fr16xg4cKDM0RcAaNiwoRjsAIinFM+ePYvc3FxkZ2cjPDwcbdu2hba2tszRwzp16uDw4cO4f/++TPh4N+S8r1mzZjKvXVxcYG5ujlu3bonhLjk5Gdu3b8e1a9fw6tUrmaOTT58+zRfu5G0vMzNTPM354sUL5ObmivNiYmLyjff09JR5bWNjg0uXLqFu3br5poeFhYmvr1y5ggoVKsDT01OmN1WqVIGxsTFu3br10VOmV65cgb29PapUqSKzjtq1a0MikeDWrVsy/a1Vq1aRgh0AuLq6ypzuzfts5W3z/ekvXryQCXe6urr53odmzZrh2LFjuH37Npo3b47w8HBoa2vLhGwAaNGiBbZs2ZLv6LKi+5WTkyOeDo+Li5Ppnbz3+P16K1euDABISEiAs7Mz7t69i4yMDLRr1y7fn5M8cXFxiImJwcCBA8Ua8tStWxdXr17Fs2fPYGtrW+j9oM8bwx3RZ8zGxkY8qvMuPT09mXAHAK1atcKOHTtw5MgR9OvXD6GhodDS0kLLli3zLW9sbCx3WnZ2NjIzM5GZmYmcnBwcPnwYhw8flltbamqqzGsTE5MC96Og7eWtIzc3F3PmzEFiYiJ69uwJe3t7aGtrQxAETJs2Te5F9vK2t3z5coSHh6Nnz56oWrUqdHV1IZFIMH/+fLnreD9U5J36lTf93eWTk5Px+vVr9OvXT+7+vt8beZKTkxEXF4e+ffsWah3yeqgoRfYXeHuk713yTgXn1ZWWlib+19jYOF9QMjIygrq6epH3a+PGjQgNDYW3tzdq1qwJfX19SCQSrF69Wu57bGBgIHff8sbmHSWsWLFigdtMSkoCAAQFBSEoKEjumMK850R5GO6IqFD09PTg5eWFf//9F926dcOJEyfQtGlT8Zq2d+X9snp/moaGBnR0dKCurg41NTU0b94c7du3l7u9SpUqybwu6KjHh7aXd8H+kydPEB0djdGjR8tcuxQXF1fgOt+Xnp6Oq1evwsfHB927dxenS6VSMXgUFwMDAxgYGGDq1Kly5+vq6hZqHVpaWjKnq9+f/64P9be0JCcn55uW997mBUR9fX3cv38fgiDI1JycnIycnJx81z0qul+nT5+Gl5dXvmCdmpoq97P+MXn1vP+PJXljunfvXuAR6vev9SP6EIY7Iiq0jh074siRI/jll1/w+vVrmbta33XhwgUMGDBAPDWbkZGBK1euoEaNGlBTU4O2tjZcXV0RFRWFypUr57uZQVFnzpyROU139+5dvHjxQrxYPu8X/Lt3UgLA0aNHFdqOIAj51nHs2DGZ07PFwdPTE+fOnUNubi6cnJw+OPb9o37vrmP37t0wMDDIF5Q/VRkZGbh8+bLMqc4zZ85AIpGI18G5ubnh/PnzuHTpEho0aCCOO3nyJIC3p2E/Ju89lNc3iUSS7/N49epVvHr1Subu3sJycXGBnp4ejh49iqZNm8oNm9bW1rCyskJ0dHSBR2uJFMFwR0SFZm1tjdq1a+PatWuoXr06qlSpInecmpoa5syZgy5duiA3Nxd79+5FRkaGzB2wfn5+mD59OmbMmIF27drB3NwcGRkZiIuLw5UrVzBz5sxC1/Xw4UOsXr0ajRo1wsuXLxEcHAxTU1PxqKC1tTUsLCywdetWCIIAfX19XLlyReY6t4/R09NDjRo1sG/fPhgYGMDc3By3b9/G8ePHlTqi8yFNmzbFmTNnMH/+fHTq1AnVqlWDuro6Xr58iVu3bqF+/fpisLG3t8e5c+dw7tw5VKpUCVpaWrC3t0enTp1w4cIFzJw5E507d4a9vT0EQUBCQgJu3LiBrl27fjQ4ljYDAwOsXbsWCQkJsLKywrVr13Ds2DG0a9cOZmZmAIDmzZsjNDQUK1euRHx8POzt7REREYHdu3ejTp06MtfbFURXVxfm5ua4fPky3NzcoK+vL4bgunXr4uTJk7CxsUHlypURGRmJffv2ffC06ofo6Ohg0KBBWL16NX766Se0bt0aRkZGiIuLQ3R0tHgX8IgRIzB//nzMnTsXXl5eMDU1RVpaGmJiYhAVFVXgzURE8jDcEZFCGjdujGvXrhV41A4AOnToAKlUig0bNiA5ORl2dnb44YcfUL16dXGMra0tFixYgL/++gvBwcFITk5GhQoVYGVlle/u248ZNWoUTp06heXLl0MqlYrPucs7laehoYHvv/8egYGBWLt2LdTU1ODm5obp06dj9OjRhd7O+PHjsWHDBmzevBm5ublwcXHBjz/+iJ9//lmhej9GTU0NU6ZMwd9//41Tp05h9+7dUFdXR8WKFVGjRg2ZmxB8fX2RlJSEP/74AxkZGeJz7nR0dDBr1izs2bMH//zzD+Lj46GlpQUzMzO4ubnJ3A39qTA2NsawYcMQFBSEx48fQ19fHz169JC5a1lLSwszZ87Etm3bsH//fqSkpMDU1BRdu3bN9/icDxk5ciQ2b96MhQsXQiqVis+58/Pzg4aGBvbs2YPMzEw4ODjgu+++Q3BwsNL71apVK5iYmGDv3r1YvXo1gLd3o3t5eYljatWqhXnz5mHXrl3YuHEj0tLSYGBgAFtbW/GuYKLCkgjvP9CKiOgDFi9ejPv372PlypX5Tl/lPcR4wIAB6NatW4nXkvew4Pnz58u9MYTKj7yHGP/yyy9lXQpRuccjd0T0UVKpFFFRUXjw4AEuXbqEQYMGFfk6OSIiKhn825mIPioxMRE//vgjdHV10aZNG3Ts2LGsSyIiogLwtCwRERGRCuF3mRARERGpEIY7IiIiIhXCcEdERESkQhjuiIiIiFQIwx0RERGRCmG4IyIiIlIhDHdEREREKoThjoiIiEiFMNwRERERqZD/AwaJRGDez255AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_param_importances(study_knn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "52ff7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>27.200000</td>\n",
       "      <td>2.699794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>93.600000</td>\n",
       "      <td>3.405877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>3.281260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.394438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.902171</td>\n",
       "      <td>0.030941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.802830</td>\n",
       "      <td>0.080434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.813888</td>\n",
       "      <td>0.072922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.931330</td>\n",
       "      <td>0.032815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.806042</td>\n",
       "      <td>0.060864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.902494</td>\n",
       "      <td>0.030430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.870255</td>\n",
       "      <td>0.040609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.872606</td>\n",
       "      <td>0.040637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.742760</td>\n",
       "      <td>0.081662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.938330</td>\n",
       "      <td>0.022842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.872606</td>\n",
       "      <td>0.040637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP        27.200000     2.699794\n",
       "1                    TN        93.600000     3.405877\n",
       "2                    FP         6.900000     3.281260\n",
       "3                    FN         6.200000     2.394438\n",
       "4              Accuracy         0.902171     0.030941\n",
       "5             Precision         0.802830     0.080434\n",
       "6           Sensitivity         0.813888     0.072922\n",
       "7           Specificity         0.931330     0.032815\n",
       "8              F1 score         0.806042     0.060864\n",
       "9   F1 score (weighted)         0.902494     0.030430\n",
       "10     F1 score (macro)         0.870255     0.040609\n",
       "11    Balanced Accuracy         0.872606     0.040637\n",
       "12                  MCC         0.742760     0.081662\n",
       "13                  NPV         0.938330     0.022842\n",
       "14              ROC_AUC         0.872606     0.040637"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_knn_CV(study_knn.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9465254c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>51.700000</td>\n",
       "      <td>4.164666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>187.400000</td>\n",
       "      <td>4.599517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>12.900000</td>\n",
       "      <td>4.724640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.546061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.869403</td>\n",
       "      <td>0.861940</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.921642</td>\n",
       "      <td>0.929104</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.906716</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.892164</td>\n",
       "      <td>0.024717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.803155</td>\n",
       "      <td>0.058580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.637681</td>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.764032</td>\n",
       "      <td>0.064983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.915400</td>\n",
       "      <td>0.964800</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.935300</td>\n",
       "      <td>0.940600</td>\n",
       "      <td>0.945300</td>\n",
       "      <td>0.935610</td>\n",
       "      <td>0.023563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.741259</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.846715</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.796992</td>\n",
       "      <td>0.809160</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.781328</td>\n",
       "      <td>0.049873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.863811</td>\n",
       "      <td>0.864090</td>\n",
       "      <td>0.864986</td>\n",
       "      <td>0.895608</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.921830</td>\n",
       "      <td>0.928569</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>0.906476</td>\n",
       "      <td>0.901970</td>\n",
       "      <td>0.891201</td>\n",
       "      <td>0.024918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.815351</td>\n",
       "      <td>0.823556</td>\n",
       "      <td>0.819082</td>\n",
       "      <td>0.859153</td>\n",
       "      <td>0.822647</td>\n",
       "      <td>0.897042</td>\n",
       "      <td>0.904998</td>\n",
       "      <td>0.864997</td>\n",
       "      <td>0.873716</td>\n",
       "      <td>0.867980</td>\n",
       "      <td>0.854852</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.793715</td>\n",
       "      <td>0.834706</td>\n",
       "      <td>0.815920</td>\n",
       "      <td>0.837485</td>\n",
       "      <td>0.822647</td>\n",
       "      <td>0.898971</td>\n",
       "      <td>0.899118</td>\n",
       "      <td>0.863184</td>\n",
       "      <td>0.871812</td>\n",
       "      <td>0.860697</td>\n",
       "      <td>0.849825</td>\n",
       "      <td>0.035028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.640329</td>\n",
       "      <td>0.648864</td>\n",
       "      <td>0.638320</td>\n",
       "      <td>0.725894</td>\n",
       "      <td>0.645294</td>\n",
       "      <td>0.794126</td>\n",
       "      <td>0.810393</td>\n",
       "      <td>0.730037</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.736648</td>\n",
       "      <td>0.711738</td>\n",
       "      <td>0.064817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.883200</td>\n",
       "      <td>0.922300</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>0.905700</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.949700</td>\n",
       "      <td>0.945800</td>\n",
       "      <td>0.930700</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.926800</td>\n",
       "      <td>0.921660</td>\n",
       "      <td>0.020510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.793715</td>\n",
       "      <td>0.834706</td>\n",
       "      <td>0.815920</td>\n",
       "      <td>0.837485</td>\n",
       "      <td>0.822647</td>\n",
       "      <td>0.898971</td>\n",
       "      <td>0.899118</td>\n",
       "      <td>0.863184</td>\n",
       "      <td>0.871812</td>\n",
       "      <td>0.860697</td>\n",
       "      <td>0.849825</td>\n",
       "      <td>0.035028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   44.000000   53.000000   48.000000   49.000000   \n",
       "1                    TN  189.000000  178.000000  184.000000  192.000000   \n",
       "2                    FP   10.000000   22.000000   17.000000    7.000000   \n",
       "3                    FN   25.000000   15.000000   19.000000   20.000000   \n",
       "4              Accuracy    0.869403    0.861940    0.865672    0.899254   \n",
       "5             Precision    0.814815    0.706667    0.738462    0.875000   \n",
       "6           Sensitivity    0.637681    0.779412    0.716418    0.710145   \n",
       "7           Specificity    0.949700    0.890000    0.915400    0.964800   \n",
       "8              F1 score    0.715447    0.741259    0.727273    0.784000   \n",
       "9   F1 score (weighted)    0.863811    0.864090    0.864986    0.895608   \n",
       "10     F1 score (macro)    0.815351    0.823556    0.819082    0.859153   \n",
       "11    Balanced Accuracy    0.793715    0.834706    0.815920    0.837485   \n",
       "12                  MCC    0.640329    0.648864    0.638320    0.725894   \n",
       "13                  NPV    0.883200    0.922300    0.906400    0.905700   \n",
       "14              ROC_AUC    0.793715    0.834706    0.815920    0.837485   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    50.000000   58.000000   57.000000   53.000000   53.000000   52.000000   \n",
       "1   182.000000  189.000000  192.000000  188.000000  190.000000  190.000000   \n",
       "2    18.000000   11.000000    8.000000   13.000000   12.000000   11.000000   \n",
       "3    18.000000   10.000000   11.000000   14.000000   13.000000   15.000000   \n",
       "4     0.865672    0.921642    0.929104    0.899254    0.906716    0.902985   \n",
       "5     0.735294    0.840580    0.876923    0.803030    0.815385    0.825397   \n",
       "6     0.735294    0.852941    0.838235    0.791045    0.803030    0.776119   \n",
       "7     0.910000    0.945000    0.960000    0.935300    0.940600    0.945300   \n",
       "8     0.735294    0.846715    0.857143    0.796992    0.809160    0.800000   \n",
       "9     0.865672    0.921830    0.928569    0.899000    0.906476    0.901970   \n",
       "10    0.822647    0.897042    0.904998    0.864997    0.873716    0.867980   \n",
       "11    0.822647    0.898971    0.899118    0.863184    0.871812    0.860697   \n",
       "12    0.645294    0.794126    0.810393    0.730037    0.747475    0.736648   \n",
       "13    0.910000    0.949700    0.945800    0.930700    0.936000    0.926800   \n",
       "14    0.822647    0.898971    0.899118    0.863184    0.871812    0.860697   \n",
       "\n",
       "           ave       std  \n",
       "0    51.700000  4.164666  \n",
       "1   187.400000  4.599517  \n",
       "2    12.900000  4.724640  \n",
       "3    16.000000  4.546061  \n",
       "4     0.892164  0.024717  \n",
       "5     0.803155  0.058580  \n",
       "6     0.764032  0.064983  \n",
       "7     0.935610  0.023563  \n",
       "8     0.781328  0.049873  \n",
       "9     0.891201  0.024918  \n",
       "10    0.854852  0.033000  \n",
       "11    0.849825  0.035028  \n",
       "12    0.711738  0.064817  \n",
       "13    0.921660  0.020510  \n",
       "14    0.849825  0.035028  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_knn_test['ave'] = mat_met_knn_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test['std'] = mat_met_knn_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e11bef7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.903956</td>\n",
       "      <td>0.026697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.807549</td>\n",
       "      <td>0.063648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.813916</td>\n",
       "      <td>0.065636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.933958</td>\n",
       "      <td>0.027399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.808658</td>\n",
       "      <td>0.051339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.904096</td>\n",
       "      <td>0.026250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.872225</td>\n",
       "      <td>0.034520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.873934</td>\n",
       "      <td>0.035980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.746392</td>\n",
       "      <td>0.067806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.938210</td>\n",
       "      <td>0.020694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.873934</td>\n",
       "      <td>0.035980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.903956     0.026697\n",
       "1             Precision         0.807549     0.063648\n",
       "2           Sensitivity         0.813916     0.065636\n",
       "3           Specificity         0.933958     0.027399\n",
       "4              F1 score         0.808658     0.051339\n",
       "5   F1 score (weighted)         0.904096     0.026250\n",
       "6      F1 score (macro)         0.872225     0.034520\n",
       "7     Balanced Accuracy         0.873934     0.035980\n",
       "8                   MCC         0.746392     0.067806\n",
       "9                   NPV         0.938210     0.020694\n",
       "10              ROC_AUC         0.873934     0.035980"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_knn=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_knn = KNeighborsClassifier(n_neighbors = study_knn.best_params['n_neighbors'],\n",
    "                                                  weights= study_knn.best_params['weights'],\n",
    "                                                  metric= study_knn.best_params['metric'],\n",
    "                                                  leaf_size= study_knn.best_params['leaf_size'],\n",
    "                                                  n_jobs=8,\n",
    "                                                 )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_knn.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_knn = optimizedCV_knn.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_knn': y_pred_optimized_knn } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_knn)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_knn))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_knn))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_knn))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_knn, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_knn))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_knn))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_knn))\n",
    "        \n",
    "    data_knn['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_knn['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_knn['y_pred_knn' + str(i)] = data_inner['y_pred_knn']\n",
    "   # data_knn['correct' + str(i)] = correct_value\n",
    "   # data_knn['pred' + str(i)] = y_pred_optimized_knn\n",
    "\n",
    "mat_met_optimized_knn = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1fb53bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN baseline model f1_score 0.8577 with a standard deviation of 0.0362\n",
      "KNN optimized model f1_score 0.8772 with a standard deviation of 0.0383\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized KNN \n",
    "knn_baseline_CVscore = cross_val_score(knn_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_knn_opt_testSet = cross_val_score(optimized_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_knn_opt = cross_val_score(optimizedCV_knn, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"KNN baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(knn_baseline_CVscore), np.std(knn_baseline_CVscore, ddof=1)))\n",
    "#print(\"KNN optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (cv_knn_opt_testSet.mean(), cv_knn_opt_testSet.std()))\n",
    "print(\"KNN optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_knn_opt), np.std(cv_knn_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f21ca0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_knn_clf.joblib']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(knn_clf, \"OUTPUT/knn_clf.joblib\")\n",
    "#joblib.dump(optimized_knn, \"OUTPUT/optimized_knn.joblib\")\n",
    "joblib.dump(optimizedCV_knn, \"OUTPUT/optimizedCV_knn_clf.joblib\")\n",
    "#loaded_rf = joblib.load(\"OUTPUT/optimized_rf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb36c6",
   "metadata": {},
   "source": [
    "## Support Vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c4363225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Value (average)  Value (std)\n",
      "0                    TP        23.600000     3.470511\n",
      "1                    TN        96.500000     2.953341\n",
      "2                    FP         4.000000     2.828427\n",
      "3                    FN         9.800000     3.392803\n",
      "4              Accuracy         0.896964     0.039407\n",
      "5             Precision         0.857328     0.098769\n",
      "6           Sensitivity         0.706460     0.102594\n",
      "7           Specificity         0.960190     0.028229\n",
      "8              F1 score         0.772140     0.092280\n",
      "9   F1 score (weighted)         0.893163     0.041551\n",
      "10     F1 score (macro)         0.852736     0.058503\n",
      "11    Balanced Accuracy         0.833323     0.058771\n",
      "12                  MCC         0.713805     0.115274\n",
      "13                  NPV         0.908320     0.029692\n",
      "14              ROC_AUC         0.833323     0.058771\n",
      "CPU times: user 1.32 s, sys: 4 ms, total: 1.33 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "TP =np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "Accuracy = np.empty(10)\n",
    "Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores = np.empty(10)\n",
    "f1_scores_W = np.empty(10)\n",
    "f1_scores_M = np.empty(10)\n",
    "BA_scores = np.empty(10)\n",
    "MCC = np.empty(10)\n",
    "NPV = np.empty(10)\n",
    "ROC_AUC= np.empty(10)\n",
    "\n",
    "\n",
    "\n",
    "for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "\n",
    "    svm_clf = SVC()\n",
    "    \n",
    "    svm_clf.fit(X_train, y_train, )\n",
    "\n",
    "    y_pred = svm_clf.predict(X_test) \n",
    "   \n",
    "    #calculate the evaluation results\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TP[idx] = conf_matrix[1][1]\n",
    "    TN[idx] = conf_matrix[0][0]\n",
    "    FP[idx] = conf_matrix[0][1] \n",
    "    FN[idx] = conf_matrix[1][0]\n",
    "    Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "    Precision[idx] = precision_score(y_test, y_pred)\n",
    "    Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "    Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "    f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "    f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "    MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "    NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "    ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       }) \n",
    "    \n",
    "print(mat_met)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a0212847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_svm_CV(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu'])\n",
    "        \n",
    "    }\n",
    "    \n",
    "    \n",
    "    cv_scores=np.empty(10)\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "        cv_scores[idx] = f1_score(y_test, y_pred, average='macro')\n",
    "       \n",
    "        \n",
    "    return np.mean(cv_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d0a2e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective_svm_cv(trial, X, Y, Y_class):\n",
    "    param_grid = {\n",
    "        \"C\" : trial.suggest_categorical(\"C\", [np.exp2(-7), np.exp2(-6), np.exp2(-5), np.exp2(-4), np.exp2(-3), np.exp2(-2),\n",
    "                                              np.exp2(-1), np.exp2(0), np.exp2(1), np.exp2(2), np.exp2(3), np.exp2(4),\n",
    "                                             np.exp2(5), np.exp2(6), np.exp2(7)]),\n",
    "        \"gamma\" :trial.suggest_categorical(\"gamma\", [np.exp2(-15), np.exp2(-14), np.exp2(-13), np.exp2(-12), np.exp2(-11), \n",
    "                                                     np.exp2(-10),np.exp2(-9), np.exp2(-8), np.exp2(-7), np.exp2(-6), np.exp2(-5), \n",
    "                                                     np.exp2(-4),np.exp2(-3), np.exp2(-2), np.exp2(-1), np.exp2(0), np.exp2(1),\n",
    "                                                     np.exp2(2), np.exp2(3)]),\n",
    "        #\"kernel\" : trial.suggestegorical(\"kernel\", ['linear', 'rbf', 'sigmoid']),\n",
    "        #\"degree\": trial.suggest_int(\"degree\", 3, 10)\n",
    "        #\"device_type\": trial.suggestegorical(\"device_type\", ['gpu']),\n",
    "        \n",
    "    }\n",
    "    \n",
    "  \n",
    "    TP =np.empty(10)\n",
    "    TN = np.empty(10)\n",
    "    FP = np.empty(10)\n",
    "    FN = np.empty(10)\n",
    "    Accuracy = np.empty(10)\n",
    "    Precision = np.empty(10) #Also called Positive Predictive Value(PPV)\n",
    "    Sensitivity = np.empty(10) # Also called Recall or True Positive Rate (TPR)\n",
    "    Specificity = np.empty(10) #Also called selectivity or True Negative Rate  (TNR)\n",
    "    f1_scores = np.empty(10)\n",
    "    f1_scores_W = np.empty(10)\n",
    "    f1_scores_M = np.empty(10)\n",
    "    BA_scores = np.empty(10)\n",
    "    MCC = np.empty(10)\n",
    "    NPV = np.empty(10)\n",
    "    ROC_AUC= np.empty(10)\n",
    "\n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        \n",
    "        svm_model = SVC(**param_grid)\n",
    "        svm_model.fit(X_train,y_train)\n",
    "    \n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        \n",
    "        #calculate the evaluation results\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        Accuracy[idx] = accuracy_score(y_test, y_pred)\n",
    "        Precision[idx] = precision_score(y_test, y_pred)\n",
    "        Sensitivity[idx] = recall_score(y_test, y_pred)\n",
    "        Specificity[idx] = round( TN[idx] / (TN[idx]+FP[idx]),4 )\n",
    "        f1_scores[idx] = f1_score(y_test, y_pred)\n",
    "        f1_scores_W[idx] = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1_scores_M[idx] = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        BA_scores[idx] = balanced_accuracy_score(y_test, y_pred)\n",
    "        MCC[idx] = matthews_corrcoef(y_test, y_pred)\n",
    "        NPV[idx] = round( TN[idx] / (TN[idx]+FN[idx]),4 )\n",
    "        ROC_AUC[idx] = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "    mat_met = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        'Value (std)': [ np.std(TP,ddof=1),np.std(TN,ddof=1),np.std(FP,ddof=1),np.std(FN, ddof=1),\n",
    "                                        np.std(Accuracy, ddof=1),np.std(Precision, ddof=1),\n",
    "                                        np.std(Sensitivity,ddof=1),np.std(Specificity,ddof=1),np.std(f1_scores, ddof=1),\n",
    "                                        np.std(f1_scores_W, ddof=1),np.std(f1_scores_M, ddof=1), np.std(BA_scores, ddof=1), \n",
    "                                        np.std(MCC, ddof=1),np.std(NPV, ddof=1),np.std(ROC_AUC, ddof=1)]\n",
    "                       })  \n",
    "    \n",
    "    return(mat_met)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b7a25cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:06:36,698] A new study created in memory with name: SVM_classifier\n",
      "[I 2023-12-05 13:06:37,763] Trial 0 finished with value: 0.4294056694244429 and parameters: {'C': 0.03125, 'gamma': 0.03125}. Best is trial 0 with value: 0.4294056694244429.\n",
      "[I 2023-12-05 13:06:39,229] Trial 1 finished with value: 0.4294056694244429 and parameters: {'C': 0.125, 'gamma': 2.0}. Best is trial 0 with value: 0.4294056694244429.\n",
      "[I 2023-12-05 13:06:40,007] Trial 2 finished with value: 0.6800016591887666 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:40,797] Trial 3 finished with value: 0.6800016591887666 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:42,246] Trial 4 finished with value: 0.45567028715217983 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:43,681] Trial 5 finished with value: 0.45567028715217983 and parameters: {'C': 16.0, 'gamma': 0.5}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:44,471] Trial 6 finished with value: 0.4294056694244429 and parameters: {'C': 0.5, 'gamma': 0.00048828125}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:45,987] Trial 7 finished with value: 0.4294056694244429 and parameters: {'C': 0.125, 'gamma': 4.0}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:46,679] Trial 8 finished with value: 0.4294056694244429 and parameters: {'C': 0.0078125, 'gamma': 0.0009765625}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:48,153] Trial 9 finished with value: 0.49513439189811626 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:48,935] Trial 10 finished with value: 0.6800016591887666 and parameters: {'C': 32.0, 'gamma': 3.0517578125e-05}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:49,716] Trial 11 finished with value: 0.6800016591887666 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 2 with value: 0.6800016591887666.\n",
      "[I 2023-12-05 13:06:50,324] Trial 12 finished with value: 0.8636766003275742 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 12 with value: 0.8636766003275742.\n",
      "[I 2023-12-05 13:06:50,932] Trial 13 finished with value: 0.8636766003275742 and parameters: {'C': 32.0, 'gamma': 0.001953125}. Best is trial 12 with value: 0.8636766003275742.\n",
      "[I 2023-12-05 13:06:51,648] Trial 14 finished with value: 0.4294056694244429 and parameters: {'C': 0.015625, 'gamma': 0.001953125}. Best is trial 12 with value: 0.8636766003275742.\n",
      "[I 2023-12-05 13:06:52,383] Trial 15 finished with value: 0.744255909161784 and parameters: {'C': 1.0, 'gamma': 0.001953125}. Best is trial 12 with value: 0.8636766003275742.\n",
      "[I 2023-12-05 13:06:53,862] Trial 16 finished with value: 0.574343662437511 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 12 with value: 0.8636766003275742.\n",
      "[I 2023-12-05 13:06:55,282] Trial 17 finished with value: 0.4294056694244429 and parameters: {'C': 0.0625, 'gamma': 0.0625}. Best is trial 12 with value: 0.8636766003275742.\n",
      "[I 2023-12-05 13:06:56,980] Trial 18 finished with value: 0.4483738201782935 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 12 with value: 0.8636766003275742.\n",
      "[I 2023-12-05 13:06:57,645] Trial 19 finished with value: 0.8625825789920449 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 12 with value: 0.8636766003275742.\n",
      "[I 2023-12-05 13:06:58,382] Trial 20 finished with value: 0.8676600613061556 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8676600613061556.\n",
      "[I 2023-12-05 13:06:59,114] Trial 21 finished with value: 0.8676600613061556 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8676600613061556.\n",
      "[I 2023-12-05 13:06:59,850] Trial 22 finished with value: 0.8676600613061556 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8676600613061556.\n",
      "[I 2023-12-05 13:07:00,582] Trial 23 finished with value: 0.8676600613061556 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8676600613061556.\n",
      "[I 2023-12-05 13:07:01,315] Trial 24 finished with value: 0.8676600613061556 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8676600613061556.\n",
      "[I 2023-12-05 13:07:02,051] Trial 25 finished with value: 0.8676600613061556 and parameters: {'C': 2.0, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8676600613061556.\n",
      "[I 2023-12-05 13:07:02,891] Trial 26 finished with value: 0.7000796479782936 and parameters: {'C': 0.25, 'gamma': 0.0078125}. Best is trial 20 with value: 0.8676600613061556.\n",
      "[I 2023-12-05 13:07:04,077] Trial 27 finished with value: 0.8750695794772207 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:05,275] Trial 28 finished with value: 0.8750695794772207 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:06,183] Trial 29 finished with value: 0.4294056694244429 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:07,359] Trial 30 finished with value: 0.8750695794772207 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:08,531] Trial 31 finished with value: 0.8750695794772207 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:09,703] Trial 32 finished with value: 0.8750695794772207 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:10,881] Trial 33 finished with value: 0.8750695794772207 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:12,071] Trial 34 finished with value: 0.8750695794772207 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:12,685] Trial 35 finished with value: 0.8510840107061016 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:13,788] Trial 36 finished with value: 0.4294056694244429 and parameters: {'C': 0.015625, 'gamma': 0.03125}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:15,287] Trial 37 finished with value: 0.44107531270517264 and parameters: {'C': 0.5, 'gamma': 1.0}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:15,991] Trial 38 finished with value: 0.4294056694244429 and parameters: {'C': 0.25, 'gamma': 6.103515625e-05}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:16,872] Trial 39 finished with value: 0.4294056694244429 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:17,794] Trial 40 finished with value: 0.6255294179722432 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:18,992] Trial 41 finished with value: 0.8750695794772207 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:20,188] Trial 42 finished with value: 0.8750695794772207 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:21,697] Trial 43 finished with value: 0.4483738201782935 and parameters: {'C': 1.0, 'gamma': 2.0}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:22,398] Trial 44 finished with value: 0.4294056694244429 and parameters: {'C': 0.0625, 'gamma': 0.00048828125}. Best is trial 27 with value: 0.8750695794772207.\n",
      "[I 2023-12-05 13:07:23,574] Trial 45 finished with value: 0.8832928213560056 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 45 with value: 0.8832928213560056.\n",
      "[I 2023-12-05 13:07:25,015] Trial 46 finished with value: 0.4483738201782935 and parameters: {'C': 4.0, 'gamma': 4.0}. Best is trial 45 with value: 0.8832928213560056.\n",
      "[I 2023-12-05 13:07:25,674] Trial 47 finished with value: 0.8112141170053558 and parameters: {'C': 4.0, 'gamma': 0.0009765625}. Best is trial 45 with value: 0.8832928213560056.\n",
      "[I 2023-12-05 13:07:27,120] Trial 48 finished with value: 0.49513439189811626 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 45 with value: 0.8832928213560056.\n",
      "[I 2023-12-05 13:07:28,625] Trial 49 finished with value: 0.45567028715217983 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 45 with value: 0.8832928213560056.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 50.0\n",
      "\tBest value (f1_score): 0.8833\n",
      "\tBest params:\n",
      "\t\tC: 4.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "study_svm = optuna.create_study(direction='maximize', study_name=\"SVM_classifier\")\n",
    "func_svm_0 = lambda trial: objective_svm_CV(trial, X_trainSet0, Y_trainSet0, Y_trainSet0_class)\n",
    "study_svm.optimize(func_svm_0, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f310e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0\n",
      "0                    TP   46.000000\n",
      "1                    TN  194.000000\n",
      "2                    FP    5.000000\n",
      "3                    FN   23.000000\n",
      "4              Accuracy    0.895522\n",
      "5             Precision    0.901961\n",
      "6           Sensitivity    0.666667\n",
      "7           Specificity    0.974900\n",
      "8              F1 score    0.766667\n",
      "9   F1 score (weighted)    0.889947\n",
      "10     F1 score (macro)    0.849679\n",
      "11    Balanced Accuracy    0.820771\n",
      "12                  MCC    0.714596\n",
      "13                  NPV    0.894000\n",
      "14              ROC_AUC    0.820771\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_0 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_0.fit(X_trainSet0,Y_trainSet0,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_0 = optimized_svm_0.predict(X_testSet0)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet0, y_pred_svm_0)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "Precision = precision_score(Y_testSet0, y_pred_svm_0)\n",
    "Sensitivity = recall_score(Y_testSet0, y_pred_svm_0)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet0, y_pred_svm_0)      \n",
    "f1_scores_W = f1_score(Y_testSet0, y_pred_svm_0, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet0, y_pred_svm_0, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet0, y_pred_svm_0)\n",
    "MCC = matthews_corrcoef(Y_testSet0, y_pred_svm_0)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet0, y_pred_svm_0)\n",
    "    \n",
    "\n",
    "mat_met_svm_test = pd.DataFrame({'Metric':['TP','TN','FP','FN','Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Set0':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })    \n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f70c706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:07:29,652] Trial 50 finished with value: 0.4290845621395481 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 45 with value: 0.8832928213560056.\n",
      "[I 2023-12-05 13:07:30,797] Trial 51 finished with value: 0.8877302872518846 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:31,928] Trial 52 finished with value: 0.8877302872518846 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:32,663] Trial 53 finished with value: 0.8173730320997838 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:34,056] Trial 54 finished with value: 0.5757399123908628 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:34,727] Trial 55 finished with value: 0.859688014757471 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:36,115] Trial 56 finished with value: 0.831381820855281 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:37,778] Trial 57 finished with value: 0.44843869978590883 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:38,527] Trial 58 finished with value: 0.8645584827641992 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:39,683] Trial 59 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:40,835] Trial 60 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:41,990] Trial 61 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:43,144] Trial 62 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:44,298] Trial 63 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:44,968] Trial 64 finished with value: 0.8606729362968766 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:46,387] Trial 65 finished with value: 0.45578276481498337 and parameters: {'C': 64.0, 'gamma': 1.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:47,132] Trial 66 finished with value: 0.8173730320997838 and parameters: {'C': 64.0, 'gamma': 6.103515625e-05}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:48,277] Trial 67 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:49,607] Trial 68 finished with value: 0.8859407519703011 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:50,741] Trial 69 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:52,140] Trial 70 finished with value: 0.44843869978590883 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:53,276] Trial 71 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:54,411] Trial 72 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:55,560] Trial 73 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:56,228] Trial 74 finished with value: 0.8672919893754749 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:57,671] Trial 75 finished with value: 0.44843869978590883 and parameters: {'C': 64.0, 'gamma': 4.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:58,547] Trial 76 finished with value: 0.8167348276227535 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:07:59,919] Trial 77 finished with value: 0.4290845621395481 and parameters: {'C': 0.0078125, 'gamma': 0.25}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:00,671] Trial 78 finished with value: 0.4290845621395481 and parameters: {'C': 0.125, 'gamma': 0.0009765625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:01,540] Trial 79 finished with value: 0.4290845621395481 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:02,196] Trial 80 finished with value: 0.8593783785279312 and parameters: {'C': 64.0, 'gamma': 0.001953125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:03,353] Trial 81 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:04,508] Trial 82 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:05,639] Trial 83 finished with value: 0.8877302872518846 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:06,777] Trial 84 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:08,200] Trial 85 finished with value: 0.45578276481498337 and parameters: {'C': 1.0, 'gamma': 0.5}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:08,942] Trial 86 finished with value: 0.4290845621395481 and parameters: {'C': 0.0625, 'gamma': 3.0517578125e-05}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:09,700] Trial 87 finished with value: 0.6582884519903722 and parameters: {'C': 8.0, 'gamma': 0.0001220703125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:10,844] Trial 88 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:12,239] Trial 89 finished with value: 0.831381820855281 and parameters: {'C': 16.0, 'gamma': 0.0625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:13,677] Trial 90 finished with value: 0.5757399123908628 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:14,848] Trial 91 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:15,738] Trial 92 finished with value: 0.7141077472104864 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:16,903] Trial 93 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:18,578] Trial 94 finished with value: 0.44843869978590883 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:19,809] Trial 95 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:20,575] Trial 96 finished with value: 0.8566223408183443 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:21,818] Trial 97 finished with value: 0.8877302872518846 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:22,749] Trial 98 finished with value: 0.4290845621395481 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:23,462] Trial 99 finished with value: 0.8606729362968766 and parameters: {'C': 64.0, 'gamma': 0.000244140625}. Best is trial 51 with value: 0.8877302872518846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 100.0\n",
      "\tBest value (f1_score): 0.8877\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_1 = lambda trial: objective_svm_CV(trial, X_trainSet1, Y_trainSet1, Y_trainSet1_class)\n",
    "study_svm.optimize(func_svm_1, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dbfdb414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1\n",
      "0                    TP   46.000000   50.000000\n",
      "1                    TN  194.000000  185.000000\n",
      "2                    FP    5.000000   15.000000\n",
      "3                    FN   23.000000   18.000000\n",
      "4              Accuracy    0.895522    0.876866\n",
      "5             Precision    0.901961    0.769231\n",
      "6           Sensitivity    0.666667    0.735294\n",
      "7           Specificity    0.974900    0.925000\n",
      "8              F1 score    0.766667    0.751880\n",
      "9   F1 score (weighted)    0.889947    0.875935\n",
      "10     F1 score (macro)    0.849679    0.834997\n",
      "11    Balanced Accuracy    0.820771    0.830147\n",
      "12                  MCC    0.714596    0.670351\n",
      "13                  NPV    0.894000    0.911300\n",
      "14              ROC_AUC    0.820771    0.830147\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_1 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_1.fit(X_trainSet1,Y_trainSet1,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_1 = optimized_svm_1.predict(X_testSet1)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet1, y_pred_svm_1)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "Precision = precision_score(Y_testSet1, y_pred_svm_1)\n",
    "Sensitivity = recall_score(Y_testSet1, y_pred_svm_1)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet1, y_pred_svm_1)      \n",
    "f1_scores_W = f1_score(Y_testSet1, y_pred_svm_1, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet1, y_pred_svm_1, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet1, y_pred_svm_1)\n",
    "MCC = matthews_corrcoef(Y_testSet1, y_pred_svm_1)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet1, y_pred_svm_1)\n",
    "    \n",
    "\n",
    "set1 = pd.DataFrame({'Set1':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set1'] = set1\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3c802470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:08:24,957] Trial 100 finished with value: 0.42879040865737944 and parameters: {'C': 0.5, 'gamma': 1.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:26,040] Trial 101 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:27,144] Trial 102 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:28,238] Trial 103 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:29,519] Trial 104 finished with value: 0.8782606522408418 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:30,206] Trial 105 finished with value: 0.42879040865737944 and parameters: {'C': 0.0078125, 'gamma': 6.103515625e-05}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:31,292] Trial 106 finished with value: 0.8690653261072558 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:32,571] Trial 107 finished with value: 0.42879040865737944 and parameters: {'C': 0.015625, 'gamma': 2.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:33,655] Trial 108 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:34,385] Trial 109 finished with value: 0.42879040865737944 and parameters: {'C': 0.125, 'gamma': 0.00048828125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:35,482] Trial 110 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:36,574] Trial 111 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:37,674] Trial 112 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:38,572] Trial 113 finished with value: 0.8561518437099809 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:39,875] Trial 114 finished with value: 0.47951248960851656 and parameters: {'C': 32.0, 'gamma': 0.25}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:40,975] Trial 115 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:42,284] Trial 116 finished with value: 0.42879040865737944 and parameters: {'C': 0.0625, 'gamma': 4.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:42,918] Trial 117 finished with value: 0.8591129910761884 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:43,652] Trial 118 finished with value: 0.44020380688119565 and parameters: {'C': 0.25, 'gamma': 0.001953125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:44,752] Trial 119 finished with value: 0.8701329296594779 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:46,077] Trial 120 finished with value: 0.43605031025395197 and parameters: {'C': 8.0, 'gamma': 0.5}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:47,168] Trial 121 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:48,278] Trial 122 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:49,371] Trial 123 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:50,043] Trial 124 finished with value: 0.8017770193757248 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:51,172] Trial 125 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:51,837] Trial 126 finished with value: 0.837495211339931 and parameters: {'C': 64.0, 'gamma': 0.0001220703125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:52,674] Trial 127 finished with value: 0.42879040865737944 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:54,000] Trial 128 finished with value: 0.5574469402719124 and parameters: {'C': 64.0, 'gamma': 0.125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:55,538] Trial 129 finished with value: 0.4284860509375023 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:56,861] Trial 130 finished with value: 0.8329257566364285 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:57,985] Trial 131 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:08:59,114] Trial 132 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:00,220] Trial 133 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:00,881] Trial 134 finished with value: 0.8542365354139309 and parameters: {'C': 64.0, 'gamma': 0.00390625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:01,677] Trial 135 finished with value: 0.7997770323385982 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:02,651] Trial 136 finished with value: 0.8636531344718934 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:03,748] Trial 137 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:04,584] Trial 138 finished with value: 0.42879040865737944 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:05,313] Trial 139 finished with value: 0.42879040865737944 and parameters: {'C': 0.125, 'gamma': 0.000244140625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:06,124] Trial 140 finished with value: 0.42879040865737944 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:07,239] Trial 141 finished with value: 0.8690653261072558 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:08,353] Trial 142 finished with value: 0.8690653261072558 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:09,466] Trial 143 finished with value: 0.8690653261072558 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:10,768] Trial 144 finished with value: 0.8782606522408418 and parameters: {'C': 32.0, 'gamma': 0.03125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:12,107] Trial 145 finished with value: 0.43635426162173313 and parameters: {'C': 32.0, 'gamma': 1.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:12,845] Trial 146 finished with value: 0.42879040865737944 and parameters: {'C': 4.0, 'gamma': 6.103515625e-05}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:13,966] Trial 147 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:15,081] Trial 148 finished with value: 0.8690653261072558 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:16,435] Trial 149 finished with value: 0.4284860509375023 and parameters: {'C': 1.0, 'gamma': 2.0}. Best is trial 51 with value: 0.8877302872518846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 150.0\n",
      "\tBest value (f1_score): 0.8877\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_2 = lambda trial: objective_svm_CV(trial, X_trainSet2, Y_trainSet2, Y_trainSet2_class)\n",
    "study_svm.optimize(func_svm_2, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b15b0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2\n",
      "0                    TP   46.000000   50.000000   51.000000\n",
      "1                    TN  194.000000  185.000000  190.000000\n",
      "2                    FP    5.000000   15.000000   11.000000\n",
      "3                    FN   23.000000   18.000000   16.000000\n",
      "4              Accuracy    0.895522    0.876866    0.899254\n",
      "5             Precision    0.901961    0.769231    0.822581\n",
      "6           Sensitivity    0.666667    0.735294    0.761194\n",
      "7           Specificity    0.974900    0.925000    0.945300\n",
      "8              F1 score    0.766667    0.751880    0.790698\n",
      "9   F1 score (weighted)    0.889947    0.875935    0.897920\n",
      "10     F1 score (macro)    0.849679    0.834997    0.862179\n",
      "11    Balanced Accuracy    0.820771    0.830147    0.853234\n",
      "12                  MCC    0.714596    0.670351    0.725435\n",
      "13                  NPV    0.894000    0.911300    0.922300\n",
      "14              ROC_AUC    0.820771    0.830147    0.853234\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_2 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_2.fit(X_trainSet2,Y_trainSet2,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_2 = optimized_svm_2.predict(X_testSet2)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet2, y_pred_svm_2)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "Precision = precision_score(Y_testSet2, y_pred_svm_2)\n",
    "Sensitivity = recall_score(Y_testSet2, y_pred_svm_2)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet2, y_pred_svm_2)      \n",
    "f1_scores_W = f1_score(Y_testSet2, y_pred_svm_2, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet2, y_pred_svm_2, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet2, y_pred_svm_2)\n",
    "MCC = matthews_corrcoef(Y_testSet2, y_pred_svm_2)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet2, y_pred_svm_2)\n",
    "    \n",
    "\n",
    "Set2 = pd.DataFrame({'Set2':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set2'] = Set2\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5f35dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:09:17,454] Trial 150 finished with value: 0.4294023967607883 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:18,584] Trial 151 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:19,720] Trial 152 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:20,842] Trial 153 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:21,485] Trial 154 finished with value: 0.8490852584317166 and parameters: {'C': 64.0, 'gamma': 0.00048828125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:22,632] Trial 155 finished with value: 0.8684366506677392 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:23,999] Trial 156 finished with value: 0.4485011797610207 and parameters: {'C': 8.0, 'gamma': 4.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:25,135] Trial 157 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:26,486] Trial 158 finished with value: 0.4294023967607883 and parameters: {'C': 0.25, 'gamma': 0.25}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:27,110] Trial 159 finished with value: 0.8399546547060804 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:27,734] Trial 160 finished with value: 0.8435358154001638 and parameters: {'C': 64.0, 'gamma': 0.0009765625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:28,859] Trial 161 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:30,014] Trial 162 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:31,173] Trial 163 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:32,341] Trial 164 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:33,497] Trial 165 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:34,877] Trial 166 finished with value: 0.4481968220411436 and parameters: {'C': 64.0, 'gamma': 0.5}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:35,712] Trial 167 finished with value: 0.4294023967607883 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:36,455] Trial 168 finished with value: 0.7139640429675516 and parameters: {'C': 64.0, 'gamma': 3.0517578125e-05}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:37,094] Trial 169 finished with value: 0.8564030687097354 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:38,221] Trial 170 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:39,349] Trial 171 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:40,480] Trial 172 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:41,612] Trial 173 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:42,928] Trial 174 finished with value: 0.5569340462212715 and parameters: {'C': 32.0, 'gamma': 0.125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:44,057] Trial 175 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:45,381] Trial 176 finished with value: 0.6569985989921933 and parameters: {'C': 0.5, 'gamma': 0.0625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:46,509] Trial 177 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:48,076] Trial 178 finished with value: 0.4485011797610207 and parameters: {'C': 64.0, 'gamma': 8.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:48,885] Trial 179 finished with value: 0.4294023967607883 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:49,663] Trial 180 finished with value: 0.4294023967607883 and parameters: {'C': 0.125, 'gamma': 0.00390625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:50,788] Trial 181 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:51,916] Trial 182 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:53,050] Trial 183 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:53,846] Trial 184 finished with value: 0.4294023967607883 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:54,973] Trial 185 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:55,732] Trial 186 finished with value: 0.8553568323466347 and parameters: {'C': 64.0, 'gamma': 0.0078125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:56,358] Trial 187 finished with value: 0.8490852584317166 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:57,461] Trial 188 finished with value: 0.8690659532606684 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:58,806] Trial 189 finished with value: 0.4485011797610207 and parameters: {'C': 64.0, 'gamma': 1.0}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:09:59,931] Trial 190 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:10:01,065] Trial 191 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:10:02,208] Trial 192 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:10:03,364] Trial 193 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:10:04,500] Trial 194 finished with value: 0.8647640451905694 and parameters: {'C': 64.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:10:05,353] Trial 195 finished with value: 0.8455495716662286 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:10:06,170] Trial 196 finished with value: 0.4294023967607883 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:10:06,884] Trial 197 finished with value: 0.7139640429675516 and parameters: {'C': 32.0, 'gamma': 6.103515625e-05}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:10:08,110] Trial 198 finished with value: 0.8624419228641445 and parameters: {'C': 64.0, 'gamma': 0.03125}. Best is trial 51 with value: 0.8877302872518846.\n",
      "[I 2023-12-05 13:10:09,432] Trial 199 finished with value: 0.4485011797610207 and parameters: {'C': 64.0, 'gamma': 2.0}. Best is trial 51 with value: 0.8877302872518846.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 200.0\n",
      "\tBest value (f1_score): 0.8877\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_3 = lambda trial: objective_svm_CV(trial, X_trainSet3, Y_trainSet3, Y_trainSet3_class)\n",
    "study_svm.optimize(func_svm_3, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7fb9781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3\n",
      "0                    TP   46.000000   50.000000   51.000000   50.000000\n",
      "1                    TN  194.000000  185.000000  190.000000  190.000000\n",
      "2                    FP    5.000000   15.000000   11.000000    9.000000\n",
      "3                    FN   23.000000   18.000000   16.000000   19.000000\n",
      "4              Accuracy    0.895522    0.876866    0.899254    0.895522\n",
      "5             Precision    0.901961    0.769231    0.822581    0.847458\n",
      "6           Sensitivity    0.666667    0.735294    0.761194    0.724638\n",
      "7           Specificity    0.974900    0.925000    0.945300    0.954800\n",
      "8              F1 score    0.766667    0.751880    0.790698    0.781250\n",
      "9   F1 score (weighted)    0.889947    0.875935    0.897920    0.892722\n",
      "10     F1 score (macro)    0.849679    0.834997    0.862179    0.856311\n",
      "11    Balanced Accuracy    0.820771    0.830147    0.853234    0.839706\n",
      "12                  MCC    0.714596    0.670351    0.725435    0.716943\n",
      "13                  NPV    0.894000    0.911300    0.922300    0.909100\n",
      "14              ROC_AUC    0.820771    0.830147    0.853234    0.839706\n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_3 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_3.fit(X_trainSet3,Y_trainSet3,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_3 = optimized_svm_3.predict(X_testSet3)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet3, y_pred_svm_3)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "Precision = precision_score(Y_testSet3, y_pred_svm_3)\n",
    "Sensitivity = recall_score(Y_testSet3, y_pred_svm_3)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet3, y_pred_svm_3)      \n",
    "f1_scores_W = f1_score(Y_testSet3, y_pred_svm_3, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet3, y_pred_svm_3, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet3, y_pred_svm_3)\n",
    "MCC = matthews_corrcoef(Y_testSet3, y_pred_svm_3)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet3, y_pred_svm_3)\n",
    "    \n",
    "\n",
    "Set3 = pd.DataFrame({'Set3':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set3'] = Set3\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4b2acbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:10:10,840] Trial 200 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:12,031] Trial 201 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:13,231] Trial 202 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:14,465] Trial 203 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:15,698] Trial 204 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:16,905] Trial 205 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:18,114] Trial 206 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:19,336] Trial 207 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:20,530] Trial 208 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:21,729] Trial 209 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:22,920] Trial 210 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:24,106] Trial 211 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:25,295] Trial 212 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:26,513] Trial 213 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:27,121] Trial 214 finished with value: 0.8732516772995595 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:28,353] Trial 215 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:29,565] Trial 216 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:30,792] Trial 217 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:32,031] Trial 218 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:33,242] Trial 219 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:34,456] Trial 220 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:35,672] Trial 221 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:36,901] Trial 222 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:38,161] Trial 223 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:39,386] Trial 224 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:40,608] Trial 225 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:41,840] Trial 226 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:43,052] Trial 227 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:44,539] Trial 228 finished with value: 0.4762462209784234 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:46,024] Trial 229 finished with value: 0.4479422843069413 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:46,651] Trial 230 finished with value: 0.8710749837531424 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:47,842] Trial 231 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:49,038] Trial 232 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:50,246] Trial 233 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:50,854] Trial 234 finished with value: 0.8635692391648153 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:52,068] Trial 235 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:53,290] Trial 236 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:54,506] Trial 237 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:55,967] Trial 238 finished with value: 0.4549845467231486 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:57,184] Trial 239 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:57,856] Trial 240 finished with value: 0.8097371328923204 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:10:59,054] Trial 241 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:00,267] Trial 242 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:01,476] Trial 243 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:02,687] Trial 244 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:03,898] Trial 245 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:05,110] Trial 246 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:06,318] Trial 247 finished with value: 0.8926701478362279 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:07,770] Trial 248 finished with value: 0.568496535288285 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:08,384] Trial 249 finished with value: 0.8603844230709997 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.8926701478362279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 250.0\n",
      "\tBest value (f1_score): 0.8927\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_4 = lambda trial: objective_svm_CV(trial, X_trainSet4, Y_trainSet4, Y_trainSet4_class)\n",
    "study_svm.optimize(func_svm_4, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c80f9415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   46.000000   50.000000   51.000000   50.000000   \n",
      "1                    TN  194.000000  185.000000  190.000000  190.000000   \n",
      "2                    FP    5.000000   15.000000   11.000000    9.000000   \n",
      "3                    FN   23.000000   18.000000   16.000000   19.000000   \n",
      "4              Accuracy    0.895522    0.876866    0.899254    0.895522   \n",
      "5             Precision    0.901961    0.769231    0.822581    0.847458   \n",
      "6           Sensitivity    0.666667    0.735294    0.761194    0.724638   \n",
      "7           Specificity    0.974900    0.925000    0.945300    0.954800   \n",
      "8              F1 score    0.766667    0.751880    0.790698    0.781250   \n",
      "9   F1 score (weighted)    0.889947    0.875935    0.897920    0.892722   \n",
      "10     F1 score (macro)    0.849679    0.834997    0.862179    0.856311   \n",
      "11    Balanced Accuracy    0.820771    0.830147    0.853234    0.839706   \n",
      "12                  MCC    0.714596    0.670351    0.725435    0.716943   \n",
      "13                  NPV    0.894000    0.911300    0.922300    0.909100   \n",
      "14              ROC_AUC    0.820771    0.830147    0.853234    0.839706   \n",
      "\n",
      "          Set4  \n",
      "0    44.000000  \n",
      "1   188.000000  \n",
      "2    12.000000  \n",
      "3    24.000000  \n",
      "4     0.865672  \n",
      "5     0.785714  \n",
      "6     0.647059  \n",
      "7     0.940000  \n",
      "8     0.709677  \n",
      "9     0.861128  \n",
      "10    0.811149  \n",
      "11    0.793529  \n",
      "12    0.628332  \n",
      "13    0.886800  \n",
      "14    0.793529  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_4 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_4.fit(X_trainSet4,Y_trainSet4,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_4 = optimized_svm_4.predict(X_testSet4)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet4, y_pred_svm_4)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "Precision = precision_score(Y_testSet4, y_pred_svm_4)\n",
    "Sensitivity = recall_score(Y_testSet4, y_pred_svm_4)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet4, y_pred_svm_4)      \n",
    "f1_scores_W = f1_score(Y_testSet4, y_pred_svm_4, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet4, y_pred_svm_4, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet4, y_pred_svm_4)\n",
    "MCC = matthews_corrcoef(Y_testSet4, y_pred_svm_4)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet4, y_pred_svm_4)\n",
    "    \n",
    "\n",
    "Set4 = pd.DataFrame({'Set4':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set4'] = Set4\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "92e04028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:11:09,903] Trial 250 finished with value: 0.8274823042454033 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:11,015] Trial 251 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:12,569] Trial 252 finished with value: 0.44388121581472495 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:13,691] Trial 253 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:14,747] Trial 254 finished with value: 0.8696713727083184 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:15,862] Trial 255 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:16,991] Trial 256 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:17,693] Trial 257 finished with value: 0.8618085006246753 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:18,808] Trial 258 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:19,929] Trial 259 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:21,047] Trial 260 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:22,176] Trial 261 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:23,525] Trial 262 finished with value: 0.44388121581472495 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:24,648] Trial 263 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:25,907] Trial 264 finished with value: 0.8715350940172548 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:26,568] Trial 265 finished with value: 0.8634256893113559 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:27,706] Trial 266 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:28,388] Trial 267 finished with value: 0.8276110141277856 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:29,539] Trial 268 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:30,675] Trial 269 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:32,015] Trial 270 finished with value: 0.44388121581472495 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:33,136] Trial 271 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:34,261] Trial 272 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:34,924] Trial 273 finished with value: 0.8568063558641084 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:36,046] Trial 274 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:37,171] Trial 275 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:38,516] Trial 276 finished with value: 0.47775270099636413 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:39,867] Trial 277 finished with value: 0.44388121581472495 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:40,992] Trial 278 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:41,663] Trial 279 finished with value: 0.8440956738141392 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:42,784] Trial 280 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:43,915] Trial 281 finished with value: 0.8763811157900239 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:45,039] Trial 282 finished with value: 0.8752432814988795 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:46,372] Trial 283 finished with value: 0.42909144064338056 and parameters: {'C': 0.25, 'gamma': 0.5}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:47,047] Trial 284 finished with value: 0.8438082151293559 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:48,134] Trial 285 finished with value: 0.8692786229419314 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:49,257] Trial 286 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:50,412] Trial 287 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:51,143] Trial 288 finished with value: 0.7990288436292096 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:51,843] Trial 289 finished with value: 0.42909144064338056 and parameters: {'C': 0.03125, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:52,995] Trial 290 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:54,146] Trial 291 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:55,526] Trial 292 finished with value: 0.545979405328805 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:56,886] Trial 293 finished with value: 0.8274823042454033 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:58,031] Trial 294 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:59,176] Trial 295 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:11:59,960] Trial 296 finished with value: 0.7621482027036157 and parameters: {'C': 0.5, 'gamma': 0.0078125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:01,108] Trial 297 finished with value: 0.8752432814988795 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:02,708] Trial 298 finished with value: 0.44388121581472495 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:03,559] Trial 299 finished with value: 0.42909144064338056 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 300.0\n",
      "\tBest value (f1_score): 0.8927\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_5 = lambda trial: objective_svm_CV(trial, X_trainSet5, Y_trainSet5, Y_trainSet5_class)\n",
    "study_svm.optimize(func_svm_5, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dae92b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   46.000000   50.000000   51.000000   50.000000   \n",
      "1                    TN  194.000000  185.000000  190.000000  190.000000   \n",
      "2                    FP    5.000000   15.000000   11.000000    9.000000   \n",
      "3                    FN   23.000000   18.000000   16.000000   19.000000   \n",
      "4              Accuracy    0.895522    0.876866    0.899254    0.895522   \n",
      "5             Precision    0.901961    0.769231    0.822581    0.847458   \n",
      "6           Sensitivity    0.666667    0.735294    0.761194    0.724638   \n",
      "7           Specificity    0.974900    0.925000    0.945300    0.954800   \n",
      "8              F1 score    0.766667    0.751880    0.790698    0.781250   \n",
      "9   F1 score (weighted)    0.889947    0.875935    0.897920    0.892722   \n",
      "10     F1 score (macro)    0.849679    0.834997    0.862179    0.856311   \n",
      "11    Balanced Accuracy    0.820771    0.830147    0.853234    0.839706   \n",
      "12                  MCC    0.714596    0.670351    0.725435    0.716943   \n",
      "13                  NPV    0.894000    0.911300    0.922300    0.909100   \n",
      "14              ROC_AUC    0.820771    0.830147    0.853234    0.839706   \n",
      "\n",
      "          Set4        Set5  \n",
      "0    44.000000   56.000000  \n",
      "1   188.000000  192.000000  \n",
      "2    12.000000    8.000000  \n",
      "3    24.000000   12.000000  \n",
      "4     0.865672    0.925373  \n",
      "5     0.785714    0.875000  \n",
      "6     0.647059    0.823529  \n",
      "7     0.940000    0.960000  \n",
      "8     0.709677    0.848485  \n",
      "9     0.861128    0.924612  \n",
      "10    0.811149    0.899490  \n",
      "11    0.793529    0.891765  \n",
      "12    0.628332    0.799686  \n",
      "13    0.886800    0.941200  \n",
      "14    0.793529    0.891765  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_5 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_5.fit(X_trainSet5,Y_trainSet5,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_5 = optimized_svm_5.predict(X_testSet5)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet5, y_pred_svm_5)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "Precision = precision_score(Y_testSet5, y_pred_svm_5)\n",
    "Sensitivity = recall_score(Y_testSet5, y_pred_svm_5)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet5, y_pred_svm_5)      \n",
    "f1_scores_W = f1_score(Y_testSet5, y_pred_svm_5, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet5, y_pred_svm_5, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet5, y_pred_svm_5)\n",
    "MCC = matthews_corrcoef(Y_testSet5, y_pred_svm_5)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet5, y_pred_svm_5)\n",
    "    \n",
    "\n",
    "Set5 = pd.DataFrame({'Set5':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set5'] = Set5\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b346e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:12:04,444] Trial 300 finished with value: 0.8455859513803228 and parameters: {'C': 4.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:05,657] Trial 301 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:06,879] Trial 302 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:07,742] Trial 303 finished with value: 0.4290977058643879 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:08,626] Trial 304 finished with value: 0.5856269574485162 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:09,243] Trial 305 finished with value: 0.8517507221649627 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:10,455] Trial 306 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:11,781] Trial 307 finished with value: 0.8627733093459664 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:12,680] Trial 308 finished with value: 0.8433684169425412 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:14,089] Trial 309 finished with value: 0.4290977058643879 and parameters: {'C': 0.0625, 'gamma': 1.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:15,312] Trial 310 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:16,528] Trial 311 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:17,953] Trial 312 finished with value: 0.436269368362557 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:19,172] Trial 313 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:19,817] Trial 314 finished with value: 0.8268736682867287 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:21,039] Trial 315 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:22,271] Trial 316 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:23,478] Trial 317 finished with value: 0.8660266364830331 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:24,105] Trial 318 finished with value: 0.8530056651235952 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:24,987] Trial 319 finished with value: 0.7328986108156349 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:26,411] Trial 320 finished with value: 0.436269368362557 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:27,042] Trial 321 finished with value: 0.850488979659079 and parameters: {'C': 16.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:27,700] Trial 322 finished with value: 0.8343792122482185 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:28,909] Trial 323 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:30,320] Trial 324 finished with value: 0.4589078443082335 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:31,455] Trial 325 finished with value: 0.8587126506258752 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:32,328] Trial 326 finished with value: 0.4290977058643879 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:33,750] Trial 327 finished with value: 0.4359686164828577 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:34,973] Trial 328 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:35,640] Trial 329 finished with value: 0.8047876432625184 and parameters: {'C': 128.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:36,859] Trial 330 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:38,078] Trial 331 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:39,294] Trial 332 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:40,661] Trial 333 finished with value: 0.5558541016409827 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:41,544] Trial 334 finished with value: 0.8037828289881375 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:42,172] Trial 335 finished with value: 0.8536302859995472 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:42,983] Trial 336 finished with value: 0.4290977058643879 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:44,351] Trial 337 finished with value: 0.8148663281260733 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:45,526] Trial 338 finished with value: 0.8664573873586475 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:46,748] Trial 339 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:48,411] Trial 340 finished with value: 0.4290977058643879 and parameters: {'C': 0.125, 'gamma': 8.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:49,206] Trial 341 finished with value: 0.4290977058643879 and parameters: {'C': 0.015625, 'gamma': 0.0078125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:49,888] Trial 342 finished with value: 0.8467266297556622 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:51,164] Trial 343 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:52,382] Trial 344 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:53,292] Trial 345 finished with value: 0.8433684169425412 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:54,157] Trial 346 finished with value: 0.4290977058643879 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:54,773] Trial 347 finished with value: 0.8517507221649627 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:55,983] Trial 348 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:57,198] Trial 349 finished with value: 0.8641574358252144 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 350.0\n",
      "\tBest value (f1_score): 0.8927\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_6 = lambda trial: objective_svm_CV(trial, X_trainSet6, Y_trainSet6, Y_trainSet6_class)\n",
    "study_svm.optimize(func_svm_6, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ed5a900c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   46.000000   50.000000   51.000000   50.000000   \n",
      "1                    TN  194.000000  185.000000  190.000000  190.000000   \n",
      "2                    FP    5.000000   15.000000   11.000000    9.000000   \n",
      "3                    FN   23.000000   18.000000   16.000000   19.000000   \n",
      "4              Accuracy    0.895522    0.876866    0.899254    0.895522   \n",
      "5             Precision    0.901961    0.769231    0.822581    0.847458   \n",
      "6           Sensitivity    0.666667    0.735294    0.761194    0.724638   \n",
      "7           Specificity    0.974900    0.925000    0.945300    0.954800   \n",
      "8              F1 score    0.766667    0.751880    0.790698    0.781250   \n",
      "9   F1 score (weighted)    0.889947    0.875935    0.897920    0.892722   \n",
      "10     F1 score (macro)    0.849679    0.834997    0.862179    0.856311   \n",
      "11    Balanced Accuracy    0.820771    0.830147    0.853234    0.839706   \n",
      "12                  MCC    0.714596    0.670351    0.725435    0.716943   \n",
      "13                  NPV    0.894000    0.911300    0.922300    0.909100   \n",
      "14              ROC_AUC    0.820771    0.830147    0.853234    0.839706   \n",
      "\n",
      "          Set4        Set5        Set6  \n",
      "0    44.000000   56.000000   57.000000  \n",
      "1   188.000000  192.000000  195.000000  \n",
      "2    12.000000    8.000000    5.000000  \n",
      "3    24.000000   12.000000   11.000000  \n",
      "4     0.865672    0.925373    0.940299  \n",
      "5     0.785714    0.875000    0.919355  \n",
      "6     0.647059    0.823529    0.838235  \n",
      "7     0.940000    0.960000    0.975000  \n",
      "8     0.709677    0.848485    0.876923  \n",
      "9     0.861128    0.924612    0.939362  \n",
      "10    0.811149    0.899490    0.918757  \n",
      "11    0.793529    0.891765    0.906618  \n",
      "12    0.628332    0.799686    0.839182  \n",
      "13    0.886800    0.941200    0.946600  \n",
      "14    0.793529    0.891765    0.906618  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_6 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_6.fit(X_trainSet6,Y_trainSet6,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_6 = optimized_svm_6.predict(X_testSet6)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet6, y_pred_svm_6)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "Precision = precision_score(Y_testSet6, y_pred_svm_6)\n",
    "Sensitivity = recall_score(Y_testSet6, y_pred_svm_6)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet6, y_pred_svm_6)      \n",
    "f1_scores_W = f1_score(Y_testSet6, y_pred_svm_6, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet6, y_pred_svm_6, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet6, y_pred_svm_6)\n",
    "MCC = matthews_corrcoef(Y_testSet6, y_pred_svm_6)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet6, y_pred_svm_6)\n",
    "    \n",
    "\n",
    "Set6 = pd.DataFrame({'Set6':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set6'] = Set6\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "165e2c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:12:58,767] Trial 350 finished with value: 0.4545290262106628 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:12:59,897] Trial 351 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:01,042] Trial 352 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:02,319] Trial 353 finished with value: 0.8711472841565027 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:03,452] Trial 354 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:04,229] Trial 355 finished with value: 0.4478203304068088 and parameters: {'C': 8.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:05,054] Trial 356 finished with value: 0.7250039336905932 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:06,145] Trial 357 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:06,913] Trial 358 finished with value: 0.676765665156902 and parameters: {'C': 2.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:08,230] Trial 359 finished with value: 0.44725645496636346 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:09,329] Trial 360 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:10,432] Trial 361 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:11,756] Trial 362 finished with value: 0.47193916083857834 and parameters: {'C': 16.0, 'gamma': 0.25}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:13,111] Trial 363 finished with value: 0.428793681321034 and parameters: {'C': 0.03125, 'gamma': 4.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:14,207] Trial 364 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:14,859] Trial 365 finished with value: 0.8330112618434228 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:15,964] Trial 366 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:17,071] Trial 367 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:17,733] Trial 368 finished with value: 0.8380934452830875 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:18,838] Trial 369 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:19,709] Trial 370 finished with value: 0.7929274705695046 and parameters: {'C': 0.5, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:20,822] Trial 371 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:22,153] Trial 372 finished with value: 0.45422139582713117 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:22,958] Trial 373 finished with value: 0.428793681321034 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:23,712] Trial 374 finished with value: 0.428793681321034 and parameters: {'C': 4.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:24,393] Trial 375 finished with value: 0.8429898353448912 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:25,282] Trial 376 finished with value: 0.6093309628731738 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:26,648] Trial 377 finished with value: 0.428793681321034 and parameters: {'C': 0.015625, 'gamma': 0.125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:27,776] Trial 378 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:29,148] Trial 379 finished with value: 0.8202110654407999 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:30,282] Trial 380 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:31,418] Trial 381 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:32,996] Trial 382 finished with value: 0.44725645496636346 and parameters: {'C': 32.0, 'gamma': 8.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:33,921] Trial 383 finished with value: 0.8428679998985521 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:34,974] Trial 384 finished with value: 0.8610659634904441 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:35,672] Trial 385 finished with value: 0.855905856106894 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:36,527] Trial 386 finished with value: 0.428793681321034 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:37,645] Trial 387 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:38,756] Trial 388 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:39,858] Trial 389 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:40,497] Trial 390 finished with value: 0.8475072353360261 and parameters: {'C': 128.0, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:41,603] Trial 391 finished with value: 0.872194867315103 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:42,934] Trial 392 finished with value: 0.4545290262106628 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:43,755] Trial 393 finished with value: 0.7250039336905932 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:44,989] Trial 394 finished with value: 0.8711472841565027 and parameters: {'C': 16.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:46,075] Trial 395 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:46,844] Trial 396 finished with value: 0.428793681321034 and parameters: {'C': 2.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:47,981] Trial 397 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:49,114] Trial 398 finished with value: 0.8691216860192519 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:50,479] Trial 399 finished with value: 0.44725645496636346 and parameters: {'C': 128.0, 'gamma': 2.0}. Best is trial 200 with value: 0.8926701478362279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 400.0\n",
      "\tBest value (f1_score): 0.8927\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_7 = lambda trial: objective_svm_CV(trial, X_trainSet7, Y_trainSet7, Y_trainSet7_class)\n",
    "study_svm.optimize(func_svm_7, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3eeb8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   46.000000   50.000000   51.000000   50.000000   \n",
      "1                    TN  194.000000  185.000000  190.000000  190.000000   \n",
      "2                    FP    5.000000   15.000000   11.000000    9.000000   \n",
      "3                    FN   23.000000   18.000000   16.000000   19.000000   \n",
      "4              Accuracy    0.895522    0.876866    0.899254    0.895522   \n",
      "5             Precision    0.901961    0.769231    0.822581    0.847458   \n",
      "6           Sensitivity    0.666667    0.735294    0.761194    0.724638   \n",
      "7           Specificity    0.974900    0.925000    0.945300    0.954800   \n",
      "8              F1 score    0.766667    0.751880    0.790698    0.781250   \n",
      "9   F1 score (weighted)    0.889947    0.875935    0.897920    0.892722   \n",
      "10     F1 score (macro)    0.849679    0.834997    0.862179    0.856311   \n",
      "11    Balanced Accuracy    0.820771    0.830147    0.853234    0.839706   \n",
      "12                  MCC    0.714596    0.670351    0.725435    0.716943   \n",
      "13                  NPV    0.894000    0.911300    0.922300    0.909100   \n",
      "14              ROC_AUC    0.820771    0.830147    0.853234    0.839706   \n",
      "\n",
      "          Set4        Set5        Set6        Set7  \n",
      "0    44.000000   56.000000   57.000000   52.000000  \n",
      "1   188.000000  192.000000  195.000000  192.000000  \n",
      "2    12.000000    8.000000    5.000000    9.000000  \n",
      "3    24.000000   12.000000   11.000000   15.000000  \n",
      "4     0.865672    0.925373    0.940299    0.910448  \n",
      "5     0.785714    0.875000    0.919355    0.852459  \n",
      "6     0.647059    0.823529    0.838235    0.776119  \n",
      "7     0.940000    0.960000    0.975000    0.955200  \n",
      "8     0.709677    0.848485    0.876923    0.812500  \n",
      "9     0.861128    0.924612    0.939362    0.909007  \n",
      "10    0.811149    0.899490    0.918757    0.876838  \n",
      "11    0.793529    0.891765    0.906618    0.865672  \n",
      "12    0.628332    0.799686    0.839182    0.755278  \n",
      "13    0.886800    0.941200    0.946600    0.927500  \n",
      "14    0.793529    0.891765    0.906618    0.865672  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_7 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_7.fit(X_trainSet7,Y_trainSet7,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_7 = optimized_svm_7.predict(X_testSet7)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet7, y_pred_svm_7)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "Precision = precision_score(Y_testSet7, y_pred_svm_7)\n",
    "Sensitivity = recall_score(Y_testSet7, y_pred_svm_7)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet7, y_pred_svm_7)      \n",
    "f1_scores_W = f1_score(Y_testSet7, y_pred_svm_7, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet7, y_pred_svm_7, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet7, y_pred_svm_7)\n",
    "MCC = matthews_corrcoef(Y_testSet7, y_pred_svm_7)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet7, y_pred_svm_7)\n",
    "    \n",
    "\n",
    "Set7 = pd.DataFrame({'Set7':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set7'] = Set7\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "92faaf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:13:51,895] Trial 400 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:53,095] Trial 401 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:54,303] Trial 402 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:54,929] Trial 403 finished with value: 0.8475794149672093 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:55,821] Trial 404 finished with value: 0.4284925962648115 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:57,316] Trial 405 finished with value: 0.47841322209327625 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:13:58,825] Trial 406 finished with value: 0.44694830997505647 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:00,112] Trial 407 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:00,964] Trial 408 finished with value: 0.5085444464007317 and parameters: {'C': 0.5, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:01,604] Trial 409 finished with value: 0.8418174071860223 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:02,441] Trial 410 finished with value: 0.4284925962648115 and parameters: {'C': 0.0078125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:03,683] Trial 411 finished with value: 0.8710056107932704 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:04,884] Trial 412 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:06,274] Trial 413 finished with value: 0.4284925962648115 and parameters: {'C': 0.015625, 'gamma': 0.5}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:07,464] Trial 414 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:08,652] Trial 415 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:09,333] Trial 416 finished with value: 0.4284925962648115 and parameters: {'C': 0.125, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:10,524] Trial 417 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:11,734] Trial 418 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:12,951] Trial 419 finished with value: 0.8705762811368392 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:13,912] Trial 420 finished with value: 0.8393668171711701 and parameters: {'C': 1.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:14,564] Trial 421 finished with value: 0.8373575428575182 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:15,448] Trial 422 finished with value: 0.4284925962648115 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:16,834] Trial 423 finished with value: 0.8137111009876424 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:17,852] Trial 424 finished with value: 0.8531961668846699 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:19,085] Trial 425 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:20,755] Trial 426 finished with value: 0.44694830997505647 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:21,990] Trial 427 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:23,416] Trial 428 finished with value: 0.5668151957760893 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:24,114] Trial 429 finished with value: 0.8414157992393763 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:25,023] Trial 430 finished with value: 0.7090311174582561 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:26,260] Trial 431 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:27,529] Trial 432 finished with value: 0.8749182813148962 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:28,349] Trial 433 finished with value: 0.5417444752074417 and parameters: {'C': 2.0, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:29,637] Trial 434 finished with value: 0.8728642255974528 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:31,112] Trial 435 finished with value: 0.44694830997505647 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:32,350] Trial 436 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:33,610] Trial 437 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:34,569] Trial 438 finished with value: 0.4284925962648115 and parameters: {'C': 0.03125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:35,869] Trial 439 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:36,593] Trial 440 finished with value: 0.8351262519567706 and parameters: {'C': 128.0, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:37,911] Trial 441 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:39,286] Trial 442 finished with value: 0.8655936634610881 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:40,526] Trial 443 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:41,993] Trial 444 finished with value: 0.43958926367077683 and parameters: {'C': 0.5, 'gamma': 2.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:42,624] Trial 445 finished with value: 0.8475794149672093 and parameters: {'C': 128.0, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:43,841] Trial 446 finished with value: 0.8705762811368392 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:45,328] Trial 447 finished with value: 0.4284925962648115 and parameters: {'C': 0.0078125, 'gamma': 4.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:46,600] Trial 448 finished with value: 0.8710056107932704 and parameters: {'C': 4.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:48,071] Trial 449 finished with value: 0.47841322209327625 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 200 with value: 0.8926701478362279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 450.0\n",
      "\tBest value (f1_score): 0.8927\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_8 = lambda trial: objective_svm_CV(trial, X_trainSet8, Y_trainSet8, Y_trainSet8_class)\n",
    "study_svm.optimize(func_svm_8, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "361958ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   46.000000   50.000000   51.000000   50.000000   \n",
      "1                    TN  194.000000  185.000000  190.000000  190.000000   \n",
      "2                    FP    5.000000   15.000000   11.000000    9.000000   \n",
      "3                    FN   23.000000   18.000000   16.000000   19.000000   \n",
      "4              Accuracy    0.895522    0.876866    0.899254    0.895522   \n",
      "5             Precision    0.901961    0.769231    0.822581    0.847458   \n",
      "6           Sensitivity    0.666667    0.735294    0.761194    0.724638   \n",
      "7           Specificity    0.974900    0.925000    0.945300    0.954800   \n",
      "8              F1 score    0.766667    0.751880    0.790698    0.781250   \n",
      "9   F1 score (weighted)    0.889947    0.875935    0.897920    0.892722   \n",
      "10     F1 score (macro)    0.849679    0.834997    0.862179    0.856311   \n",
      "11    Balanced Accuracy    0.820771    0.830147    0.853234    0.839706   \n",
      "12                  MCC    0.714596    0.670351    0.725435    0.716943   \n",
      "13                  NPV    0.894000    0.911300    0.922300    0.909100   \n",
      "14              ROC_AUC    0.820771    0.830147    0.853234    0.839706   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8  \n",
      "0    44.000000   56.000000   57.000000   52.000000   53.000000  \n",
      "1   188.000000  192.000000  195.000000  192.000000  199.000000  \n",
      "2    12.000000    8.000000    5.000000    9.000000    3.000000  \n",
      "3    24.000000   12.000000   11.000000   15.000000   13.000000  \n",
      "4     0.865672    0.925373    0.940299    0.910448    0.940299  \n",
      "5     0.785714    0.875000    0.919355    0.852459    0.946429  \n",
      "6     0.647059    0.823529    0.838235    0.776119    0.803030  \n",
      "7     0.940000    0.960000    0.975000    0.955200    0.985100  \n",
      "8     0.709677    0.848485    0.876923    0.812500    0.868852  \n",
      "9     0.861128    0.924612    0.939362    0.909007    0.938573  \n",
      "10    0.811149    0.899490    0.918757    0.876838    0.915103  \n",
      "11    0.793529    0.891765    0.906618    0.865672    0.894089  \n",
      "12    0.628332    0.799686    0.839182    0.755278    0.835238  \n",
      "13    0.886800    0.941200    0.946600    0.927500    0.938700  \n",
      "14    0.793529    0.891765    0.906618    0.865672    0.894089  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_8 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_8.fit(X_trainSet8,Y_trainSet8,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_8 = optimized_svm_8.predict(X_testSet8)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet8, y_pred_svm_8)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "Precision = precision_score(Y_testSet8, y_pred_svm_8)\n",
    "Sensitivity = recall_score(Y_testSet8, y_pred_svm_8)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet8, y_pred_svm_8)      \n",
    "f1_scores_W = f1_score(Y_testSet8, y_pred_svm_8, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet8, y_pred_svm_8, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet8, y_pred_svm_8)\n",
    "MCC = matthews_corrcoef(Y_testSet8, y_pred_svm_8)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet8, y_pred_svm_8)\n",
    "    \n",
    "\n",
    "Set8 = pd.DataFrame({'Set8':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set8'] = Set8\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d15fe2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-05 13:14:49,125] Trial 450 finished with value: 0.42879695398468864 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:50,275] Trial 451 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:50,937] Trial 452 finished with value: 0.8394912005640011 and parameters: {'C': 128.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:51,601] Trial 453 finished with value: 0.8343080359921732 and parameters: {'C': 128.0, 'gamma': 0.001953125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:52,458] Trial 454 finished with value: 0.6490823447457588 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:53,604] Trial 455 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:54,949] Trial 456 finished with value: 0.446951988990807 and parameters: {'C': 32.0, 'gamma': 0.5}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:56,097] Trial 457 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:57,243] Trial 458 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:58,103] Trial 459 finished with value: 0.42879695398468864 and parameters: {'C': 0.0625, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:58,863] Trial 460 finished with value: 0.42879695398468864 and parameters: {'C': 1.0, 'gamma': 3.0517578125e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:14:59,508] Trial 461 finished with value: 0.8492213129377021 and parameters: {'C': 128.0, 'gamma': 0.0001220703125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:00,649] Trial 462 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:01,981] Trial 463 finished with value: 0.5629012912381478 and parameters: {'C': 128.0, 'gamma': 0.125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:03,127] Trial 464 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:04,279] Trial 465 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:05,863] Trial 466 finished with value: 0.4472559403585882 and parameters: {'C': 128.0, 'gamma': 8.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:07,002] Trial 467 finished with value: 0.87213329510008 and parameters: {'C': 8.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:08,307] Trial 468 finished with value: 0.805734081354528 and parameters: {'C': 128.0, 'gamma': 0.0625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:09,165] Trial 469 finished with value: 0.8619453889496475 and parameters: {'C': 128.0, 'gamma': 0.0078125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:09,998] Trial 470 finished with value: 0.7289271593370682 and parameters: {'C': 0.25, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:11,098] Trial 471 finished with value: 0.8695949080454961 and parameters: {'C': 16.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:11,770] Trial 472 finished with value: 0.8484182483061279 and parameters: {'C': 128.0, 'gamma': 0.00390625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:12,854] Trial 473 finished with value: 0.8765639151885287 and parameters: {'C': 2.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:13,975] Trial 474 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:15,097] Trial 475 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:15,808] Trial 476 finished with value: 0.42879695398468864 and parameters: {'C': 0.03125, 'gamma': 0.000244140625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:16,923] Trial 477 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:18,043] Trial 478 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:19,292] Trial 479 finished with value: 0.8674592408451829 and parameters: {'C': 128.0, 'gamma': 0.03125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:20,605] Trial 480 finished with value: 0.4472559403585882 and parameters: {'C': 128.0, 'gamma': 1.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:21,729] Trial 481 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:22,844] Trial 482 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:23,596] Trial 483 finished with value: 0.42879695398468864 and parameters: {'C': 0.5, 'gamma': 6.103515625e-05}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:24,752] Trial 484 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:26,111] Trial 485 finished with value: 0.4472559403585882 and parameters: {'C': 4.0, 'gamma': 2.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:27,262] Trial 486 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:27,980] Trial 487 finished with value: 0.42879695398468864 and parameters: {'C': 0.0078125, 'gamma': 0.00048828125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:28,797] Trial 488 finished with value: 0.42879695398468864 and parameters: {'C': 0.015625, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:29,919] Trial 489 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:30,763] Trial 490 finished with value: 0.6490823447457588 and parameters: {'C': 0.125, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:32,100] Trial 491 finished with value: 0.4472559403585882 and parameters: {'C': 128.0, 'gamma': 4.0}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:33,411] Trial 492 finished with value: 0.47195107764810623 and parameters: {'C': 128.0, 'gamma': 0.25}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:34,528] Trial 493 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:35,262] Trial 494 finished with value: 0.6762853602659769 and parameters: {'C': 1.0, 'gamma': 0.0009765625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:36,416] Trial 495 finished with value: 0.867038689975357 and parameters: {'C': 32.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:37,567] Trial 496 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:38,328] Trial 497 finished with value: 0.42879695398468864 and parameters: {'C': 0.0625, 'gamma': 0.001953125}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:39,451] Trial 498 finished with value: 0.8659211512149696 and parameters: {'C': 128.0, 'gamma': 0.015625}. Best is trial 200 with value: 0.8926701478362279.\n",
      "[I 2023-12-05 13:15:40,766] Trial 499 finished with value: 0.446951988990807 and parameters: {'C': 128.0, 'gamma': 0.5}. Best is trial 200 with value: 0.8926701478362279.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tNumber of trials: 500.0\n",
      "\tBest value (f1_score): 0.8927\n",
      "\tBest params:\n",
      "\t\tC: 128.0\n",
      "\t\tgamma: 0.015625\n"
     ]
    }
   ],
   "source": [
    "#Execute optuna and set hyperparameters\n",
    "func_svm_9 = lambda trial: objective_svm_CV(trial, X_trainSet9, Y_trainSet9, Y_trainSet9_class)\n",
    "study_svm.optimize(func_svm_9, n_trials=50)\n",
    "print(f\"\\tNumber of trials: {len(study_svm.trials):.1f}\")\n",
    "print(f\"\\tBest value (f1_score): {study_svm.best_value:.4f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_svm.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3def860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric        Set0        Set1        Set2        Set3  \\\n",
      "0                    TP   46.000000   50.000000   51.000000   50.000000   \n",
      "1                    TN  194.000000  185.000000  190.000000  190.000000   \n",
      "2                    FP    5.000000   15.000000   11.000000    9.000000   \n",
      "3                    FN   23.000000   18.000000   16.000000   19.000000   \n",
      "4              Accuracy    0.895522    0.876866    0.899254    0.895522   \n",
      "5             Precision    0.901961    0.769231    0.822581    0.847458   \n",
      "6           Sensitivity    0.666667    0.735294    0.761194    0.724638   \n",
      "7           Specificity    0.974900    0.925000    0.945300    0.954800   \n",
      "8              F1 score    0.766667    0.751880    0.790698    0.781250   \n",
      "9   F1 score (weighted)    0.889947    0.875935    0.897920    0.892722   \n",
      "10     F1 score (macro)    0.849679    0.834997    0.862179    0.856311   \n",
      "11    Balanced Accuracy    0.820771    0.830147    0.853234    0.839706   \n",
      "12                  MCC    0.714596    0.670351    0.725435    0.716943   \n",
      "13                  NPV    0.894000    0.911300    0.922300    0.909100   \n",
      "14              ROC_AUC    0.820771    0.830147    0.853234    0.839706   \n",
      "\n",
      "          Set4        Set5        Set6        Set7        Set8        Set9  \n",
      "0    44.000000   56.000000   57.000000   52.000000   53.000000   53.000000  \n",
      "1   188.000000  192.000000  195.000000  192.000000  199.000000  193.000000  \n",
      "2    12.000000    8.000000    5.000000    9.000000    3.000000    8.000000  \n",
      "3    24.000000   12.000000   11.000000   15.000000   13.000000   14.000000  \n",
      "4     0.865672    0.925373    0.940299    0.910448    0.940299    0.917910  \n",
      "5     0.785714    0.875000    0.919355    0.852459    0.946429    0.868852  \n",
      "6     0.647059    0.823529    0.838235    0.776119    0.803030    0.791045  \n",
      "7     0.940000    0.960000    0.975000    0.955200    0.985100    0.960200  \n",
      "8     0.709677    0.848485    0.876923    0.812500    0.868852    0.828125  \n",
      "9     0.861128    0.924612    0.939362    0.909007    0.938573    0.916590  \n",
      "10    0.811149    0.899490    0.918757    0.876838    0.915103    0.887102  \n",
      "11    0.793529    0.891765    0.906618    0.865672    0.894089    0.875622  \n",
      "12    0.628332    0.799686    0.839182    0.755278    0.835238    0.775829  \n",
      "13    0.886800    0.941200    0.946600    0.927500    0.938700    0.932400  \n",
      "14    0.793529    0.891765    0.906618    0.865672    0.894089    0.875622  \n"
     ]
    }
   ],
   "source": [
    "#Create an instance with tuned hyperparameters\n",
    "\n",
    "optimized_svm_9 = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "#learn\n",
    "optimized_svm_9.fit(X_trainSet9,Y_trainSet9,)\n",
    "\n",
    "# predict\n",
    "y_pred_svm_9 = optimized_svm_9.predict(X_testSet9)\n",
    "\n",
    "#calculate the evaluation results\n",
    "conf_matrix = confusion_matrix(Y_testSet9, y_pred_svm_9)\n",
    "TP = conf_matrix[1][1]\n",
    "TN = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1] \n",
    "FN = conf_matrix[1][0]\n",
    "Accuracy = accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "Precision = precision_score(Y_testSet9, y_pred_svm_9)\n",
    "Sensitivity = recall_score(Y_testSet9, y_pred_svm_9)\n",
    "Specificity = round( TN / (TN+FP),4 )\n",
    "f1_scores = f1_score(Y_testSet9, y_pred_svm_9)      \n",
    "f1_scores_W = f1_score(Y_testSet9, y_pred_svm_9, average=\"weighted\")\n",
    "f1_scores_M = f1_score(Y_testSet9, y_pred_svm_9, average=\"macro\")\n",
    "BA_scores = balanced_accuracy_score(Y_testSet9, y_pred_svm_9)\n",
    "MCC = matthews_corrcoef(Y_testSet9, y_pred_svm_9)\n",
    "NPV = round( TN / (TN+FN),4 )\n",
    "ROC_AUC = roc_auc_score(Y_testSet9, y_pred_svm_9)\n",
    "    \n",
    "\n",
    "Set9 = pd.DataFrame({'Set9':[np.mean(TP),np.mean(TN),np.mean(FP),np.mean(FN),np.mean(Accuracy),np.mean(Precision),\n",
    "                                           np.mean(Sensitivity),np.mean(Specificity),np.mean(f1_scores),\n",
    "                                            np.mean(f1_scores_W), np.mean(f1_scores_M), np.mean(BA_scores), \n",
    "                                           np.mean(MCC),np.mean(NPV),np.mean(ROC_AUC)],\n",
    "                        \n",
    "                       })\n",
    "\n",
    "mat_met_svm_test['Set9'] = Set9\n",
    "print(mat_met_svm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "95aa0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvYAAAHJCAYAAADuJX3FAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACahklEQVR4nO3deXwTZf4H8M/k6n1SoIWWQjmqcisqR7GAB+ryk1NAPEAXy6rrCp64riKsJ6virsdKPcBjEYFyiYuwKCAIgnhQAQGhUArlKr0LbXPM74+SmDTXJJ0kk+Tzfr3cpclk5skzk+T7PPN9nkcQRVEEEREREREFNVWgC0BERERERC3HwJ6IiIiIKAQwsCciIiIiCgEM7ImIiIiIQgADeyIiIiKiEMDAnoiIiIgoBDCwJyIiIiIKAQzsiYiIiIhCAAN7IiIiIqIQwMCeKECGDBkCQRB8eowpU6ZAEAQcPXrUp8eRauHChRAEAQsXLgx0UWQRau/Hl/xxvRMRhTsG9hR2du3ahbvvvhtZWVmIiopCfHw8evbsicceewwnTpyQ7ThKC6r9YdOmTRAEAc8++2ygiyKZOTifMmWK023M72vIkCGyHvvZZ5+FIAjYtGmTrPv1B/P1bf1fTEwMevbsib/+9a+orKz0yXF9cR6IiEKFJtAFIPIXURQxc+ZMzJ07FxqNBtdffz1uvfVWNDY2Ytu2bXjllVfw9ttv48MPP8S4ceN8Xp6PPvoI58+f9+kxXnzxRcycORPt27f36XGkGj16NPr374+0tLRAF0UWofZ+vDFy5Ej06dMHAHDq1Cl8/vnnePHFF7Fs2TLs3LkTiYmJAS0fEVE4YWBPYWPOnDmYO3cuOnbsiDVr1qB79+42zxcUFOCOO+7AxIkTsX79egwbNsyn5enQoYNP9w8AaWlpigo6ExISkJCQEOhiyCbU3o83Ro0aZXO345VXXsHVV1+Nffv24Y033sDTTz8duMIREYUZpuJQWDhy5Aiee+45aLVarF692i6oB4CxY8di3rx5MBqNuO+++2AymSzPWedSr1mzBgMHDkRMTAySkpIwbtw4/Pbbbzb7EgQBH374IQCgU6dOllSFjh07WrZxlHNsncqya9cu3HjjjUhMTERiYiLGjh2LkpISAMBvv/2G8ePHo3Xr1oiKisLQoUNRWFho954cpQN17NjRLoXC+j/rIO3gwYOYOXMm+vXrh9atWyMiIgKZmZm49957cezYMbtjDR06FAAwe/Zsm32aU01c5aTv2rULY8aMQZs2bSzHue+++1BaWuryfc2fPx89e/ZEZGQk2rZti3vvvddnaSDNOXs/P/30EyZMmIDMzExERESgVatW6NWrFx566CHo9XoATedh9uzZAIChQ4fa1Je10tJS3H///ejYsSN0Oh1at26N0aNH4/vvv3dZni+++ALXXHMN4uPjIQgCKioqEB0djc6dO0MURYfvZ8SIERAEAT/88IPXdRIbG4vJkycDAHbs2OF2e5PJhLfffhtXXnklYmNjERMTg379+uHtt992+BkEgM2bN9vUVzClfhER+RJ77CksLFiwAAaDAbfeeit69uzpdLupU6dizpw5OHjwIDZv3mwJVM2WL1+OtWvXYvTo0RgyZAh+/vlnFBQUYOPGjdi2bRuys7MBALNmzcLKlSuxe/duPPTQQ5Z0BKlpCd9//z1efvll5ObmYurUqfjll1+wfPly7NmzBytWrEBOTg4uu+wy3HXXXTh27BgKCgpw3XXXoaioCLGxsS73PX36dIeB7+eff44ff/wR0dHRNu/3nXfewdChQzFw4EDodDrs2bMH77//PlavXo0ffvgB6enpAJp6bgHgww8/RG5urk0etHWDxpFVq1bh1ltvhSAIGDduHDp06IBdu3bhnXfewapVq7B161ZkZWXZve7xxx/HunXr8H//93+44YYbsHHjRrz33nuW8xcIP//8MwYMGACVSoVbbrkFnTp1QnV1NQ4dOoR///vfeP7556HVajF9+nSsXLkSmzdvxuTJkx3WUVFREXJycnDy5Elce+21uO2221BSUoKlS5fiiy++wNKlSzFy5Ei71y1duhRffvklbr75ZvzpT3/CkSNHkJSUhIkTJ2LBggXYsGEDrr/+epvXlJSUYO3atbjiiitwxRVXtKgOnDUcHJk0aRI+++wzdOjQAVOnToUgCFixYgUeeOABfPPNN1i8eDEAoE+fPpg1axZmz56NzMxMmwYoc+6JiC4SicLA0KFDRQBifn6+221vu+02EYD497//3fLYggULRAAiAPHzzz+32f71118XAYjDhg2zeXzy5MkiAPHIkSMOj5Obmys2/whu3LjRcpxPPvnE5rl77rlHBCAmJCSIzz33nM1zzz//vAhAfP311z0qg9n69etFjUYjdunSRTx79qzl8ePHj4v19fV22//3v/8VVSqVOG3aNIflnzVrlsPjmOtxwYIFlsdqamrE5ORkUa1Wi99++63N9i+88IIIQLzuuuscvq8OHTqIxcXFlsf1er04ePBgEYD43XffuXzPzcvUu3dvcdasWQ7/Mx8vNzfX7fuZMWOGCEBcsWKF3bHKy8tFo9Fo+XvWrFkiAHHjxo0Oy3b99deLAMSXXnrJ5vEtW7aIKpVKTEpKEqurq+3KIwiCuHbtWrv97dq1SwQgjh071u65p59+WvJnRBR/PwfW710URbGurk7s3r27CECcPXu25XFH1/t//vMfEYDYr18/sba21vJ4bW2tePnllzv8HDg6D0RE1IQ99hQWTp06BQDIyMhwu615G0cpIMOGDcOIESNsHvvzn/+MN954A19//TWKi4uRmZnZ4vIOHjwYt99+u81jkydPxgcffICkpCTMnDnT5rk77rgDTz31FH7++WePj7Vnzx6MGzcOCQkJ+O9//4uUlBTLc84G3d5000247LLLsH79eo+P19zKlStRXl6O22+/HQMHDrR57tFHH8X8+fOxYcMGh3X7zDPP2IxV0Gg0uPvuu7FlyxZ8//33uPrqqyWXY/fu3di9e3fL3gxgSRexvvNhlpSUJHk/x48fx//+9z9kZmbikUcesXkuJycHEydOxKJFi7BixQrcddddNs/fcsstuPHGG+32ecUVV+DKK6/E6tWrcfr0abRt2xYAYDQa8f777yMuLg6TJk2SXEag6fyZU71Onz6Nzz//HCdOnEDnzp3x4IMPunztBx98AKBpkHdMTIzl8ZiYGLz00ku44YYb8P7779t9FoiIyDHm2FNYEC+mBkiZR9u8jaNtc3Nz7R5Tq9XIyckB0JRbLQdHqRDt2rUD0JSSoFarHT53/Phxj45z8uRJ/OEPf0BDQwNWrFiBrl272jwviiI++eQTXHfddWjdujU0Go0lr3nPnj2yTA9qrrPmaU8AoNVqLXXuqG779etn95i5YVZRUeFROSZPngxRFB3+t3HjRsn7mThxItRqNUaNGoXJkyfjo48+wuHDhz0qC/D7+x08eDA0Gvs+mOuuuw4A8OOPP9o956pBc//990Ov11uCaqApDau0tBR33HGHTYAtxapVqzB79mzMnj0bH374IeLj4/HYY49h586dbhsyP/30E1QqlcPP1dChQ6FWqx2+PyIicoyBPYUF88ww5sGnrpiDY0ezyZh7OJtLTU0FAFRVVXlbRBuOZloxB3eunjMPzJSirq4OI0aMQElJCRYsWIDBgwfbbfPwww/jzjvvxL59+zB8+HA88sgjmDVrFmbNmoXMzEw0NjZKPp4z5joz12Fz5vPgqG5d1YXRaGxx2bxx5ZVXYsuWLRg2bBiWLl2KyZMno0uXLrj00kvx2WefSd5PS+rF2WsAYMKECUhOTsZ7771nafDOnz8fAPCnP/1JcvnMFixYYGkAnT9/Hvv27cPcuXORnJzs9rVVVVVITk6GVqu1e06j0SAlJQXV1dUel4mIKFwxFYfCQk5ODjZu3IgNGzZg6tSpTrczGo2W3tlBgwbZPX/69GmHrzOn+gTL1Icmkwm33XYbfvzxRzz//PO47bbb7LY5c+YM/vWvf6FHjx7Ytm0b4uLibJ7/9NNPZSmLuc7MddjcyZMnbbYLBgMGDMCaNWvQ0NCAH374AV9++SXeeOMN3HbbbWjdurWkqVRbUi+u7kxFRUVhypQpeO211/C///0P3bp1w/r169G/f3/06tVLytuTTUJCAsrLy6HX6+2Ce4PBgLKyMsTHx/u1TEREwYw99hQWpkyZArVajeXLl2Pfvn1Ot/vggw9QWlqK7Oxsh+kBjmZaMRqN2Lp1KwCgb9++lsfN6TKB6jl2Zfr06fj8889xzz334K9//avDbYqKimAymXDDDTfYBfXHjx9HUVGR3Wu8ec/mOnO0+qrBYLDU7eWXXy55n0oRERGBgQMHYs6cOfjXv/4FURSxcuVKy/Ou6stcL1u3boXBYLB73twA9aZe7rvvPgiCgPnz5+Pdd9+FyWTCtGnTPN5PS/Xt2xcmkwnffPON3XPffPMNjEaj3ftTqVSK/EwRESkBA3sKC1lZWfjrX/8KvV6P//u//3MY3K9cuRIPPfQQ1Go13n77bahU9h+Pr7/+GmvWrLF57M0338Thw4cxdOhQm8GdrVq1AiAt/cefXn/9dbzxxhu49tpr8c477zjdzjz94tatW20CqdraWtx7770Og01v3vOoUaOQnJyMTz/9FN99951dWYuKinDdddf5ZUEvOWzZssVheoz5bk9kZKTlMVf1lZ6ejuuvvx5Hjx7F66+/bvPcjh07sGjRIiQlJWH06NEel7FLly64/vrrsXr1auTn5yMxMRETJkzweD8tdc899wAAnnzySZtVmM+fP28ZIP7HP/7R5jWtWrVS3GeKiEgpmIpDYePZZ59FXV0dXnvtNfTu3RvDhw9H9+7dodfrsW3bNuzYsQNRUVH49NNPnaZK3HLLLRg9ejRGjx6NLl26YPfu3fjvf/+L5ORkvP322zbbXnvttfjHP/6Be++9F2PHjkVsbCwSExPx5z//2R9v16FTp07hkUcegSAI6NmzJ55//nm7bfr06YNRo0YhNTUVEydOxOLFi9GnTx/ccMMNqKqqwv/+9z9ERkaiT58+drPwZGdno3379li8eDG0Wi06dOgAQRBw5513Op0tKDY2Fh988AFuvfVW5Obm4tZbb0WHDh3www8/YP369UhNTbXkgAeDV199FevXr8eQIUOQlZWF2NhY7N27F2vXrkViYiLy8vIs2w4dOhQqlQpPPvkkfvnlF8tg07/97W8AgHfeeQeDBg3CY489hvXr16Nfv36WeexVKhUWLFhgdzdFqvvuuw/r169HWVkZ/vKXvyAqKqrlb95DkyZNwqpVq7BkyRJ0794do0aNgiAIWLlyJY4cOYLx48fbzYhz7bXXYvHixRg5ciT69u0LjUaDa665Btdcc43fy09EpDiBmWWTKHB27Ngh3nXXXWLHjh3FyMhIMSYmRuzevbv4yCOPiCUlJQ5fYz1f+Zo1a8T+/fuL0dHRYkJCgjhmzBjxwIEDDl/36quvipdccomo0+lEAGJmZqblOVfz2DuaB/7IkSMiAHHy5MkOjwUH83s3n8fevA9X/1nvv66uTvzrX/8qdu7cWYyIiBDT09PF+++/XywrK3NYflEUxZ07d4rDhg0T4+PjRUEQbOZpdzTvu/XrRo0aJaakpIharVbMyMgQ//SnP4knTpyw29bV/Pzu5tJvzlwmZ/VqvU8p89ivW7dOnDJlinjppZeK8fHxYnR0tNitWzfxwQcfFI8ePWq3748//ljs3bu3GBkZaTkH1o4fPy7+6U9/Ejt06CBqtVqxVatW4siRI8WdO3c6fS+O6rc5g8EgpqSkiADEvXv3ut2+OWfz2Dvj7HoxGo3iW2+9JV5xxRViVFSUGBUVJV5++eXim2++aTPnv9np06fF2267TWzTpo2oUqk8OtdERKFOEEUPlggkClMLFy7E3XffjQULFtiseEkUrA4fPoyuXbsiJyfHYY47EREFH+bYExGFoX/84x8QRTGgqWFERCQv5tgTEYWJ4uJifPzxx/jtt9/w8ccfo2/fvhg3blygi0VERDJhYE9EFCaOHDmCp59+GjExMRg+fDj+/e9/O5z9iYiIghNz7ImIiIiIQgC7aoiIiIiIQgADeyIiIiKiEMDAnoiIiIgoBDCwJyIiIiIKAWE9K05FRQUMBoPs+23dujXOnj0r+37JFuvZf1jX/sF69g/Ws//IXdcajQZJSUmy7Y8o1IR1YG8wGKDX62XdpyAIln1zwiHfYT37D+vaP1jP/sF69h/WNZH/MRWHiIiIiCgEMLAnIiIiIgoBDOyJiIiIiEIAA3siIiIiohCgiMGz69atw+rVq1FZWYn09HRMmTIFl156qdPtv/zyS6xbtw5nzpxBSkoKxowZg9zcXD+WmIiIiMLVhQsXcPr0aYiiyIHB5FOCIEAQBLRt2xZRUVFutw94YL9t2zYsXLgQU6dORXZ2NjZs2IAXXngB8+bNQ0pKit3269evx6effopp06ahc+fOOHToEObPn4+YmBj069cvAO+AiIiIwsWFCxdw4sQJxMXFQaVi4gP5nslkwokTJ9C+fXu3wX3Ar8g1a9Zg2LBhuPbaay299SkpKVi/fr3D7b/55htcd911GDhwINq2bYtBgwZh2LBhWLVqlZ9LTkREROHm9OnTDOrJr1QqFeLi4nD69Gn32/qhPE4ZDAYUFRWhd+/eNo/36tULBw4ccPgavV4PrVZr85hOp8OhQ4d8stgUERERkZkoigzqye9UKpWktK+ApuJUV1fDZDIhISHB5vGEhARUVlY6fE3v3r3x9ddf46qrrkKnTp1QVFSEjRs3wmg0oqamxuGKdHq93mYhKkEQLLcyzAtoyMW8P7n3S7ZYz/7DuvYP1rN/sJ79J1Trmjn1FCiKD+zNHH3onX0RjBs3DpWVlXjqqacgiiISEhKQm5uL1atXO21Br1ixAsuWLbP83alTJ7z88sto3bq1PG/AgdTUVJ/tm37HevYPURTRpk0bST/Q5t4sV19Aoija7cv6b/OAtOb7EQTB8lrz4833FexBBK9p/2A9+w/rmsh/AhrYx8fHQ6VS2fXOV1VV2fXim+l0Otx///3Iy8tDVVUVkpKSsGHDBkRFRSEuLs7ha0aPHo0RI0ZY/jb/8J89e1b29B1BEJCamopTp06xVe9DrGffq2s0Yt6mEqz9tRwmPxwvSiOgdZwGJyr1MLo4pQIAV2c8WqvCDZck4c856YjRqeUups/wmvYP1rP/+KKuNRqNTzvlqMkVV1yBvLw8TJs2rUXbtNTixYvxt7/9DYcOHfLZMeSgpHIGNLDXaDTIyspCYWEhrrrqKsvjhYWFuPLKK92+tlWrVgCAb7/9FpdffrnTHnutVmuXl2/mqy92ToHlH6xn36hrNOKexftRUtnot2NeMIg4VqF3u527s31eb8LKX87hp+O1eG9CdlAF9wCvaX9hPfsP61o5Tpw4gX/84x/46quvUF5ejrZt2+Kmm27CI488guTkZI/2tW7dOkRHR8tWNkcNhZEjR+Laa6+V7RjNff7557j33nuxa9cupKen2z0/cOBADBkyBC+88ILPyiC3gKfijBgxAm+88QaysrLQrVs3bNiwAWVlZbj++usBAIsWLUJ5eTn+/Oc/AwBKS0tx6NAhdO3aFXV1dVizZg1KSkrwwAMPBPJthATzF6+jVAZHX8rm7axTI6xTJJzty9H+mu/L2X6t/598J397qSWo1xr10BmDb2B62ekLWLjxN9w/yP7LWolEQYCxuhqm2lqAQZDPsJ79iANMJfHXb9rRo0dx8803o3Pnzpg/fz46dOiAAwcOYPbs2fjqq6+wdu1ah+MUnXE0JbncoqKiJM3d7q0bb7wRycnJ+Oyzz/DII4/YPLdjxw4cOnQI+fn5Pju+LwQ8sB84cCBqampQUFCAiooKZGRk4Mknn7TcaquoqEBZWZlle5PJhDVr1qC0tBRqtRrdu3fHc889hzZt2gTqLQS1ukYj3tp6HOsOVKLB0JRwEalR4YbsJNx9VSoW7DyJL/dXoN5g/wOoFgCdWoAJQOPF55tvFa1t2tcDOe0BAG9tPe5wf2oBiNCoEKkVcEEvAqJo2a8gNP3+mr/3dGoBCVEa3NSzHHf0TkC0lj8ecttSVA0ASLlQiRuKd0II0gAo7rgaDSeD5LtBAMpj49BQW+P+tgR5j/XsN0JMNPDQQ4EuhiLVNRrx763H8c3hChhMIjQqAdd0TsJ9PkwhnDlzJnQ6HZYsWWIJltPT09GjRw9cffXVeOGFF/CPf/zDsn1tbS3+9Kc/4csvv0RcXBweeughTJ061fJ88x726upqzJ49G2vXrkV9fT369OmDOXPmoEePHpbXfPnll3j11Vexf/9+xMTEoH///li4cCFGjRqFkpISPP3003j66acBAGfOnLFJcTl06BAGDhyIb7/9Fl27drXs89///jfee+897Nq1C4Ig4MCBA3j22Wexfft2REdHY8iQIfj73/9uyfKwptVqMW7cOCxevBgPP/ywTQPr008/Re/evdGjRw/8+9//xuLFi1FcXIzExETccMMNeOaZZxAbG+uwrh988EFUVVXho48+sjz2t7/9DXv27MHKlSsBNDXo3nzzTXz44Yc4c+YMsrKy8Mgjj+D//u//JJ9TRwIe2APA8OHDMXz4cIfPNe+JT09Px9y5c/1RrJBX12jE1M8OoLiiwebx83oTVu45hzV7z8FBPG9hFJvSJ1wx7+uH4zUA4DS1wyg2bXveQSaGOaY0/3+9QUR9jR4fbT+KzfsjkT++W9ClWyiZKIpovDj2pEP1aQiiCFEQYBKCrwGlhwpQq9CUma9sggAIGjUEtZodyT7EevYfQcXvZUfqGo24Z9FeHD1XbzN+aenPp/H9sSp8MKm77L9pFRUV2LhxI/7617/a9YC3bdsWY8eOxapVqzB37lxLcPvWW29h+vTpeOyxx7Bx40Y8/fTT6NKlC4YMGWK3f1EUMWnSJCQlJWHRokWIj4/Hhx9+iHHjxmH79u1ISkrC//73P9x9992YPn063nrrLTQ2NmLDhg0AgAULFmDo0KG48847cccddzh8D126dEHv3r1RUFCAmTNnWh5fvnw5xowZA0EQcPr0aYwaNQp33HEH5syZg/r6esyZMwf33nsvli9f7nC/t99+O9555x1s27YNgwYNAgDU1dVh1apVeOaZZwA0TTX5/PPPIyMjA8eOHcMTTzyBOXPmtCgeffHFF/HFF19g7ty5yMrKwnfffYf7778frVq1wsCBA73eryICewqM/O2ldkG9NTcxu0d8kattEoHiinrkby/FjNwM2fcfrgRBgE6jAdCItufLAQDb0nriaEJaYAvmhdQ4Hf54Z/dAF0MSQRCQkpYG/cmTzEf2Idaz/zBl0rF/bz1uF9QDTb9pR8vr8e+tx/HosExZj1lUVARRFG16uq117doVlZWVKCsrs2RMXHXVVfjLX/4CAOjcuTN27tyJ+fPnOwzst27dil9//RX79u1DREQEAFh67z///HPcddddmDdvHkaNGoUnnnjC8jpzb35SUhLUajViY2PRtm1bp+9j7NixeP/99y2B/eHDh7F79268+eabAJoaCD179sRTTz1lec0///lP9OnTB4cPH0bnzp3t9pmdnY0rrrgCn376qSWwX716NUwmE8aMGQMANnn/mZmZmDlzJh5//HGvA/u6ujq88847KCgosIwp7dixI3bs2IGPPvqIgT15ThRFS7oFALSvPWsJ4oJNxdbD0Ee7X42NpJsgnsWWM1VIami603I6WnrepZIMzooPdBGIiGx8c7jC6UxjJhHYcrhC9sDeHUdj7Pr162ezTb9+/Zzmm+/evRt1dXXIzs62eby+vh5Hjx4FAOzduxd33nlni8o5evRozJ49G7t27UK/fv2wbNky9OjRw3LcwsJCfPvtt+jYsaPda48ePeowsAeASZMm4emnn8ZLL72E2NhYLFq0CDfffLNlhsatW7fi9ddfx8GDB1FTUwOj0Yj6+nrU1dUhJibG4/dx8OBB1NfX49Zbb7V5XK/Xo2fPnh7vzxoD+zBS12hE/vZSbCmqht5oRPl5IwBAZ9TjmuM/QyX6Y1JD+cXo1DDsrYUQBOkWweIm0QR9XRmqRBFVEbG4oI0MdJE81jEpAnkD2gW6GEREFqIowmByfadIbxJlH1DbqVMnCIKAgwcP4uabb7Z7/tChQ0hMTHSYhy6FyWRC27ZtsWLFCrvnzMFxZGTLf0fatm2LQYMGYfny5ejXrx9WrFiBu+66y6YcN9xwgyVPv/lrnRk9ejSefvpprFy5EgMHDsSOHTssdxZKSkowadIkTJ48GTNnzkRSUhJ27NiB6dOnO50y3dEsjdYLpZpMTfHWokWL7NZ5MN/x8BYD+zBR12hE3pKDKC63v/2XWncOKtGE89pIHI0PvnSLpCgNND3827sR6qIAjOxuwvLCMnxZ6XlvhFfH1AhoHavBiSoZ5rG/OGCbYy+ISEkEQYBG5Tpg16gE2dOYkpOTkZubiwULFmDatGk2efanT59GQUEBbr31Vpvj/vDDDzb7+OGHH5ym8vTq1QtnzpyBRqNBhw4dHG5z2WWX4ZtvvsFtt93m8HmtVguj0ej2vYwbNw5z5szB6NGjcfToUYwePdqmHGvWrEGHDh2g0UgPcWNjY3HLLbfg008/RXFxMTIzMy1pOT///DMMBgNmz55tCdhXrVrlcn+tWrXC/v37bR7bs2ePZer17OxsRERE4Pjx4y1Ku3GEgX2YyN9eiuLyeoiiCannK6Az/t5y7FxVCgAojkvFT226BaqIXlEJwNheKdD2y1DcNJhKK4+ntAAm9wemXFxkprS0VNLruPIsEZFz13ROwtKfT8NRx71KaHreF1566SX84Q9/wIQJE/Dkk0/aTHeZmpqKv/71rzbb79y5E2+88QZuvvlmbNq0CatXr8Z//vMfh/vOzc1Fv379MHnyZMsg21OnTuGrr77CTTfdhD59+uDRRx/F2LFj0bFjR4wePRoGgwFfffUVHnzwQQBARkYGvvvuO4wePRo6nc7p3YM//OEPePzxx/H4449j0KBBSEv7vUPynnvuwSeffIJp06bhgQceQHJyMo4cOYKVK1fitddeg1rtvLNn0qRJuOWWW3Dw4EHcf//9lt+Sjh07wmAw4L333sMNN9yAnTt34sMPP3RZ1zk5OXjrrbfw2Wef4corr8TSpUuxf/9+S5pNbGws7r//fjzzzDMwmUy4+uqrUVtbi507dyImJgYTJ050uX9XGNiHiS1F1TAB6Fh9GoNKCx1uczLW9kOkEeQbQNshUQcR8g6iVQlAh6QI6I0ixizYC4PJBI1KhcFZ8cgb0C4gvbXW6U5KKI9cBEFwG6xbb2v9/662cfW8s/00f5yBPBEFk/ty0vH9sSocLa+3Ce5VAtAxOQr35fhm7Y2srCysX78e//jHP3DvvfeioqICbdq0wU033YRHH33Ubg77++67D4WFhXj11VcRExOD2bNnY9iwYQ73LQgCPv30U7zwwguYPn06zp07hzZt2qB///6WwbiDBg3Ce++9h9deew1vvPEG4uLi0L9/f8s+nnjiCTz66KO46qqr0NDQgDNnzjg8VlxcHG644QasXr0a//znP22eS01NxZo1azBnzhxMmDABjY2NSE9Px7Bhw5wuYmrWv39/dOnSBUVFRZgwYYLl8Z49e2LOnDl444038Pzzz6N///546qmnLOsrOTJs2DA8/PDDmDNnDhoaGnDbbbdh/Pjx+PXXXy3bzJw5EykpKfjXv/6F4uJiJCQkoGfPnpg+fbrLcrojiGE8LcDZs2dtcp7kIAgC0tLScFJBMy6IooiRH+xBWZ0B/U7/iuzyY6jTRqFW+/utuOqIGPyQeglEQWU3j/26/RVup7V0xtE89o72Zz2Pfb2+qbdWBNDgcB57FRKj1LjusjRsPXgaxyoabNKLVAKQmeT/aTCdpTsFqjxyUeI1rUQtvUPDevYP1vPvfH1X0Rd1rdVqLYFioBQVFSEuLq5F+zDPY7/lcAX0JhFalYDBPp7HXm49evTAzJkznU5PSfKrqalBVlaWy23YYx+ErEevu1oR1vpvzcWWalzjeQDAnpQsHEr8vVcgNU6Hb6ZcZvf6x4dl4vFhmXbHqWs0YtrS31BcYd/jkJkYgfkXg9jmZXG2P09XnlWpVJj/fbldUA8EbhpMc7qTUspDsEvbARx/bpxdZ823FwQBJpPJ0vNT22DAu9+dtNyhUQsCrumcgLwB7SwLp1lfz47+tn6MQpdSUvNC9a5isInRqfHosEw8evH3UAnXhlTnz5/Hzp07cfbsWbtZcCjwGNgHCesVYuv1JkmDBy097gcqcUHfFG7GXwzsq3XRlu1VQtO0gJ6kTsRGaJA/vhvyt5dia1G1ZeW8HIk/EM6O5ShX2tn/b/j1tMspw7YWVWNGrstiyMqc7qSU8gRSIH+ozIHL5sNVqLqgR4P7sVgAmj4HlsXQpGx/cbvm2y7dXYalu8ss+xPQdLdJxO9/A7C5VqK1KtxwSRKeGxfYnkiSl6MgOqdTHKYNDMzAbmd3FQsKy7CrpDZo7yoGu2AK6gHg448/xmuvvYa8vDzLHOykHAzsg4CzFWKdcbZyrMpkRIz+AgCgWtc004lKADomRXo1LWCMTo0ZuRmYkev/QE4URehdTZ0CwOCDKcNclcdgcj1dqD/LEwhK6Ak0By5Hy+slBefW3MxAZ7+9xP2Jlv+x+ruZ83oTVv5yDntOb8U7Y7tYevvJO0r4nDkLopcVnsOKPefwf5e18uvMTaIo8q4iyWLatGk2CzaRsjCwDwLWK8TqjHq0qz0LtRf5ipHGRgiiiEa1FvVqHaK1KvzhsmRZAi9//4gKggCt2vUx1RKmDJMrALBOd2pJeYKVnD2BLTknltmfvHp14B06U4f8baWYnuubwXO+poQ7Nc0blvf2T0NshO1PnXnWJUflbZ4OCNinY7lKozK/fv42x0E0ABhNwMo95/DziVq8OyHbJm3LVTpiS+uk/LyBdxWJQhwD+yBgvUJsv9P70alK2rSDzlTrYgBBQEKkJqh7Z667tC0+2n7U6ZRhOZ3iHP4AN8+LNt8eNzdwnP2QOvsxN287OCseBYVlDssjoKk8jgQqGJLzuC3tCZSrt99VOlSw2HKkKqgC+7pGI+ZvK8XWI47HGTQPigHXYx+cXZOugmlnDculu8uwvLAMraI1GNgpHo1G4OvfKlBvdSszWqvC0C6J0KoFfFdcg0ajEecbTdAbRZhE92lZ1q/fdrQa1fUGNF58rTtHKxpw/Tu/z1ImANCpAdXF92cCoDeK0Kmbvq/N9SrlM+Fq7RJnQv2uIlE4YGCvYOYeJb3Vgg0pFyoBAGejk9Co8vz0iYKAA0lNi0cYTCaff4n7Yv9N4w1OYN3+cqc/niax6Zb3ssJzlsfUAqBTC2hw8KNrva0AIELT9EM6oGMc9Ebgq2bBQJRGQFq8DnWNJuhNJpxvNKHBIDoNAkQAy385h//+WmEZ//DJD6f9nrbiq3SZlowvkKu3X0o6VDAwGIMjuKprNOL1zSX4768VTscZ6FRA48VTIqCpwa1TCzCKIhqbjX2w/tyZA1igaSatL/fbB+Pmz1H+9lJ88WuF03IaReBMnQEr95Q7fP683oQvfnX8nBQtfb01Ebg4JsS2RusNIupr9R59Jpw1tl0x31X09voLhuuWKNQxsFcY8yDZ5j9kAKA2GRF7MUf+m/a9Ua9p2bLDapXKJ1/Cvsy19nS8gTWjCEnTdor4/YfUWTBwwSCiqNyzMpjE38c/fLGvvKl3zOp5Xw9g89XAuZaOL5Ar71dKOlQw0Ki9S9mSM6hyty+pn8NGq5MqwvVn0PpzV1BYhp3HamASRYdrX5g/R5/vOQeJY6NDQvPPhKMZnICm8+fp3SsBQKxOsFkTZFDHWPxpULrNnZfmd12c3bG5t38a4iK1MtcAEbnDwF5B3P1YxjfWQRBFNGh0qFfrWny8wVnxkrbzJGDw9awL1uMNgpnewa0G6x/t6dfYp2I0Pwcmk8lmpiBn+bjm7VwF0EfK6zF/2wnMyM3wODhs6fgCOWcTcpUOFSwGd0qQvK2cjWhP9uXrz2HTZ8H9/sMpqDczicDywjJsOlSJ6nqDZdYn6zsiBpMIvYc3r0QAh87Z1nnBL+Uo+KWpc0MlAFpVU5qQIAiI1ApO71Qu3V2GZbvL0DpWi5t7leOO3gkcEE7kJwzsA8y6p8Xmx1IUITT7ukxsqAUAVF3MkXfH1cqxHZMiXM6E423A4OtZF6zHG4Qik/h7GkNz0VoVBndKwIGzdThaYd+LqbIs4CUgRqeC3mBCdaP0CNecjhSlVWH4xUXFpAaHrgJq83Sqjsg9m1DegHbYVVKLI+X1ksqtNF3axODeAWkOe0bNzN8Z5/UmSY1oR69vnt8uZV/WAzxD/XOodEYROFtnsHnM3R2RljKJ1mlCIs67WdtRBHCmVo+Pth/F5v3Bu0AfUbDhyrMBWHnWek76BkPTz2ikpulH87zeBIgirj/2Pdqcd5w3ejApA9+nXua0DM3nsV9/oBL1Vse5wU3Q1pIVVMcs2ItTNfZBp1lanA4Fd3d3+rwroijilvd/wbnz4dhP53+ZSRF4d3w3xEVqJV3TeUsOOlywrGNSpGXBMkfcXTMqARjbK0VyL3RtgwG3vL/HLpVNbmoB0KiaxmxIYT2vPWA7XWaURkDbeC3O1BpRrzdCFJu2j9CoEKVTQQUgLkKNk9WNqHcxlsNMANC5VSRqG00wmExQCQLiI9SoqjfY9PJavxdXb0NtNQ8/IG2efyIz82dYjskaQmXlWWq5Bx98EFVVVfjoo48CXRS/4cqzCuQs3ea81X3TlAtVToN6k6DC8dg2v28brcbKe3o4XNgJsF/pVUqvp9Red0c9gb6cy71piks1wvMGvP8VVzTglvf3IClai+E9XN9Oj9GpvV6wzF36jEn0LJUrNkKDxCity8aCAGBsr1YY2ysFdy066DA1SiMAy6ZchjZxEU5XnjVfy+4aJ6lxOhQ0W9nZvM+6RiPuXXIQR5uN2TBeHJNh/m5o3kPrSlNahe1dizO1zjsx3LVNJLZdiBziVJrK8OCDD+Kzzz6z/J2UlIQ+ffrgmWeeQffu3nW4NTd37lysXbsWGzdudLrNk08+ia+//ho7duywe+7kyZPo27cv3nvvPYwYMUKWMoUbBvZ+Zp1uozUaEGG0DwY6VTdNZ1kcn4qdzXrmjYIKRtXvgY1GrbYEGa54Eki7y3n+Yl+50xQdKbnW3qprNCJWxzxNf6o3iDhZ3Sjpdrq3C5aZ02ea9/Zbk5LKZZ0+VnnB9Z048yxF6w5UwODkoCYA//nxjN24A+vPm/lxKalIzevD/Pe7350MiXEjRK5wKk1lGDZsGP75z38CAM6cOYOXXnoJd9xxB3766Se/lWHSpEl4//338d1336F///42zy1evBjJyckYPny438oTahgl+Zk5NzVKX48xhzZj5OEtdv91qygBAByLa4tGtdbmP+ugHpA+AFaq2gaD26DovN6EUzWNKKsz4FRNIwoKy5C35CDqGo0YnBUPV7F7db0BIz/YgzEL9mLe5hLUNZ/zzglzqsfhc8GZOx3srANrKTz58Tb39o/tleLy2jH3+jlivj4KdpfhVE2jpDQckwjUNJicppW4Ol5zeQPaITMp0q78UlZ2Zr66vaZ6i0BGYssnCXAnSqvCHy5NxqgerZAWp0OraA2iNALUwu9pU65EW72+bawWkRqhKY1KLSBKKyBaq0JKjAZpcTqM6pGMP1yaHJYDSUN5gb5gotPp0LZtW7Rt2xY9e/bEgw8+iBMnTqCs7PdxXSdPnsS9996Lrl27Ijs7G3fddReOHTtmef7bb7/F8OHD0bFjR3Tp0gV/+MMfUFJSgsWLF+OVV17B3r170aZNG7Rp0waLFy+2K0PPnj3Rq1cvLFq0yO65xYsX49Zbb4VKpcL06dPRr18/dOjQAQMGDEB+fr7L93bFFVdg/vz5No8NHToUc+fOtfxdXV2NRx55BJdddhmysrIwZswY7NmzR3L9BQP22PuR9Zz08Y110JgMEAUBRsG+B7QqIgalsSku9+duAKyn6hqNmLb0N49zk62DPne9r9apBd7MyeyqZOZ56k2iaJdDDDS1YkW4zw+2zKcdpcGAzKZ57L/+rcLjQWlRGgFt47QormgMiZxkX95Oj9GpMf2adGw8VIkyFyknznr9vJmzW4rmx3PW4+htKlLzdSpCQYQaUKmakvKtP4vWs7Y4+oxaf+6uybKdx37dfumfPwFAp+QIPP+HTlheWIZvDleh8oLecjyVAERp1bi+WwLuH9Te4aq0cqw86+q6eer6pvTIsQv3uUzhitI4Xncj2LgaQB8KRFEEDNJT5WSj0bSosVRbW4tly5ahU6dOSE5OBgCcP38eo0ePRv/+/bFq1SpoNBq89tprmDhxIjZt2gSVSoXJkyfjjjvuwDvvvAO9Xo8ff/wRgiBg5MiR+PXXX7Fx40YsXboUABAf7/i8T5o0CXPmzMELL7yA2NhYAMC2bdtw5MgRTJo0CSaTCWlpaXj33XeRnJyM77//Ho8++ijatm2LkSNHevV+RVHEpEmTkJSUhEWLFiE+Ph4ffvghxo0bh+3btyMpKcmr/SoNA3s/ss4RN38Uq3Qx+CJrkEf7MQ+O9WTWEinMwZE3fg/6MuwCnLpGo80YAuvXSF2V9It95S6DttRYLZbf0wOA7Q9rU3rGSWw90pSe4azRYs65NqddePtD3DZWi+UXBwebXz9vc4nDWW6CkS9vp7dk2kxfrTirVgk4rzdJmiHKm1SkUBs3Yh5L4GyFWW9WnrUeJ+T28xejwYo/9rT83XQ+MuyC7Xbt2jkdDO5svJKzfzt7vbvt3a1WrRKA4ZckYXfpeZdpakqnEoCOya7vWgU9gwHnP/7Y74eNvvNOQOvZWgH/+9//0LFjRwBNQXzbtm3xn//8x5JiuHLlSqhUKsybN89y3f7rX/9C165d8e2336JPnz6orq7GDTfcgE6dOgEAunXrZtl/TEwM1Go12rZt67IcY8eOxbPPPovPP/8ct912GwBg0aJF6NevH7KzswEATzzxhGX7zMxMfP/991i1apXXgf3WrVvx66+/Yt++fYiIaFoHaPbs2Vi7di0+//xz3HXXXV7tV2kY2PvZ4Kx4LN1d9vtUlhIDJEcBo9xaGhyZg77mAc7YhftwXu/4x1jKqqT3fnbAYcPAmlH8PVgw1895vQnTlv4mqSdXBPDtkRo8PMT+R9lMyg/xNZ0T7F6bN6Addh6rCYk8al/fTvdm2kxfrTirEoD+mbFercvgSR2ZvxNCQfOxBI6CXCmPO+N2cL7ouFFlfTwlrbTt7A6nOYXrgZym9SysO0pUQtMMSVUNBlRfcDyPvbO7li3RNI+9AJUKENA0j329vqnQ5n+LYtOMTY1GETq1ColRatzUsz1u5zz2ijFo0CBLakplZSUWLFiAiRMnYt26dcjIyMDu3btx5MgRS9BuVl9fj6NHj2Lo0KGYOHEiJkyYgNzcXFxzzTUYOXKk20C+uYSEBNx8881YtGgRbrvtNtTW1mLNmjV47rnnLNssXLgQ//nPf3D8+HFcuHABer0ePXr08Pq97969G3V1dZaGQ/P3FioY2PuZOchrbJqSXlKKhrOAUU5yBEfOgr6Wrkp6TEJA7OjYnqZnuOuNrms0Qm80QSXALvB0lUsdo1PjvQnZDlMKojQC0uJ1qNObYDCKON9oRINB9KqB9fs89irERAjQ6z2bx17K/n19O91doOOofluy4qyApiCpttHo8HiA4NN1GQD5G34aAWiXEIFjlf5tSMqdGticlPNcVW/Aeb3Jr/Olt2SRMKkpXM7uBEm5IzJv83Es/8V5Y3lMT9s7ldb7Mc8A5S69yNG/RVGESqVyO1VuSNBomnrPA3BcT0VHR9tMl9i7d2907twZn3zyCZ588kmYTCb07t0bb7/9tt1rU1Ka0oP/9a9/4d5778XXX3+NlStX4sUXX8TSpUvRr18/j8py++23Y+zYsSgqKsK2bdsAAKNGjQIArFq1Cs888wyeffZZXHnllYiJicFbb72FH3/80en+mt8FBACDVYqUyWRC27ZtsWLFCrvXJiRIXxhQ6RjY+5k5yPt4dS1UpQIg/D7QSqMWUNdokhzQOONNqoSUH80orQoNBpNHvam+XJXU3bE9vQPhqhzO5vYHAI0KGHFZK5epUTE6td3UowAc/kib/23NXQpCWpwOyy6mQDQ//69tOoblhecc1oX5h336NekwLw8/belvjgNrP9xO9zZX3VVPv7vgfd6ozvjkh9MOj3fnf/bLtiquq/f83oRsvL31BNYfrMQFvdHrtAu1ACTHaNG3fTTO1EobROx6XxrE6dQ4WdPYtMLoxXn4BeH3Tgkpa2PIxd3dDYMJsjS2pJJjpW1PUrg8vfMhCAKmDWyHH447byxPG9je6X7M6Rnu0ouk3KUJZYIgeJwSoxSCIEClUuHChQsAgF69emHVqlVo3bq1y7n6e/bsiZ49e+Khhx7CTTfdhOXLl6Nfv37Q6XQwSewkzMnJQWZmJhYvXoytW7di5MiRlnz77777DldeeSXuuecey/buetVTUlJw+vRpy981NTU2g3579eqFM2fOQKPRoEOHDpLKGIwY2AdAjE6Ne/u3g742FUKrZDw1oo8lqPJmHnBAnqXl3eZ7Zic6zPd01/jw5aqkQNOiWc2P7ekdCHe90a56/00ioFULkuvZWTDv7IdS6voAjl4LAFuP1LgMTptSkJp+wGMjNPaBtVrAjT3a+e12uje56u56+l0F786O5+t1GZq/58evzcS8O/qjtLQUtQ0GvP3tCbtF7G7ITsL9g5rKLAgCahsMNulmRrFpvvpVex2vg2GtbawWg7MSsLywzOl1PaRzoiVINo9Zefe7k9hSVA290QiNSoVrOid49D3TEnkD2mH5L2Uwujgt/pwvXe6VtuUKhq2vyZasMUGhp7Gx0RL8VlVV4f3330ddXZ1lesmxY8firbfewl133YUnnngCaWlpOHHiBL744gs88MAD0Ov1+PjjjzF8+HCkpqbi0KFDKCoqwvjx4wEAGRkZKC4uxi+//IJ27dohNjbWks/enCAIuO222/DOO++gsrISs2bNsjzXqVMnLFmyBF9//TUyMzOxdOlS/Pzzzy4D8pycHCxevBjDhw9HQkICXnrpJZvpiXNzc9GvXz9MnjwZTz/9NLp06YJTp07hq6++wk033YQ+ffq0tHoVgYF9oFhueapsvoC9mQdcjl4jwLt8Tyk/EN6kVwDSevujtSqH78+T9AypUxLK1XPraSOsJXc9pASnjUaTXSBgfR0G8na61M+AlOBFymereeOqJXebvOVsbEi9wYTdpXU2eeLvfnfSq9mAzOl9rq5rEbbXteNyGT3+nmmJaK0KiVEanPNi5iRfkPN7oaXcfa80v/45p3x4+vrrr9GzZ9MA89jYWHTt2hXvvfceBg1qmsQjOjoaq1atwt///nfcfffdqK2tRWpqKq655hrExcXhwoUL+O233/DZZ5+hoqICbdu2xT333IPJkycDAEaMGIEvvvgCY8aMQVVVFf71r39h4sSJTsszceJEzJ07F126dMHVV19teXzy5MnYs2cP8vLyIAgCRo8ejbvvvhtfffWV03099NBDKC4uxu233474+Hg88cQTNj32giDg008/xQsvvIDp06fj3LlzaNOmDfr37x/w1YzlJIghnfjm2tmzZ6HXu56z3VOCIEgKgozHSqD/+muoWqdA94c/tOiY8zaXoGC34143T5fy9uSugaeND296jOZtLnHZ2z+uV2tMz033+LVAUw9oUpRG0pSEIz/Y43IaxtYxWqy8p7vb+nDWCFMJTXcenAVH7urB1Tl2tyqqRgWszevl9P1LvaaVRK6gpSX17ilzPT/26U4U7D4r6fPs7tw6Ym7IvnNrV0z65FdJ1zUAvP7Ncdm+Z1rC3XtuG6vFinucD7Brfj17e63I+b3gbP8t7dxp/r0ix51dT/jiu0Or1QY8CCsqKnKZpkLkKzU1NTZjJBxhj33AeDYrjity9hq1JN9Trv1ac9Xb36VNLPIGOu9ld3en4J1bu9rNYe2InD233t669/auB+B5XnIo9OTJVf6W1Ls3RFHE1iNVkj7P3qSbtY3V2TRk3V3XtY1GjF24DwaTCeXnDYronXaV2gc0DaAds2Cvy4C1tsGA1zaVYEtRldcBri/u6HgbeEv5Xskb0E6WO7tEpGwM7ANFpt4LX+YB+yq487RB4CjFYnBWAp4Zczlqys867QmSM7fU23ECzUlthDU/Xy15L1Lykr85XGUpX/OAQkrjJ1RJqXdHny2v7mYdqYZR3IuyWtc98NafZ09mA0qO0lgGWJu5C5Iv6E244GSqWlfl8iV3i+DVG0TLitiOAta6RiMmv/0tDp2ubXGAK9f3grlc3gbeUr5XAPh8hiciCrzw/cUONFGeHvtA5QH7k6PefkEQEBuhQY0Xr/WGHD23UhphFRf0GLNgr8PeOm/fi5S85LI6vV2ahTmgeHdCttPXhQNH9e6oZ7V/ZiwAAd8V10jubXU125Iz1p9nd4G5NY1aZXfNuAuSPeGL7xlH17mlsbWtqTFU4WThOWcB6/xtpTh0plaWAFfOOzre3s2T2rmjpPEAROQ7DOwDxfIj0PIfQjl7jZSuJYFDS14rR++/lEaYubfRzFlvnSfvRRAEaN0c1+jg2rEEFNtKMTfT8TiGcGMO6h0F4yv3lNtt76631dO1FgAgVqdCXaMRMTq15MDc2feAs+vaPB+8VHJ+z7hKRwFg95xKEOBsRRBHAevWI1VO68rVXTNHvL2j44i3gbeU7xWV0LL1RIgoeDCwDxhzj33L9+TvPOBwJUfvvyc9rIB8t8k9Pa718bccqfL6uKHIk2Dc3fnzZrXnovJ65C05aGksmAPLbw5XoaxOb9dIc/c90Py6BoCRH+yRHNjL+T3jKh1l57Gm+3MlFQ0e1Zl1wCqKIgyOWrFWXN01c0TqHR1X+/EkpRKwb9hLWRF7y8V0HGeC/c6uP7GeKFCkXHtc4znQZPiCMP+4j+2VgrQ4HVrHaJEWp8PYXimYzwFRPtGSlJ7MpEjLKrFSWOfIesvZcQU0LUTkisEoOh3HEI48DcadnT9vV3u2biwAvweWK+7pgbV5PXFrb++/B8xpbp7k7mclR7boe8b62nKdjtKAYg+DesA2YBWEpoUAXTHfNSurM1hy9fOWHERdo9Fl2c37NzdOCnaXSd6PlDo3D2Qe+cEejFmwF/M2l1j25ezzbd3oGpwV7/R7J9Tu7PqaIAiSF2EikovJZJIUe7DHPlBkyrE3kyuXnH7ni3p0dOtegIizdQYnyQRNWnqb3FXKwDeHq3C61vm0rxo1e/LMvA3GHZ0/TwNoa85SM2IjNLJ8D3hyh6eu0eRxUO+sR/ubw85nBPKGo4A1p1MCCgrPen3XrHnZ1YJgs0iXt7nyng5kbp7m5S4liHd25dO2bVucOHECcXFxNgsgEfmKyWRCTU0N2rdv73ZbBvaBYunpkT9gYhDmPX/M89y8Efb6N8ddTkcJyHOb3FXjz+UYjU4JLTpuKPE2GHd2/rxNkQLcN/Zacr2Yg8Cj5fUuG5xSytGcs3SbZbvLPLqT5Y6zgHXawHbYfepC0wBaD4L7rUXVyBvQVPbm9bJ0dxm+3F+OT26/1OtceU8HMjdvKLjr3OEKtPKJiopC+/btcfr0actaCES+Yr6T2r59e0RFRbndnoE90UVyreDrCUEQ3Oa+AvLfJrf+0Xfbk+dirYBw5Gkw7irNwVXdNw14dL5fKY09b3vtrYNAd+/V00ansx5tEY4HcUsVpVUhMVLjNmCN0amx/P5BmLP8x4vz2ItQC0BlvcHh7DpmBpOI+dtKnTZ2ahpMuOOTX6HTum74OWsImVfR9mQgs7OGgrPzwTu78omKikLHjh0DXQwiOwzsA8WSihPYYtDvXN1CP1rum3mepaR2qAXg3v5psh7XGnvyPONJz6qUgauWuj9SDREqCDAhp1M89EYRq/ee83i2K7nuOpmDQMDNHR0PG53eDBh2RyUAIy5LxozcDEkBa2yEBjOGZGB6brple3cr2qpVwsVz5FxNownRbq4J64aQs3P10e2XIPpiA8HdQOZGo0lR65QQUWAxsA8UmXPsfSUQvTqB6klyFXCIaApuAMga7EpJ7WgVo/X5IlHsyZPOWUPo6ovz2O8orvGocWSu+4eHCEhNTcWpU6cgiiLqGo3YXVrnUU60L+46yZmbXdtgQOUF5+M5gKaGrAjYHatDYgREACWVDS7L4W2A625mmZxOcdh0WNoMUSoBbhtCUs+Vu+8Hc48+G+BEBDCwDxwFB/b+yDNXwjGtSek5N4m+SctxF1DkdvZvjjuDevfcNYS8bRy1dLVhbwduuiLXHZ26RiOmLf3NZboL0NSQze2c4PBY5vfoiztL7how0wa2l5Q2F6VVoW2czm1DSOq5GpwV73IMjsEErhpLRBYM7ANOWUFUIPLMA3HM5qQOivTF8uucrSK4OQrg5WoceXonxVeri8pxR8ccyLpibsi6Opav7ixJacBc0znB7UB3rdpxrnzzBojUc5U3oB2W/1IGo4t+B64aS0RmDOwDTVlxvU96/JR4TEekDoqUe/l15rj7RksCP1+nI/kiJ9qTRY4CsYKzlNz65g1ZV8fyxflx14DJG9AOX+4vR02D43diTrWRckdH6rmK1qqQGKXBuTqD2215t42IGNgHiKjQVBxf9fgp7ZiO+HKKP3eY4y6PlqR0+TodzFf7N18vUu46BWp1USmBbKRGwDu3dlVMQ9ZRPcXo1Pjk9ktx53/2o7rBdqEpZ3fYnN3R8eRcaRV6XolIeRjYB4oCA3t/9fgF+pjO+HKKP0/wB9o7LUnp8nU6mJT9ezJA2lkjoX9mnFcz6fialEA2Mcr3g8Tl0DpWh4K7u7f4Dpu7sTXW58qTbYkovHHJtEBR4HoWgejxU1ovo7nnfGyvFC6/HmSkpHT54rW+Lltz5kZCwe4ynKppRFmdAadqGlFQWIafTtQiIzHC7tpVwniNwVnxIfOZMn9PFNzdHSvv6Y6Cu7tbFomSKm9AO2QmRUo6V55sS0ThjYF9wDRF9oLCkuwD8eOrxB98/pAGHykpXb54ra/L1pyrRkJJZQP6to/B2F4pSIvToXWMFmlxOoztlYL5fhiE7kqofqa87XQw3yGUcq482ZaIwpvy73uGKoUuUBWIGVqUOCsMB7QGl5akdPk6HcyT/UvhrpGwo7j2Yg+yssZrtPQzpaT3IhdPxtZwHA4RScHAPlAUmGMPBCagVWoQzR/S4NGSlC5fp4PJuX9PGyFKu2Y9/UwFen0Lf/LkXCntvBKRcjCwDzjlfUFHa1Ue/fjKEfQqPYhWWnnIXksGGPp6cKJc+1famJSWkBLUB3p9CyKiYMPAPlDMP/AK+QH2tGfM0+09CdaDISgh5WlJSpev08Hk3H+4zJCilPUtiIiCCQP7gFFOjr2nPWNStw+n2+gUeC1J6fJ1Opic+/d1I0Qpd8yUsr4FEVEwYWAfKArKsfe0Z0zK9nkD2vE2OvldS1K6fJ0OJtf+fdEIUVojXEnrW0ilpLIQUfhiYB8oEmfA8AdPe8akTt3H2+gUSC0JsnwdoClpTIoSc9mDZSyB0hpEREScxz5QLHF94H6YRFH0eCo+qdv7el5wImrS0uDW14tzeUuJ61tYc7VQWN6Sg6hrNAa0fEQUnthjHzCBybF31MN0vtF1oG7dMyalJ00lIOhuo1Nw4bUjH6XmsitxfQtrHNxLRErEwD5QApBj7+yWuyuOesbczcpxTecEbHHTI6+E2+gUXJj2ID8l57IrdX0LM6U2iIgovDGwDzQ//lg662FyxlnPmNSetHCYko/8Q4l54KFA6bnsSl3fQskNIiIKb8yxDxTL4Fn/fem76mECmhamSovToXWMFmlxOoztlYL5DgImc0/a2F4pTrfPG9AOmUmRdjmySrmNTsFFqXngoUDpuexmSgqQld4gckZU0KQNROQb7LEPFD/H9VJ6mGJ0aiybchkA9z+i7nrSlH4bnYIL0x7kZf2ZVXouu1IFy0JhTGEjCi8M7APGvzn2vuxhcvYapd5Gp+DCtAd5OArw+mfGAhBQ12iETi2g0ShCp1YhIUqNa7ISGPy5EAwNIqawEYUfBvaBEoBbooHsYQqmgIsBorIEa9qDkjgL8FbuKbfbtsFgQrRWx6DejWC4K8mZe4jCDwP7ABEDMCtOMPQwBYo3t6vZAPCfYEl7UCpPBs6LYNAnldLvSjKFjSj8KCKwX7duHVavXo3Kykqkp6djypQpuPTSS51uv2XLFqxevRonT55EdHQ0+vTpgzvvvBNxcXF+LHULBWCBqmDoYQoET25XM181MIK5UaqEgM/dwPnmGPR5LtDnuDmmsBGFp4AH9tu2bcPChQsxdepUZGdnY8OGDXjhhRcwb948pKSk2G2/f/9+vPnmm5g8eTL69euH8vJyvPvuu3jnnXfw2GOPBeAdeCswC1QpvYcpEKTerma+auAEW6NUSQ1AKQGeIwz6ghtT2IjCU8AD+zVr1mDYsGG49tprAQBTpkzB7t27sX79ekyaNMlu+4MHD6JNmza4+eabAQBt2rTBddddh9WrV/u13HIJ5Jcqv9CbSL1dzXzVwAqWRqnSGoBSAjxHGPQFP6awEYWfgAb2BoMBRUVFGDVqlM3jvXr1woEDBxy+Jjs7G4sXL8aPP/6Ivn37oqqqCt999x369u3r9Dh6vR56vd7ytyAIiIqKsvxbTub9uduvYP4fQcUfTy9IrWd3RFGE0dGvnhXDxee3HnHTADhSjYeHhN65lKuu5aSksjSXv/2kywbgu9tPYsYQ+wagL+t5cFYCCgrPOgzwHFEJwDVZCYquZ28p8Xr2lWkD2ztPYUuOxLSB7X1aD+FU10RKEdDAvrq6GiaTCQkJCTaPJyQkoLKy0uFrsrOz8Ze//AWvv/469Ho9jEYj+vXrh3vuucfpcVasWIFly5ZZ/u7UqRNefvlltG7dWpb34UhqaqrL52uTknAhNg7RKa0Qk5bms3KEOnf1LEWEbj9Qp3fxvAZpaWkw4VeX+xGhQmpqasj+iMlR1+Fg+7FfXTYAtx2rxVwXn3lf1POsMa2x+9S3OHSm1m1wrxKALm1i8cyYyxEb4b+fCH/fhQmX6/nzh1Lx6roD+N+vp2EwitCoBVx/aVs8Mjzbb+c3XOqaSAkCnooDOG7NO/uCP378OBYsWIBx48ahd+/eqKiowCeffIJ3330X9913n8PXjB49GiNGjLDb99mzZ2EwGGR4B7blTk1NxalTp1yu8qc/dw6G2hpcKC9H9cmTspYhHEitZykGdIhFQeUFp7erB3aIxalTp6ByM/xQgAmnTp1qUVmUSM66DnWiKKKh0fV3SkOjAaWlpXbfcb6u57fHdEb+tlJsOVJlCfD6d4gDBOC74hrLY4M7JSBvYDvUlJ9FjeylsFXXaMT8baXYalWmnE4JmDbQd2MRwvF6zrsyGXlXJts0nvxxfn1R1xqNxqedckTBLqCBfXx8PFQqlV3vfFVVlV0vvtmKFSuQnZ2NW265BQCQmZmJyMhIPPPMM5g4cSKSkpLsXqPVaqHVah3uz1df7KIouty3KIpN42fdbEeuuatnKfIGpGFXSY3TGVfuHZAGURSR08l1vmpOp/iQPpdy1HU4UKtc9zqbn3dWl76q52itCtNz0zE9N91h73jzx3x9rp2PRTiLXSU1Ph+LEK7XcyDec7jWNVEgeD6iSkYajQZZWVkoLCy0ebywsBDZ2dkOX9PQ0GD3g6S6ODAsqL44AjCPPTlmnnFlbK8UpMXp0DpGi7Q4Hcb2SsF8q+Aib0A7ZCZFonncFgxTLpL/DM6Kt7tGzJQyYNGTu6S+ImUwOhEReSbgqTgjRozAG2+8gaysLHTr1g0bNmxAWVkZrr/+egDAokWLUF5ejj//+c8AgH79+mH+/PlYv369JRXnww8/RJcuXZCcnBzIt+IZBvaKImXGlWCbcpECI5jn3PcnLp5ERCS/gAf2AwcORE1NDQoKClBRUYGMjAw8+eSTlhy6iooKlJWVWbYfMmQILly4gC+//BIfffQRYmJi0L17d9xxxx2BegveCcACVSSNq57LYJlykQKHDUD3uHgSEZFvBDywB4Dhw4dj+PDhDp974IEH7B676aabcNNNN/m6WP7B36ygxYCDnGED0DUunkRE5BsBzbEPb0zFIQoHDE4dC4axCEREwYaBfaAwx56IwhgHoxMRyU8RqThhKYgm8CEikhvHIhARyY+BfaBYpuZkjz0RhSeORSAikhdTcQLGnIoT2FIQESkBg3oiopZjYB8ozLEnIiIiIhkxsA8USyYOA3siIiIiajkG9gHHwJ6IiIiIWo6BfYCIzLEnIiIiIhkxsA8U5tgTERERkYwY2AeKyInsiYiIiEg+DOwDxdJhzx57IiIiImo5BvYBwx57IiIiIpIPA/tAYY49EREREcmIgX2gMbAnIiIiIhkwsA8U9tgThT2Rg+iJiEhGmkAXIGzx95woLNU1GpG/vRRbiqphMJmgUakwOCse0wa2D3TRiIgoyDGwDxj22BOFm7pGI/KWHERxeT1MVo8XFJZhV0ktPn8oNWBlIyKi4MdUnEDhLXiisJO/vdQuqAcAkwgUV9Tj1XUHAlIuIiIKDQzsA4U59kRhZ0tRtV1Qb2YSgf/9etqv5SEiotDCwD5QLB32DOyJwoEoijCYnIX1TQxGkQNqiYjIawzsA8bcYx/YUhCRfwiCAI3K9VeuRi1wNWoiIvIaA/tA4484UdgYnBUPlZOPvEoArr+0rX8LREREIYWBfaAwx54o7OQNaIfMpEi74F4lAB2TI/HI8OzAFIyIiEICp7sMFEseLQN7onARo1Mjf3w35G8vxdaiahhMIjQqATkX57GPjdCgJtCFJCKioMXAPlAY1xOFpRidGjNyMzAjt2lArTmnnrn1RETUUkzFCRjOfEEU7hjMExGRnBjYB4jIHHsiIiIikhED+0CxpOL8Hthz/moiIiIi8hZz7APlYhB/QW/Cu5tLsKWoGgaTCRqVCoOz4pE3oB1idOoAF5KIiIiIgoXXgf2JEyewb98+1NTUYNiwYUhMTER5eTliY2Oh0+nkLGPIajSa8Lf/HsWPhlibZeYLCsuwq6QW+eO7MbgnIiIiIkk8DuxNJhPmz5+PTZs2WR7r06cPEhMTkZ+fj06dOmHChAlyljFEifjxeA1O6Bpgiom1ecYkAsUV9cjfXooZuRkBKh8RERERBROPc+yXL1+OrVu34s4778Srr75q81zfvn3x888/y1W20CaKKK5ogNHJfJcmEdhaVO3nQhERERFRsPK4x37Tpk0YO3YsRowYAZPJZPNcmzZtcObMGdkKF9JEESZRdDmPvcEk2sxzTURERETkjMc99uXl5ejWrZvD57RaLerr61tcqLAgAio3AbtaJTCoJyIiIiJJPA7sExISnPbKl5aWIjk5ucWFCg8iMpMinAbuKgEYnBXv5zIRERERUbDyOLDv27cvli9fjvLycstjgiDg/PnzWLt2La644gpZCxiyRBGXp8ehfUIkVM1ie5UAdEyKRN6AdoEpGxEREREFHY9z7MePH4+ffvoJM2bMQPfu3QEAn376KUpKSqBWqzFu3DjZCxmSRECnVuGlG7Pw7iE9thZVw2ASoVEJyOE89kTkJY7LISIKXx4H9omJiXjxxRexZMkS/PTTT1CpVCguLsbll1+OCRMmIDY21v1OyCJap8aM3FTMyOUPMhF5p67RiPztpVzojogozHm1QFViYiLy8vLkLkt4ubjyLKwCeQb1ROSpukYj8pYcRHF5PRe6I0nYiUQUurxeeZZayj6wJyLyVP72UrugHuBCd2SLd3WIwoPHgf3bb7/t8nlBEHDfffd5XaCwYe6xJyJqgS1F1XZBvZl5obsZuX4tEikM7+oQhQ+PA/u9e/faPVZbW4v6+npER0cjJiZGloKFPHNczx57IvKSKIowmJyF9U0CtdCdEtI9lFAGJeBdHaLw4XFg/9Zbbzl8fM+ePXjvvffw8MMPt7hQYYE99kTUQoIgQKNyPWuxPxe6U0K6hxLKoDS8q0MUPjyex96ZHj164MYbb8SCBQvk2mWIY449EbXc4Kx4u7UwzPy50J053aNgdxlO1TSirM6AUzWNKCgsQ96Sg6hrNIZFGZTGk7s6RBT8ZAvsASA9PR2HDh2Sc5ehy8GsOEREnsob0A6ZSYFf6E5Kukc4lEFplHZXh4h8S9bAft++fYiP90/vUMjglykRtUCMTo388d0wtlcK0uJ0aB2jRVqcDmN7pWC+HwdFSkn3CIcyKJFS7uoQke95nGO/bNkyu8f0ej2Ki4vx888/45ZbbpGlYKGOdz2JSC4xOjVm5GYEbKE7JQziZcqJc3kD2mFXSS2KK+phsnr7/r6rQ0S+53Fgv3TpUvudaDRo06YNxo8fz8BeMqbiEJH8ApFSoYR0DyWUQanMd3Xyt5dia1E1DCYRGpWAnDAfVEwUijwO7D/77DNflCP8hGGvERGFrsFZ8SgoLLPpETbzV7qHEsqgVIG+q0NE/iFrjj15wNJhzy9XV8Lxtrk7/qoT1j15QgmDeJVQhmDA3x2i0OVxjz3JhLPiOMV5qO35q04cHycBs8a0lu0YFJqUkO6hhDIQEQWSIErolpswYYL0HQoCFi9e3KJC+cvZs2eh1+tl3acgCEhLS8PJkydd9njWf/wxYDQhYtxYCLGxspYhmDlb+lwlAJlJkZalz6XWcyiQWie+PE6XNrF4e0xnRGt5k89XQu2aVkK6h6MyhFo9K5kv6lqr1aJ1a3Y0EDkjqcd+7NixAf+CDlmsVxtc+tyev+rE1XEOnalF/rZSTM9Nb/FxKDwo4TdDCWUgIvInSYH9+PHjfV2O8MNUHIe49Lk9f9WJu+NsOVLFwJ7IC0q4e0FE4YE59oFivivJL3sLJcyFrTT+qhNJxzGGV90TtQTHChFRIHgd2B87dgwnTpxAY2Oj3XO5uWHWpeoN5nba4TzU9vxVJ1KOo1GHV90TecvZeJWCwjLsKqmVbVwMEVFzHgf2DQ0NmDt3Lvbs2eN0Gwb2rnHAlnOch9qev+rE7XE6JchyHKJQx7FCRBQoHk9xUVBQgDNnzuDZZ58FADzyyCP429/+hquvvhppaWl4+eWX5S5j6LEO7NkDaoPzUNvzV524Ok6XNrHIGxh+dU/kDSnjYoiIfMHjwP7777/HyJEjkZ2dDQBISUlBz5498fDDD6NTp05Yv3697IUMOQzsnTLPQz22VwrS4nRoHaNFWpwOY3ulYH6Y3r72V504O864Xq2x/P5BYVn3RJ7yZFwMEZHcPE7FOXv2LNq3bw/VxXxc6xz7wYMH49///jfy8vLkK2Go8zCwD4fBi1z63J6/6sTRcQRBQGyEBjU+OSJRaOFYISIKJI8D+5iYGDQ0NAAAEhIScPLkSVxyySUAAIPBYHnOE+vWrcPq1atRWVmJ9PR0TJkyBZdeeqnDbd966y1s3rzZ7vH09HS89tprHh87GITz7Ar88bPnrzph3RN5h2OFiChQPA7sO3TogNLSUvTp0wfdu3fHihUrkJaWBo1Gg4KCAmRmZnq0v23btmHhwoWYOnUqsrOzsWHDBrzwwguYN28eUlJS7La/++67cfvtt1v+NhqNeOyxx9C/f39P30rgeJCKw9kViIiCS96AdthVUoviinqb4D6cxwoRkX94nGM/dOhQ1NfXAwBuu+02NDQ0YNasWXjqqadw9uxZ3HXXXR7tb82aNRg2bBiuvfZaS299SkqK01z96OhoJCYmWv47fPgw6urqMHToUE/fSuB4ENhLmV2BiIiUg2OFiChQJPXYL1y4EMOGDUOHDh0wcOBAy+Nt2rTBP//5T+zZsweCICA7OxuxsbGSD24wGFBUVIRRo0bZPN6rVy8cOHBA0j6+/vpr9OzZE61bt3a6jV6vh16vt/wtCAKioqIs/5aTeX9u93vxaUGlcrnt1iNuZlc4Uo2Hh4RfyoTkeqYWY137B+vZP/xVz7ERGjw8pAMeHhK+Y4V4TRP5n6TAfu3atVi7di2ysrIwbNgwDBo0CNHR0QCAyMhI9OvXz6uDV1dXw2QyISHBdn7shIQEVFZWun19RUUFfv75Z/zlL39xud2KFSuwbNkyy9+dOnXCyy+/7LIx0FKpqalOnzM1NOBcbBwAICU1FYLG8WkQRREm7HN5HBEqpKamhu0Xp6t6Jnmxrv2D9ewfrGf/YV0T+Y+kwP6f//wnvv76a2zZsgXvvfcePvroI1x99dUYNmwYLrvsshYXwlFQKiVQ3bRpE2JiYnDVVVe53G706NEYMWKE3b7Pnj0Lg8HgYWldEwQBqampOHXqlNPpzMSGBtTXNs0xoj91CoLa+W1ZldP++ovHgwmnTp3yvsBBSko9kzxY1/7BevYP1rP/+KKuNRqNTzvliIKdpMA+NTUVkyZNwsSJE7F7925s3LgR27dvx5YtW9CmTRsMGzYMubm5SE5O9ujg8fHxUKlUdr3zVVVVdr34zYmiiI0bN2Lw4MHQOOnxNtNqtdBqtU734wui6HyeYtFkAi4+JTZt7HQ/OZ1cz66Q0yk+rH+cXNUzyYt17R+sZ/9gPfsP65rIfzyaFUelUqFv377o27cvamtrsWXLFmzatAmLFy/GkiVL0KtXLwwbNgxXX321tINrNMjKykJhYaFNr3thYSGuvPJKl6/dt28fTp06hWHDhnnyFpTBg8GznF2BiIiIiKTweLpLs9jYWNx000246aabUFxcjHXr1uGrr77C7t27sXjxYsn7GTFiBN544w1kZWWhW7du2LBhA8rKynD99dcDABYtWoTy8nL8+c9/tnnd119/ja5du6JDhw7evgVFcJdyZJ5dIX97KbYWVcNgEqFRCcgJk3nsiYiIiEgarwN7s6KiImzcuBHfffcdgKb0Gk8MHDgQNTU1KCgoQEVFBTIyMvDkk09acugqKipQVlZm85rz589jx44dmDJlSkuLH1gSB7xyJVYiIiIicserwL6mpgZbtmzBxo0bcezYMahUKvTu3RvDhg3DFVdc4fH+hg8fjuHDhzt87oEHHrB7LDo6Gp988onHx1EMcyqOF/E5g3oiIiIickRyYC+KIn766Sds2rQJP/zwAwwGA9q2bYuJEydiyJAhSEpK8mU5Q4slsGeQTkRERETykBTYL1q0CN988w0qKiqg0+kwYMAA2aa6DEucHYCIiIiIZCYpsF+1ahWysrIwZswY5OTkWBanopZijz0RERERyUNSYD937lxkZmb6uiwhxzxvr11evChChMhMHCIiIiKSjaTAnkG9dHWNRjy14hes+PE46g1Nq8ZGalS4ITsJd1+VigU7T+LbX05g+IFTMKg0WF2zGzdkJ+GBnPacupKIiIiIvNbi6S7pd3WNRkz97ACKKxpsHj+vN2HlnnNYs/ccDCIQqxchioDJ6rmfTtTivQnZDO6JKGRwel4iIv9iYC+j/O2llqA+wtCAKEOjw+1i9BcAAKJVjn1xRQPyt5diRm6G7wtKROQjdY1G5G8vxZaiahhMJmhUKgzmgnohj404ImVgYC+jLUXVAIDYxvO4pWgrBA9nv9laVI0Zub4oGRGR79U1GpG35CCKy+thsnq8oLAMu0pqkT++G4P7EMJGHJHyMLCXiSiK0BuNAICExjoIoghRENCg1jl9zeGEdjZ/G0wm9noQUdDK315qF9QDgEkEiivqeVcyhEhpxMVGMMQg8jevP3Xnz5/HwYMHUVNTg759+yI2NlbOcgUdQRCgVasBGC2PlUfG48uO/SXvQ61SMagnoqC1pajaLqg3M4m8KxlKpDTiHh7SISBlIwpnKm9etGzZMkybNg0vvvgi3nzzTZw5cwYAMGfOHKxcuVLO8gWVwVnxTf/wcgEqy+uJiIKMKIowmJyF9U0MJtEyDTAFNymNOCLyP48D+3Xr1mHZsmUYOnQoZs6cafPc5Zdfjh9//FG2wgWbvAHtkJkUYRkSKzZbgErjojO+Y1IE8ga0c74BEZGCCYIAjcr1T4paJfCuZAhgI45IuTwO7L/88kuMGDEC99xzD3r37m3zXFpaGk6ePClb4YJNjE6N9ydeglt6pUGrFiAIgEoAorUqjOrRCgV3d8eoHsmI1qqgavbcu5zqkoiC3OCseKicxO0qgXclQwUbcUTK5XGO/ZkzZ+wCerOoqCicP3++xYUKZjE6NR66ritOlP8EIaU1nr65j82X2+PDMvH4sEznq9ISEQWpvAHtsKukFsUV9TBZddaqBKBjUiTvSoaQwVnxKCgssznPZmzEEQWOxz320dHRqKqqcvjcmTNnEB/PD7OZ4KLHQhDYm0FEoSVGp0b++G4Y2ysFaXE6tI7RIi1Oh7G9UjCfU12GlKbU00i7OzRsxBEFlsc99j169MCqVavQr18/6HRNUzkKggCj0Yj//e9/Tnvzw4olr5CBOxGFlxidGjNyMzAjl4sWhTJzIy5/eym2FlXDYBKhUQnI4Tz2RAHlcWA/YcIEPPnkk3j44Ydx1VVXAWjKuz969CjKysowY8YM2QsZdDhgiIiIQX2IYyOOSHk8TsVJTU3F3//+d7Rv3x7r1q0DAHzzzTeIi4vD7NmzkZKSInshgxa/44iIKAwwqCdSBq8WqEpPT8dTTz0FvV6PmpoaxMbGWtJyCJzii4iIiIj8zuMe+x9++AGmi/PXarVaJCcnM6hvzpJizx4MT7FRREREROQdj3vs586di4SEBFxzzTUYMmQI0tPTfVGu0MDAXpK6RiPyt5diS1E1DCYTNCoVBnMAlleY50pERBS+PA7sZ86ciU2bNmHt2rX4/PPP0aVLFwwdOhSDBg1CVFSUL8oYhNjrLFVdoxF5Sw6iuLzeZnnygsIy7CqpRT6nyHOLDSMiIiICvAjs+/bti759+6Kurg5bt27F5s2b8e677+LDDz/EVVddhaFDh6JHjx6+KGvw4HSXkuVvL7UL6gHAJALFFfXI316KGbkZASlbMGDDiIiIiMw8zrE3i4mJwfDhw/HCCy/g1VdfxfDhw1FYWIjnnntOzvIFN8b1bm0pqrYL6s1MIrC1qNqv5Qk2UhpGREREFB68DuzNRFHEuXPnUFZWhvPnz3PwI8B57CUSRREGk7OwvonBJPKacoENIyIiIjLzarpLADh16hQ2bdqEzZs3o7y8HMnJyRgxYgSGDh0qZ/mCGwcxuiQIAjQq121LtUrgYFAnPGkYsQ6JQosSPtdKKAMR2fI4sN+4cSM2bdqE/fv3Q6PRoF+/fhg6dCh69eoFlZsgLWyYe5j5hefW4Kx4FBSWweSgU14lND1PjrFhRBRelDBQXgllICLnPA7s33nnHXTs2BF33303cnJyEBsb64tyBTWmjkiXN6AddpXUorii3ia4VwlAx6RI5A1oF7jCBQE2jIjCgxIGyiuhDETkmlfz2GdmZvqiLCGIPaXuxOjUyB/fDfnbS7G1qBoGkwiNSkAOe4AkYcOIKDwoYQYxJZSBiFzzOLBnUC8BO+w9EqNTY0ZuBmbkMmfTU2wYEYUHKQPlZ+SGfhmIyDVJgf2yZcswbNgwJCcnY9myZW63HzduXIsLFtQsOfaBLUYwYlDvOTaMiEKbEgbKK6EMROSepMB+6dKl6NOnD5KTk7F06VK324d9YG/GLzfyM/6gEoUeJQyUV0IZiMg9SYH9Z5995vDf5AxzcYiISD5KGCivhDIQkWucn9IXON0lERHJKG9AO2QmRULV7GfFnwPllVAGInLN48B+woQJOHTokMPnioqKMGHChBYXKlQITLInIiIZmAfKj+2VgrQ4HVrHaJEWp8PYXimY76dpJpVQBiJyzeuVZx0xmUzMrwN+77EnIiKSiRIGyiuhDETknKypOEVFRYiOjpZzl8GN33dEROQDSgiolVAGIrIlqcf+v//9L/773/9a/v7HP/4BrVZrs01jYyOqqqrQv39/eUsYjJhjT0RERER+Jimwj4+PR3p6OgDg7NmzaNu2rV3PvFarRYcOHXDzzTfLX8ogIzIVh4iIiIj8TFJgn5OTg5ycHADA7NmzMXXqVLRv396nBQsN7LEnIiIiIv/wePDsrFmzfFGO0MIOeyIiIiLyM48Hz27cuBFLlixx+NySJUuwefPmFhcq+DHHnoiIiIj8y+PAfu3atYiNjXX4XHx8PNauXdviQoUMhcT1zPknIiIiCn0ep+KcOnUKGRkZDp9LT0/HyZMnW1yooKeAQLqu0Yj87aXYUlQNg8kEjUqFwVnxyBvQjouIEBEREYUgrxaoOn/+vNPHTSZTiwoUEgI83WVdoxF5Sw6iuLwe1mejoLAMu0pqkc8VAomIiIhCjsepOB06dMC3337r8LmtW7eiQ4cOLS5U6AhMYJ+/vdQuqAcAkwgUV9Qjf3tpQMpFRERERL7jcWB/4403YseOHXjzzTfx22+/oby8HL/99hveeust7NixAzfeeKMvyhlcApyKs6Wo2i6oNzOJwNaiar+Wh4iIiIh8z+NUnJycHJw4cQIrV67Eli1bLI+rVCqMHTsWgwcPlrWAwcgyWDUAHfaiKMLgJh3KYBIhiiKXAyciIiIKIV7l2E+YMAFDhw5FYWEhqqurER8fj969e6N169Zyly+4BSBwFgQBGpXrGzFqlcCgnoiIiCjEeBXYA0CbNm1w3XXXyVmW0BHgSXEGZ8WjoLAMJgflUAlNzxMRERFRaPEqsNfr9di0aRP27t2L2tpa/PGPf0RaWhq+//57dOjQAW3btpW7nMEpQL3ieQPaYVdJLYor6m2Ce5UAdEyKRN6AdgEpFxERERH5jseBfXV1NWbPno3jx48jMTERlZWVuHDhAgDg+++/x+7duzF16lTZCxpUAjx4NkanRv74bsjfXoqtRdUwmERoVAJyOI89ERERUcjyOLD/5JNPcP78ebz44ovIzMzEpEmTLM91794dq1atkrWAwckc2Acujz1Gp8aM3AzMyAUHyhIRERGFAY+nu/zxxx8xfvx4ZGVl2QWLrVq1wrlz52QrXNBTSCzNoJ6IiIgo9Hkc2F+4cMHp7DcGg4ErzwIBT8UhIgoVIr9PiYgk8zgVp02bNjh48CB69Ohh99yhQ4fQrh0HZloCe/aUExF5rK7RiPztpdhSVA2DyQSNSoXBHCNEROSWxz32OTk5WLVqFb7//ntLT4ogCDh06BDWrl3LBaqsMbAnIvJIXaMReUsOomB3GU7VNKKszoBTNY0oKCxD3pKDqGs0BrqIRESK5XGP/ciRI3HgwAG88soriImJAQA8//zzqKmpQZ8+fXDzzTfLXshgw1vHRETeyd9eiuLyejRP6jSJQHFFPfK3l2JGbkZAykZEpHQeB/YajQZPPvkktm3bhh9//BFVVVWIi4vDFVdcgYEDB0LlZtXTsGCJ69ljT0TkiS1F1XZBvZlJBLYWVWNGrl+LREQUNLxaoEoQBAwaNAiDBg2SuzyhhXE9EZFkoijC4GYCBoNJ5BS+REROsHvdF5iKQ0TkMUEQoHFz11etEhjUExE5IanHfvbs2Zg6dSrat2+P2bNnu9xWEATExsYiOzsbN9xwA7Rardv9r1u3DqtXr0ZlZSXS09MxZcoUXHrppU631+v1WLZsGbZs2YLKykq0atUKo0ePxrBhw6S8Hb/hjw8RkWcGZ8WjoLAMJgf9Iyqh6XkiInLM41Qcd7dARVHE6dOn8f3336OkpAR/+tOfXO5v27ZtWLhwIaZOnYrs7Gxs2LABL7zwAubNm4eUlBSHr5k3bx6qqqrwpz/9CampqaiurobRqKSZEthjT0TkjbwB7bCrpBbFFfU2wb1KADomRSJvAKdUJiJyRlJgP2vWLMu/n332WUk7/vrrr7Fo0SK3261ZswbDhg3DtddeCwCYMmUKdu/ejfXr12PSpEl22//888/Yt28f3nzzTcTGxgJomltfUTiPPRGRV2J0auSP74b87aXYWlQNg0mERiUgh/PYExG55dXgWSkuvfRSXH755S63MRgMKCoqwqhRo2we79WrFw4cOODwNbt27ULnzp2xatUqfPPNN4iMjMQVV1yBiRMnQqfTyVV8mTCwJyLyVIxOjRm5GZiR6/4uMRER/c6rwN5kMmHbtm3Yu3cvampqEBcXh+7du2PAgAFQq5t6U9LS0nD//fe73E91dTVMJhMSEhJsHk9ISEBlZaXD15w+fRr79++HVqvFY489hurqarz//vuora11ejy9Xg+9Xm/5WxAEREVFWf4tJ0EQAFGEAAEQmGfvK+Z6Zf36HuvaP1jPjvnkO9oH+yV7rGsi//M4sK+ursYLL7yAI0eOQKVSIS4uDjU1Nfj666/x+eef46mnnkJ8vGeDmxx96J19EZgXf/rLX/6C6OhoAE2B+2uvvYapU6c67LVfsWIFli1bZvm7U6dOePnll9G6dWuPyilVzf79iI2NRXSrFMSkpfnkGNQkNTU10EUIG6xr/2A9+wfr2X9Y10T+43Fg/+GHH6K0tBQPPvigZUEqcw/+u+++iw8//BAPPvigpH3Fx8dDpVLZ9c5XVVXZ9eKbJSYmIjk52RLUA0D79u0hiiLOnTuHNAeB9OjRozFixAjL3+ZGw9mzZ2EwGCSVVSpBEBADoLa2FhcqylF98qSs+6cmgiAgNTUVp06d4kq/Psa69g/Ws3+wnv3HF3Wt0Wh81ilHFAo8Dux/+OEHTJw4ETk5OZbHVCoVcnJyUFVVhaVLl0o/uEaDrKwsFBYW4qqrrrI8XlhYiCuvvNLhay655BJ89913qK+vR2RkJADg5MmTEAQBrVq1cvgarVbrdNpNn3yxiyJEiE3/zx8OnxJZx37DuvYP1rN/sJ79h3VN5D8eL1AliiLS09MdPpeRkeHxh3fEiBH46quv8PXXX+P48eNYuHAhysrKcP311wMAFi1ahDfffNOyfU5ODuLi4vD222/j+PHj2LdvHz755BMMHTpUOYNnzVXAvEIiIiIi8hOPe+x79uyJX375Bb169bJ7rrCwEN27d/dofwMHDkRNTQ0KCgpQUVGBjIwMPPnkk5ZbbRUVFSgrK7NsHxkZib/97W/44IMPMHPmTMTFxWHAgAGYOHGip2/FDxjYExEREZF/SArsa2trLf8eN24cXnnlFZhMJuTk5CAxMRGVlZXYsmULdu7ciUcffdTjQgwfPhzDhw93+NwDDzxg91j79u3x9NNPe3wc/+EtRyJqOU71SEREnpAU2P/xj3+0e2zNmjVYs2aN3eNPPPEEPvvss5aXLJhZFqgKbDGIKPjUNRoxf9sJbCmqhsFkgkalwmAuzkRERBJICuzHjh3LXiNvsM6IyAO1DQbc+9kBFJfXw2T1eEFhGXaV1CJ/fDcG90RE5JSkwH78+PG+Lkdo4eh/IvLCK+vsg3oAMIlAcUU98reXYkZuRkDKRkREyufxrDhAU95ndXU1ampqOIWVK+yxJyIPbPj1tF1Qb2YSga1F1X4tDxERBRePZsU5ePAgVq5ciT179qChoQEAEBERgR49emD06NHo2rWrTwoZbNjYISJPiaIIvdH1d4fBJHJALREROSU5sF+3bh0WLlwIAMjKyrJMR3n27Fn89NNP+OmnnzBlyhSns9uEFUtgzx9fIpJGEARo1a6/M9QqgUE9ERE5JSmwP3jwIBYsWIC+ffti6tSpdiu8njt3Du+++y4WLlyIzp07o0uXLj4pbNAJ4t/fQPcKBvr4RIFw3aVt8dH2ozA56LhXCcDgrHj/F4qIiIKGpMB+zZo16Nq1Kx577DGoVPZp+a1atcLjjz+OWbNmYfXq1Xj44YdlL2hQ8TITJ9DBbF2jEfnbSwM2zV6gj08UaI8Oz8bm/adQXFFvE9yrBKBjUiTyBrQLXOGIiEjxJAX2+/fvx1133eUwqDdTqVS44YYb8PHHH8tWuKBlmcfefZCulGC2rtGIvCUHAzbNXqCPT6QEsREavDshG/O3ncDWomoYTCI0KgE5bOASEZEEkleeTUlJcbtd69atbVapDXtuAnslBbP520sDOs1eoI9PpBQxOjVm5GZgRm7g7+IREVFwkTTdZVxcHM6ePet2u7KyMsTFxbW4UMFPWi6OlGDWX7YUVQd0mr1AH59IiRjUExGRJyQF9tnZ2Vi/fj1MJmehF2AymfDll1/ikksuka1wQUvirDhKCWZFUYTBxbkFfp9mLxSPT0RERBQKJAX2I0aMwG+//YZXXnkFFRUVds+Xl5fjlVdeweHDh/F///d/shcyaLmI65UUzAqCAI2L8ROAb6fZC/TxKbSwAUhEROFKUo59t27dMHnyZHz44Ye4//770blzZ7Rp0wYAcObMGRw+fBiiKGLKlCmc6hKw6rF3TmnB7OCseBQUlgVsmr1AH5+Cm1IGoRNZ4xgJIvI3yQtU3XTTTejUqRNWrlyJvXv34rfffgMA6HQ69O7dG6NHj0Z2drbPChqU3HyhKymYzRvQDrtKagM2zV6gj0/BS0mD0InYyCSiQJIc2APAJZdcgpkzZ8JkMqGmpgZA08BaV9NghiOpqQBKCmZjdGrkj++G/O2lAZlmL9DHp+DFGZVIKdjIJKJA8yiwN1OpVEhISJC7LKHDMo296x57pQWzgZ5mL9DHp+AkZRD6jFy/FonClJIamfwOJQpPXgX2JB+lBrOBLkegj0/BwZNB6LymyNcC3chkGhARMbD3BS9n5WDgQeQZpQ1Cp/AV6EYm04CICJA43SV5ypKLE9hiEIWBwVnxUDn5qHFGJfKXQDcylbTgIREFDgN7X2JgHxKCYV70YCijr+QNaIfMpEi74J4zKpG/BbKRqZQFD4kosJiK4wthHGSFimDIVQ2GMvqD0gahByOOQZBHoGY6C3QaEBEpBwN7XxCZihPMgiFXNRjK6E9KHYSuZGwYyi9QjcxApwERkXIwsCdqRklT1jkTDGUMFAYv7rFh6DuBamQqacFDIgoc5tj7QDjnO4eCYMhVDYYyknJxoKV/+LORybEmRAQwsPcNc1zPnsOg40muaqAEQxlJ2dgwDD3mNKCxvVKQFqdD6xgt0uJ0GNsrBfN5B4YobDAVh8hKMOSqBkMZSbk40DJ0cawJEbHH3hc4eDaoBcO86MFQRlImNgzDA88fUXhiYO9T/GINRsGQqxoMZSTHlJAixYYhEVFoYiqOLyjgh5u8FwzzogdDGel3SptaMlDzrRMRkW8xsPcJcypOYEtB3guGXNVgKCMpc2pJNgyJiEITA3tfYqAVEoIhYA6GMoYrpa45wIah/7GeicjXGNj7AlNxiOgiKVNLzsj1a5HsMNj0HaWlYRFRaGNg7wMiZ8UhInBqyXCnxDQsIgptnBXHp/hDTRTOOLVkeOMKv0TkbwzsfYGpOER0EaeWDF9c4ZeI/I2BvS+Y43p2whGFPa45EJ48ScMiIpILc+yJiHyIU0uGJ6ZhEVEgMLD3BQ6eJSIrnFoyPA3OikdBYZnNImBmTMMiIl9gKo4v8cebiJphUB8+mIZFRP7GHnufYM4kEVG4YxoWEfkbA3tfEDl6loiImIZFRP7FVBwf4vc3ERGZMagnIl9jYO8DnL6MiIiIiPyNgb0vcFYcIiIiIvIzBva+xMCeiIiIiPyEgb0vMBOHiIiIiPyMgb0vcFYcIiIiIvIzBvZERERERCGAgb0vWAbPBrYYRERERBQ+GNj7EgfPEhEREZGfMLD3CY6eJSIiIiL/YmDvC5zHnoiIiIj8jIE9EREREVEIYGDvA6LIVBwiIiIi8i8G9r5gmcaeqThERMGGnTNEFKw0gS5ASGNgT0QUFOoajcjfXootRdUwmEzQqFQYnBWPvAHtEKNTB7p4RESSMLD3Bfb2EBEFjbpGI/KWHERxeT1MVo8XFJZhV0kt8sd3Y3BPREGBqTi+wFlxiIiCRv72UrugHgBMIlBcUY/87aUBKRcRkacY2BMRUVjbUlRtF9SbmURga1G1X8tDROQtBvY+wVQcIqJgIIoiDCZnYX0Tg0nkgFoiCgoM7H2BqThEREFBEARoVK5/CtUqAQK/z4koCChi8Oy6deuwevVqVFZWIj09HVOmTMGll17qcNu9e/di9uzZdo/PmzcP7du393VRiYgoxAzOikdBYRlMDjrlVULT80REwSDggf22bduwcOFCTJ06FdnZ2diwYQNeeOEFzJs3DykpKU5f9/rrryM6Otryd3y8cr54RfbYExEFjbwB7bCrpBbFFfU2wb1KADomRSJvQLvAFY6IyAMBT8VZs2YNhg0bhmuvvdbSW5+SkoL169e7fF1CQgISExMt/6nc3EoNCAb2RESKF6NTI398N4ztlYK0OB1ax2iRFqfD2F4pmM+pLokoiAS0x95gMKCoqAijRo2yebxXr144cOCAy9c+/vjj0Ov1SE9Px5gxY9CjRw+n2+r1euj1esvfgiAgKirK8m85CYIAiICAppxMR/sXRVEx+ZpKKosnzGUOxrIHG9a1f7Ce/cNZPcdGaPDwkA54eEjwfi8qDa9pIv8LaGBfXV0Nk8mEhIQEm8cTEhJQWVnp8DVJSUnIy8tDVlYWDAYDvvnmG/z973/HrFmzcNlllzl8zYoVK7Bs2TLL3506dcLLL7+M1q1by/ZerJWJImJjY5HUti00SUkAgNoGA15ZdwAbfj0NvVGEVi3gukvb4tHh2YiN8O9pUFJZWio1NTXQRQgbrGv/YD37B+vZf1jXRP6jiCjOUWveWQu/Xbt2aNfu93zHbt26oaysDJ9//rnTwH706NEYMWKE3b7Pnj0Lg8HQkqLbEQQBGgC1tbVoPHMGqvp61DUace9nB+wWQPlo+1Fs3n8K707I9tutXiWVpSUEQUBqaipOnTrFaeh8jHXtH6xn/2A9+48v6lqj0fisU44oFAQ0sI+Pj4dKpbLrna+qqrLrxXelW7du2LJli9PntVottFqtw+d88sUuihDRNO+xKIqYv+2Ey1UN5287gRm5GfKXwwEllUUO5jom32Nd+wfr2T9Yz/7Duibyn4COONVoNMjKykJhYaHN44WFhcjOzpa8nyNHjiAxMVHm0rVE0xeY+c6AklY1VFJZiIiIiEg+AU/FGTFiBN544w1kZWWhW7du2LBhA8rKynD99dcDABYtWoTy8nL8+c9/BgB88cUXaN26NTIyMmAwGLBlyxbs2LEDjzzySCDfhmOC4NGqhr4eYKSkshARERGRvAIe2A8cOBA1NTUoKChARUUFMjIy8OSTT1py6CoqKlBWVmbZ3mAw4OOPP0Z5eTl0Oh0yMjIwc+ZMXH755YF6C/asbjkqaVVDJZWFiIiIiOQV8MAeAIYPH47hw4c7fO6BBx6w+XvkyJEYOXKkP4rltea5hEpa1VBJZSEiIiIi+ShwVafQkzegHTKTIqFq1hEeiFUNlVQWIiIiIpKPInrsQ465x/5iSot5VcP87aXYWlQNg0mERiUgJyseeQPa+XV6SSWVhYiIiIjkw8Del6xy1WN0aszIzcCM3MCvaqikshARERGRPJiK4wtuputVUiCtpLL4A+dSJiIiolDFHntfaJaKQ4FV12hE/vZSbCmqhsFkgkalwmCmHlEL8E5X6FH6OVV6+YhIGRjYy4w9wspS12hE3pKDdqvtFhSWYVdJLfLHd2NwrxBKD1zYQAw9Sj+nSi8fESkPA3sKafnbS+2CeqBpld3iinrkby/FjNyMgJSNgidwYQMx9Cj9nCq9fESkTMyxl5t1j72Cex/DxZaiarug3swkAluLqv1aHvqdOXAp2F2GUzWNKKsz4FRNIwoKy5C35CDqGo2BLqKFlAYiBZf525R9TnnNEZE3GNj7EgP7gBJFEQaTs7C+icEkMn0qQIIpcGEDUR5K+qxtPVKl6HMaTNecks4rUbhjKo7c+AWnGIIgQKNy3XZVqwRF53WHMimBy4xcvxbJIU8aiLyW7Ckx3UoURRiMrr+rA3lOg+GaU+J5JSIG9r7FH/mAG5wVj4LCMpgc/IarhKbnyf+CIXAxYwPRe0rNExcEARq16/MVyHOq9GtOqeeViJiKQyEub0A7ZCZFQtXs908lAB2TIpE3oF1gChbmlB64NDc4K97uGjJjA9E5Jadb5XRKUPQ5VfI1p+TzShTuGNjLjak4ihKjUyN/fDeM7ZWCtDgdWsdokRanw9heKZjPXqWAUnLg0hwbiN5Rcp74tIHKPqdKvuaUfF6Jwh1TceTGWXEUJ0anxozcDMzIVf5c6eEkb0A77CqpRXFFvU2qlBICl+bMDcT87aXYWlQNg0mERiUghznFTik93Urp51Sp5VP6eSUKdwzsKazwh0Y5lBq4OMMGomeCId1K6edUieULhvNKFM4Y2MuNPfZEkikxcJEiWMoZaME0eF3p51RJ5Qum80oUbphj70sK+iImUjolBS4kDyXniZP3eF6JlIs99kRE5BPBlm5F0vC8EikXA3u5cVYcIlKYQKY5BWu6FbnG80qkTAzsfYlfdEQUIEpcGZTBX2jieSVSDgb2cnPQY28ymaByM4tAILWkt0Wunhr2+BDJhyuDEhGFJwb2crsY2J9vNCJv0X4UVTRAFJs677OSI/HqyM5oHasLcCFb1psnV0+gEnsUSdnYAJRGysqgM3IzZDmW3OeEnQVERN5jYO8DtQ0GLPnpLA5l11vScUQROHSuHuMW7sOyKZcFNLhvSW+eXD2B7FEkqdgA9JyUlUFn5Hq/f7nPCTsLiIjkodz8kCC26qcTMJq76ZvRm0Q8supwAEr1Oym9eb54rS/2Q6HN3AAs2F2GUzWNKKsz4FRNIwoKy5C35CDqGo2BLqLieLIyqDfkPidy7Y/XChERA3v5iSLO1TVCdHELuKi83o8FsielN88Xr/XFfii0sQHoOV+vDCr3OWFnARGRfBjYy8xkMkGEm54wsWm7QPC0N8+6V0/Ka/VGk11PoKO/Pe1R9LZ3MVjI/f6k7k/KdoGsezYAvTM4K95u8SCzlq4MKvc5YWcBEZF8mGMvM5UgQICbnjABLmfJ8fWgL3e9eYIAvP7NcYd5qu5eW37BgLEL96F/ZiwAAd8V1zjMdZXSo3heb3KaLxsbEfyXrqf5wO6uC6n7k7Kd9TZGk4gI3X4M6BCLvAFpPs1Vtn6PnjQAOUjSVt6AdthVUoviinqYrNplLV0ZVOo5sZ4JzNX58fYcO/rbl+lHwc6bz4i7Om/JvonId4I/OlKgVjE6nK7TO30+KznS7jFfDvqy3nej0YiqC85zTQUA9XoTCnaXORzU2j8zDqv3nrMJFqyZROBUTSNW7im3e856YOzgrHgUFJY53I9KAPpnxrocXPvuhGxJ712ppA4e9iRYl7o/d9sBsN+mTo+CygvYVVIj+8BmV+/RlyklocxXK4NKSfM5d16PWz7Ygwt6EQKAKJ0KWifXrSdpQ+4+C+72U9toxNiF+1w2VP01I4+cwbCzfXnzm9L8NSpBQHyEGjUNRhhF0bKPO65oi09+OO123+HakCIKJEEM40/e2bNnodc7D8C9UleHhlVf4P1vi/FJ9nV2T2tVgt2sOM6CLZUAZCZFtiiQcrZvR1QCEKtToabB5DCZSCUAt3RPxu7S83Y9gVKpBGBsrxTkDWjXVC4nPYq92sVg9Z5zDsusEoBxvVpj7m1X4eTJk0H54zFvc4ld48nMro4kXBdS9jcjN0PSdgAk7UsO7q793u1inDYk5S5LoAmCgLS0NJ9c03IGkvM2lzhtlLvi7PvM1f48+Szkby/1qFzm174+qrOkINUdd4G0nJ03Uo7l6W+K1N8KAYBGJTTdAXGwb+v6bGpEaWS926fVatG6desW74coVDHHXmYigNgIDSZe0QZdWkVCJTRVskoAurSKdDjVpS8HfTnbtyNZyZGI0qqdjhAwicCO4qZe3bG9UpAWp3Oax+uMOdfV3KNo3k/rGC3S4nQY2ysF88d3w3fFNS7zZbccqfLswAojJR/Yk+tCan6xlO38mavs7j0CIjKTIu2uM09SSlo6jsDZc3IG375unMp5VyNvQDuH58QdZ99nzvZnfY6lfBY8LZdJBI6W1+PO/+z3+Yw8Z2sbZZuxR8rsP978pkj9rRDRNLtb8yvWUX2erdPjeMUFFBSe5cxERH7CVBwfiYnQ4KPbLwXgfuVZX8457WrfzdVevN3qisEkIlqrwozcDEy/RsTID/agrM7gUZnMua4xOjVm5GZgRq4XudXG4M2XlZoPLPW68CTvWcrgZ3dDROTMa3f3HncU1+Kj2y/xOKXE03EEzbcB4PA5qSkIUjQ/vlatwvAe5bijdwKitcrtc3GU5nPuvF5ST7mj7zMpaUPSPgsZDvdTVW/Aeb3jV4sAqhvsg01PF/JyF0g/suqwbAuGSQnavflN8eS3whm56pOIvMfAXm4Ogk13A2V9MUDQHPS627c1o+h+YK11TrOU/Fh3+zCz/lvKfjVqeXKrAzHwS8r7Uwnuz531dSHlvKlUKgn16v58ypXXLvXaNzckmzcAnfF6HMHFbXYeqwEAlFQ02Dy3bHcZVv5yzi4FwZtF1ZyV8aPtR7F5f8vS7/zBulFuMpkwasFeyQ18R99nzhr5gGffkc33AwAjP9jjNLB3xZNOFXeBdJGLnnBPO2/cHWvL4SoYJHTQeNqZ0lJyLIxGRO4pt1soWFm+UKUFPnLOOV3XaMS8zSUYs2AvRn6wB2MX7sP5Rulf1mqV4PE0ea62l7oPR9yWo1OC9IM207yexizYi3mbS/x6m9jd+7umc4JH14XU8yZlO19OlWjNm2tfyudASo+m620aUNwsqAdcpyB4mjIXSnOuS2kwWnP3feao0e/Nd6QgCF53PphJmUlHUlDs5m6G1Ol9pRzL0w4awPtOGk+F88xERP7CwF4B5AiknOVdSu2pMh9HSr6rNU/yWj3Ji3ZbjoHeTdenlNUppdSzJ9eF1PMmZTtPr4GW8EUjoqXjCLzh6diDUJtzXWoD39tz2pLrxNPOB2tSOlUkBcVujm+e3tddh4PURo439dWSepKKs1gR+R4De7mZeyM8+PKSI5DyZJBsc9bHcTeotXl6gLPtR/VIxqgerSTtwxFPyyGVv3tKnfVOSXl/nlwXUutLynaOtklPisK4Xq1bVPeOyN2IkLqImi/SDqT2Rso157o3A359RUoDvyUNw5ZcJ65eGx+hkqVh6S6Qzkp2XjfW0/tK6XCQErR7U19SO2kENM3u5sv6JCLvcbpLmae7FKurofvfBtQ2NiDittskv848kM7bOafHLNiLUzWNTp+P1qqQEKlBo9GECxd78aMvzi/t6jje5PY3316OPPbm+/B2akB39dQ2VosV9/RoUVm9mdbO3VzUnl4XUutc6nbt2rXz2dSiLb32m3N3jlPjmmalcrWNN1LjdFh+d3dJ20opo6N9eTPg17oefTmmxPo8evo94+n+Pb1Omr82QqfBwA6xuP2KNpi+8rDTaXelNmQtYyac7GfeqM4uj+Nuel/raV3dHctcZm/qq/lrVAIQF6FGTaMRJhMs+zAPIm++7zuuaCtLfbrC6S6JXGNgr5DA3mYfXgTT7manaR2jxcp7mgIF6wFTwXpb1JvAXko9qQVgbV5Pr1e29eWaBIE6X76cX705Od6jlHnRAXg1F7szns6pL6WMzffl6trKSIwAYD/g1/xc3/axTleB9oXmAzPlvm5buk/rhqpcDUt3+3H1/J3/2e+yoZcWp0OBVUPP0zL7c+VZZ42oezmPPZFfcFYcuXmRitOcp1/ALRlcFk6k1JNRBN797qTXU7JJSfXxdt/hcL7keI95A9phV0mt015Dc++2s206JEZABFBS2WDznICmHkujKLrcb4vLmOx4X+4G/DpiPRjYmjcz+XjC0wHPLdl/S1/rakYeT7jbT4um9202i42nZfbmPUn9vXD1PgHf3u0jInvMsZebh7PiyMVfM5kEOyn10JKBi6E2KDIYeTuOwLzNuxOy8d6EbLvnxvVOwbIpl8ky7sPh8eN1mDygI/LHZzvcl5wDfoNx9h1/kKsR4kmg3dKZ0ZTc4Fdy2YhCFXvsfaBpQjzb3gnr3gpXtzM96TGy3tZZD6AA+95EX+XBu2MymSxT0Dni6Xv3xr3907C8sAxGFy/3dgEmqQM3Xe3b3XG9qSNvUiKCOU0LkNaj6W4bZ8+526/Uz5ej41v3bvp6nnFn84o3Py5g/50V7NeH0gzOineZmsWOGSKSioG9TOoajXhr63Hs2F2Ca3/7DfVqHb4o/xlt47Q4Vd2IeqtZFKO1KtyQnYS7r0rFJz+cxubDVaiuN6DRKEKnFpAQqcE1nRMc5ks6Gzx3xxVt0btdNE5WN6DeYDUfMpoGCb6++Ti0asEmz7Z/ZiwAwae5t2drGzFj5SEUlf+eCqBRATdekoyHrkkH4H7An6P3bjSJ0Gl/xcDMOORZ5W5aN5AA+4AkNkKDlBgtTtc6H1th3TsmtcFlDsTc9byVXzBg7MJ9Nu/R3WBbTwbjmq/DdQcq0WAwQRSbAoMIjQpRFwcxSqlf6+NMG9je5XtSOikBqCfzqjt63FHdSfl8Nb+mtdpfEa0Gai6uAm29n/Lznq3wLIW5EXteb7KUo9FoxPlGE/TG3+fsj9SoMLRLot13iDffF1IaBZ42SqXu05MGtbNxAnJ3jIii6DI1KzMxwtIxI6UeAO96yqU2VKUcgw0/osDh4FkZBs/WNRox9bMDKK5oQGJ9Df5wZBvqNREo6DrE5eu0KgF6J6P3HA22dDZ4zpz762xfnpBjkKfZ2dpGjF2wFwYnxUpP0EGtEhwO+HP23o+W19ut9RKrEzCkSxJ2HqtB1QU9zCuaq4SmgOSG7CQ8kNMUnOZvL8WafeWWGTuaUwnALd2ToVWr3Da46hqNeH1zCb7cX+HyDoAzmUkR+NfoLk2zSDgZbPu6eTYNCYNxra9DVzy5tszbfv5QLmrKzzJP1gFndeeIdd0Djle/9ae2sVp8cselXpdD6veFlMapp41Sqfu03yYBs8ZcjlOnTmH+thNOG2ONRiMu6EUIACK1guXf5rK0pGPEVUNwR3GNzaxCjo7dvB6s6w2w/d7z5ryYZ70xP64SBMTqVDhZ3YiGi192zY/hqq7l/O7g4Fki1xjYyxDYz9tcgqW7ywAASfXVuPnIdkmBvTvNZ8eYt7kEBbvLfB4EeDrDhzN3/edXHDpXL0sZ5m0uwbLdZe4WcHQqI1EHlWDfiGh+TPPAyWMVDQ6PZR1w/3n5byipbNmUiVnJETha7rhM5vmvi845Drgc1ZH5OnTHk2tLJQCTB3RE3pXJDOwd8PRzaTMzjwyfZ1cDft3p0ioSfdNjW1QOd98XUmaKAuBRo1RKgxdw3HBSCUBWSgwMBgOOufg+8IaUho67+nD23hwd5/VRnfGXFYec1ltmUgTem+B4zIa7jiKDg1WWnR3DVQdFlzaxeHtMZ0Rr5RnSx8CeyDUOnpXBlosDIiMMjbjx6A4AblcQl6T5YEu5V8uUelxvFZV7F9Q7KsOWouoW1WlJZSOKXfyIR2tVGNsrBX3ax6LESVBvLldxRT0eWXW4xUE9ABxxEtSbj1Xk4sfdUR1J5cm1ZRKB//16WvK+w42nn0s5V781B9XOBvxGaVynQ9Q0GFtcDnffF1JmisrfXuo2qLd+zSOrHAe+dvt0ss2hs3U4KnNQ3/z4zrirD2fvzem2LuqtuKLBaVmclUMEoJcY1JuP4ep8HDpTi/xtHKRN5C8M7FtIFEXojU25HwJEqMSmr7Y6bZQs+zfnwPpi8JyU43rLZDK1eI5wf773hEgNZuRm4LviGrc/9uaAWw5uq8jNBtZ1ZL4OpfKkfg3Gll0Pocrba1Ou1W9bRWsx/Zp0y0w/M3IzUHB3d6y8pzuWTbkMMRGuU0KMJnk+W66+L6TMFOVpo1RKg9dfHSHOju+Mu/pw9d4cbeuOs7LIWT/uzseWI1UyHYmI3OHg2RYSBAFatRqAEY0qDTZlXA4AOBuVKMv+rQdyuhuYKSdX06tJoVI1LS3ekuDeugxqHw/EMphEmEzSgy25FjZyS4DL4N66jszXoVSeXFsatWAzKJmaSJmq0BGNWp7PsrPPqfTz6ttySJ4pytP7cW421xtN/p5x2IazmbUkNQQ9qAopH0eDyX4mLtk7S9x1QBi9m2mMiDzHHnsZmKciM6nUOBHbGidiW6NRrW3xfptPc+Zqrno5yTW9WlZypGxluKZzQovL44paJUClUkkO0uQ6D2rB+b7MOfZS1yfw5Jx5cm2pBOD6S9tK3ne48fRzaa77ln6epXxOpaxv4ctySGn4aNSqi41SD7gpr0Yt/bPsC64aXG7L5cG5kBInq1Uqh4tNyVo/bs9HyzqKiEg6BvYyyBvQDplJER6/TqsSnH4fOlrNsuk49oGe4GZfnvBmFU1nXh3ZGa5SfNMTdMhMirB7P87ee1yEby5X68BESpBjDrjlcOMlSQ7PqbkOXh3Z2eXz9teH++vQk2vLvBLqI8OzPX5v4cJZ3TliXfeevM7Vfrwpmz/LIbVx4ckxpTR4/dUR4uz4zrirD1fvzdG27jgri5z14/Z8dPJtxwwR/Y6z4sgwKw7w+1Rt6w9Uov7ilGMRaqFpHvuaRtRbTUHdfB77bw5XocoyraIKCVFqXJPleh77rUXVMJhEaFQCcqymJ/vmcBUqHUz5aJ6DekdxjeV1V1tNr2a9r0DMY9/8/Tgqw9naRtz5n/2obvAslxwAOiTqIAiC3awh5sDEvHKoq2k1rbefJ2FWHBXgMoe1Y1IE3p3QFDC7qgNn59zVPPbm69B6ysDoi1PluXqto+NMG9geXTLTuSy8C47qTsrnq/nrtFp10zz2jUaYTJDlcyrl+rHeptFowvlGIxqdzGPvaTkss684mKPd/NkDpM+KY/78TV952O0+nR3XMiuOh7MIudP8+8QRd/Xh7L05Os48N7PimL9jXM6K42BRQ41KgFEUJdVNx6QI/NM8K46D98RZcYj8i4G9TIG9mSAISE1NxcmTJ20e9/XKs8725eh4wbzyrHUAIgoqCDDh6g5Ngc/2o9UOGzXN57F3FyCbj+GuweVoHnsBQKfkCDz/h05YXlhmCZTMc1JHaQXo1GqnAZESV54VBAFpaWkM7CVqyefL2cqznu7Hk7K52kaulWelNi48aZR62mAxbzM4KwHPWM1j76wx1nwu+Xp9U12Yy9KSBpe7sjdvaJnL4awemncqeTqPvbOOIvPjKgGI0alwsqYRDQbX89g7qmvOY0/kPwzsfRDYMwjyPXMD6tSpUzb17K5R03w7dzxdeVblIG/V3aqVSsdr2j/CpZ4DvfKso3oO9MqzUt9bsK0864trmoE9kWucFYeClqvZQNz9sEn94fNkf862sX482IJ6IrlJ+Qw4Guwp9z7dPefsc+tuW0958t7k+l7z5LWenouWloOIWoaDZ4mIiIiIQgADeyIiIiKiEMDAnoiIiIgoBDCwJyIiIiIKAQzsiYiIiIhCAAN7IiIiIqIQwMCeiIiIiCgEMLAnIiIiIgoBDOyJiIiIiEJAWK88q9H47u37ct/0O9az/7Cu/YP17B+sZ/+Rs6553ohcE0RRFANdCCIiIiIiahmm4sjswoULeOKJJ3DhwoVAFyWksZ79h3XtH6xn/2A9+w/rmsj/GNjLTBRFHDlyBLwR4lusZ/9hXfsH69k/WM/+w7om8j8G9kREREREIYCBPRERERFRCGBgLzOtVotx48ZBq9UGuighjfXsP6xr/2A9+wfr2X9Y10T+x1lxiIiIiIhCAHvsiYiIiIhCAAN7IiIiIqIQwMCeiIiIiCgEMLAnIiIiIgoBmkAXIJSsW7cOq1evRmVlJdLT0zFlyhRceumlgS5WUNm3bx9Wr16NI0eOoKKiAo8++iiuuuoqy/OiKGLp0qX46quvUFtbi65du+KPf/wjMjIyLNvo9Xp8/PHH+Pbbb9HY2IgePXpg6tSpaNWqVSDekuKsWLECO3fuxIkTJ6DT6dCtWzfccccdaNeunWUb1rM81q9fj/Xr1+Ps2bMAgPT0dIwbNw59+/YFwHr2lRUrVuDTTz/FzTffjClTpgBgXctlyZIlWLZsmc1jCQkJePfddwGwnokCjT32Mtm2bRsWLlyIMWPG4OWXX8all16KF154AWVlZYEuWlBpaGhAx44dcc899zh8ftWqVfjiiy9wzz334MUXX0RiYiKee+45myXLFy5ciJ07d+Khhx7CnDlzUF9fj5deegkmk8lfb0PR9u3bh+HDh+P555/H3/72N5hMJjz33HOor6+3bMN6lkdycjImTZqEF198ES+++CJ69OiBuXPnoqSkBADr2RcOHTqEDRs2IDMz0+Zx1rV8MjIykJ+fb/nv1VdftTzHeiYKMJFk8eSTT4r5+fk2j02fPl38z3/+E6ASBb9bb71V3LFjh+Vvk8kk3nvvveKKFSssjzU2NoqTJ08W169fL4qiKNbV1YkTJ04Uv/32W8s2586dE8ePHy/+9NNP/ip6UKmqqhJvvfVWce/evaIosp59bcqUKeJXX33FevaBCxcuiH/5y1/E3bt3i7NmzRIXLFggiiKvaTl99tln4qOPPurwOdYzUeCxx14GBoMBRUVF6N27t83jvXr1woEDBwJUqtBz5swZVFZW2tSzVqvFZZddZqnnoqIiGI1G9OrVy7JNcnIyOnTogIMHD/q9zMHg/PnzAIDY2FgArGdfMZlM+Pbbb9HQ0IBu3bqxnn3gvffeQ9++fW3qC+A1LbdTp05h2rRpeOCBB/D666/j9OnTAFjPRErAHHsZVFdXw2QyISEhwebxhIQEVFZWBqZQIchcl47q2ZzyVFlZCY1GYwlSrbfhubAniiI+/PBDXHLJJejQoQMA1rPcjh07hqeeegp6vR6RkZF49NFHkZ6ebgl0WM/y+Pbbb3HkyBG8+OKLds/xmpZP165d8cADD6Bdu3aorKzE8uXL8be//Q2vvfYa65lIARjYy0gQBEmPUcs0r1NRwuLJUrYJR++//z6OHTuGOXPm2D3HepZHu3bt8I9//AN1dXXYsWMH3nrrLcyePdvyPOu55crKyrBw4UI89dRT0Ol0TrdjXbeceeA3AHTo0AHdunXDgw8+iM2bN6Nr164AWM9EgcRUHBnEx8dDpVLZ9TZUVVXZ9VyQ9xITEwHArp6rq6st9ZyYmAiDwYDa2lq7bcyvpyYffPABfvjhB8yaNctmNgrWs7w0Gg1SU1PRuXNnTJo0CR07dsR///tf1rOMioqKUFVVhZkzZ2LixImYOHEi9u3bh7Vr12LixImW+mRdyy8yMhIdOnTAyZMneU0TKQADexloNBpkZWWhsLDQ5vHCwkJkZ2cHqFShp02bNkhMTLSpZ4PBgH379lnqOSsrC2q12mabiooKHDt2DN26dfN7mZVIFEW8//772LFjB5555hm0adPG5nnWs2+Jogi9Xs96llHPnj3xyiuvYO7cuZb/OnfujJycHMydOxdt27ZlXfuIXq/HiRMnkJSUxGuaSAGYiiOTESNG4I033kBWVha6deuGDRs2oKysDNdff32gixZU6uvrcerUKcvfZ86cwdGjRxEbG4uUlBTcfPPNWLFiBdLS0pCamooVK1YgIiICOTk5AIDo6GgMGzYMH3/8MeLi4hAbG4uPP/4YHTp0sBtQF67ef/99bN26FY8//jiioqIsvWvR0dHQ6XQQBIH1LJNFixahb9++aNWqFerr6/Htt99i7969eOqpp1jPMoqKirKMETGLiIhAXFyc5XHWtTw++ugj9OvXDykpKaiqqkJBQQEuXLiA3NxcXtNECiCITGyTjXmBqoqKCmRkZGDy5Mm47LLLAl2soLJ3716b/GOz3NxcPPDAA5bFTzZs2IC6ujp06dIFf/zjH21+1BsbG/HJJ59g69atNoufpKSk+POtKNb48eMdPn7//fdjyJAhAMB6lsm///1v7NmzBxUVFYiOjkZmZiZGjhxpCWBYz77z7LPPomPHjnYLVLGuW+b111/Hr7/+iurqasTHx6Nr166YOHEi0tPTAbCeiQKNgT0RERERUQhgjj0RERERUQhgYE9EREREFAIY2BMRERERhQAG9kREREREIYCBPRERERFRCGBgT0REREQUAhjYExERERGFAK48S0SK4mwBreZmzZqF7t272z3+7LPP2vy/J1ryWiIiokBjYE9EivLcc8/Z/F1QUIC9e/fimWeesXncvNJlc1OnTvVZ2YiIiJSMgT0RKUq3bt1s/o6Pj4cgCHaPN9fQ0ICIiAinAT8REVGoY2BPREHn2WefRU1NDf74xz9i0aJFOHr0KPr164fp06c7TKdZunQpfvrpJ5w8eRImkwmpqakYPnw4hg4dCkEQAvMmiIiIZMbAnoiCUkVFBd544w2MHDkSt912m8sA/ezZs7juuuuQkpICAPjtt9/wwQcfoLy8HOPGjfNXkYmIiHyKgT0RBaXa2lo8/PDD6NGjh9tt77//fsu/TSYTunfvDlEUsXbtWowdO5a99kREFBIY2BNRUIqJiZEU1APAnj17sGLFChw6dAgXLlywea6qqgqJiYk+KCEREZF/MbAnoqCUlJQkabtDhw7hueeeQ/fu3TFt2jS0atUKGo0G33//PZYvX47GxkYfl5SIiMg/GNgTUVCSmj7z7bffQq1W44knnoBOp7M8/v333/uqaERERAHBlWeJKKQJggC1Wg2V6vevu8bGRnzzzTcBLBUREZH82GNPRCHt8ssvx5o1a/Cvf/0L1113HWpqavD5559Dq9UGumhERESyYo89EYW0Hj164L777sOxY8fw8ssvY/Hixejfvz9GjhwZ6KIRERHJShBFUQx0IYiIiIiIqGXYY09EREREFAIY2BMRERERhQAG9kREREREIYCBPRERERFRCGBgT0REREQUAhjYExERERGFAAb2REREREQhgIE9EREREVEIYGBPRERERBQCGNgTEREREYUABvZERERERCGAgT0RERERUQj4f4d7QN0CFZLcAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "plot_optimization_history(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "24970ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from optuna.visualization.matplotlib import plot_param_importances\n",
    "\n",
    "#plot_param_importances(study_svm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f8f09d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>26.900000</td>\n",
       "      <td>2.282786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>95.700000</td>\n",
       "      <td>2.983287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>3.047768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>2.223611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.915621</td>\n",
       "      <td>0.032414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.853409</td>\n",
       "      <td>0.089281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.805406</td>\n",
       "      <td>0.066891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.952280</td>\n",
       "      <td>0.030256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.826886</td>\n",
       "      <td>0.065757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.914947</td>\n",
       "      <td>0.032340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.885526</td>\n",
       "      <td>0.043559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.878841</td>\n",
       "      <td>0.041346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.773251</td>\n",
       "      <td>0.088050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.936590</td>\n",
       "      <td>0.021015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.878841</td>\n",
       "      <td>0.041346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0                    TP        26.900000     2.282786\n",
       "1                    TN        95.700000     2.983287\n",
       "2                    FP         4.800000     3.047768\n",
       "3                    FN         6.500000     2.223611\n",
       "4              Accuracy         0.915621     0.032414\n",
       "5             Precision         0.853409     0.089281\n",
       "6           Sensitivity         0.805406     0.066891\n",
       "7           Specificity         0.952280     0.030256\n",
       "8              F1 score         0.826886     0.065757\n",
       "9   F1 score (weighted)         0.914947     0.032340\n",
       "10     F1 score (macro)         0.885526     0.043559\n",
       "11    Balanced Accuracy         0.878841     0.041346\n",
       "12                  MCC         0.773251     0.088050\n",
       "13                  NPV         0.936590     0.021015\n",
       "14              ROC_AUC         0.878841     0.041346"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_objective_svm_cv(study_svm.best_trial, X, Y, Y_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b1e849c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Set0</th>\n",
       "      <th>Set1</th>\n",
       "      <th>Set2</th>\n",
       "      <th>Set3</th>\n",
       "      <th>Set4</th>\n",
       "      <th>Set5</th>\n",
       "      <th>Set6</th>\n",
       "      <th>Set7</th>\n",
       "      <th>Set8</th>\n",
       "      <th>Set9</th>\n",
       "      <th>ave</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>51.200000</td>\n",
       "      <td>4.022161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TN</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>191.800000</td>\n",
       "      <td>3.881580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FP</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>3.597839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FN</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>4.453463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.876866</td>\n",
       "      <td>0.899254</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.925373</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.917910</td>\n",
       "      <td>0.906716</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.858904</td>\n",
       "      <td>0.056191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.761194</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.776119</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>0.756681</td>\n",
       "      <td>0.063640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.974900</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.945300</td>\n",
       "      <td>0.954800</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.955200</td>\n",
       "      <td>0.985100</td>\n",
       "      <td>0.960200</td>\n",
       "      <td>0.957550</td>\n",
       "      <td>0.017976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.876923</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.803506</td>\n",
       "      <td>0.053599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.889947</td>\n",
       "      <td>0.875935</td>\n",
       "      <td>0.897920</td>\n",
       "      <td>0.892722</td>\n",
       "      <td>0.861128</td>\n",
       "      <td>0.924612</td>\n",
       "      <td>0.939362</td>\n",
       "      <td>0.909007</td>\n",
       "      <td>0.938573</td>\n",
       "      <td>0.916590</td>\n",
       "      <td>0.904580</td>\n",
       "      <td>0.025902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.849679</td>\n",
       "      <td>0.834997</td>\n",
       "      <td>0.862179</td>\n",
       "      <td>0.856311</td>\n",
       "      <td>0.811149</td>\n",
       "      <td>0.899490</td>\n",
       "      <td>0.918757</td>\n",
       "      <td>0.876838</td>\n",
       "      <td>0.915103</td>\n",
       "      <td>0.887102</td>\n",
       "      <td>0.871161</td>\n",
       "      <td>0.034892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.820771</td>\n",
       "      <td>0.830147</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.839706</td>\n",
       "      <td>0.793529</td>\n",
       "      <td>0.891765</td>\n",
       "      <td>0.906618</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.894089</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.857115</td>\n",
       "      <td>0.036276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.714596</td>\n",
       "      <td>0.670351</td>\n",
       "      <td>0.725435</td>\n",
       "      <td>0.716943</td>\n",
       "      <td>0.628332</td>\n",
       "      <td>0.799686</td>\n",
       "      <td>0.839182</td>\n",
       "      <td>0.755278</td>\n",
       "      <td>0.835238</td>\n",
       "      <td>0.775829</td>\n",
       "      <td>0.746087</td>\n",
       "      <td>0.068578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.911300</td>\n",
       "      <td>0.922300</td>\n",
       "      <td>0.909100</td>\n",
       "      <td>0.886800</td>\n",
       "      <td>0.941200</td>\n",
       "      <td>0.946600</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>0.932400</td>\n",
       "      <td>0.920990</td>\n",
       "      <td>0.020235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.820771</td>\n",
       "      <td>0.830147</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.839706</td>\n",
       "      <td>0.793529</td>\n",
       "      <td>0.891765</td>\n",
       "      <td>0.906618</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.894089</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.857115</td>\n",
       "      <td>0.036276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric        Set0        Set1        Set2        Set3  \\\n",
       "0                    TP   46.000000   50.000000   51.000000   50.000000   \n",
       "1                    TN  194.000000  185.000000  190.000000  190.000000   \n",
       "2                    FP    5.000000   15.000000   11.000000    9.000000   \n",
       "3                    FN   23.000000   18.000000   16.000000   19.000000   \n",
       "4              Accuracy    0.895522    0.876866    0.899254    0.895522   \n",
       "5             Precision    0.901961    0.769231    0.822581    0.847458   \n",
       "6           Sensitivity    0.666667    0.735294    0.761194    0.724638   \n",
       "7           Specificity    0.974900    0.925000    0.945300    0.954800   \n",
       "8              F1 score    0.766667    0.751880    0.790698    0.781250   \n",
       "9   F1 score (weighted)    0.889947    0.875935    0.897920    0.892722   \n",
       "10     F1 score (macro)    0.849679    0.834997    0.862179    0.856311   \n",
       "11    Balanced Accuracy    0.820771    0.830147    0.853234    0.839706   \n",
       "12                  MCC    0.714596    0.670351    0.725435    0.716943   \n",
       "13                  NPV    0.894000    0.911300    0.922300    0.909100   \n",
       "14              ROC_AUC    0.820771    0.830147    0.853234    0.839706   \n",
       "\n",
       "          Set4        Set5        Set6        Set7        Set8        Set9  \\\n",
       "0    44.000000   56.000000   57.000000   52.000000   53.000000   53.000000   \n",
       "1   188.000000  192.000000  195.000000  192.000000  199.000000  193.000000   \n",
       "2    12.000000    8.000000    5.000000    9.000000    3.000000    8.000000   \n",
       "3    24.000000   12.000000   11.000000   15.000000   13.000000   14.000000   \n",
       "4     0.865672    0.925373    0.940299    0.910448    0.940299    0.917910   \n",
       "5     0.785714    0.875000    0.919355    0.852459    0.946429    0.868852   \n",
       "6     0.647059    0.823529    0.838235    0.776119    0.803030    0.791045   \n",
       "7     0.940000    0.960000    0.975000    0.955200    0.985100    0.960200   \n",
       "8     0.709677    0.848485    0.876923    0.812500    0.868852    0.828125   \n",
       "9     0.861128    0.924612    0.939362    0.909007    0.938573    0.916590   \n",
       "10    0.811149    0.899490    0.918757    0.876838    0.915103    0.887102   \n",
       "11    0.793529    0.891765    0.906618    0.865672    0.894089    0.875622   \n",
       "12    0.628332    0.799686    0.839182    0.755278    0.835238    0.775829   \n",
       "13    0.886800    0.941200    0.946600    0.927500    0.938700    0.932400   \n",
       "14    0.793529    0.891765    0.906618    0.865672    0.894089    0.875622   \n",
       "\n",
       "           ave       std  \n",
       "0    51.200000  4.022161  \n",
       "1   191.800000  3.881580  \n",
       "2     8.500000  3.597839  \n",
       "3    16.500000  4.453463  \n",
       "4     0.906716  0.025000  \n",
       "5     0.858904  0.056191  \n",
       "6     0.756681  0.063640  \n",
       "7     0.957550  0.017976  \n",
       "8     0.803506  0.053599  \n",
       "9     0.904580  0.025902  \n",
       "10    0.871161  0.034892  \n",
       "11    0.857115  0.036276  \n",
       "12    0.746087  0.068578  \n",
       "13    0.920990  0.020235  \n",
       "14    0.857115  0.036276  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_met_svm_test['ave'] = mat_met_svm_test.iloc[:,1:11].mean(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test['std'] = mat_met_svm_test.iloc[:,1:11].std(axis='columns', numeric_only=True)\n",
    "mat_met_svm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "297d96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Value (average)</th>\n",
       "      <th>Value (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.911573</td>\n",
       "      <td>0.023640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.845996</td>\n",
       "      <td>0.055761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensitivity</td>\n",
       "      <td>0.792287</td>\n",
       "      <td>0.071779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Specificity</td>\n",
       "      <td>0.951274</td>\n",
       "      <td>0.020460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F1 score</td>\n",
       "      <td>0.816262</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>F1 score (weighted)</td>\n",
       "      <td>0.910440</td>\n",
       "      <td>0.024245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F1 score (macro)</td>\n",
       "      <td>0.878984</td>\n",
       "      <td>0.033509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balanced Accuracy</td>\n",
       "      <td>0.871778</td>\n",
       "      <td>0.037523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MCC</td>\n",
       "      <td>0.760500</td>\n",
       "      <td>0.065620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPV</td>\n",
       "      <td>0.932758</td>\n",
       "      <td>0.021634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ROC_AUC</td>\n",
       "      <td>0.871778</td>\n",
       "      <td>0.037523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Metric  Value (average)  Value (std)\n",
       "0              Accuracy         0.911573     0.023640\n",
       "1             Precision         0.845996     0.055761\n",
       "2           Sensitivity         0.792287     0.071779\n",
       "3           Specificity         0.951274     0.020460\n",
       "4              F1 score         0.816262     0.052000\n",
       "5   F1 score (weighted)         0.910440     0.024245\n",
       "6      F1 score (macro)         0.878984     0.033509\n",
       "7     Balanced Accuracy         0.871778     0.037523\n",
       "8                   MCC         0.760500     0.065620\n",
       "9                   NPV         0.932758     0.021634\n",
       "10              ROC_AUC         0.871778     0.037523"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change to fit into these X values \n",
    "\n",
    "TP = np.empty(10)\n",
    "FP = np.empty(10)\n",
    "TN = np.empty(10)\n",
    "FN = np.empty(10)\n",
    "\n",
    "\n",
    "Accuracy_outer = []\n",
    "Precision_outer = [] #Also called Positive Predictive Value(PPV)\n",
    "Sensitivity_outer = [] # Also called Recall or True Positive Rate (TPR)\n",
    "Specificity_outer = [] #Also called selectivity or True Negative Rate  (TNR)\n",
    "f1_scores_outer = []\n",
    "f1_scores_W_outer = []\n",
    "f1_scores_M_outer = []\n",
    "BA_scores_outer = []\n",
    "MCC_outer = []\n",
    "NPV_outer = []\n",
    "ROC_AUC_outer = []\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "random_state= [687266, 98656, 56, 280189, 76543] # \n",
    "data_svm=pd.DataFrame()\n",
    "\n",
    "for i in range(NUM_TRIALS):\n",
    "    cv_change = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state[i])\n",
    "    data_inner = pd.DataFrame({'y_test_idx': pd.Series(dtype='int'),\n",
    "                   'y_test': pd.Series(dtype='float'),\n",
    "                   'y_pred': pd.Series(dtype='float')})\n",
    "    \n",
    "    \n",
    "    for idx, (train_idx, test_idx) in enumerate(cv_change.split(X, Y_class)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = Y[train_idx], Y[test_idx]\n",
    "        eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "        optimizedCV_svm = SVC(C = study_svm.best_params['C'], \n",
    "                        gamma=study_svm.best_params[\"gamma\"],\n",
    "                        )\n",
    "\n",
    "\n",
    "        #learn\n",
    "        \n",
    "        optimizedCV_svm.fit(X_train,y_train)\n",
    "                          \n",
    "                  \n",
    "        #print(test_idx)\n",
    "        y_pred_optimized_svm = optimizedCV_svm.predict(X_test) \n",
    "        data_inner = data_inner.append(pd.DataFrame({'y_test_idx': test_idx, 'y_test': y_test, 'y_pred_svm': y_pred_optimized_svm } ), )\n",
    "        data_inner.reset_index(inplace=True, drop=True) \n",
    "        data_inner.sort_values(by='y_test_idx', inplace=True) \n",
    "        \n",
    "        \n",
    "        conf_matrix = confusion_matrix(y_test, y_pred_optimized_svm)\n",
    "        TP[idx] = conf_matrix[1][1]\n",
    "        TN[idx] = conf_matrix[0][0]\n",
    "        FP[idx] = conf_matrix[0][1] \n",
    "        FN[idx] = conf_matrix[1][0]\n",
    "        \n",
    "        Accuracy_outer.append(accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        Precision_outer.append(precision_score(y_test, y_pred_optimized_svm))\n",
    "        Sensitivity_outer.append(recall_score(y_test, y_pred_optimized_svm))\n",
    "        Specificity_outer.append(round( TN[idx] / (TN[idx]+FP[idx]),4 ))\n",
    "        f1_scores_outer.append(f1_score(y_test, y_pred_optimized_svm))\n",
    "        f1_scores_W_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"weighted\"))\n",
    "        f1_scores_M_outer.append(f1_score(y_test, y_pred_optimized_svm, average=\"macro\"))\n",
    "        BA_scores_outer.append(balanced_accuracy_score(y_test, y_pred_optimized_svm))\n",
    "        MCC_outer.append(matthews_corrcoef(y_test, y_pred_optimized_svm))\n",
    "        NPV_outer.append(round( TN[idx] / (TN[idx]+FN[idx]),4 ))\n",
    "        ROC_AUC_outer.append(roc_auc_score(y_test, y_pred_optimized_svm))\n",
    "        \n",
    "    data_svm['y_test_idx' + str(i)] = data_inner['y_test_idx']\n",
    "    data_svm['y_test' + str(i)] = data_inner['y_test']\n",
    "    data_svm['y_pred_svm' + str(i)] = data_inner['y_pred_svm']\n",
    "   # data_svm['correct' + str(i)] = correct_value\n",
    "   # data_svm['pred' + str(i)] = y_pred_optimized_svm\n",
    "\n",
    "mat_met_optimized_svm = pd.DataFrame({'Metric':['Accuracy','Precision','Sensitivity','Specificity','F1 score','F1 score (weighted)','F1 score (macro)','Balanced Accuracy','MCC','NPV','ROC_AUC'],     \n",
    "                        'Value (average)':[ np.mean(Accuracy_outer),np.mean(Precision_outer),\n",
    "                                           np.mean(Sensitivity_outer),np.mean(Specificity_outer),np.mean(f1_scores_outer),\n",
    "                                           np.mean(f1_scores_W_outer), np.mean(f1_scores_M_outer), np.mean(BA_scores_outer), \n",
    "                                           np.mean(MCC_outer),np.mean(NPV_outer),np.mean(ROC_AUC_outer)],\n",
    "                        'Value (std)': [ np.std(Accuracy_outer, ddof=1),np.std(Precision_outer, ddof=1),\n",
    "                                        np.std(Sensitivity_outer,ddof=1),np.std(Specificity_outer,ddof=1),np.std(f1_scores_outer, ddof=1),\n",
    "                                        np.std(f1_scores_W_outer, ddof=1),np.std(f1_scores_M_outer, ddof=1), np.std(BA_scores_outer, ddof=1), \n",
    "                                        np.std(MCC_outer, ddof=1),np.std(NPV_outer, ddof=1),np.std(ROC_AUC_outer, ddof=1)]\n",
    "                       }) \n",
    "\n",
    "mat_met_optimized_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d226e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM baseline model f1_score 0.8468 with a standard deviation of 0.0377\n",
      "SVM optimized model f1_score 0.8844 with a standard deviation of 0.0306\n"
     ]
    }
   ],
   "source": [
    "#cross valide using this optimized SVC \n",
    "svm_baseline_CVscore = cross_val_score(svm_clf, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "#cv_svm_opt_testSet = cross_val_score(optimized_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "cv_svm_opt = cross_val_score(optimizedCV_svm, X, Y, cv=10, scoring=\"f1_macro\")\n",
    "print(\"SVM baseline model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(svm_baseline_CVscore), np.std(svm_baseline_CVscore, ddof=1)))\n",
    "#print(\"SVM optimized model (tested on Y_te) f1_score %0.4f with a standard deviation of %0.4f\" % (svm_baseline_CVscore.mean(), svm_baseline_CVscore.std()))\n",
    "print(\"SVM optimized model f1_score %0.4f with a standard deviation of %0.4f\" % (np.mean(cv_svm_opt), np.std(cv_svm_opt, ddof=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "515bb7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OUTPUT/optimizedCV_svm_clf.joblib']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svm_clf, \"OUTPUT/svm_clf.joblib\")\n",
    "#joblib.dump(optimized_svm, \"OUTPUT/optimized_svm.joblib\")\n",
    "joblib.dump(optimizedCV_svm, \"OUTPUT/optimizedCV_svm_clf.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0df6e856-a45a-4138-980e-0dbc63f5627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/TestSet_EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_rf_test.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_lgbm_test.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_xgb_test.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_knn_test.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_svm_test.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4321aec2-c4f9-48b5-89f3-5e139c63c644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the evaluation results of Optimized and saved models to an Excel file\n",
    "\n",
    "with pd.ExcelWriter(\"OUTPUT/EvaluationResults.xlsx\") as writer:\n",
    "   \n",
    "    # use to_excel function and specify the sheet_name and index \n",
    "    # to store the dataframe in specified sheet\n",
    "    mat_met_optimized_rf.to_excel(writer, sheet_name=\"RF\", )\n",
    "    mat_met_optimized_lgbm.to_excel(writer, sheet_name=\"LGBM\", )\n",
    "    mat_met_optimized_xgb.to_excel(writer, sheet_name=\"XGB\", )\n",
    "    mat_met_optimized_knn.to_excel(writer, sheet_name=\"KNN\", )\n",
    "    mat_met_optimized_svm.to_excel(writer, sheet_name=\"SVM\", )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
